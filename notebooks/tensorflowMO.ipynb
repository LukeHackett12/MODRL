{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOC4agzUourzMSNqg6FqCKm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LukeHackett12/MODRL/blob/main/tensorflowMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qolSvSkW9J3Z",
        "outputId": "28d4b6e9-043f-4f2b-9004-4a42cdbae265"
      },
      "source": [
        "!pip install piglet\n",
        "!pip install pygame\n",
        "!pip install gym --upgrade\n",
        "!pip install pyvirtualdisplay\n",
        "\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: piglet in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "Requirement already satisfied: piglet-templates in /usr/local/lib/python3.7/dist-packages (from piglet) (1.1.0)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (1.6.3)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (1.1.1)\n",
            "Requirement already satisfied: Parsley in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (1.3)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (20.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse->piglet-templates->piglet) (0.36.2)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n",
            "Collecting pygame\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/9e/c400554dd1d0e562bd4379f35ad5023c68fc120003a58991405850f56f95/pygame-2.0.1-cp37-cp37m-manylinux1_x86_64.whl (11.8MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8MB 10.1MB/s \n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "Successfully installed pygame-2.0.1\n",
            "Requirement already up-to-date: gym in /usr/local/lib/python3.7/dist-packages (0.18.0)\n",
            "Requirement already satisfied, skipping upgrade: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: Pillow<=7.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.7/dist-packages (2.0)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.7/dist-packages (from pyvirtualdisplay) (0.3)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1c6S005_XTo",
        "outputId": "c4bc8756-313a-49c5-f6e6-c692a77a7e7d"
      },
      "source": [
        "!git clone https://github.com/LukeHackett12/MODRL.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'MODRL'...\n",
            "remote: Enumerating objects: 82, done.\u001b[K\n",
            "remote: Counting objects: 100% (82/82), done.\u001b[K\n",
            "remote: Compressing objects: 100% (59/59), done.\u001b[K\n",
            "remote: Total 82 (delta 36), reused 64 (delta 21), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (82/82), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8Mz1B_D9Twr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2067dd94-cca2-4b69-d847-08e0ed2ac33f"
      },
      "source": [
        "import os\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
        "from MODRL.custom_envs.mountain_car.engine import MountainCar\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras, Tensor\n",
        "from keras import backend as K\n",
        "K.set_image_data_format('channels_last')\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms as T\n",
        "from PIL import Image\n",
        "\n",
        "import enum\n",
        "import random\n",
        "from collections import namedtuple, deque\n",
        "from typing import NamedTuple\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#################  RESET GAME  ##################\n",
            "Episode terminated after: 295 (s)\n",
            "Total score: -1\n",
            "#################################################\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k8DKUo1-tWo"
      },
      "source": [
        "\n",
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "\n",
        "class Transition(NamedTuple):\n",
        "    currStates: Tensor\n",
        "    actions: Tensor\n",
        "    rewards: Tensor\n",
        "    nextStates: Tensor\n",
        "    dones: Tensor\n",
        "\n",
        "class DQN():\n",
        "    def __init__(self, n_input, n_output, learningRate):\n",
        "        self.model = keras.models.Sequential()\n",
        "        self.model.add(keras.layers.Conv2D(32, input_shape=np.array(n_input), kernel_size=8, strides=4, activation='relu'))\n",
        "        self.model.add(keras.layers.Conv2D(32, kernel_size=4, strides=2, activation='relu'))\n",
        "        self.model.add(keras.layers.Conv2D(64, kernel_size=3, strides=1, activation='relu'))\n",
        "        self.model.add(keras.layers.Flatten())\n",
        "        self.model.add(keras.layers.Dense(512,activation='relu'))\n",
        "        self.model.add(keras.layers.Dense(n_output,activation='linear'))\n",
        "        self.model.compile(loss='mse', optimizer=keras.optimizers.Adam(lr=learningRate))\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, stateShape, actionSpace, numPicks, memorySize):\n",
        "        self.numPicks = numPicks\n",
        "        self.memorySize = memorySize\n",
        "        self.replayMemory = deque(maxlen=memorySize)\n",
        "        self.stateShape = stateShape\n",
        "        self.actionSpace = actionSpace\n",
        "\n",
        "        self.step = 0\n",
        "        self.sync = 1000\n",
        "\n",
        "        self.alpha = 0.001\n",
        "        self.epsilon = 1\n",
        "        self.epsilon_decay = 0.01\n",
        "        self.epsilon_min=0.01\n",
        "        self.eps_threshold = 0\n",
        "\n",
        "        self.gamma = 0.99\n",
        "        self.tau = 0.01\n",
        "\n",
        "        self.trainNetwork = DQN(stateShape, len(actionSpace), self.alpha)\n",
        "        self.targetNetwork = DQN(stateShape, len(actionSpace), self.alpha)\n",
        "        self.targetNetwork.model.set_weights(self.trainNetwork.model.get_weights())\n",
        "\n",
        "    def trainDQN(self):\n",
        "        if len(self.replayMemory) <= self.numPicks:\n",
        "            return 0\n",
        "        \n",
        "        samples = random.sample(self.replayMemory, self.numPicks)\n",
        "        batch = Transition(*zip(*samples))\n",
        "        currStates,actions,rewards,nextStates,dones = batch\n",
        "\n",
        "        currStates = np.squeeze(np.array(currStates), 1)\n",
        "        Q_currents = self.trainNetwork.model.predict(currStates)\n",
        "\n",
        "        nextStates = np.squeeze(np.array(nextStates), 1)\n",
        "        Q_futures = self.targetNetwork.model.predict(nextStates).max(axis = 1)\n",
        "\n",
        "        rewards = np.array(rewards).reshape(self.numPicks,).astype(float)\n",
        "        actions = np.array(actions).reshape(self.numPicks,).astype(int)\n",
        "\n",
        "        dones = np.array(dones).astype(bool)\n",
        "        notDones = (~dones).astype(float)\n",
        "        dones = dones.astype(float)\n",
        "\n",
        "        Q_currents[np.arange(self.numPicks), actions] = rewards*dones + (rewards + Q_futures * self.gamma)*notDones\n",
        "\n",
        "        hist = self.trainNetwork.model.fit(tf.convert_to_tensor(currStates), tf.convert_to_tensor(Q_currents), epochs=1, verbose=0)  \n",
        "        return hist.history['loss'][0]\n",
        "\n",
        "    def selectAction(self, state):\n",
        "      self.step += 1\n",
        "      self.epsilon = max(self.epsilon, self.epsilon_min)\n",
        "\n",
        "      q = -100000\n",
        "      if np.random.rand(1) < self.epsilon:\n",
        "          action = np.random.randint(0, 3)\n",
        "      else:\n",
        "          preds = np.squeeze(self.trainNetwork.model(state, training=False).numpy(), axis=0)\n",
        "          action = np.argmax(preds)\n",
        "          q = preds[action]\n",
        "      return action,q\n",
        "\n",
        "    def addMemory(self, memory):\n",
        "        self.replayMemory.append(memory)\n",
        "\n",
        "    def save(self):\n",
        "        save_path = (\n",
        "            f\"./mountain_car_{int(self.step)}.chkpt\"\n",
        "        )\n",
        "        agent.trainNetwork.model.save(\n",
        "            save_path\n",
        "        )\n",
        "        print(f\"MountainNet saved to {save_path} done!\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIb3g8A6-z_a"
      },
      "source": [
        "resize = T.Compose([T.ToPILImage(),\n",
        "                    T.Resize((40,40), interpolation=Image.CUBIC),\n",
        "                    T.ToTensor()])\n",
        "\n",
        "def process_screen(observation):\n",
        "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
        "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
        "    screen = observation\n",
        "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "    screen = torch.from_numpy(screen)\n",
        "    # Resize, and add a batch dimension (BCHW)\n",
        "    return resize(screen).numpy().transpose((2,1,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_M62dNb-3w1"
      },
      "source": [
        "env = MountainCar(speed=1e8, graphical_state=True, render=True, is_debug=True)\n",
        "agent = DQNAgent(stateShape=(40, 40, 3),\n",
        "                   actionSpace=env.get_action_space(), numPicks=32, memorySize=10000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        },
        "id": "EV8zErtaAFCS",
        "outputId": "a764148c-ce56-4b6c-fa82-89ce6f8399f3"
      },
      "source": [
        "\n",
        "episode_score = []\n",
        "episode_qs = []\n",
        "episode_height = []\n",
        "episode_loss = []\n",
        "episode_decay = []\n",
        "\n",
        "fig, ax = plt.subplots(2, 2)\n",
        "fig.canvas.draw()\n",
        "plt.show(block=False)\n",
        "\n",
        "\n",
        "def plot_episode():\n",
        "    ax[0][0].title.set_text('Training Score')\n",
        "    ax[0][0].set_xlabel('Episode')\n",
        "    ax[0][0].set_ylabel('Score')\n",
        "    ax[0][0].plot(episode_score, 'b')\n",
        "\n",
        "    ax[0][1].title.set_text('Training Height')\n",
        "    ax[0][1].set_xlabel('Episode')\n",
        "    ax[0][1].set_ylabel('Height')\n",
        "    ax[0][1].plot(episode_height, 'g')\n",
        "\n",
        "    ax[1][0].title.set_text('Training Loss')\n",
        "    ax[1][0].set_xlabel('Episode')\n",
        "    ax[1][0].set_ylabel('Loss')\n",
        "    ax[1][0].plot(episode_loss, 'r')\n",
        "\n",
        "    ax[1][1].title.set_text('Training Q Vals')\n",
        "    ax[1][1].set_xlabel('Episode')\n",
        "    ax[1][1].set_ylabel('Qs')\n",
        "    ax[1][1].plot(episode_qs, 'c')\n",
        "    fig.canvas.draw()\n",
        "    plt.show(block=False)\n",
        "    plt.pause(.001)\n",
        "\n",
        "def episode():\n",
        "    done = False\n",
        "    rewardsSum = 0\n",
        "    qSum = 0\n",
        "    qActions = 1\n",
        "    lossSum = 0\n",
        "\n",
        "    currState = process_screen(env.reset())\n",
        "    lastState = currState\n",
        "    state = currState - lastState\n",
        "    state = tf.expand_dims(tf.constant(state), 0)\n",
        "\n",
        "    maxHeight = -10000\n",
        "\n",
        "    while not done:\n",
        "        action, q = agent.selectAction(state)\n",
        "        if q != -100000:\n",
        "            qSum += q\n",
        "            qActions += 1\n",
        "\n",
        "        obs, reward, done, info = env.step_all(action)\n",
        "\n",
        "        reward = reward[0]\n",
        "\n",
        "        maxHeight = max(info[0], maxHeight)\n",
        "        if info[0] >= 0.5:\n",
        "            reward += 10\n",
        "\n",
        "        lastState = currState\n",
        "        currState = process_screen(obs)\n",
        "\n",
        "        nextState = currState - lastState\n",
        "        nextState = tf.expand_dims(tf.constant(nextState), 0)\n",
        "        rewardsSum = np.add(rewardsSum, reward)\n",
        "\n",
        "        loss = agent.trainDQN()\n",
        "        agent.addMemory((state, action, reward, nextState, done))\n",
        "        state = nextState\n",
        "        lossSum += loss\n",
        "\n",
        "    episode_score.append(rewardsSum)\n",
        "    episode_qs.append(qSum/qActions)\n",
        "    episode_height.append(maxHeight)\n",
        "    episode_loss.append(lossSum)\n",
        "    episode_decay.append(agent.epsilon)\n",
        "    plot_episode()\n",
        "\n",
        "    agent.epsilon -= agent.epsilon_decay\n",
        "\n",
        "    if ep % agent.sync == 0:\n",
        "      agent.targetNetwork.model.set_weights(agent.trainNetwork.model.get_weights())\n",
        "\n",
        "    if rewardsSum != -202:\n",
        "        agent.save()\n",
        "\n",
        "    print(\"now epsilon is {}, the reward is {} with loss {} in episode {}\".format(agent.epsilon, rewardsSum, lossSum, ep)) \n",
        "\n",
        "ep = 1\n",
        "while ep < 10000:\n",
        "    episode()\n",
        "    ep += 1\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "#################  RESET GAME  ##################\n",
            "Episode terminated after: 330 (s)\n",
            "Total score: -1\n",
            "#################################################\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-eaacb94e6b0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0mep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mepisode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0mep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-eaacb94e6b0c>\u001b[0m in \u001b[0;36mepisode\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mlossSum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mcurrState\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mlastState\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrState\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlastState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-7f7bf3f10472>\u001b[0m in \u001b[0;36mprocess_screen\u001b[0;34m(observation)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mscreen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Resize, and add a batch dimension (BCHW)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pic should be 2/3 dimensional. Got {} dimensions.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: pic should be 2/3 dimensional. Got 1 dimensions."
          ]
        }
      ]
    }
  ]
}
/home/luke/.pyenv/versions/3.8.8/bin/python: can't open file 'agentRunner': [Errno 2] No such file or directory
pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)
Hello from the pygame community. https://www.pygame.org/contribute.html
2021-03-24 23:43:18.252721: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
libpng warning: iCCP: known incorrect sRGB profile
2021-03-24 23:43:21.141478: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-24 23:43:21.145052: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-24 23:43:21.298205: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2021-03-24 23:43:21.298284: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ml-vm): /proc/driver/nvidia/version does not exist
2021-03-24 23:43:21.299161: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
rescaling (Rescaling)        (None, 84, 84, 4)         0         
_________________________________________________________________
conv2d (Conv2D)              (None, 20, 20, 32)        8224      
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 9, 9, 64)          32832     
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 7, 7, 64)          36928     
_________________________________________________________________
flatten (Flatten)            (None, 3136)              0         
_________________________________________________________________
dense (Dense)                (None, 512)               1606144   
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 1539      
=================================================================
Total params: 1,685,667
Trainable params: 1,685,667
Non-trainable params: 0
_________________________________________________________________
None
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
rescaling_1 (Rescaling)      (None, 84, 84, 4)         0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 20, 20, 32)        8224      
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 9, 9, 64)          32832     
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 7, 7, 64)          36928     
_________________________________________________________________
flatten_1 (Flatten)          (None, 3136)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               1606144   
_________________________________________________________________
dense_3 (Dense)              (None, 3)                 1539      
=================================================================
Total params: 1,685,667
Trainable params: 1,685,667
Non-trainable params: 0
_________________________________________________________________
None
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.95, the reward is -446.1556324276157 with loss 0 in episode 0
Report: 
rewardSum:-446.1556324276157
height:-0.426280601395377
loss:0
qAverage:[0.0]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.95, the reward is -443.3856053179627 with loss 0 in episode 1
Report: 
rewardSum:-443.3856053179627
height:-0.38476228203180624
loss:0
qAverage:[0.40336867372194923]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.95, the reward is -438.7667174629714 with loss 0 in episode 2
Report: 
rewardSum:-438.7667174629714
height:-0.38565711652874335
loss:0
qAverage:[0.4052270901830573]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.95, the reward is -435.10523209852636 with loss 0 in episode 3
Report: 
rewardSum:-435.10523209852636
height:-0.42320962911924886
loss:0
qAverage:[0.3856017399917949]
libpng warning: iCCP: known incorrect sRGB profile
2021-03-24 23:43:31.728770: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-03-24 23:43:31.731689: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2596995000 Hz
now epsilon is 0.95, the reward is -450.42015040197913 with loss 0.6441649720072746 in episode 4
Report: 
rewardSum:-450.42015040197913
height:-0.4257940937253982
loss:0.6441649720072746
qAverage:[0.26700067945889067]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.95, the reward is -444.6166032997667 with loss 0.25635824736673385 in episode 5
Report: 
rewardSum:-444.6166032997667
height:-0.4152712798809662
loss:0.25635824736673385
qAverage:[-1.0046047687530517]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.95, the reward is -434.2660135738178 with loss 0.0981826654242468 in episode 6
Report: 
rewardSum:-434.2660135738178
height:-0.44475418766433794
loss:0.0981826654242468
qAverage:[-1.0172451092646673]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.95, the reward is -441.9587612399281 with loss 0.0746021409140667 in episode 7
Report: 
rewardSum:-441.9587612399281
height:-0.45253185389629264
loss:0.0746021409140667
qAverage:[-1.0148481527964275]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.95, the reward is -429.53564088487315 with loss 0.09286264328693505 in episode 8
Report: 
rewardSum:-429.53564088487315
height:-0.451432424284397
loss:0.09286264328693505
qAverage:[-0.9965481201807658]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.95, the reward is -439.2984667632506 with loss 0.07233418312534923 in episode 9
Report: 
rewardSum:-439.2984667632506
height:-0.406588737994455
loss:0.07233418312534923
qAverage:[-0.9874000549316406]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8999999999999999, the reward is -435.8786582898002 with loss 0.0786629563299357 in episode 10
Report: 
rewardSum:-435.8786582898002
height:-0.372731681039428
loss:0.0786629563299357
qAverage:[-0.989060714840889]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8999999999999999, the reward is -444.5092535789453 with loss 2.28666731499834 in episode 11
Report: 
rewardSum:-444.5092535789453
height:-0.4793281917147446
loss:2.28666731499834
qAverage:[-2.4516110731207807]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8999999999999999, the reward is -453.2895906043184 with loss 0.6607286024373025 in episode 12
Report: 
rewardSum:-453.2895906043184
height:-0.4681035688275266
loss:0.6607286024373025
qAverage:[-2.466177073392001]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8999999999999999, the reward is -445.7849217585543 with loss 0.617627054307377 in episode 13
Report: 
rewardSum:-445.7849217585543
height:-0.43427325275975837
loss:0.617627054307377
qAverage:[-2.4818652735816107]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8999999999999999, the reward is -445.4306945072954 with loss 0.6553363547718618 in episode 14
Report: 
rewardSum:-445.4306945072954
height:-0.38237281631018893
loss:0.6553363547718618
qAverage:[-2.457327561719077]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8999999999999999, the reward is -448.51575918669346 with loss 0.6863960715709254 in episode 15
Report: 
rewardSum:-448.51575918669346
height:-0.403310543465788
loss:0.6863960715709254
qAverage:[-2.4060579888960896]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8999999999999999, the reward is -443.6315705348634 with loss 0.6249696925879107 in episode 16
Report: 
rewardSum:-443.6315705348634
height:-0.4174389554386702
loss:0.6249696925879107
qAverage:[-2.5041838032858714]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8999999999999999, the reward is -443.24404765333475 with loss 0.6378032865031855 in episode 17
Report: 
rewardSum:-443.24404765333475
height:-0.45114643705898333
loss:0.6378032865031855
qAverage:[-2.4372457133399115]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8999999999999999, the reward is -452.3090378653862 with loss 0.6776019490789622 in episode 18
Report: 
rewardSum:-452.3090378653862
height:-0.31990942439703823
loss:0.6776019490789622
qAverage:[-2.3977983278386734]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8999999999999999, the reward is -447.22250511514284 with loss 0.7057563109265175 in episode 19
Report: 
rewardSum:-447.22250511514284
height:-0.3732575293468575
loss:0.7057563109265175
qAverage:[-2.533181570194386]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8499999999999999, the reward is -435.5649178335679 with loss 0.6361186988506233 in episode 20
Report: 
rewardSum:-435.5649178335679
height:-0.4026020417509205
loss:0.6361186988506233
qAverage:[-2.461189389228821]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8499999999999999, the reward is -458.2478503477856 with loss 4.543621612014249 in episode 21
Report: 
rewardSum:-458.2478503477856
height:-0.48914785969560853
loss:4.543621612014249
qAverage:[-3.7485523986816407]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8499999999999999, the reward is -450.32037186075934 with loss 2.3440305898257066 in episode 22
Report: 
rewardSum:-450.32037186075934
height:-0.44109722416529473
loss:2.3440305898257066
qAverage:[-3.888110653559367]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8499999999999999, the reward is -445.70389075264825 with loss 3.1194382689427584 in episode 23
Report: 
rewardSum:-445.70389075264825
height:-0.3605120537809636
loss:3.1194382689427584
qAverage:[-3.889299255428892]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8499999999999999, the reward is -443.3864641133987 with loss 2.6833764701586915 in episode 24
Report: 
rewardSum:-443.3864641133987
height:-0.33348352695223255
loss:2.6833764701586915
qAverage:[-3.9070013863699775]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8499999999999999, the reward is -436.60483506447014 with loss 2.7248711898719193 in episode 25
Report: 
rewardSum:-436.60483506447014
height:-0.37141903068549087
loss:2.7248711898719193
qAverage:[-3.8132123453863738]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8499999999999999, the reward is -443.9089973063611 with loss 2.5638719514681725 in episode 26
Report: 
rewardSum:-443.9089973063611
height:-0.4532528467366288
loss:2.5638719514681725
qAverage:[-3.8801011613437106]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8499999999999999, the reward is -450.701557261267 with loss 2.3380776576523203 in episode 27
Report: 
rewardSum:-450.701557261267
height:-0.3799023289019116
loss:2.3380776576523203
qAverage:[-3.7921482691398034]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8499999999999999, the reward is -443.6401657844279 with loss 2.457560515438672 in episode 28
Report: 
rewardSum:-443.6401657844279
height:-0.4132825466214699
loss:2.457560515438672
qAverage:[-3.8735069036483765]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8499999999999999, the reward is -453.7711658556051 with loss 2.7725666951737367 in episode 29
Report: 
rewardSum:-453.7711658556051
height:-0.39470963731749864
loss:2.7725666951737367
qAverage:[-3.9064788748236263]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7999999999999998, the reward is -449.42876467485934 with loss 2.5152350721909897 in episode 30
Report: 
rewardSum:-449.42876467485934
height:-0.42360439870736133
loss:2.5152350721909897
qAverage:[-3.8043224811553955]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7999999999999998, the reward is -451.69255794691037 with loss 6.852215384657029 in episode 31
Report: 
rewardSum:-451.69255794691037
height:-0.3663207710940306
loss:6.852215384657029
qAverage:[-5.275309381030855]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7999999999999998, the reward is -447.40612611669235 with loss 4.976630106713856 in episode 32
Report: 
rewardSum:-447.40612611669235
height:-0.3760244386944149
loss:4.976630106713856
qAverage:[-5.35571999776931]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7999999999999998, the reward is -446.474724600288 with loss 5.126376479049213 in episode 33
Report: 
rewardSum:-446.474724600288
height:-0.4628609598451476
loss:5.126376479049213
qAverage:[-5.34684524752877]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7999999999999998, the reward is -443.6219708736898 with loss 5.145197536825435 in episode 34
Report: 
rewardSum:-443.6219708736898
height:-0.4250085559918248
loss:5.145197536825435
qAverage:[-5.2399555512194365]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7999999999999998, the reward is -439.1846444373337 with loss 5.602620392921381 in episode 35
Report: 
rewardSum:-439.1846444373337
height:-0.4158150667910719
loss:5.602620392921381
qAverage:[-5.231749866319739]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7999999999999998, the reward is -450.7699225199953 with loss 4.839942671940662 in episode 36
Report: 
rewardSum:-450.7699225199953
height:-0.4053252026444428
loss:4.839942671940662
qAverage:[-5.346800724665324]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7999999999999998, the reward is -454.9758430941441 with loss 5.108472585852724 in episode 37
Report: 
rewardSum:-454.9758430941441
height:-0.3956385564512479
loss:5.108472585852724
qAverage:[-5.276785755157471]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7999999999999998, the reward is -443.47026750474345 with loss 5.072200657130452 in episode 38
Report: 
rewardSum:-443.47026750474345
height:-0.4478159109990977
loss:5.072200657130452
qAverage:[-5.2881169705777555]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7999999999999998, the reward is -433.6631997312177 with loss 4.6802107502298895 in episode 39
Report: 
rewardSum:-433.6631997312177
height:-0.45917287409816415
loss:4.6802107502298895
qAverage:[-5.295504786751487]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7499999999999998, the reward is -432.7802571807607 with loss 4.9567526363243815 in episode 40
Report: 
rewardSum:-432.7802571807607
height:-0.35326340295002306
loss:4.9567526363243815
qAverage:[-5.268043010131173]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7499999999999998, the reward is -443.5396172018967 with loss 9.72235291195102 in episode 41
Report: 
rewardSum:-443.5396172018967
height:-0.4284822396124752
loss:9.72235291195102
qAverage:[-6.75380521254106]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7499999999999998, the reward is -446.95336801119134 with loss 7.04464048857335 in episode 42
Report: 
rewardSum:-446.95336801119134
height:-0.29686494663043544
loss:7.04464048857335
qAverage:[-6.615671492637472]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7499999999999998, the reward is -440.95230140724266 with loss 6.919820782728493 in episode 43
Report: 
rewardSum:-440.95230140724266
height:-0.3872819298853518
loss:6.919820782728493
qAverage:[-6.550193512881243]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7499999999999998, the reward is -454.8509722513373 with loss 5.959401239466388 in episode 44
Report: 
rewardSum:-454.8509722513373
height:-0.35112493575932974
loss:5.959401239466388
qAverage:[-6.608900200236928]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7499999999999998, the reward is -454.4020848529906 with loss 6.029139711637981 in episode 45
Report: 
rewardSum:-454.4020848529906
height:-0.4104964607392343
loss:6.029139711637981
qAverage:[-6.753580012563932]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7499999999999998, the reward is -442.2233860816085 with loss 7.07746586704161 in episode 46
Report: 
rewardSum:-442.2233860816085
height:-0.4183455456580137
loss:7.07746586704161
qAverage:[-6.721419821182887]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7499999999999998, the reward is -445.8409564549193 with loss 5.936505330493674 in episode 47
Report: 
rewardSum:-445.8409564549193
height:-0.45005826108200647
loss:5.936505330493674
qAverage:[-6.70660162986593]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7499999999999998, the reward is -443.901970467335 with loss 5.334758692770265 in episode 48
Report: 
rewardSum:-443.901970467335
height:-0.4666703236563878
loss:5.334758692770265
qAverage:[-6.726772816284843]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7499999999999998, the reward is -437.50314586416494 with loss 5.933437321800739 in episode 49
Report: 
rewardSum:-437.50314586416494
height:-0.4571987598046428
loss:5.933437321800739
qAverage:[-6.752726824052872]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6999999999999997, the reward is -438.02006042393106 with loss 5.386888952227309 in episode 50
Report: 
rewardSum:-438.02006042393106
height:-0.4673142603126653
loss:5.386888952227309
qAverage:[-6.649159383773804]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6999999999999997, the reward is -454.8502654006533 with loss 8.575662878341973 in episode 51
Report: 
rewardSum:-454.8502654006533
height:-0.34215163628634454
loss:8.575662878341973
qAverage:[-8.076391266841515]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6999999999999997, the reward is -427.30273243829106 with loss 6.53072847018484 in episode 52
Report: 
rewardSum:-427.30273243829106
height:-0.4608185202414463
loss:6.53072847018484
qAverage:[-8.278677398508245]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6999999999999997, the reward is -430.53213702166767 with loss 6.779544060816988 in episode 53
Report: 
rewardSum:-430.53213702166767
height:-0.3211624071895935
loss:6.779544060816988
qAverage:[-8.150130831400553]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6999999999999997, the reward is -454.06410468583437 with loss 5.501951204962097 in episode 54
Report: 
rewardSum:-454.06410468583437
height:-0.40186855927708476
loss:5.501951204962097
qAverage:[-8.070324921607972]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6999999999999997, the reward is -450.5039798909964 with loss 6.043590429821052 in episode 55
Report: 
rewardSum:-450.5039798909964
height:-0.4550428610532372
loss:6.043590429821052
qAverage:[-8.025649428367615]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6999999999999997, the reward is -444.180415229194 with loss 6.044461141107604 in episode 56
Report: 
rewardSum:-444.180415229194
height:-0.4315823390550448
loss:6.044461141107604
qAverage:[-8.204514626533754]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6999999999999997, the reward is -451.8967607208152 with loss 5.87853747967165 in episode 57
Report: 
rewardSum:-451.8967607208152
height:-0.4723390918139337
loss:5.87853747967165
qAverage:[-8.111548406427557]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6999999999999997, the reward is -447.5213716980482 with loss 5.48278487555217 in episode 58
Report: 
rewardSum:-447.5213716980482
height:-0.4410428924071192
loss:5.48278487555217
qAverage:[-8.064053739820208]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6999999999999997, the reward is -440.2072236374442 with loss 5.484354414278641 in episode 59
Report: 
rewardSum:-440.2072236374442
height:-0.40530349937357013
loss:5.484354414278641
qAverage:[-8.193790117899576]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6499999999999997, the reward is -442.7492228961034 with loss 4.333907013526186 in episode 60
Report: 
rewardSum:-442.7492228961034
height:-0.4133445238592948
loss:4.333907013526186
qAverage:[-8.08238326526079]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6499999999999997, the reward is -445.6835413943231 with loss 8.828105391236022 in episode 61
Report: 
rewardSum:-445.6835413943231
height:-0.45856087448020333
loss:8.828105391236022
qAverage:[-9.606147725817184]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6499999999999997, the reward is -438.41879360378414 with loss 6.031454920070246 in episode 62
Report: 
rewardSum:-438.41879360378414
height:-0.39715198831870546
loss:6.031454920070246
qAverage:[-9.511119070507231]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6499999999999997, the reward is -448.78048656134393 with loss 5.9633091350551695 in episode 63
Report: 
rewardSum:-448.78048656134393
height:-0.41484381149685673
loss:5.9633091350551695
qAverage:[-9.544922816288935]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6499999999999997, the reward is -437.01824221189554 with loss 5.857572314329445 in episode 64
Report: 
rewardSum:-437.01824221189554
height:-0.39864704628771647
loss:5.857572314329445
qAverage:[-9.57863256964885]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6499999999999997, the reward is -442.671391586385 with loss 4.769376564538106 in episode 65
Report: 
rewardSum:-442.671391586385
height:-0.421572149942492
loss:4.769376564538106
qAverage:[-9.532410011916864]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6499999999999997, the reward is -447.1126771711234 with loss 5.918620568700135 in episode 66
Report: 
rewardSum:-447.1126771711234
height:-0.4269812206951096
loss:5.918620568700135
qAverage:[-9.49125215369211]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6499999999999997, the reward is -445.78069486998425 with loss 5.361465527443215 in episode 67
Report: 
rewardSum:-445.78069486998425
height:-0.379348853069145
loss:5.361465527443215
qAverage:[-9.523611923743939]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6499999999999997, the reward is -433.7189695610302 with loss 4.926899865735322 in episode 68
Report: 
rewardSum:-433.7189695610302
height:-0.35896275336235295
loss:4.926899865735322
qAverage:[-9.356905389476466]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6499999999999997, the reward is -450.39366526125445 with loss 4.163009660784155 in episode 69
Report: 
rewardSum:-450.39366526125445
height:-0.3901767866789722
loss:4.163009660784155
qAverage:[-9.37717398915972]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5999999999999996, the reward is -445.5311131418774 with loss 4.329695399384946 in episode 70
Report: 
rewardSum:-445.5311131418774
height:-0.4004872264791973
loss:4.329695399384946
qAverage:[-9.32286052270369]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5999999999999996, the reward is -455.137792286975 with loss 7.591862320899963 in episode 71
Report: 
rewardSum:-455.137792286975
height:-0.41665186897779966
loss:7.591862320899963
qAverage:[-10.693163654078608]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5999999999999996, the reward is -444.1469221934689 with loss 6.630089896963909 in episode 72
Report: 
rewardSum:-444.1469221934689
height:-0.37293539316572666
loss:6.630089896963909
qAverage:[-10.77298694703637]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5999999999999996, the reward is -451.91968738945747 with loss 5.727465366013348 in episode 73
Report: 
rewardSum:-451.91968738945747
height:-0.41919075481110757
loss:5.727465366013348
qAverage:[-10.575244866060407]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5999999999999996, the reward is -447.7309805396276 with loss 4.414243417326361 in episode 74
Report: 
rewardSum:-447.7309805396276
height:-0.42252843716624733
loss:4.414243417326361
qAverage:[-10.76326456524077]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5999999999999996, the reward is -436.8790341299142 with loss 5.222729394212365 in episode 75
Report: 
rewardSum:-436.8790341299142
height:-0.3190779021602914
loss:5.222729394212365
qAverage:[-10.723448532383616]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5999999999999996, the reward is -443.85269027100213 with loss 4.832974830875173 in episode 76
Report: 
rewardSum:-443.85269027100213
height:-0.44853659893508685
loss:4.832974830875173
qAverage:[-10.788657595713934]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5999999999999996, the reward is -448.33247105710143 with loss 4.587699001654983 in episode 77
Report: 
rewardSum:-448.33247105710143
height:-0.3414913344157338
loss:4.587699001654983
qAverage:[-10.798191289792115]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5999999999999996, the reward is -443.7769160417536 with loss 4.359100469853729 in episode 78
Report: 
rewardSum:-443.7769160417536
height:-0.358397159171428
loss:4.359100469853729
qAverage:[-10.656891376414197]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5999999999999996, the reward is -443.1128241232678 with loss 4.094996088184416 in episode 79
Report: 
rewardSum:-443.1128241232678
height:-0.4414261890877104
loss:4.094996088184416
qAverage:[-10.739798344761493]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5499999999999996, the reward is -441.4934642493891 with loss 4.183494086842984 in episode 80
Report: 
rewardSum:-441.4934642493891
height:-0.3996168496548562
loss:4.183494086842984
qAverage:[-10.650630719521466]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5499999999999996, the reward is -451.30153295061785 with loss 9.42524852277711 in episode 81
Report: 
rewardSum:-451.30153295061785
height:-0.3504637374696139
loss:9.42524852277711
qAverage:[-12.137313431881843]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5499999999999996, the reward is -445.8830009960481 with loss 6.392948332708329 in episode 82
Report: 
rewardSum:-445.8830009960481
height:-0.4762918465252811
loss:6.392948332708329
qAverage:[-12.107097869334014]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5499999999999996, the reward is -442.31220925361373 with loss 6.398946529254317 in episode 83
Report: 
rewardSum:-442.31220925361373
height:-0.44218167898977784
loss:6.398946529254317
qAverage:[-12.229382211086797]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5499999999999996, the reward is -441.01489502197603 with loss 5.443043765611947 in episode 84
Report: 
rewardSum:-441.01489502197603
height:-0.41717706604458976
loss:5.443043765611947
qAverage:[-12.02640619683773]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5499999999999996, the reward is -444.4217752032713 with loss 5.0717048505321145 in episode 85
Report: 
rewardSum:-444.4217752032713
height:-0.19859000284058262
loss:5.0717048505321145
qAverage:[-12.024486714667017]
libpng warning: iCCP: known incorrect sRGB profile
pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)
Hello from the pygame community. https://www.pygame.org/contribute.html
2021-03-25 00:12:50.330048: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
libpng warning: iCCP: known incorrect sRGB profile
2021-03-25 00:12:53.407119: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 00:12:53.411944: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-25 00:12:53.579565: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2021-03-25 00:12:53.579697: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ml-vm): /proc/driver/nvidia/version does not exist
2021-03-25 00:12:53.580824: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
rescaling (Rescaling)        (None, 84, 84, 4)         0         
_________________________________________________________________
conv2d (Conv2D)              (None, 20, 20, 32)        8224      
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 9, 9, 64)          32832     
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 7, 7, 64)          36928     
_________________________________________________________________
flatten (Flatten)            (None, 3136)              0         
_________________________________________________________________
dense (Dense)                (None, 512)               1606144   
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 1539      
=================================================================
Total params: 1,685,667
Trainable params: 1,685,667
Non-trainable params: 0
_________________________________________________________________
None
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
rescaling_1 (Rescaling)      (None, 84, 84, 4)         0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 20, 20, 32)        8224      
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 9, 9, 64)          32832     
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 7, 7, 64)          36928     
_________________________________________________________________
flatten_1 (Flatten)          (None, 3136)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               1606144   
_________________________________________________________________
dense_3 (Dense)              (None, 3)                 1539      
=================================================================
Total params: 1,685,667
Trainable params: 1,685,667
Non-trainable params: 0
_________________________________________________________________
None
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.95, the reward is -440.1219641179649 with loss 0 in episode 0
Report: 
rewardSum:-440.1219641179649
height:-0.3955068398801299
loss:0
qAverage:[0.0]
libpng warning: iCCP: known incorrect sRGB profile
Traceback (most recent call last):
  File "agentRunner.py", line 57, in <module>
    agent.train()
  File "/home/luke/MODRL/tensorflow_agents/mountain_car_graphical_ddqn.py", line 156, in train
    self.episode()
  File "/home/luke/MODRL/tensorflow_agents/mountain_car_graphical_ddqn.py", line 179, in episode
    obs, reward, done, height = self.env.step_all(action)
  File "/home/luke/MODRL/custom_envs/mountain_car/engine.py", line 471, in step_all
    self.frames.append(self.get_state())
  File "/home/luke/MODRL/custom_envs/mountain_car/engine.py", line 501, in get_state
    pygame.pixelcopy.surface_to_array(self.current_buffer, self.screen)
KeyboardInterrupt
pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)
Hello from the pygame community. https://www.pygame.org/contribute.html
2021-03-25 00:13:12.899826: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
libpng warning: iCCP: known incorrect sRGB profile
2021-03-25 00:13:15.158872: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 00:13:15.159952: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-25 00:13:15.322977: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2021-03-25 00:13:15.323057: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ml-vm): /proc/driver/nvidia/version does not exist
2021-03-25 00:13:15.324507: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
rescaling (Rescaling)        (None, 84, 84, 4)         0         
_________________________________________________________________
conv2d (Conv2D)              (None, 20, 20, 32)        8224      
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 9, 9, 64)          32832     
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 7, 7, 64)          36928     
_________________________________________________________________
flatten (Flatten)            (None, 3136)              0         
_________________________________________________________________
dense (Dense)                (None, 512)               1606144   
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 1539      
=================================================================
Total params: 1,685,667
Trainable params: 1,685,667
Non-trainable params: 0
_________________________________________________________________
None
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
rescaling_1 (Rescaling)      (None, 84, 84, 4)         0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 20, 20, 32)        8224      
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 9, 9, 64)          32832     
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 7, 7, 64)          36928     
_________________________________________________________________
flatten_1 (Flatten)          (None, 3136)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               1606144   
_________________________________________________________________
dense_3 (Dense)              (None, 3)                 1539      
=================================================================
Total params: 1,685,667
Trainable params: 1,685,667
Non-trainable params: 0
_________________________________________________________________
None
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.95, the reward is -441.6021907415519 with loss 0 in episode 0
Report: 
rewardSum:-441.6021907415519
height:-0.29107371591098835
loss:0
qAverage:[0.0]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.95, the reward is -441.6939760014739 with loss 0 in episode 1
Report: 
rewardSum:-441.6939760014739
height:-0.45552274503331447
loss:0
qAverage:[0.10905162786895578]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.95, the reward is -443.8220734665584 with loss 0 in episode 2
Report: 
rewardSum:-443.8220734665584
height:-0.4384665223775072
loss:0
qAverage:[0.10995563492178917]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.95, the reward is -436.44012800018396 with loss 0 in episode 3
Report: 
rewardSum:-436.44012800018396
height:-0.39798123706240607
loss:0
qAverage:[0.10854071006178856]
libpng warning: iCCP: known incorrect sRGB profile
2021-03-25 00:13:27.309153: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-03-25 00:13:27.312802: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2596995000 Hz
now epsilon is 0.95, the reward is -436.9798805564086 with loss 0.6706611756235361 in episode 4
Report: 
rewardSum:-436.9798805564086
height:-0.3273386083513886
loss:0.6706611756235361
qAverage:[-0.0019965342739049125]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.95, the reward is -435.68263113850816 with loss 0.3545459639863111 in episode 5
Report: 
rewardSum:-435.68263113850816
height:-0.2305524539947878
loss:0.3545459639863111
qAverage:[-1.282058624120859]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.95, the reward is -445.7961311432321 with loss 0.2498080805526115 in episode 6
Report: 
rewardSum:-445.7961311432321
height:-0.4181763024186925
loss:0.2498080805526115
qAverage:[-1.2790248577411358]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.95, the reward is -429.5121582747123 with loss 0.2027714231517166 in episode 7
Report: 
rewardSum:-429.5121582747123
height:-0.4469720046232899
loss:0.2027714231517166
qAverage:[-1.2957283343587602]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.95, the reward is -433.38253212228705 with loss 0.14119540198589675 in episode 8
Report: 
rewardSum:-433.38253212228705
height:-0.47818415764463945
loss:0.14119540198589675
qAverage:[-1.2311978191137314]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.95, the reward is -438.03805695975916 with loss 0.08981710523221409 in episode 9
Report: 
rewardSum:-438.03805695975916
height:-0.4148029620746094
loss:0.08981710523221409
qAverage:[-1.2203185160954793]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8999999999999999, the reward is -438.82206076685566 with loss 0.06425358154228888 in episode 10
Report: 
rewardSum:-438.82206076685566
height:-0.4047602408249844
loss:0.06425358154228888
qAverage:[-1.2808369214718158]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8999999999999999, the reward is -441.7885476945177 with loss 2.401158822933212 in episode 11
Report: 
rewardSum:-441.7885476945177
height:-0.4762124432399717
loss:2.401158822933212
qAverage:[-2.520091070069207]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8999999999999999, the reward is -432.7037365261452 with loss 0.9893380781868473 in episode 12
Report: 
rewardSum:-432.7037365261452
height:-0.4327544805112347
loss:0.9893380781868473
qAverage:[-2.645400649622867]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8999999999999999, the reward is -444.3863094042016 with loss 1.0138917628210038 in episode 13
Report: 
rewardSum:-444.3863094042016
height:-0.29025615952987005
loss:1.0138917628210038
qAverage:[-2.6638171076774597]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8999999999999999, the reward is -440.40876898851116 with loss 1.1564105212455615 in episode 14
Report: 
rewardSum:-440.40876898851116
height:-0.4650770154823921
loss:1.1564105212455615
qAverage:[-2.7017933229605355]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8999999999999999, the reward is -451.3914433047474 with loss 0.9557059180224314 in episode 15
Report: 
rewardSum:-451.3914433047474
height:-0.4625844203461389
loss:0.9557059180224314
qAverage:[-2.71297752338907]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8999999999999999, the reward is -426.9458191574839 with loss 0.9456019548815675 in episode 16
Report: 
rewardSum:-426.9458191574839
height:-0.3678925324969124
loss:0.9456019548815675
qAverage:[-2.628274217247963]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8999999999999999, the reward is -449.6679506423779 with loss 0.9789266939624213 in episode 17
Report: 
rewardSum:-449.6679506423779
height:-0.4060807116952319
loss:0.9789266939624213
qAverage:[-2.69886771440506]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8999999999999999, the reward is -434.7209160286854 with loss 1.0431457832455635 in episode 18
Report: 
rewardSum:-434.7209160286854
height:-0.34580079731084595
loss:1.0431457832455635
qAverage:[-2.664183486591686]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8999999999999999, the reward is -449.33387475838504 with loss 0.9020434395351913 in episode 19
Report: 
rewardSum:-449.33387475838504
height:-0.4213647766673452
loss:0.9020434395351913
qAverage:[-2.7149759101867676]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8499999999999999, the reward is -438.09718399183976 with loss 0.854512473451905 in episode 20
Report: 
rewardSum:-438.09718399183976
height:-0.36060106812098003
loss:0.854512473451905
qAverage:[-2.5982515811920166]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8499999999999999, the reward is -439.4514494901976 with loss 3.894069288042374 in episode 21
Report: 
rewardSum:-439.4514494901976
height:-0.46518264838251827
loss:3.894069288042374
qAverage:[-3.96809314644855]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8499999999999999, the reward is -427.95183095068904 with loss 2.952635702502448 in episode 22
Report: 
rewardSum:-427.95183095068904
height:-0.4437833755596274
loss:2.952635702502448
qAverage:[-4.0388800038231745]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8499999999999999, the reward is -446.2109646919773 with loss 3.0278020639379974 in episode 23
Report: 
rewardSum:-446.2109646919773
height:-0.32630868802999
loss:3.0278020639379974
qAverage:[-4.100682944962473]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8499999999999999, the reward is -451.56238826706806 with loss 3.1947133871144615 in episode 24
Report: 
rewardSum:-451.56238826706806
height:-0.4138186237378438
loss:3.1947133871144615
qAverage:[-4.062073809759958]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8499999999999999, the reward is -439.36576527858557 with loss 3.015603057850967 in episode 25
Report: 
rewardSum:-439.36576527858557
height:-0.44631517451832514
loss:3.015603057850967
qAverage:[-4.109766821066539]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8499999999999999, the reward is -438.57134520596924 with loss 3.1187374595174333 in episode 26
Report: 
rewardSum:-438.57134520596924
height:-0.3960471619788134
loss:3.1187374595174333
qAverage:[-4.104722829426036]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8499999999999999, the reward is -437.7399122440528 with loss 3.1513637976386235 in episode 27
Report: 
rewardSum:-437.7399122440528
height:-0.3030417331912129
loss:3.1513637976386235
qAverage:[-4.049581144537244]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8499999999999999, the reward is -446.23533927184826 with loss 3.2467377728025895 in episode 28
Report: 
rewardSum:-446.23533927184826
height:-0.4817781631702659
loss:3.2467377728025895
qAverage:[-4.0631193351745605]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8499999999999999, the reward is -444.5729466011881 with loss 3.549913107897737 in episode 29
Report: 
rewardSum:-444.5729466011881
height:-0.40317385176504833
loss:3.549913107897737
qAverage:[-3.9629954566126284]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7999999999999998, the reward is -437.39854381219277 with loss 3.238322981480451 in episode 30
Report: 
rewardSum:-437.39854381219277
height:-0.41906452440483766
loss:3.238322981480451
qAverage:[-4.093018217086792]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7999999999999998, the reward is -430.6014831530954 with loss 6.8228749866830185 in episode 31
Report: 
rewardSum:-430.6014831530954
height:-0.36843508110428147
loss:6.8228749866830185
qAverage:[-5.581725358963013]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7999999999999998, the reward is -438.4095577620491 with loss 5.145826102758292 in episode 32
Report: 
rewardSum:-438.4095577620491
height:-0.4061064353954953
loss:5.145826102758292
qAverage:[-5.546993186076482]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7999999999999998, the reward is -453.2353920837055 with loss 6.089720734074945 in episode 33
Report: 
rewardSum:-453.2353920837055
height:-0.4195145374136749
loss:6.089720734074945
qAverage:[-5.522748613357544]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7999999999999998, the reward is -456.31040216659835 with loss 5.776493968645809 in episode 34
Report: 
rewardSum:-456.31040216659835
height:-0.4312345634971307
loss:5.776493968645809
qAverage:[-5.595195477659052]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7999999999999998, the reward is -439.0848552399568 with loss 5.874503132668906 in episode 35
Report: 
rewardSum:-439.0848552399568
height:-0.39154632817961
loss:5.874503132668906
qAverage:[-5.491113553728376]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7999999999999998, the reward is -446.12935514202286 with loss 5.556592328277475 in episode 36
Report: 
rewardSum:-446.12935514202286
height:-0.4564063409260858
loss:5.556592328277475
qAverage:[-5.589614018149998]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7999999999999998, the reward is -451.4077492817675 with loss 6.52620559168281 in episode 37
Report: 
rewardSum:-451.4077492817675
height:-0.3175537346824186
loss:6.52620559168281
qAverage:[-5.568977479934692]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7999999999999998, the reward is -422.4828856559154 with loss 6.605131539952708 in episode 38
Report: 
rewardSum:-422.4828856559154
height:-0.4346907816153065
loss:6.605131539952708
qAverage:[-5.545310890674591]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7999999999999998, the reward is -427.37832351826114 with loss 6.219101906535798 in episode 39
Report: 
rewardSum:-427.37832351826114
height:-0.3771904745566806
loss:6.219101906535798
qAverage:[-5.511501935812143]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7499999999999998, the reward is -441.28009026418147 with loss 6.076807848294266 in episode 40
Report: 
rewardSum:-441.28009026418147
height:-0.38681935911857807
loss:6.076807848294266
qAverage:[-5.622104687160916]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7499999999999998, the reward is -433.27754249337 with loss 9.98608529905323 in episode 41
Report: 
rewardSum:-433.27754249337
height:-0.3399360077720366
loss:9.98608529905323
qAverage:[-6.95281602751534]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7499999999999998, the reward is -444.56396078709787 with loss 8.106224090734031 in episode 42
Report: 
rewardSum:-444.56396078709787
height:-0.3931969085646722
loss:8.106224090734031
qAverage:[-6.958754320939382]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7499999999999998, the reward is -443.0283873887527 with loss 8.612822580063948 in episode 43
Report: 
rewardSum:-443.0283873887527
height:-0.40358972139119653
loss:8.612822580063948
qAverage:[-6.944937846877358]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7499999999999998, the reward is -433.18134012461815 with loss 9.063363967637997 in episode 44
Report: 
rewardSum:-433.18134012461815
height:-0.3804854670338479
loss:9.063363967637997
qAverage:[-6.9664773380055145]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7499999999999998, the reward is -434.7917559174939 with loss 8.437223087908933 in episode 45
Report: 
rewardSum:-434.7917559174939
height:-0.46747976421498577
loss:8.437223087908933
qAverage:[-7.034907837303317]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7499999999999998, the reward is -429.2715721426787 with loss 9.31857826426858 in episode 46
Report: 
rewardSum:-429.2715721426787
height:-0.46188608651332297
loss:9.31857826426858
qAverage:[-6.9263291561857185]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7499999999999998, the reward is -439.4324189577741 with loss 8.730650897399755 in episode 47
Report: 
rewardSum:-439.4324189577741
height:-0.3958704662183926
loss:8.730650897399755
qAverage:[-7.058116294719555]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7499999999999998, the reward is -438.3725517343653 with loss 9.441861439816421 in episode 48
Report: 
rewardSum:-438.3725517343653
height:-0.42673596797814994
loss:9.441861439816421
qAverage:[-6.976613371979957]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7499999999999998, the reward is -440.9508961387264 with loss 9.511942706711125 in episode 49
Report: 
rewardSum:-440.9508961387264
height:-0.33566484278493747
loss:9.511942706711125
qAverage:[-7.0103282162121365]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6999999999999997, the reward is -433.8763168973266 with loss 8.48201030443306 in episode 50
Report: 
rewardSum:-433.8763168973266
height:-0.41748077542261025
loss:8.48201030443306
qAverage:[-6.997729309818201]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6999999999999997, the reward is -427.8085811452027 with loss 13.4671750239213 in episode 51
Report: 
rewardSum:-427.8085811452027
height:-0.3554041900862388
loss:13.4671750239213
qAverage:[-8.40077239177266]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6999999999999997, the reward is -433.3561945952438 with loss 12.64692973846104 in episode 52
Report: 
rewardSum:-433.3561945952438
height:-0.3297540537770204
loss:12.64692973846104
qAverage:[-8.427933585259222]
libpng warning: iCCP: known incorrect sRGB profile
pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)
Hello from the pygame community. https://www.pygame.org/contribute.html
2021-03-25 01:02:32.080405: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
libpng warning: iCCP: known incorrect sRGB profile
2021-03-25 01:02:44.975069: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 01:02:45.129194: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-25 01:02:45.443189: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2021-03-25 01:02:45.443268: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ml-vm): /proc/driver/nvidia/version does not exist
2021-03-25 01:02:45.444209: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
rescaling (Rescaling)        (None, 84, 84, 4)         0         
_________________________________________________________________
conv2d (Conv2D)              (None, 20, 20, 32)        8224      
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 9, 9, 64)          32832     
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 7, 7, 64)          36928     
_________________________________________________________________
flatten (Flatten)            (None, 3136)              0         
_________________________________________________________________
dense (Dense)                (None, 512)               1606144   
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 1539      
=================================================================
Total params: 1,685,667
Trainable params: 1,685,667
Non-trainable params: 0
_________________________________________________________________
None
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
rescaling_1 (Rescaling)      (None, 84, 84, 4)         0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 20, 20, 32)        8224      
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 9, 9, 64)          32832     
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 7, 7, 64)          36928     
_________________________________________________________________
flatten_1 (Flatten)          (None, 3136)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               1606144   
_________________________________________________________________
dense_3 (Dense)              (None, 3)                 1539      
=================================================================
Total params: 1,685,667
Trainable params: 1,685,667
Non-trainable params: 0
_________________________________________________________________
None
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.95, the reward is -429.56464213086014 with loss 0 in episode 0
Report: 
rewardSum:-429.56464213086014
height:-0.4406222271805625
loss:0
qAverage:[0.0]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.95, the reward is -449.4688519770539 with loss 0 in episode 1
Report: 
rewardSum:-449.4688519770539
height:-0.37520007736660177
loss:0
qAverage:[-0.13450929866387293]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.95, the reward is -453.61595171818317 with loss 0 in episode 2
Report: 
rewardSum:-453.61595171818317
height:-0.46223604602056817
loss:0
qAverage:[-0.1301479423418641]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.95, the reward is -447.8124085488658 with loss 0 in episode 3
Report: 
rewardSum:-447.8124085488658
height:-0.45805871653576913
loss:0
qAverage:[-0.12928873977877878]
libpng warning: iCCP: known incorrect sRGB profile
2021-03-25 01:02:58.126433: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-03-25 01:02:58.129822: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2596985000 Hz
now epsilon is 0.95, the reward is -448.20806945640555 with loss 0.66352902026847 in episode 4
Report: 
rewardSum:-448.20806945640555
height:-0.37847022152502185
loss:0.66352902026847
qAverage:[-0.41329156810587103]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.95, the reward is -441.19911058600997 with loss 0.3020023069693707 in episode 5
Report: 
rewardSum:-441.19911058600997
height:-0.4209528894927616
loss:0.3020023069693707
qAverage:[-1.4665094397284768]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.95, the reward is -438.244076052337 with loss 0.12446465838002041 in episode 6
Report: 
rewardSum:-438.244076052337
height:-0.4001717369827371
loss:0.12446465838002041
qAverage:[-1.4548105663723416]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.95, the reward is -448.41125205377267 with loss 0.14642098045442253 in episode 7
Report: 
rewardSum:-448.41125205377267
height:-0.34614150666818105
loss:0.14642098045442253
qAverage:[-1.5363606770833333]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.95, the reward is -459.84979297815227 with loss 0.07213280579890124 in episode 8
Report: 
rewardSum:-459.84979297815227
height:-0.4576452499338344
loss:0.07213280579890124
qAverage:[-1.4980311393737793]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.95, the reward is -449.2836042398478 with loss 0.0489170733744686 in episode 9
Report: 
rewardSum:-449.2836042398478
height:-0.42122547600284505
loss:0.0489170733744686
qAverage:[-1.5550247788429261]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8999999999999999, the reward is -439.039487403238 with loss 0.04268656242493307 in episode 10
Report: 
rewardSum:-439.039487403238
height:-0.4568186897373046
loss:0.04268656242493307
qAverage:[-1.5242904765265328]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8999999999999999, the reward is -435.24177200814114 with loss 3.2452584197744727 in episode 11
Report: 
rewardSum:-435.24177200814114
height:-0.3684301785852746
loss:3.2452584197744727
qAverage:[-2.8980937685285295]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8999999999999999, the reward is -437.1520542550112 with loss 1.1934526526601985 in episode 12
Report: 
rewardSum:-437.1520542550112
height:-0.43833148741369526
loss:1.1934526526601985
qAverage:[-2.969973643620809]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8999999999999999, the reward is -452.41553285815814 with loss 1.109869082341902 in episode 13
Report: 
rewardSum:-452.41553285815814
height:-0.42881463516862045
loss:1.109869082341902
qAverage:[-2.9609186398355583]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8999999999999999, the reward is -438.69436129723033 with loss 1.2273710895242402 in episode 14
Report: 
rewardSum:-438.69436129723033
height:-0.4618543729377218
loss:1.2273710895242402
qAverage:[-2.9999366998672485]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8999999999999999, the reward is -441.8641632194045 with loss 1.5187184012247599 in episode 15
Report: 
rewardSum:-441.8641632194045
height:-0.4309728929745078
loss:1.5187184012247599
qAverage:[-2.9761525903429304]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8999999999999999, the reward is -441.30843543372026 with loss 1.4203355808786 in episode 16
Report: 
rewardSum:-441.30843543372026
height:-0.452563777455578
loss:1.4203355808786
qAverage:[-2.948035648890904]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8999999999999999, the reward is -433.6135229354045 with loss 1.4342220645048656 in episode 17
Report: 
rewardSum:-433.6135229354045
height:-0.438036324043256
loss:1.4342220645048656
qAverage:[-2.9391229493277415]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8999999999999999, the reward is -445.30531761138576 with loss 1.3678923592306091 in episode 18
Report: 
rewardSum:-445.30531761138576
height:-0.38427526879794943
loss:1.3678923592306091
qAverage:[-2.9661745899601986]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8999999999999999, the reward is -439.6236001388782 with loss 1.4968190968284034 in episode 19
Report: 
rewardSum:-439.6236001388782
height:-0.4164038992816983
loss:1.4968190968284034
qAverage:[-2.903695837656657]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8499999999999999, the reward is -430.83289247656785 with loss 1.47298837438575 in episode 20
Report: 
rewardSum:-430.83289247656785
height:-0.4022660880316879
loss:1.47298837438575
qAverage:[-2.979177737236023]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8499999999999999, the reward is -431.2515616075703 with loss 5.1761051179491915 in episode 21
Report: 
rewardSum:-431.2515616075703
height:-0.3708132186526905
loss:5.1761051179491915
qAverage:[-4.376594815935407]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8499999999999999, the reward is -448.7750739739251 with loss 3.9539704356575385 in episode 22
Report: 
rewardSum:-448.7750739739251
height:-0.45815174986204643
loss:3.9539704356575385
qAverage:[-4.493910463844857]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8499999999999999, the reward is -430.01857279334706 with loss 4.547457614848099 in episode 23
Report: 
rewardSum:-430.01857279334706
height:-0.41327777468215915
loss:4.547457614848099
qAverage:[-4.37954680621624]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8499999999999999, the reward is -443.99633284499754 with loss 3.78165354259545 in episode 24
Report: 
rewardSum:-443.99633284499754
height:-0.4212087208387169
loss:3.78165354259545
qAverage:[-4.353628220096711]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8499999999999999, the reward is -430.148107438528 with loss 4.392487540404545 in episode 25
Report: 
rewardSum:-430.148107438528
height:-0.40749429477135174
loss:4.392487540404545
qAverage:[-4.353393439588876]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8499999999999999, the reward is -444.93973211918797 with loss 3.922975018431316 in episode 26
Report: 
rewardSum:-444.93973211918797
height:-0.45135616178511623
loss:3.922975018431316
qAverage:[-4.416739634105137]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8499999999999999, the reward is -434.74603735683405 with loss 4.466445016383659 in episode 27
Report: 
rewardSum:-434.74603735683405
height:-0.4412450007305539
loss:4.466445016383659
qAverage:[-4.340235480555782]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8499999999999999, the reward is -441.51186674348384 with loss 4.028844825021224 in episode 28
Report: 
rewardSum:-441.51186674348384
height:-0.44087290147036384
loss:4.028844825021224
qAverage:[-4.390404582023621]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.8499999999999999, the reward is -435.5029791579363 with loss 4.27668676693429 in episode 29
Report: 
rewardSum:-435.5029791579363
height:-0.4279112158554455
loss:4.27668676693429
qAverage:[-4.415831422805786]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7999999999999998, the reward is -432.28542257522014 with loss 3.8352200002409518 in episode 30
Report: 
rewardSum:-432.28542257522014
height:-0.39755752857070004
loss:3.8352200002409518
qAverage:[-4.418403106577256]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7999999999999998, the reward is -440.9280102171597 with loss 8.78515486753895 in episode 31
Report: 
rewardSum:-440.9280102171597
height:-0.4496585364523214
loss:8.78515486753895
qAverage:[-5.701769041460614]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7999999999999998, the reward is -442.97302297407634 with loss 6.4782371524197515 in episode 32
Report: 
rewardSum:-442.97302297407634
height:-0.3265102265749305
loss:6.4782371524197515
qAverage:[-5.764076153437297]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7999999999999998, the reward is -443.8301629912008 with loss 6.571398560583475 in episode 33
Report: 
rewardSum:-443.8301629912008
height:-0.394904118161464
loss:6.571398560583475
qAverage:[-5.784196246754039]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7999999999999998, the reward is -437.946898388826 with loss 7.374193124152953 in episode 34
Report: 
rewardSum:-437.946898388826
height:-0.4194579176501186
loss:7.374193124152953
qAverage:[-5.765101523012729]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7999999999999998, the reward is -442.07320619425957 with loss 6.559670285278116 in episode 35
Report: 
rewardSum:-442.07320619425957
height:-0.3204574632782573
loss:6.559670285278116
qAverage:[-5.772590549368608]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7999999999999998, the reward is -443.72519506900477 with loss 7.508009236553335 in episode 36
Report: 
rewardSum:-443.72519506900477
height:-0.3686152050304775
loss:7.508009236553335
qAverage:[-5.815423292991443]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7999999999999998, the reward is -442.6657248134104 with loss 6.664670948724961 in episode 37
Report: 
rewardSum:-442.6657248134104
height:-0.4091941888291049
loss:6.664670948724961
qAverage:[-5.787521733178033]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7999999999999998, the reward is -437.8185264284205 with loss 5.860550692232209 in episode 38
Report: 
rewardSum:-437.8185264284205
height:-0.4199876008254066
loss:5.860550692232209
qAverage:[-5.854548429187975]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7999999999999998, the reward is -431.4841004080097 with loss 6.739047261449741 in episode 39
Report: 
rewardSum:-431.4841004080097
height:-0.4572777858669434
loss:6.739047261449741
qAverage:[-5.800030594286711]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7499999999999998, the reward is -433.1793815288771 with loss 6.262940231710672 in episode 40
Report: 
rewardSum:-433.1793815288771
height:-0.4932649534032961
loss:6.262940231710672
qAverage:[-5.835304493599749]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7499999999999998, the reward is -446.4326861499027 with loss 10.303400252130814 in episode 41
Report: 
rewardSum:-446.4326861499027
height:-0.2324786655625186
loss:10.303400252130814
qAverage:[-7.263271152973175]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7499999999999998, the reward is -448.95575391554524 with loss 8.921099602099275 in episode 42
Report: 
rewardSum:-448.95575391554524
height:-0.4491119097878968
loss:8.921099602099275
qAverage:[-7.278408460617065]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7499999999999998, the reward is -448.7388247256632 with loss 9.293697862798581 in episode 43
Report: 
rewardSum:-448.7388247256632
height:-0.3293998988349839
loss:9.293697862798581
qAverage:[-7.253844892978668]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7499999999999998, the reward is -453.0433456105658 with loss 8.68346341844881 in episode 44
Report: 
rewardSum:-453.0433456105658
height:-0.34327149030756793
loss:8.68346341844881
qAverage:[-7.356668519973755]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7499999999999998, the reward is -443.16438655138074 with loss 7.3483173116110265 in episode 45
Report: 
rewardSum:-443.16438655138074
height:-0.45265079356118487
loss:7.3483173116110265
qAverage:[-7.279601898193359]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7499999999999998, the reward is -435.33635454396926 with loss 8.401460490538739 in episode 46
Report: 
rewardSum:-435.33635454396926
height:-0.4478269109639997
loss:8.401460490538739
qAverage:[-7.305745204289754]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7499999999999998, the reward is -448.6925560912023 with loss 8.455211622233037 in episode 47
Report: 
rewardSum:-448.6925560912023
height:-0.42597421870519686
loss:8.455211622233037
qAverage:[-7.225477861803632]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7499999999999998, the reward is -450.16077183816395 with loss 7.504950307426043 in episode 48
Report: 
rewardSum:-450.16077183816395
height:-0.4043035086710846
loss:7.504950307426043
qAverage:[-7.317624160221645]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.7499999999999998, the reward is -437.5327980309412 with loss 6.615405177639332 in episode 49
Report: 
rewardSum:-437.5327980309412
height:-0.4259587144603187
loss:6.615405177639332
qAverage:[-7.35686299434075]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6999999999999997, the reward is -445.38016201490774 with loss 5.26586773066083 in episode 50
Report: 
rewardSum:-445.38016201490774
height:-0.4173100777567382
loss:5.26586773066083
qAverage:[-7.217036703799633]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6999999999999997, the reward is -443.9781929762548 with loss 7.061735408962704 in episode 51
Report: 
rewardSum:-443.9781929762548
height:-0.403631873414203
loss:7.061735408962704
qAverage:[-8.678750111506535]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6999999999999997, the reward is -436.25969977943475 with loss 5.465293035143986 in episode 52
Report: 
rewardSum:-436.25969977943475
height:-0.351535866119847
loss:5.465293035143986
qAverage:[-8.552386372776354]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6999999999999997, the reward is -459.545940688879 with loss 3.498107048915699 in episode 53
Report: 
rewardSum:-459.545940688879
height:-0.35218750012465705
loss:3.498107048915699
qAverage:[-8.53898682016315]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6999999999999997, the reward is -452.1801266266138 with loss 3.0222915144404396 in episode 54
Report: 
rewardSum:-452.1801266266138
height:-0.36606338828443064
loss:3.0222915144404396
qAverage:[-8.680458389629017]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6999999999999997, the reward is -456.9039577193092 with loss 2.037871836218983 in episode 55
Report: 
rewardSum:-456.9039577193092
height:-0.3838934935958909
loss:2.037871836218983
qAverage:[-8.800843268632889]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6999999999999997, the reward is -441.0512788878709 with loss 1.3379699578508735 in episode 56
Report: 
rewardSum:-441.0512788878709
height:-0.31255170314325775
loss:1.3379699578508735
qAverage:[-8.623343855994088]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6999999999999997, the reward is -451.34653335749744 with loss 1.3776031758170575 in episode 57
Report: 
rewardSum:-451.34653335749744
height:-0.4563258275036038
loss:1.3776031758170575
qAverage:[-8.602864128048138]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6999999999999997, the reward is -443.15451842458174 with loss 1.1349034500308335 in episode 58
Report: 
rewardSum:-443.15451842458174
height:-0.4146464553036199
loss:1.1349034500308335
qAverage:[-8.6157742597289]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6999999999999997, the reward is -446.4913821477595 with loss 1.0137875641230494 in episode 59
Report: 
rewardSum:-446.4913821477595
height:-0.24313685487557007
loss:1.0137875641230494
qAverage:[-8.708782337081264]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6499999999999997, the reward is -438.7652725211714 with loss 0.9136092981789261 in episode 60
Report: 
rewardSum:-438.7652725211714
height:-0.38484205770114627
loss:0.9136092981789261
qAverage:[-8.620543778988353]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6499999999999997, the reward is -438.72826648127744 with loss 4.1293352484935895 in episode 61
Report: 
rewardSum:-438.72826648127744
height:-0.4826916141176773
loss:4.1293352484935895
qAverage:[-9.909918887274605]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6499999999999997, the reward is -450.56384935931425 with loss 1.7966479376191273 in episode 62
Report: 
rewardSum:-450.56384935931425
height:-0.38217306286709096
loss:1.7966479376191273
qAverage:[-10.007806181907654]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6499999999999997, the reward is -441.91416298886264 with loss 1.6212473410414532 in episode 63
Report: 
rewardSum:-441.91416298886264
height:-0.40826181800291134
loss:1.6212473410414532
qAverage:[-9.83782566246921]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6499999999999997, the reward is -427.5971062766154 with loss 0.994598058226984 in episode 64
Report: 
rewardSum:-427.5971062766154
height:-0.44292624223349375
loss:0.994598058226984
qAverage:[-10.082577561678951]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6499999999999997, the reward is -435.1942159068774 with loss 0.7422313172137365 in episode 65
Report: 
rewardSum:-435.1942159068774
height:-0.45052177086091716
loss:0.7422313172137365
qAverage:[-9.97766620462591]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6499999999999997, the reward is -432.44779623906004 with loss 0.6784570916788653 in episode 66
Report: 
rewardSum:-432.44779623906004
height:-0.36990948536660934
loss:0.6784570916788653
qAverage:[-9.818133845258115]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6499999999999997, the reward is -438.266955325701 with loss 0.638187721488066 in episode 67
Report: 
rewardSum:-438.266955325701
height:-0.46905788794546266
loss:0.638187721488066
qAverage:[-10.134065950618071]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6499999999999997, the reward is -450.69534188791334 with loss 0.6245228382758796 in episode 68
Report: 
rewardSum:-450.69534188791334
height:-0.37848122984053256
loss:0.6245228382758796
qAverage:[-10.09162373696604]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.6499999999999997, the reward is -450.80479158385947 with loss 0.5853927342686802 in episode 69
Report: 
rewardSum:-450.80479158385947
height:-0.33364275309636215
loss:0.5853927342686802
qAverage:[-10.106816119835026]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5999999999999996, the reward is -453.6659743506913 with loss 0.5780341493082233 in episode 70
Report: 
rewardSum:-453.6659743506913
height:-0.3600369697218099
loss:0.5780341493082233
qAverage:[-10.060320436954498]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5999999999999996, the reward is -444.02829430066527 with loss 5.343320190790109 in episode 71
Report: 
rewardSum:-444.02829430066527
height:-0.23573507037897715
loss:5.343320190790109
qAverage:[-11.370097550478848]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5999999999999996, the reward is -455.1343582491576 with loss 2.9323663051472977 in episode 72
Report: 
rewardSum:-455.1343582491576
height:-0.3984330077492909
loss:2.9323663051472977
qAverage:[-11.201320853031858]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5999999999999996, the reward is -454.63960947587964 with loss 2.3982042770367116 in episode 73
Report: 
rewardSum:-454.63960947587964
height:-0.3446371393695354
loss:2.3982042770367116
qAverage:[-11.513296767126155]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5999999999999996, the reward is -439.33739964517184 with loss 2.3219833777984604 in episode 74
Report: 
rewardSum:-439.33739964517184
height:-0.4121328925111582
loss:2.3219833777984604
qAverage:[-11.3830000265145]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5999999999999996, the reward is -441.18865422658325 with loss 2.2438611591933295 in episode 75
Report: 
rewardSum:-441.18865422658325
height:-0.4625943934528295
loss:2.2438611591933295
qAverage:[-11.399790604909262]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5999999999999996, the reward is -439.3411968809377 with loss 2.3986453761463054 in episode 76
Report: 
rewardSum:-439.3411968809377
height:-0.4120481336335111
loss:2.3986453761463054
qAverage:[-11.187251235397769]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5999999999999996, the reward is -455.83690067396304 with loss 2.3944940726505592 in episode 77
Report: 
rewardSum:-455.83690067396304
height:-0.3153742793250372
loss:2.3944940726505592
qAverage:[-11.395971585469074]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5999999999999996, the reward is -441.86060894398315 with loss 2.4340324974036776 in episode 78
Report: 
rewardSum:-441.86060894398315
height:-0.4770104423393315
loss:2.4340324974036776
qAverage:[-11.262603991410948]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5999999999999996, the reward is -446.00046580031244 with loss 2.4651894852286205 in episode 79
Report: 
rewardSum:-446.00046580031244
height:-0.471089247281428
loss:2.4651894852286205
qAverage:[-11.478583852450052]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5499999999999996, the reward is -450.67618689954537 with loss 2.4121072662528604 in episode 80
Report: 
rewardSum:-450.67618689954537
height:-0.4362308696988082
loss:2.4121072662528604
qAverage:[-11.478592368993866]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5499999999999996, the reward is -425.1053405916848 with loss 7.943143403972499 in episode 81
Report: 
rewardSum:-425.1053405916848
height:-0.3906275151387511
loss:7.943143403972499
qAverage:[-12.554257211196854]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5499999999999996, the reward is -452.0461831614372 with loss 6.473841108963825 in episode 82
Report: 
rewardSum:-452.0461831614372
height:-0.39157044924105255
loss:6.473841108963825
qAverage:[-12.77137777977383]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5499999999999996, the reward is -446.2977403896614 with loss 7.519316577585414 in episode 83
Report: 
rewardSum:-446.2977403896614
height:-0.4247971702004782
loss:7.519316577585414
qAverage:[-12.666014187037945]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5499999999999996, the reward is -452.3894959072424 with loss 7.763020703918301 in episode 84
Report: 
rewardSum:-452.3894959072424
height:-0.42289913389542444
loss:7.763020703918301
qAverage:[-12.596311921543546]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5499999999999996, the reward is -436.5872565951479 with loss 7.544139114907011 in episode 85
Report: 
rewardSum:-436.5872565951479
height:-0.45373722832896135
loss:7.544139114907011
qAverage:[-12.573689618742609]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5499999999999996, the reward is -439.0999545618204 with loss 6.108409744338132 in episode 86
Report: 
rewardSum:-439.0999545618204
height:-0.3765941298377339
loss:6.108409744338132
qAverage:[-12.634624746938547]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5499999999999996, the reward is -444.56308010635894 with loss 6.751727405004203 in episode 87
Report: 
rewardSum:-444.56308010635894
height:-0.42822108305383616
loss:6.751727405004203
qAverage:[-12.727435899068075]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5499999999999996, the reward is -445.75727108429373 with loss 7.665083984611556 in episode 88
Report: 
rewardSum:-445.75727108429373
height:-0.47671472360259776
loss:7.665083984611556
qAverage:[-12.950382852554322]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.5499999999999996, the reward is -450.9508794011609 with loss 5.891866712132469 in episode 89
Report: 
rewardSum:-450.9508794011609
height:-0.33902593244407014
loss:5.891866712132469
qAverage:[-12.671606587891532]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.4999999999999996, the reward is -442.51517314450257 with loss 5.861433382262476 in episode 90
Report: 
rewardSum:-442.51517314450257
height:-0.42847711877826317
loss:5.861433382262476
qAverage:[-12.708274051981064]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.4999999999999996, the reward is -467.41329307372666 with loss 9.968702930375002 in episode 91
Report: 
rewardSum:-467.41329307372666
height:-0.3343576245734338
loss:9.968702930375002
qAverage:[-13.747824680346708]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.4999999999999996, the reward is -450.6148905158899 with loss 9.451828577322885 in episode 92
Report: 
rewardSum:-450.6148905158899
height:-0.4099151435733573
loss:9.451828577322885
qAverage:[-14.008597895339294]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.4999999999999996, the reward is -441.68432759209 with loss 7.767499604029581 in episode 93
Report: 
rewardSum:-441.68432759209
height:-0.44014279996130845
loss:7.767499604029581
qAverage:[-13.876407757401466]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.4999999999999996, the reward is -442.36067108559143 with loss 7.540146040497348 in episode 94
Report: 
rewardSum:-442.36067108559143
height:-0.3699972514248567
loss:7.540146040497348
qAverage:[-13.782630314610222]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.4999999999999996, the reward is -439.6067130124787 with loss 7.900869198841974 in episode 95
Report: 
rewardSum:-439.6067130124787
height:-0.3121927182044568
loss:7.900869198841974
qAverage:[-13.827021207725792]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.4999999999999996, the reward is -444.37378069420026 with loss 7.0483812999445945 in episode 96
Report: 
rewardSum:-444.37378069420026
height:-0.41507643836508756
loss:7.0483812999445945
qAverage:[-13.972457802172789]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.4999999999999996, the reward is -447.0006725852895 with loss 6.929115296690725 in episode 97
Report: 
rewardSum:-447.0006725852895
height:-0.39472409092816946
loss:6.929115296690725
qAverage:[-13.914846511320635]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.4999999999999996, the reward is -453.5432800444485 with loss 6.748331208364107 in episode 98
Report: 
rewardSum:-453.5432800444485
height:-0.3466608653995199
loss:6.748331208364107
qAverage:[-13.938938774002922]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.4999999999999996, the reward is -446.6405608764939 with loss 7.170503836125135 in episode 99
Report: 
rewardSum:-446.6405608764939
height:-0.2912655344919568
loss:7.170503836125135
qAverage:[-13.859535694122314]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.4499999999999996, the reward is -466.75933785109754 with loss 6.565690623130649 in episode 100
Report: 
rewardSum:-466.75933785109754
height:-0.3289868123010957
loss:6.565690623130649
qAverage:[-13.943892635740676]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.4499999999999996, the reward is -458.7407483186705 with loss 10.308316227281466 in episode 101
Report: 
rewardSum:-458.7407483186705
height:-0.43180350212510277
loss:10.308316227281466
qAverage:[-15.560818680401507]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.4499999999999996, the reward is -449.10819410029467 with loss 9.878988930140622 in episode 102
Report: 
rewardSum:-449.10819410029467
height:-0.2964374474206181
loss:9.878988930140622
qAverage:[-14.968990126770475]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.4499999999999996, the reward is -442.61268244390936 with loss 8.29525399080012 in episode 103
Report: 
rewardSum:-442.61268244390936
height:-0.40028044909396476
loss:8.29525399080012
qAverage:[-14.923679078618685]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.4499999999999996, the reward is -449.1579233377727 with loss 9.535831440938637 in episode 104
Report: 
rewardSum:-449.1579233377727
height:-0.3142108132379559
loss:9.535831440938637
qAverage:[-14.881147309294287]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.4499999999999996, the reward is -446.59537810445 with loss 6.981532778590918 in episode 105
Report: 
rewardSum:-446.59537810445
height:-0.3603958331075289
loss:6.981532778590918
qAverage:[-15.328808822254143]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.4499999999999996, the reward is -459.4845388636667 with loss 7.924151743762195 in episode 106
Report: 
rewardSum:-459.4845388636667
height:-0.3743392049048473
loss:7.924151743762195
qAverage:[-14.873886327330883]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.4499999999999996, the reward is -447.27493428767184 with loss 7.827932278160006 in episode 107
Report: 
rewardSum:-447.27493428767184
height:-0.42185784234257995
loss:7.827932278160006
qAverage:[-15.08625236738508]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.4499999999999996, the reward is -431.26449710258146 with loss 7.859101366484538 in episode 108
Report: 
rewardSum:-431.26449710258146
height:-0.3819396189734308
loss:7.859101366484538
qAverage:[-15.15293262101183]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.4499999999999996, the reward is -456.3598043068634 with loss 6.839391643065028 in episode 109
Report: 
rewardSum:-456.3598043068634
height:-0.399962515958697
loss:6.839391643065028
qAverage:[-14.805959935582012]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.39999999999999963, the reward is -441.6257961899527 with loss 6.8960080454126 in episode 110
Report: 
rewardSum:-441.6257961899527
height:-0.47819173057627784
loss:6.8960080454126
qAverage:[-15.45175450216464]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.39999999999999963, the reward is -441.2041017281674 with loss 11.64134185295552 in episode 111
Report: 
rewardSum:-441.2041017281674
height:-0.21986123585378423
loss:11.64134185295552
qAverage:[-16.39183325426919]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.39999999999999963, the reward is -445.37639112447266 with loss 9.250499260611832 in episode 112
Report: 
rewardSum:-445.37639112447266
height:-0.3180193279474224
loss:9.250499260611832
qAverage:[-16.32116764241999]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.39999999999999963, the reward is -442.8503142820906 with loss 9.809337369399145 in episode 113
Report: 
rewardSum:-442.8503142820906
height:-0.4379059317759136
loss:9.809337369399145
qAverage:[-16.399326477880063]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.39999999999999963, the reward is -443.81461968810356 with loss 8.925421042367816 in episode 114
Report: 
rewardSum:-443.81461968810356
height:-0.3109450131902087
loss:8.925421042367816
qAverage:[-16.573349960148335]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.39999999999999963, the reward is -429.0671110034721 with loss 7.385914244223386 in episode 115
Report: 
rewardSum:-429.0671110034721
height:-0.32387970637614155
loss:7.385914244223386
qAverage:[-16.55721454222997]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.39999999999999963, the reward is -445.0772867949491 with loss 7.673401185311377 in episode 116
Report: 
rewardSum:-445.0772867949491
height:-0.42207662664840023
loss:7.673401185311377
qAverage:[-16.70861103088875]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.39999999999999963, the reward is -454.12993965363603 with loss 7.799409661674872 in episode 117
Report: 
rewardSum:-454.12993965363603
height:-0.3645062691538962
loss:7.799409661674872
qAverage:[-16.573937956302885]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.39999999999999963, the reward is -436.58758286257427 with loss 6.130186466500163 in episode 118
Report: 
rewardSum:-436.58758286257427
height:-0.3827779725734826
loss:6.130186466500163
qAverage:[-16.62515982471858]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.39999999999999963, the reward is -439.05735186760626 with loss 8.18535593803972 in episode 119
Report: 
rewardSum:-439.05735186760626
height:-0.2564832517651046
loss:8.18535593803972
qAverage:[-16.522050121307373]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.34999999999999964, the reward is -461.40463280764675 with loss 8.263503596419469 in episode 120
Report: 
rewardSum:-461.40463280764675
height:-0.21002179381350547
loss:8.263503596419469
qAverage:[-16.670361991820297]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.34999999999999964, the reward is -459.4147491272422 with loss 11.03780939616263 in episode 121
Report: 
rewardSum:-459.4147491272422
height:-0.21326107993308147
loss:11.03780939616263
qAverage:[-17.80317172970805]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.34999999999999964, the reward is -436.98978087659333 with loss 10.419312234269455 in episode 122
Report: 
rewardSum:-436.98978087659333
height:-0.31086869810976886
loss:10.419312234269455
qAverage:[-17.59803483603706]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.34999999999999964, the reward is -448.81122262809544 with loss 9.793337563052773 in episode 123
Report: 
rewardSum:-448.81122262809544
height:-0.19534411250761197
loss:9.793337563052773
qAverage:[-17.683854793057296]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.34999999999999964, the reward is -462.5704032614628 with loss 9.2894140065182 in episode 124
Report: 
rewardSum:-462.5704032614628
height:-0.31783566545040387
loss:9.2894140065182
qAverage:[-17.796515660023125]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.34999999999999964, the reward is -457.3780146553735 with loss 10.003547030035406 in episode 125
Report: 
rewardSum:-457.3780146553735
height:-0.30308844581580124
loss:10.003547030035406
qAverage:[-17.94764127220426]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.34999999999999964, the reward is -444.7452262587039 with loss 9.052143699489534 in episode 126
Report: 
rewardSum:-444.7452262587039
height:-0.40452463162679503
loss:9.052143699489534
qAverage:[-17.617833213698596]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.34999999999999964, the reward is -450.82433182338616 with loss 10.131356621161103 in episode 127
Report: 
rewardSum:-450.82433182338616
height:-0.30542291897508367
loss:10.131356621161103
qAverage:[-17.693893363981537]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.34999999999999964, the reward is -448.38174972262004 with loss 9.883696400094777 in episode 128
Report: 
rewardSum:-448.38174972262004
height:-0.3506174739219253
loss:9.883696400094777
qAverage:[-17.581882740222458]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.34999999999999964, the reward is -448.3084548120207 with loss 7.513655418762937 in episode 129
Report: 
rewardSum:-448.3084548120207
height:-0.2515831703411323
loss:7.513655418762937
qAverage:[-17.558451499494296]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.29999999999999966, the reward is -441.84580367890385 with loss 9.23266505706124 in episode 130
Report: 
rewardSum:-441.84580367890385
height:-0.4249163166106599
loss:9.23266505706124
qAverage:[-17.98617902421762]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.29999999999999966, the reward is -453.6790252480828 with loss 12.823370412457734 in episode 131
Report: 
rewardSum:-453.6790252480828
height:-0.3550507786852111
loss:12.823370412457734
qAverage:[-19.03111312588727]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.29999999999999966, the reward is -442.69585407149606 with loss 12.01539907651022 in episode 132
Report: 
rewardSum:-442.69585407149606
height:-0.36619233482916796
loss:12.01539907651022
qAverage:[-19.115186776090788]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.29999999999999966, the reward is -449.8897416549902 with loss 10.548593380954117 in episode 133
Report: 
rewardSum:-449.8897416549902
height:-0.3720960019365798
loss:10.548593380954117
qAverage:[-18.97435946793392]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.29999999999999966, the reward is -425.6469376765804 with loss 10.127592119388282 in episode 134
Report: 
rewardSum:-425.6469376765804
height:-0.386271696209853
loss:10.127592119388282
qAverage:[-18.9252929892761]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.29999999999999966, the reward is -459.5344966548534 with loss 10.01761045656167 in episode 135
Report: 
rewardSum:-459.5344966548534
height:-0.3510008976724615
loss:10.01761045656167
qAverage:[-18.83698334130976]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.29999999999999966, the reward is -453.78726380165097 with loss 9.490556167205796 in episode 136
Report: 
rewardSum:-453.78726380165097
height:-0.3977933786814017
loss:9.490556167205796
qAverage:[-19.05260533094406]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.29999999999999966, the reward is -431.1100349427001 with loss 9.94014031579718 in episode 137
Report: 
rewardSum:-431.1100349427001
height:-0.344132033419686
loss:9.94014031579718
qAverage:[-18.91305889716515]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.29999999999999966, the reward is -446.4267677358669 with loss 8.388124433346093 in episode 138
Report: 
rewardSum:-446.4267677358669
height:-0.3476766009647168
loss:8.388124433346093
qAverage:[-19.024272747283433]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.29999999999999966, the reward is -443.18700360163274 with loss 10.564916441449896 in episode 139
Report: 
rewardSum:-443.18700360163274
height:-0.3887797157478103
loss:10.564916441449896
qAverage:[-19.08701746745242]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.24999999999999967, the reward is -433.71371737937733 with loss 9.42804183147382 in episode 140
Report: 
rewardSum:-433.71371737937733
height:-0.3657371268607923
loss:9.42804183147382
qAverage:[-18.688955320517223]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.24999999999999967, the reward is -432.58801076694255 with loss 13.787925158860162 in episode 141
Report: 
rewardSum:-432.58801076694255
height:-0.41696892815223363
loss:13.787925158860162
qAverage:[-20.172580961939655]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.24999999999999967, the reward is -460.4350153685658 with loss 9.591773646883667 in episode 142
Report: 
rewardSum:-460.4350153685658
height:-0.40529693919436377
loss:9.591773646883667
qAverage:[-20.14306798255701]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.24999999999999967, the reward is -462.40532277177124 with loss 12.346940509043634 in episode 143
Report: 
rewardSum:-462.40532277177124
height:-0.2887021071774351
loss:12.346940509043634
qAverage:[-19.918629978217332]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.24999999999999967, the reward is -461.68618194780373 with loss 12.372764906613156 in episode 144
Report: 
rewardSum:-461.68618194780373
height:-0.2855103357321405
loss:12.372764906613156
qAverage:[-19.991301379747107]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.24999999999999967, the reward is -447.5410738073925 with loss 9.632586100604385 in episode 145
Report: 
rewardSum:-447.5410738073925
height:-0.45487914802837054
loss:9.632586100604385
qAverage:[-20.234183963243062]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.24999999999999967, the reward is -454.2723138808166 with loss 9.685798547230661 in episode 146
Report: 
rewardSum:-454.2723138808166
height:-0.29762056844796664
loss:9.685798547230661
qAverage:[-20.371114899714787]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.24999999999999967, the reward is -448.0682335236121 with loss 9.30706692347303 in episode 147
Report: 
rewardSum:-448.0682335236121
height:-0.40062249650859616
loss:9.30706692347303
qAverage:[-19.865416650648243]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.24999999999999967, the reward is -446.91313594602934 with loss 10.537727086804807 in episode 148
Report: 
rewardSum:-446.91313594602934
height:-0.1954984689090683
loss:10.537727086804807
qAverage:[-19.935590270339258]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.24999999999999967, the reward is -437.3729554866955 with loss 9.089096116367728 in episode 149
Report: 
rewardSum:-437.3729554866955
height:-0.4039048829672416
loss:9.089096116367728
qAverage:[-19.971042468630035]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.19999999999999968, the reward is -441.933627787759 with loss 10.166473738150671 in episode 150
Report: 
rewardSum:-441.933627787759
height:-0.3192417835088678
loss:10.166473738150671
qAverage:[-20.036360234996074]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.19999999999999968, the reward is -439.036535413513 with loss 14.148181254509836 in episode 151
Report: 
rewardSum:-439.036535413513
height:-0.35240007450457006
loss:14.148181254509836
qAverage:[-20.98266190215002]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.19999999999999968, the reward is -462.60887600067576 with loss 11.354652732843533 in episode 152
Report: 
rewardSum:-462.60887600067576
height:-0.1668041195955388
loss:11.354652732843533
qAverage:[-20.65822689957414]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.19999999999999968, the reward is -435.0428430369674 with loss 11.672694257926196 in episode 153
Report: 
rewardSum:-435.0428430369674
height:-0.22671510675816833
loss:11.672694257926196
qAverage:[-21.306714153909063]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.19999999999999968, the reward is -452.662352279834 with loss 10.897562716389075 in episode 154
Report: 
rewardSum:-452.662352279834
height:-0.3243696512648844
loss:10.897562716389075
qAverage:[-20.857276443777412]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.19999999999999968, the reward is -444.2783000114751 with loss 11.13182833371684 in episode 155
Report: 
rewardSum:-444.2783000114751
height:-0.4113047690889526
loss:11.13182833371684
qAverage:[-19.421049329212735]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.19999999999999968, the reward is -467.04381670914677 with loss 11.80518962466158 in episode 156
Report: 
rewardSum:-467.04381670914677
height:-0.2508483861594172
loss:11.80518962466158
qAverage:[-21.084728055205083]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.19999999999999968, the reward is -462.32460993128325 with loss 10.663435112917796 in episode 157
Report: 
rewardSum:-462.32460993128325
height:-0.30066659306880883
loss:10.663435112917796
qAverage:[-20.969469485668146]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.19999999999999968, the reward is -440.5943793192353 with loss 13.320952841779217 in episode 158
Report: 
rewardSum:-440.5943793192353
height:-0.3117529901917547
loss:13.320952841779217
qAverage:[-20.808650002593087]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.19999999999999968, the reward is -470.9826901506781 with loss 12.545624245656654 in episode 159
Report: 
rewardSum:-470.9826901506781
height:-0.26713619621802664
loss:12.545624245656654
qAverage:[-21.222303792202112]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.1499999999999997, the reward is -443.43754046171307 with loss 11.156333559891209 in episode 160
Report: 
rewardSum:-443.43754046171307
height:-0.3084064359124099
loss:11.156333559891209
qAverage:[-21.353844272423974]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.1499999999999997, the reward is -459.4581694277531 with loss 15.271963897859678 in episode 161
Report: 
rewardSum:-459.4581694277531
height:-0.27326626227334455
loss:15.271963897859678
qAverage:[-22.271226196970257]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.1499999999999997, the reward is -449.1882395320147 with loss 12.169888369273394 in episode 162
Report: 
rewardSum:-449.1882395320147
height:-0.24416755979677565
loss:12.169888369273394
qAverage:[-22.084554982682068]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.1499999999999997, the reward is -443.9603035403081 with loss 12.946453863522038 in episode 163
Report: 
rewardSum:-443.9603035403081
height:-0.29955306544597116
loss:12.946453863522038
qAverage:[-21.906368902751378]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.1499999999999997, the reward is -476.2312592367987 with loss 11.80690857861191 in episode 164
Report: 
rewardSum:-476.2312592367987
height:-0.1890680431697239
loss:11.80690857861191
qAverage:[-21.698120594024658]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.1499999999999997, the reward is -454.13645796915785 with loss 13.839492429513484 in episode 165
Report: 
rewardSum:-454.13645796915785
height:-0.2016049469815622
loss:13.839492429513484
qAverage:[-21.89022544636364]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.1499999999999997, the reward is -454.5279385932934 with loss 11.121404178906232 in episode 166
Report: 
rewardSum:-454.5279385932934
height:-0.25805011221592744
loss:11.121404178906232
qAverage:[-21.8866833959307]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.1499999999999997, the reward is -444.0724771471521 with loss 12.015970003092661 in episode 167
Report: 
rewardSum:-444.0724771471521
height:-0.13222846308992126
loss:12.015970003092661
qAverage:[-21.19017115651562]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.1499999999999997, the reward is -444.1497169722214 with loss 11.278680845862255 in episode 168
Report: 
rewardSum:-444.1497169722214
height:-0.2830007501811811
loss:11.278680845862255
qAverage:[-21.710788779948132]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.1499999999999997, the reward is -444.71356446892213 with loss 11.67073313659057 in episode 169
Report: 
rewardSum:-444.71356446892213
height:-0.1947045029889824
loss:11.67073313659057
qAverage:[-22.36063289642334]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.09999999999999969, the reward is -460.137493860263 with loss 11.61136676883325 in episode 170
Report: 
rewardSum:-460.137493860263
height:-0.2898037545020505
loss:11.61136676883325
qAverage:[-22.022861251831056]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.09999999999999969, the reward is -462.06527186871483 with loss 13.933212718227878 in episode 171
Report: 
rewardSum:-462.06527186871483
height:-0.3483122061565029
loss:13.933212718227878
qAverage:[-23.1857368616768]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.09999999999999969, the reward is -465.92363270263337 with loss 12.395267574582249 in episode 172
Report: 
rewardSum:-465.92363270263337
height:-0.29757788593713486
loss:12.395267574582249
qAverage:[-21.42446607318732]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.09999999999999969, the reward is -447.98232579321405 with loss 11.739269184414297 in episode 173
Report: 
rewardSum:-447.98232579321405
height:-0.37910109037568135
loss:11.739269184414297
qAverage:[-21.909439924600964]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.09999999999999969, the reward is -450.91123231192034 with loss 12.142161877593026 in episode 174
Report: 
rewardSum:-450.91123231192034
height:-0.292428862191494
loss:12.142161877593026
qAverage:[-23.095039758789405]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.09999999999999969, the reward is -463.84405753249916 with loss 11.265740943141282 in episode 175
Report: 
rewardSum:-463.84405753249916
height:-0.31604703761585534
loss:11.265740943141282
qAverage:[-22.665444430484566]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.09999999999999969, the reward is -449.64108171877854 with loss 11.250905473250896 in episode 176
Report: 
rewardSum:-449.64108171877854
height:-0.2721836342926769
loss:11.250905473250896
qAverage:[-21.881220786966743]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.09999999999999969, the reward is -464.3115577798877 with loss 12.74326324602589 in episode 177
Report: 
rewardSum:-464.3115577798877
height:-0.3337313701419271
loss:12.74326324602589
qAverage:[-20.50687259121945]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.09999999999999969, the reward is -462.0954995756071 with loss 12.690571784973145 in episode 178
Report: 
rewardSum:-462.0954995756071
height:-0.34470450402676167
loss:12.690571784973145
qAverage:[-22.700607977278246]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.09999999999999969, the reward is -431.02147179348714 with loss 11.238317831419408 in episode 179
Report: 
rewardSum:-431.02147179348714
height:-0.3557657681591743
loss:11.238317831419408
qAverage:[-22.855505150269696]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.049999999999999684, the reward is -470.0083387425053 with loss 11.831947692437097 in episode 180
Report: 
rewardSum:-470.0083387425053
height:-0.3257205754309275
loss:11.831947692437097
qAverage:[-22.258217878963638]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.049999999999999684, the reward is -435.45409380561557 with loss 13.457812432199717 in episode 181
Report: 
rewardSum:-435.45409380561557
height:-0.2326191443924384
loss:13.457812432199717
qAverage:[-24.087613901326076]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.049999999999999684, the reward is -454.78063078482165 with loss 11.573820274323225 in episode 182
Report: 
rewardSum:-454.78063078482165
height:-0.33599684137144514
loss:11.573820274323225
qAverage:[-24.217577204108238]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.049999999999999684, the reward is -451.46586714991986 with loss 11.2590521145612 in episode 183
Report: 
rewardSum:-451.46586714991986
height:-0.3665261635815502
loss:11.2590521145612
qAverage:[-23.25579804151486]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.049999999999999684, the reward is -456.41593146122403 with loss 13.285629874328151 in episode 184
Report: 
rewardSum:-456.41593146122403
height:-0.34930339991271286
loss:13.285629874328151
qAverage:[-22.67212852805552]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.049999999999999684, the reward is -465.28974100276025 with loss 13.103685830719769 in episode 185
Report: 
rewardSum:-465.28974100276025
height:-0.3411655147184365
loss:13.103685830719769
qAverage:[-24.044033824461295]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.049999999999999684, the reward is -457.6306446394046 with loss 12.269133158028126 in episode 186
Report: 
rewardSum:-457.6306446394046
height:-0.3531148936552087
loss:12.269133158028126
qAverage:[-22.849530526393437]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.049999999999999684, the reward is -477.70085563125474 with loss 11.697170758619905 in episode 187
Report: 
rewardSum:-477.70085563125474
height:-0.30856507214166784
loss:11.697170758619905
qAverage:[-21.272010732951916]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.049999999999999684, the reward is -424.62569797132824 with loss 12.192005902063102 in episode 188
Report: 
rewardSum:-424.62569797132824
height:-0.32318957253999603
loss:12.192005902063102
qAverage:[-23.72040846187216]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.049999999999999684, the reward is -460.5427851823085 with loss 12.896331909112632 in episode 189
Report: 
rewardSum:-460.5427851823085
height:-0.21597092181860686
loss:12.896331909112632
qAverage:[-23.593613087503535]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -439.71268151946316 with loss 10.76428273320198 in episode 190
Report: 
rewardSum:-439.71268151946316
height:-0.30266750744816456
loss:10.76428273320198
qAverage:[-22.610342410222398]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -462.95334922301066 with loss 15.154741244390607 in episode 191
Report: 
rewardSum:-462.95334922301066
height:-0.22614346401131713
loss:15.154741244390607
qAverage:[-25.555318077011865]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -436.7416779242128 with loss 11.048569944687188 in episode 192
Report: 
rewardSum:-436.7416779242128
height:-0.3568783638287291
loss:11.048569944687188
qAverage:[-24.203700075008598]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -456.1770958548194 with loss 12.040742981247604 in episode 193
Report: 
rewardSum:-456.1770958548194
height:-0.2810982093787605
loss:12.040742981247604
qAverage:[-25.088790431022645]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -439.9179994576524 with loss 12.679413624340668 in episode 194
Report: 
rewardSum:-439.9179994576524
height:-0.34644703307771446
loss:12.679413624340668
qAverage:[-24.38947898864746]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -439.70823074884464 with loss 12.237437684088945 in episode 195
Report: 
rewardSum:-439.70823074884464
height:-0.358341662516889
loss:12.237437684088945
qAverage:[-23.895975906069918]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -461.3543033448258 with loss 11.263814015779644 in episode 196
Report: 
rewardSum:-461.3543033448258
height:-0.28980443524842275
loss:11.263814015779644
qAverage:[-22.64805552482605]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -458.1489523815813 with loss 10.21573962783441 in episode 197
Report: 
rewardSum:-458.1489523815813
height:-0.2501691181791391
loss:10.21573962783441
qAverage:[-25.157668459415437]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -455.26051848822607 with loss 9.800503747770563 in episode 198
Report: 
rewardSum:-455.26051848822607
height:-0.025595506702403313
loss:9.800503747770563
qAverage:[-24.034695007094186]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -457.63203053696554 with loss 12.174203631468117 in episode 199
Report: 
rewardSum:-457.63203053696554
height:-0.34543621690493964
loss:12.174203631468117
qAverage:[-23.90947497717225]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -458.44147433654786 with loss 9.73013612581417 in episode 200
Report: 
rewardSum:-458.44147433654786
height:-0.30730808009772
loss:9.73013612581417
qAverage:[-24.611797633429465]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -446.71156172724534 with loss 14.779320110566914 in episode 201
Report: 
rewardSum:-446.71156172724534
height:-0.14423026626073657
loss:14.779320110566914
qAverage:[-26.77310364281953]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -412.36455165488286 with loss 11.835419204086065 in episode 202
Report: 
rewardSum:-412.36455165488286
height:-0.3765722735408932
loss:11.835419204086065
qAverage:[-26.139815587715564]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -434.4401764123876 with loss 12.908849074039608 in episode 203
Report: 
rewardSum:-434.4401764123876
height:-0.35102801616680585
loss:12.908849074039608
qAverage:[-26.015738147320132]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -419.58797358605807 with loss 11.654377602040768 in episode 204
Report: 
rewardSum:-419.58797358605807
height:-0.3763362971818396
loss:11.654377602040768
qAverage:[-25.68404472418848]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -450.3826247270539 with loss 10.694951766636223 in episode 205
Report: 
rewardSum:-450.3826247270539
height:-0.18206932415394456
loss:10.694951766636223
qAverage:[-27.035223267295144]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -449.975881736578 with loss 10.986331969499588 in episode 206
Report: 
rewardSum:-449.975881736578
height:-0.0860771207250319
loss:10.986331969499588
qAverage:[-26.029043126814436]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -449.1037129370261 with loss 12.464909981936216 in episode 207
Report: 
rewardSum:-449.1037129370261
height:-0.20143471511367766
loss:12.464909981936216
qAverage:[-24.910359041014715]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -424.32268182621095 with loss 11.553389669861645 in episode 208
Report: 
rewardSum:-424.32268182621095
height:-0.3202812411051328
loss:11.553389669861645
qAverage:[-26.04910956697511]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -473.08569700385164 with loss 10.706498363986611 in episode 209
Report: 
rewardSum:-473.08569700385164
height:-0.3507354684662056
loss:10.706498363986611
qAverage:[-25.205186488023443]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -430.96301772696194 with loss 11.401298020966351 in episode 210
Report: 
rewardSum:-430.96301772696194
height:-0.39757342601204143
loss:11.401298020966351
qAverage:[-25.634238024849203]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -448.27503466299726 with loss 15.475195383653045 in episode 211
Report: 
rewardSum:-448.27503466299726
height:-0.20071180201668692
loss:15.475195383653045
qAverage:[-26.80229859281643]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -434.75131801696585 with loss 13.486672797240317 in episode 212
Report: 
rewardSum:-434.75131801696585
height:-0.4048649359887237
loss:13.486672797240317
qAverage:[-26.496336990564913]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -422.57474966867215 with loss 11.558426353614777 in episode 213
Report: 
rewardSum:-422.57474966867215
height:-0.10384989776339969
loss:11.558426353614777
qAverage:[-27.209726164493656]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -418.52277038498784 with loss 12.594139216467738 in episode 214
Report: 
rewardSum:-418.52277038498784
height:-0.337004008921083
loss:12.594139216467738
qAverage:[-27.310967247084815]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -432.9532462737262 with loss 11.311745455488563 in episode 215
Report: 
rewardSum:-432.9532462737262
height:-0.32790757619976824
loss:11.311745455488563
qAverage:[-27.007673775971824]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -443.29636741653815 with loss 12.48060983326286 in episode 216
Report: 
rewardSum:-443.29636741653815
height:-0.30622892799805546
loss:12.48060983326286
qAverage:[-26.299445171167356]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -438.1655321995301 with loss 11.811769158113748 in episode 217
Report: 
rewardSum:-438.1655321995301
height:-0.317365795139473
loss:11.811769158113748
qAverage:[-27.05312843322754]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -449.4165146277647 with loss 12.04565291898325 in episode 218
Report: 
rewardSum:-449.4165146277647
height:-0.3411307913791261
loss:12.04565291898325
qAverage:[-25.81561833088941]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -440.89457863098977 with loss 11.391381166409701 in episode 219
Report: 
rewardSum:-440.89457863098977
height:-0.34935140870433745
loss:11.391381166409701
qAverage:[-26.81140145216838]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -469.0421496055936 with loss 10.494058272335678 in episode 220
Report: 
rewardSum:-469.0421496055936
height:-0.3580579375583377
loss:10.494058272335678
qAverage:[-26.36332354215112]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -457.64875414469867 with loss 14.182609476149082 in episode 221
Report: 
rewardSum:-457.64875414469867
height:-0.3482364844078521
loss:14.182609476149082
qAverage:[-27.23722048561172]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -454.65459782848285 with loss 11.156722475774586 in episode 222
Report: 
rewardSum:-454.65459782848285
height:-0.33764604833725453
loss:11.156722475774586
qAverage:[-27.45686582743828]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -447.5445805478451 with loss 10.484109541401267 in episode 223
Report: 
rewardSum:-447.5445805478451
height:-0.31059927563377976
loss:10.484109541401267
qAverage:[-27.991414843507073]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -469.12237492891984 with loss 10.296801901422441 in episode 224
Report: 
rewardSum:-469.12237492891984
height:-0.31082445489453914
loss:10.296801901422441
qAverage:[-26.58073980331421]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -456.61064571002925 with loss 10.570074875839055 in episode 225
Report: 
rewardSum:-456.61064571002925
height:-0.29336971855321703
loss:10.570074875839055
qAverage:[-28.04516335289077]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -465.73431390475605 with loss 9.578732228372246 in episode 226
Report: 
rewardSum:-465.73431390475605
height:-0.10252058808478154
loss:9.578732228372246
qAverage:[-27.34401681206443]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -455.9042856488915 with loss 9.715818383265287 in episode 227
Report: 
rewardSum:-455.9042856488915
height:-0.3653544293618558
loss:9.715818383265287
qAverage:[-27.5563298274975]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -436.9352284657604 with loss 10.134218621067703 in episode 228
Report: 
rewardSum:-436.9352284657604
height:-0.23852895066118934
loss:10.134218621067703
qAverage:[-27.495161716938018]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -436.6299978966903 with loss 9.216764559503645 in episode 229
Report: 
rewardSum:-436.6299978966903
height:-0.09074907875244619
loss:9.216764559503645
qAverage:[-27.93434540274108]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -446.586413334799 with loss 8.390752226114273 in episode 230
Report: 
rewardSum:-446.586413334799
height:-0.30448949860658775
loss:8.390752226114273
qAverage:[-27.604490432408777]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -463.10142490586185 with loss 14.264689321629703 in episode 231
Report: 
rewardSum:-463.10142490586185
height:-0.2083597221919163
loss:14.264689321629703
qAverage:[-26.597318786382676]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -450.8265154076933 with loss 11.469944026786834 in episode 232
Report: 
rewardSum:-450.8265154076933
height:-0.3352918690303135
loss:11.469944026786834
qAverage:[-28.908615193367005]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -441.92413192645955 with loss 10.362936571706086 in episode 233
Report: 
rewardSum:-441.92413192645955
height:-0.08655842032422377
loss:10.362936571706086
qAverage:[-28.82988954539323]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -450.32107381781674 with loss 12.511700835078955 in episode 234
Report: 
rewardSum:-450.32107381781674
height:-0.2560018545898639
loss:12.511700835078955
qAverage:[-27.546382873365193]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -454.4453140535562 with loss 11.708056330680847 in episode 235
Report: 
rewardSum:-454.4453140535562
height:-0.3630568438918
loss:11.708056330680847
qAverage:[-28.77185035233546]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -447.3225175553114 with loss 9.906421824358404 in episode 236
Report: 
rewardSum:-447.3225175553114
height:-0.3244463152079377
loss:9.906421824358404
qAverage:[-28.666651248042264]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -455.13677521831653 with loss 10.489587612450123 in episode 237
Report: 
rewardSum:-455.13677521831653
height:-0.36268186876365827
loss:10.489587612450123
qAverage:[-28.76837140026659]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -439.3067908445031 with loss 9.345014021731913 in episode 238
Report: 
rewardSum:-439.3067908445031
height:-0.3163779626858951
loss:9.345014021731913
qAverage:[-28.929187254690046]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -441.2368416933782 with loss 11.233470828272402 in episode 239
Report: 
rewardSum:-441.2368416933782
height:-0.2120188129640921
loss:11.233470828272402
qAverage:[-28.47143838193157]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -445.1807678444757 with loss 8.580302060581744 in episode 240
Report: 
rewardSum:-445.1807678444757
height:-0.13090702643969718
loss:8.580302060581744
qAverage:[-28.45709713732842]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -459.77238650720255 with loss 15.205684018321335 in episode 241
Report: 
rewardSum:-459.77238650720255
height:-0.2496419741057131
loss:15.205684018321335
qAverage:[-30.011985332658977]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -468.6753853597154 with loss 11.91194512322545 in episode 242
Report: 
rewardSum:-468.6753853597154
height:-0.321171299336778
loss:11.91194512322545
qAverage:[-29.210096564918462]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -473.3321425634778 with loss 11.307128421030939 in episode 243
Report: 
rewardSum:-473.3321425634778
height:-0.2756994911773517
loss:11.307128421030939
qAverage:[-27.99664496693445]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -456.0178928251543 with loss 11.55719917314127 in episode 244
Report: 
rewardSum:-456.0178928251543
height:-0.29974573341275423
loss:11.55719917314127
qAverage:[-30.09215007039714]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -434.7506977276867 with loss 13.997369995340705 in episode 245
Report: 
rewardSum:-434.7506977276867
height:-0.3040872033953207
loss:13.997369995340705
qAverage:[-29.45333771097778]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -465.4112101766738 with loss 22.665755094960332 in episode 246
Report: 
rewardSum:-465.4112101766738
height:-0.29010278440245996
loss:22.665755094960332
qAverage:[-29.371616109838627]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -450.33347712980515 with loss 13.102375735528767 in episode 247
Report: 
rewardSum:-450.33347712980515
height:-0.3130517277862419
loss:13.102375735528767
qAverage:[-29.3445520065356]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -478.97312003004794 with loss 13.391908151097596 in episode 248
Report: 
rewardSum:-478.97312003004794
height:-0.31389188054593997
loss:13.391908151097596
qAverage:[-27.695606056392798]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -452.7850793688645 with loss 12.50830509327352 in episode 249
Report: 
rewardSum:-452.7850793688645
height:-0.2314207429088097
loss:12.50830509327352
qAverage:[-29.679100668845486]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -460.273177903286 with loss 11.619151752442122 in episode 250
Report: 
rewardSum:-460.273177903286
height:-0.2665962621478526
loss:11.619151752442122
qAverage:[-29.198046073688204]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -461.7224554165984 with loss 15.052054151892662 in episode 251
Report: 
rewardSum:-461.7224554165984
height:-0.3733042900452687
loss:15.052054151892662
qAverage:[-31.083659771069957]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -443.6454992484851 with loss 12.910045770928264 in episode 252
Report: 
rewardSum:-443.6454992484851
height:-0.08241849934586476
loss:12.910045770928264
qAverage:[-30.303732917010784]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -464.66238463644265 with loss 13.972008985467255 in episode 253
Report: 
rewardSum:-464.66238463644265
height:-0.31390910146854273
loss:13.972008985467255
qAverage:[-31.173936897870338]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -462.2369093976348 with loss 11.53601533640176 in episode 254
Report: 
rewardSum:-462.2369093976348
height:-0.2993712514483928
loss:11.53601533640176
qAverage:[-30.381310262154823]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -443.1458349207061 with loss 11.103969025425613 in episode 255
Report: 
rewardSum:-443.1458349207061
height:-0.1463232889810794
loss:11.103969025425613
qAverage:[-30.557401344327644]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -467.4572097938311 with loss 11.187900374643505 in episode 256
Report: 
rewardSum:-467.4572097938311
height:-0.3388089268129883
loss:11.187900374643505
qAverage:[-30.695294552478032]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -452.4699511618063 with loss 12.281174972653389 in episode 257
Report: 
rewardSum:-452.4699511618063
height:-0.2652410569352916
loss:12.281174972653389
qAverage:[-30.5457192165683]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -455.59513890552506 with loss 10.154517129994929 in episode 258
Report: 
rewardSum:-455.59513890552506
height:-0.2824707119001485
loss:10.154517129994929
qAverage:[-30.936757352092478]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -468.5255417159573 with loss 11.057173969689757 in episode 259
Report: 
rewardSum:-468.5255417159573
height:-0.2689495900226076
loss:11.057173969689757
qAverage:[-30.417341637139273]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -448.95071362509304 with loss 11.824286698363721 in episode 260
Report: 
rewardSum:-448.95071362509304
height:-0.21109040711857566
loss:11.824286698363721
qAverage:[-30.505441295270302]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -446.89499538899406 with loss 15.821874618530273 in episode 261
Report: 
rewardSum:-446.89499538899406
height:-0.2930894493793127
loss:15.821874618530273
qAverage:[-32.17590813850289]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -455.1603550166529 with loss 14.859083789400756 in episode 262
Report: 
rewardSum:-455.1603550166529
height:-0.21809434370171382
loss:14.859083789400756
qAverage:[-32.59024978637695]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -433.34322860197136 with loss 16.197152456268668 in episode 263
Report: 
rewardSum:-433.34322860197136
height:-0.29512079923856954
loss:16.197152456268668
qAverage:[-31.83346911072731]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -462.55770871531246 with loss 12.266966765746474 in episode 264
Report: 
rewardSum:-462.55770871531246
height:-0.3001145474027519
loss:12.266966765746474
qAverage:[-32.943876238152534]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -476.9298415187933 with loss 12.293391949962825 in episode 265
Report: 
rewardSum:-476.9298415187933
height:-0.3171663100954997
loss:12.293391949962825
qAverage:[-31.779695304472057]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -468.14395506137305 with loss 12.832576910965145 in episode 266
Report: 
rewardSum:-468.14395506137305
height:-0.2467778743135886
loss:12.832576910965145
qAverage:[-30.256054030850603]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -435.1144617202389 with loss 14.027263993397355 in episode 267
Report: 
rewardSum:-435.1144617202389
height:-0.15732850375857954
loss:14.027263993397355
qAverage:[-32.45654111539964]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -442.5955009033653 with loss 13.68863117787987 in episode 268
Report: 
rewardSum:-442.5955009033653
height:-0.24857093968421903
loss:13.68863117787987
qAverage:[-31.37391031084962]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -467.48277908780784 with loss 13.510565247386694 in episode 269
Report: 
rewardSum:-467.48277908780784
height:-0.2662542551670542
loss:13.510565247386694
qAverage:[-32.70025119214955]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -466.2905903439453 with loss 14.647564782295376 in episode 270
Report: 
rewardSum:-466.2905903439453
height:-0.28882262508717443
loss:14.647564782295376
qAverage:[-31.67794366143829]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -443.14942265279115 with loss 19.36472586169839 in episode 271
Report: 
rewardSum:-443.14942265279115
height:0.06895885187293677
loss:19.36472586169839
qAverage:[-31.671914470316185]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -452.7396696962739 with loss 16.30912233144045 in episode 272
Report: 
rewardSum:-452.7396696962739
height:-0.26505028281038867
loss:16.30912233144045
qAverage:[-31.97247391408033]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -460.5532793168489 with loss 16.353137355297804 in episode 273
Report: 
rewardSum:-460.5532793168489
height:-0.36563767230193245
loss:16.353137355297804
qAverage:[-32.29035757482052]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -473.3633473452503 with loss 13.38608465064317 in episode 274
Report: 
rewardSum:-473.3633473452503
height:-0.23558005007652827
loss:13.38608465064317
qAverage:[-33.33712896970239]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -472.29480382270793 with loss 13.982200462371111 in episode 275
Report: 
rewardSum:-472.29480382270793
height:-0.27087697040077874
loss:13.982200462371111
qAverage:[-32.13178315565954]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -478.90915769592726 with loss 16.360427957959473 in episode 276
Report: 
rewardSum:-478.90915769592726
height:-0.2665962621478526
loss:16.360427957959473
qAverage:[-33.18983143773572]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.2131634595606 with loss 14.69766256492585 in episode 277
Report: 
rewardSum:-477.2131634595606
height:-0.2665962621478526
loss:14.69766256492585
qAverage:[-33.38134557038105]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -472.17779619635144 with loss 15.285877846181393 in episode 278
Report: 
rewardSum:-472.17779619635144
height:-0.28952055671650334
loss:15.285877846181393
qAverage:[-32.17924422910898]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -457.4011430873413 with loss 14.684690842404962 in episode 279
Report: 
rewardSum:-457.4011430873413
height:-0.2747362802557133
loss:14.684690842404962
qAverage:[-32.57927098677526]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -467.56445628906107 with loss 14.332686715759337 in episode 280
Report: 
rewardSum:-467.56445628906107
height:-0.2665962621478526
loss:14.332686715759337
qAverage:[-32.38626355648041]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -478.39562661980193 with loss 19.384680546820164 in episode 281
Report: 
rewardSum:-478.39562661980193
height:-0.2665962621478526
loss:19.384680546820164
qAverage:[-32.678327913331515]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -468.11062675881954 with loss 15.815789218991995 in episode 282
Report: 
rewardSum:-468.11062675881954
height:-0.28965438789731074
loss:15.815789218991995
qAverage:[-33.26394274460142]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -473.93140679926466 with loss 15.420479807071388 in episode 283
Report: 
rewardSum:-473.93140679926466
height:-0.27742241758286845
loss:15.420479807071388
qAverage:[-34.30681884348096]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -476.3155700447539 with loss 15.869159817695618 in episode 284
Report: 
rewardSum:-476.3155700447539
height:-0.2665962621478526
loss:15.869159817695618
qAverage:[-34.233423431320944]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -466.27858061921 with loss 15.455919548869133 in episode 285
Report: 
rewardSum:-466.27858061921
height:-0.3226719104276514
loss:15.455919548869133
qAverage:[-33.41999638258521]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -467.67355670696486 with loss 15.92000978346914 in episode 286
Report: 
rewardSum:-467.67355670696486
height:-0.17709645384814632
loss:15.92000978346914
qAverage:[-32.3612607618173]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -466.78946574027793 with loss 13.385686158202589 in episode 287
Report: 
rewardSum:-466.78946574027793
height:-0.23991710020834825
loss:13.385686158202589
qAverage:[-33.09088253615489]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -468.9641908130997 with loss 13.55266239400953 in episode 288
Report: 
rewardSum:-468.9641908130997
height:-0.1927069412334544
loss:13.55266239400953
qAverage:[-34.37183424505857]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -471.5976792279785 with loss 13.7462099744007 in episode 289
Report: 
rewardSum:-471.5976792279785
height:-0.26160153322309604
loss:13.7462099744007
qAverage:[-34.46042096138]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -476.0992554256796 with loss 15.031351024284959 in episode 290
Report: 
rewardSum:-476.0992554256796
height:-0.2665962621478526
loss:15.031351024284959
qAverage:[-34.311112247296236]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -473.4029448802167 with loss 20.07777869515121 in episode 291
Report: 
rewardSum:-473.4029448802167
height:-0.2784039272394171
loss:20.07777869515121
qAverage:[-35.45113756160925]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -470.8036376161415 with loss 16.300832269713283 in episode 292
Report: 
rewardSum:-470.8036376161415
height:-0.18420713607292458
loss:16.300832269713283
qAverage:[-33.3583815387902]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -478.1120334959488 with loss 16.18574563320726 in episode 293
Report: 
rewardSum:-478.1120334959488
height:-0.27647149707548896
loss:16.18574563320726
qAverage:[-33.79796449936445]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.3299480762916 with loss 16.38878034800291 in episode 294
Report: 
rewardSum:-477.3299480762916
height:-0.2634816051763596
loss:16.38878034800291
qAverage:[-33.57703359199293]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -473.37278197843165 with loss 17.408587688580155 in episode 295
Report: 
rewardSum:-473.37278197843165
height:-0.3652732789883189
loss:17.408587688580155
qAverage:[-34.25896292629808]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 16.626196114346385 in episode 296
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:16.626196114346385
qAverage:[-35.18576559689966]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -467.3407744359138 with loss 14.905220008455217 in episode 297
Report: 
rewardSum:-467.3407744359138
height:-0.29195624384131447
loss:14.905220008455217
qAverage:[-34.06852235555649]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -473.6416158946565 with loss 16.80722356773913 in episode 298
Report: 
rewardSum:-473.6416158946565
height:-0.27949123186497726
loss:16.80722356773913
qAverage:[-33.568256757745694]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -478.0025595372414 with loss 14.310929385013878 in episode 299
Report: 
rewardSum:-478.0025595372414
height:-0.2536579242246789
loss:14.310929385013878
qAverage:[-35.187509356446526]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -474.3691744791823 with loss 15.723228688817471 in episode 300
Report: 
rewardSum:-474.3691744791823
height:-0.2547546215254444
loss:15.723228688817471
qAverage:[-33.66006810672581]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -471.830836626794 with loss 20.188171120360494 in episode 301
Report: 
rewardSum:-471.830836626794
height:-0.2665962621478526
loss:20.188171120360494
qAverage:[-34.99586497783661]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -473.72194867673807 with loss 19.877311689779162 in episode 302
Report: 
rewardSum:-473.72194867673807
height:-0.29147313284011483
loss:19.877311689779162
qAverage:[-36.447527316990744]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -474.5570173910485 with loss 17.167462274432182 in episode 303
Report: 
rewardSum:-474.5570173910485
height:-0.27145145857480335
loss:17.167462274432182
qAverage:[-36.281014862060545]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -478.9692928628314 with loss 18.395451496355236 in episode 304
Report: 
rewardSum:-478.9692928628314
height:-0.26647745558391805
loss:18.395451496355236
qAverage:[-36.13134905371336]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -473.563318222548 with loss 19.456491108983755 in episode 305
Report: 
rewardSum:-473.563318222548
height:-0.2665962621478526
loss:19.456491108983755
qAverage:[-36.35265536378757]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -471.46518073909476 with loss 16.822834321297705 in episode 306
Report: 
rewardSum:-471.46518073909476
height:-0.3599196885813169
loss:16.822834321297705
qAverage:[-34.86138991336917]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 15.271025924012065 in episode 307
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:15.271025924012065
qAverage:[-36.06194622998167]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -461.16485664900836 with loss 17.117571549490094 in episode 308
Report: 
rewardSum:-461.16485664900836
height:-0.29291396553901805
loss:17.117571549490094
qAverage:[-34.96165074586868]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 18.01065926393494 in episode 309
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:18.01065926393494
qAverage:[-36.07863866220607]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.39242287899816 with loss 15.206266823224723 in episode 310
Report: 
rewardSum:-479.39242287899816
height:-0.2665962621478526
loss:15.206266823224723
qAverage:[-34.57610382488118]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -465.9676689331792 with loss 20.400451397523284 in episode 311
Report: 
rewardSum:-465.9676689331792
height:-0.27188334251492857
loss:20.400451397523284
qAverage:[-35.89074188947678]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -463.68936943870665 with loss 16.229315100237727 in episode 312
Report: 
rewardSum:-463.68936943870665
height:-0.25164123644399383
loss:16.229315100237727
qAverage:[-35.722881559133526]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -480.2661239197091 with loss 18.214968977496028 in episode 313
Report: 
rewardSum:-480.2661239197091
height:-0.2580440672988092
loss:18.214968977496028
qAverage:[-35.38528059363365]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.2521432933843 with loss 19.423733153380454 in episode 314
Report: 
rewardSum:-477.2521432933843
height:-0.2665962621478526
loss:19.423733153380454
qAverage:[-37.24298065487701]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -480.0451992796263 with loss 17.022010255604982 in episode 315
Report: 
rewardSum:-480.0451992796263
height:-0.24617057534098832
loss:17.022010255604982
qAverage:[-35.25276401314405]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -470.5116561250196 with loss 15.706291079055518 in episode 316
Report: 
rewardSum:-470.5116561250196
height:-0.1412313525425381
loss:15.706291079055518
qAverage:[-35.34931269420191]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -475.81786510527064 with loss 14.862965693697333 in episode 317
Report: 
rewardSum:-475.81786510527064
height:-0.2665962621478526
loss:14.862965693697333
qAverage:[-35.40393290042877]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -472.68703452639016 with loss 17.502798485569656 in episode 318
Report: 
rewardSum:-472.68703452639016
height:-0.31110962986653895
loss:17.502798485569656
qAverage:[-35.95772831201553]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -470.04541374602786 with loss 16.024829061701894 in episode 319
Report: 
rewardSum:-470.04541374602786
height:-0.3567317101424002
loss:16.024829061701894
qAverage:[-37.80894952271115]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -476.1465993953489 with loss 15.977251171134412 in episode 320
Report: 
rewardSum:-476.1465993953489
height:-0.34080589781922616
loss:15.977251171134412
qAverage:[-37.71810420196835]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -480.2866463756096 with loss 22.274098359048367 in episode 321
Report: 
rewardSum:-480.2866463756096
height:-0.26121683063568785
loss:22.274098359048367
qAverage:[-35.88080311647736]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -469.9011247030665 with loss 17.132032324559987 in episode 322
Report: 
rewardSum:-469.9011247030665
height:-0.2665962621478526
loss:17.132032324559987
qAverage:[-37.87990814892214]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -476.64067808782806 with loss 21.58986342512071 in episode 323
Report: 
rewardSum:-476.64067808782806
height:-0.2665962621478526
loss:21.58986342512071
qAverage:[-37.97685660938225]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.75465608072125 with loss 22.159136562608182 in episode 324
Report: 
rewardSum:-479.75465608072125
height:-0.2567145831291786
loss:22.159136562608182
qAverage:[-35.3878387273103]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -478.4823897240186 with loss 18.40789586212486 in episode 325
Report: 
rewardSum:-478.4823897240186
height:-0.29947568790206697
loss:18.40789586212486
qAverage:[-35.871469091660906]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -471.40006255270595 with loss 16.78702476527542 in episode 326
Report: 
rewardSum:-471.40006255270595
height:-0.35482776438292507
loss:16.78702476527542
qAverage:[-36.51120820923231]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -480.4207404454065 with loss 17.22834700718522 in episode 327
Report: 
rewardSum:-480.4207404454065
height:-0.2665962621478526
loss:17.22834700718522
qAverage:[-36.05997416510511]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -470.64520630602243 with loss 16.171778690069914 in episode 328
Report: 
rewardSum:-470.64520630602243
height:-0.2665962621478526
loss:16.171778690069914
qAverage:[-38.231629324431466]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -476.9039471668127 with loss 16.048414853401482 in episode 329
Report: 
rewardSum:-476.9039471668127
height:-0.2665962621478526
loss:16.048414853401482
qAverage:[-38.029509931507675]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -468.23897255763825 with loss 15.170814673416317 in episode 330
Report: 
rewardSum:-468.23897255763825
height:-0.27550298663819567
loss:15.170814673416317
qAverage:[-38.26247101963156]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.6842105209781 with loss 21.825783512555063 in episode 331
Report: 
rewardSum:-479.6842105209781
height:-0.24051240602975066
loss:21.825783512555063
qAverage:[-36.64546870654172]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -476.48471188405443 with loss 19.00310256704688 in episode 332
Report: 
rewardSum:-476.48471188405443
height:-0.3170460932166482
loss:19.00310256704688
qAverage:[-36.99627959069295]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.38108290135114 with loss 18.31438382063061 in episode 333
Report: 
rewardSum:-477.38108290135114
height:-0.2554194503884401
loss:18.31438382063061
qAverage:[-38.83173189068785]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -480.4337913251363 with loss 19.527507570572197 in episode 334
Report: 
rewardSum:-480.4337913251363
height:-0.2665962621478526
loss:19.527507570572197
qAverage:[-36.7985119167252]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -468.50356241635325 with loss 19.171914646402 in episode 335
Report: 
rewardSum:-468.50356241635325
height:-0.2590802049618546
loss:19.171914646402
qAverage:[-37.232685108089925]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -478.5749524140826 with loss 18.201905116438866 in episode 336
Report: 
rewardSum:-478.5749524140826
height:-0.264981506253529
loss:18.201905116438866
qAverage:[-36.80827890396118]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.6668821409556 with loss 19.595326530747116 in episode 337
Report: 
rewardSum:-477.6668821409556
height:-0.2822797834044533
loss:19.595326530747116
qAverage:[-38.98703080356711]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.9145880166685 with loss 20.126454347744584 in episode 338
Report: 
rewardSum:-477.9145880166685
height:-0.2665962621478526
loss:20.126454347744584
qAverage:[-36.641139637231824]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.2650006415508 with loss 15.842366728931665 in episode 339
Report: 
rewardSum:-479.2650006415508
height:-0.2784233043373287
loss:15.842366728931665
qAverage:[-38.855968720842114]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -468.9727708563703 with loss 15.345166375860572 in episode 340
Report: 
rewardSum:-468.9727708563703
height:-0.2665962621478526
loss:15.345166375860572
qAverage:[-39.14286155131326]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -467.7955396284678 with loss 20.60905978269875 in episode 341
Report: 
rewardSum:-467.7955396284678
height:-0.23849253499219422
loss:20.60905978269875
qAverage:[-39.76749104792529]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.17325678465414 with loss 20.200504848733544 in episode 342
Report: 
rewardSum:-479.17325678465414
height:-0.27529871182951393
loss:20.200504848733544
qAverage:[-39.721422812238856]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.84211943926454 with loss 19.361223853193223 in episode 343
Report: 
rewardSum:-477.84211943926454
height:-0.2665962621478526
loss:19.361223853193223
qAverage:[-39.700977532147185]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -468.8555173337064 with loss 21.106246208772063 in episode 344
Report: 
rewardSum:-468.8555173337064
height:-0.2992805012700466
loss:21.106246208772063
qAverage:[-38.09221934567531]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 17.72854194138199 in episode 345
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:17.72854194138199
qAverage:[-39.646465000847876]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.34487894927275 with loss 19.43900483287871 in episode 346
Report: 
rewardSum:-479.34487894927275
height:-0.2665962621478526
loss:19.43900483287871
qAverage:[-37.42115875500352]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -469.23628616133243 with loss 19.02916363440454 in episode 347
Report: 
rewardSum:-469.23628616133243
height:-0.2819386221790638
loss:19.02916363440454
qAverage:[-37.695886841928115]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 15.445140783675015 in episode 348
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:15.445140783675015
qAverage:[-39.66914694649832]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -478.38693327845175 with loss 19.23012274969369 in episode 349
Report: 
rewardSum:-478.38693327845175
height:-0.2665962621478526
loss:19.23012274969369
qAverage:[-39.633123331402075]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -473.0984938560881 with loss 18.001474741846323 in episode 350
Report: 
rewardSum:-473.0984938560881
height:-0.2792663937111483
loss:18.001474741846323
qAverage:[-40.06080405077144]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -468.7704798495853 with loss 22.519294656813145 in episode 351
Report: 
rewardSum:-468.7704798495853
height:-0.26288298345746963
loss:22.519294656813145
qAverage:[-40.94387199138773]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 17.330330839380622 in episode 352
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:17.330330839380622
qAverage:[-40.84362253649481]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -469.3456882751954 with loss 23.091958543285728 in episode 353
Report: 
rewardSum:-469.3456882751954
height:-0.32306725745590115
loss:23.091958543285728
qAverage:[-38.92141528010368]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.6956882639932 with loss 20.357925073243678 in episode 354
Report: 
rewardSum:-479.6956882639932
height:-0.2665962621478526
loss:20.357925073243678
qAverage:[-38.41750103798672]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 18.372638426721096 in episode 355
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:18.372638426721096
qAverage:[-40.810202721321936]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -474.02292429761167 with loss 19.543637690134346 in episode 356
Report: 
rewardSum:-474.02292429761167
height:-0.35624522305029616
loss:19.543637690134346
qAverage:[-39.2128269013448]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 21.56153896264732 in episode 357
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:21.56153896264732
qAverage:[-40.81341603471728]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -476.4546750197821 with loss 17.34576760046184 in episode 358
Report: 
rewardSum:-476.4546750197821
height:-0.32062435777199627
loss:17.34576760046184
qAverage:[-38.37755968594792]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -473.2625898905202 with loss 18.635121885687113 in episode 359
Report: 
rewardSum:-473.2625898905202
height:-0.26512108850227123
loss:18.635121885687113
qAverage:[-38.74985148310661]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -481.7358305059055 with loss 21.08522567152977 in episode 360
Report: 
rewardSum:-481.7358305059055
height:-0.2665962621478526
loss:21.08522567152977
qAverage:[-38.65891838552964]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 23.825016614049673 in episode 361
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:23.825016614049673
qAverage:[-41.30859095004979]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -474.2705034926433 with loss 20.988999681547284 in episode 362
Report: 
rewardSum:-474.2705034926433
height:-0.1948520484848772
loss:20.988999681547284
qAverage:[-38.701075290441516]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.14018107284505 with loss 20.023990377783775 in episode 363
Report: 
rewardSum:-479.14018107284505
height:-0.2665962621478526
loss:20.023990377783775
qAverage:[-41.42353619627691]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 23.688219389878213 in episode 364
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:23.688219389878213
qAverage:[-41.336969442035425]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.2062412638144 with loss 22.474884930066764 in episode 365
Report: 
rewardSum:-479.2062412638144
height:-0.2665962621478526
loss:22.474884930066764
qAverage:[-41.29809770489683]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -476.8765174896513 with loss 18.44156879093498 in episode 366
Report: 
rewardSum:-476.8765174896513
height:-0.2699023746907862
loss:18.44156879093498
qAverage:[-41.435490660406465]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -475.53961526913275 with loss 25.33939401153475 in episode 367
Report: 
rewardSum:-475.53961526913275
height:-0.27674813137852755
loss:25.33939401153475
qAverage:[-38.912015500694814]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -474.12076737196975 with loss 20.454119231551886 in episode 368
Report: 
rewardSum:-474.12076737196975
height:-0.2093686979957644
loss:20.454119231551886
qAverage:[-38.55939069537833]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.84123556295725 with loss 18.165557214990258 in episode 369
Report: 
rewardSum:-477.84123556295725
height:-0.2665962621478526
loss:18.165557214990258
qAverage:[-41.41403457566435]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 22.344643889926374 in episode 370
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:22.344643889926374
qAverage:[-41.35859009771064]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -475.01889511601024 with loss 27.219551814720035 in episode 371
Report: 
rewardSum:-475.01889511601024
height:-0.2665962621478526
loss:27.219551814720035
qAverage:[-42.3583870878314]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.7632070043349 with loss 27.45248427335173 in episode 372
Report: 
rewardSum:-477.7632070043349
height:-0.3131208219048617
loss:27.45248427335173
qAverage:[-39.68151987317818]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -470.8574499909905 with loss 23.48108663316816 in episode 373
Report: 
rewardSum:-470.8574499909905
height:-0.1729025411344444
loss:23.48108663316816
qAverage:[-41.67851568920778]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -474.5665770591561 with loss 25.497986384667456 in episode 374
Report: 
rewardSum:-474.5665770591561
height:-0.2665962621478526
loss:25.497986384667456
qAverage:[-42.409686685195695]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -478.7391548213782 with loss 22.578888434916735 in episode 375
Report: 
rewardSum:-478.7391548213782
height:-0.24155125877131492
loss:22.578888434916735
qAverage:[-39.179363536834714]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 25.753026428632438 in episode 376
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:25.753026428632438
qAverage:[-42.12658713956185]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.24596873631515 with loss 21.85353526379913 in episode 377
Report: 
rewardSum:-479.24596873631515
height:-0.25169538755429816
loss:21.85353526379913
qAverage:[-39.66659161686897]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -480.7811259417337 with loss 20.969937283545732 in episode 378
Report: 
rewardSum:-480.7811259417337
height:-0.2665962621478526
loss:20.969937283545732
qAverage:[-39.24187601208687]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 20.8927398705855 in episode 379
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:20.8927398705855
qAverage:[-42.13264112189265]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -473.6919653702224 with loss 22.822288438677788 in episode 380
Report: 
rewardSum:-473.6919653702224
height:-0.20619037359141765
loss:22.822288438677788
qAverage:[-39.21400710299611]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -474.6373153626751 with loss 29.990128005854785 in episode 381
Report: 
rewardSum:-474.6373153626751
height:-0.2667520685087463
loss:29.990128005854785
qAverage:[-40.87673145977419]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 28.313290551304817 in episode 382
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:28.313290551304817
qAverage:[-43.280302531683624]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -461.7657821026893 with loss 26.186855564825237 in episode 383
Report: 
rewardSum:-461.7657821026893
height:-0.17821402963295305
loss:26.186855564825237
qAverage:[-43.08486665998186]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 22.395290594547987 in episode 384
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:22.395290594547987
qAverage:[-43.29088388575185]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.9053599428006 with loss 26.28172762133181 in episode 385
Report: 
rewardSum:-477.9053599428006
height:-0.27605099059374055
loss:26.28172762133181
qAverage:[-41.625452789393336]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -480.57098846390284 with loss 27.057005953975022 in episode 386
Report: 
rewardSum:-480.57098846390284
height:-0.2665962621478526
loss:27.057005953975022
qAverage:[-40.59056429862976]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.3316582034689 with loss 29.92095584049821 in episode 387
Report: 
rewardSum:-479.3316582034689
height:-0.27926346000729485
loss:29.92095584049821
qAverage:[-43.34659270112738]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -472.33551467136436 with loss 27.78482087701559 in episode 388
Report: 
rewardSum:-472.33551467136436
height:-0.20950882533468143
loss:27.78482087701559
qAverage:[-42.88449572107475]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.8279473707722 with loss 24.175393285229802 in episode 389
Report: 
rewardSum:-479.8279473707722
height:-0.27271158406410967
loss:24.175393285229802
qAverage:[-40.48892759084701]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -475.1994218708427 with loss 21.255476320162416 in episode 390
Report: 
rewardSum:-475.1994218708427
height:-0.252567281631881
loss:21.255476320162416
qAverage:[-43.25076427459717]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.09519444342436 with loss 24.158183973282576 in episode 391
Report: 
rewardSum:-479.09519444342436
height:-0.257395133594186
loss:24.158183973282576
qAverage:[-41.55358281301622]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.8135356518861 with loss 23.365837481804192 in episode 392
Report: 
rewardSum:-479.8135356518861
height:-0.24050895372083886
loss:23.365837481804192
qAverage:[-41.09536054952821]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -478.7698292791715 with loss 24.298464351333678 in episode 393
Report: 
rewardSum:-478.7698292791715
height:-0.2665962621478526
loss:24.298464351333678
qAverage:[-41.421940797626384]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9577651310791 with loss 26.45654060691595 in episode 394
Report: 
rewardSum:-479.9577651310791
height:-0.2665962621478526
loss:26.45654060691595
qAverage:[-43.80595426512237]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -471.6898898950276 with loss 28.194473074749112 in episode 395
Report: 
rewardSum:-471.6898898950276
height:-0.2755746713246352
loss:28.194473074749112
qAverage:[-42.054985204533715]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.0133801252701 with loss 21.154666637070477 in episode 396
Report: 
rewardSum:-479.0133801252701
height:-0.2581454447817715
loss:21.154666637070477
qAverage:[-44.43065544128418]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -478.74702751731166 with loss 24.783427485264838 in episode 397
Report: 
rewardSum:-478.74702751731166
height:-0.29074521161878064
loss:24.783427485264838
qAverage:[-44.62704900485366]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -476.22966104463984 with loss 26.383705323562026 in episode 398
Report: 
rewardSum:-476.22966104463984
height:-0.3343611681153583
loss:26.383705323562026
qAverage:[-41.51073105931282]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -478.9996582423139 with loss 28.22928696591407 in episode 399
Report: 
rewardSum:-478.9996582423139
height:-0.23324923716314144
loss:28.22928696591407
qAverage:[-40.9835656529274]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.7846272608974 with loss 26.349372333846986 in episode 400
Report: 
rewardSum:-477.7846272608974
height:-0.27449669247730135
loss:26.349372333846986
qAverage:[-44.597500891234745]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 27.23115193657577 in episode 401
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:27.23115193657577
qAverage:[-45.62534116990496]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -463.5638531304937 with loss 28.170823107473552 in episode 402
Report: 
rewardSum:-463.5638531304937
height:-0.2407086122542385
loss:28.170823107473552
qAverage:[-45.67933955050931]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.000928893959 with loss 24.144236190244555 in episode 403
Report: 
rewardSum:-479.000928893959
height:-0.2665962621478526
loss:24.144236190244555
qAverage:[-42.28558454321856]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 26.640165478922427 in episode 404
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:26.640165478922427
qAverage:[-45.62586006164551]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -478.8853189448617 with loss 32.66511931736022 in episode 405
Report: 
rewardSum:-478.8853189448617
height:-0.2537114904339002
loss:32.66511931736022
qAverage:[-45.58958025261907]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -478.0654278128398 with loss 24.173916057683527 in episode 406
Report: 
rewardSum:-478.0654278128398
height:-0.2930714052970307
loss:24.173916057683527
qAverage:[-45.7602735840448]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.1672247120766 with loss 29.725659610703588 in episode 407
Report: 
rewardSum:-479.1672247120766
height:-0.2531252872282575
loss:29.725659610703588
qAverage:[-42.506759678963384]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -472.2614201530277 with loss 27.748316705226898 in episode 408
Report: 
rewardSum:-472.2614201530277
height:-0.2792663937111483
loss:27.748316705226898
qAverage:[-42.56122161446149]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.41397312267424 with loss 25.099575439468026 in episode 409
Report: 
rewardSum:-477.41397312267424
height:-0.2665962621478526
loss:25.099575439468026
qAverage:[-42.248617731746116]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.4787528716742 with loss 25.317100360058248 in episode 410
Report: 
rewardSum:-479.4787528716742
height:-0.2851910909665062
loss:25.317100360058248
qAverage:[-42.974839956916156]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.1022709985081 with loss 30.093470321968198 in episode 411
Report: 
rewardSum:-479.1022709985081
height:-0.2639804508690956
loss:30.093470321968198
qAverage:[-46.68417595630854]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -476.82451390439195 with loss 29.01201308518648 in episode 412
Report: 
rewardSum:-476.82451390439195
height:-0.27087697040077874
loss:29.01201308518648
qAverage:[-46.66012391133524]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 30.793621090240777 in episode 413
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:30.793621090240777
qAverage:[-46.674280965269496]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.40141779409004 with loss 31.537758314050734 in episode 414
Report: 
rewardSum:-479.40141779409004
height:-0.2669182464760195
loss:31.537758314050734
qAverage:[-43.75041649699211]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.05773323270625 with loss 29.629578918218613 in episode 415
Report: 
rewardSum:-477.05773323270625
height:-0.2665962621478526
loss:29.629578918218613
qAverage:[-46.706490915984354]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.00243380786696 with loss 33.47938213869929 in episode 416
Report: 
rewardSum:-479.00243380786696
height:-0.2501543342698661
loss:33.47938213869929
qAverage:[-43.14217888050942]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -470.8524659165808 with loss 28.427610066719353 in episode 417
Report: 
rewardSum:-470.8524659165808
height:-0.2665962621478526
loss:28.427610066719353
qAverage:[-46.95271819918027]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.3334384609186 with loss 28.672956372611225 in episode 418
Report: 
rewardSum:-479.3334384609186
height:-0.2789921274406046
loss:28.672956372611225
qAverage:[-46.65346889684696]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -478.80690446344295 with loss 28.336771967820823 in episode 419
Report: 
rewardSum:-478.80690446344295
height:-0.25715508041099916
loss:28.336771967820823
qAverage:[-46.656262747131954]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.46701845489747 with loss 25.29307333473116 in episode 420
Report: 
rewardSum:-477.46701845489747
height:-0.3026297106389472
loss:25.29307333473116
qAverage:[-43.66588052868843]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -470.4610320472768 with loss 31.088140268810093 in episode 421
Report: 
rewardSum:-470.4610320472768
height:-0.2658693832653915
loss:31.088140268810093
qAverage:[-47.65344964981079]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 25.95833703223616 in episode 422
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:25.95833703223616
qAverage:[-47.525744414681874]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -475.1884491496393 with loss 31.635232605040073 in episode 423
Report: 
rewardSum:-475.1884491496393
height:-0.2665962621478526
loss:31.635232605040073
qAverage:[-47.74483450527849]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 30.92838755901903 in episode 424
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:30.92838755901903
qAverage:[-47.557584800342525]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -481.0059187761736 with loss 27.880423948168755 in episode 425
Report: 
rewardSum:-481.0059187761736
height:-0.3123965183871239
loss:27.880423948168755
qAverage:[-44.17548217535019]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 27.07509871199727 in episode 426
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:27.07509871199727
qAverage:[-47.573503879490744]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -473.5604414701191 with loss 32.50711013376713 in episode 427
Report: 
rewardSum:-473.5604414701191
height:-0.2665962621478526
loss:32.50711013376713
qAverage:[-47.690036172350055]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.84184294843226 with loss 30.462590417824686 in episode 428
Report: 
rewardSum:-477.84184294843226
height:-0.2536046678965669
loss:30.462590417824686
qAverage:[-47.480188064575195]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -478.2760992401245 with loss 25.987307709641755 in episode 429
Report: 
rewardSum:-478.2760992401245
height:-0.2665962621478526
loss:25.987307709641755
qAverage:[-44.18148401822194]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -471.51384276559264 with loss 27.970638846978545 in episode 430
Report: 
rewardSum:-471.51384276559264
height:-0.2665962621478526
loss:27.970638846978545
qAverage:[-44.12573234975338]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -478.45303918412463 with loss 34.16433184593916 in episode 431
Report: 
rewardSum:-478.45303918412463
height:-0.2665962621478526
loss:34.16433184593916
qAverage:[-44.837007632806674]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -476.2204561193659 with loss 34.99243996385485 in episode 432
Report: 
rewardSum:-476.2204561193659
height:-0.2747385765081613
loss:34.99243996385485
qAverage:[-45.227694227695466]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.8845211505985 with loss 27.620775317773223 in episode 433
Report: 
rewardSum:-479.8845211505985
height:-0.2790123814208796
loss:27.620775317773223
qAverage:[-44.937831936496316]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -475.94138118954305 with loss 27.420403636991978 in episode 434
Report: 
rewardSum:-475.94138118954305
height:-0.27169634346916194
loss:27.420403636991978
qAverage:[-48.50321639710991]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -481.0943543373038 with loss 32.99269520211965 in episode 435
Report: 
rewardSum:-481.0943543373038
height:-0.28471955273879657
loss:32.99269520211965
qAverage:[-45.020086673361746]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -478.93250047734267 with loss 33.22304461617023 in episode 436
Report: 
rewardSum:-478.93250047734267
height:-0.2549450822605535
loss:33.22304461617023
qAverage:[-48.41215629008279]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.25697439323693 with loss 33.02739425748587 in episode 437
Report: 
rewardSum:-479.25697439323693
height:-0.2665962621478526
loss:33.02739425748587
qAverage:[-48.463811378479]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.5074149714313 with loss 25.38563639484346 in episode 438
Report: 
rewardSum:-477.5074149714313
height:-0.25903636356944815
loss:25.38563639484346
qAverage:[-44.705500685389914]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -478.1401093141837 with loss 32.978132992982864 in episode 439
Report: 
rewardSum:-478.1401093141837
height:-0.2637632556755923
loss:32.978132992982864
qAverage:[-48.434361984480674]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 33.641507930122316 in episode 440
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:33.641507930122316
qAverage:[-48.48891199046168]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -474.4738701110863 with loss 35.00757651217282 in episode 441
Report: 
rewardSum:-474.4738701110863
height:-0.32446713706486596
loss:35.00757651217282
qAverage:[-49.44658220291138]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -472.9755779064172 with loss 35.69351743161678 in episode 442
Report: 
rewardSum:-472.9755779064172
height:-0.2665962621478526
loss:35.69351743161678
qAverage:[-45.79487981769576]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 31.938715847209096 in episode 443
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:31.938715847209096
qAverage:[-49.33422532722132]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.1637772463626 with loss 30.19757946114987 in episode 444
Report: 
rewardSum:-477.1637772463626
height:-0.2665962621478526
loss:30.19757946114987
qAverage:[-49.318689957040874]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.77911435679795 with loss 31.03940542973578 in episode 445
Report: 
rewardSum:-479.77911435679795
height:-0.2709020538166837
loss:31.03940542973578
qAverage:[-45.58236549624163]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -473.3136909749826 with loss 33.159902231767774 in episode 446
Report: 
rewardSum:-473.3136909749826
height:-0.26231646419913174
loss:33.159902231767774
qAverage:[-45.728905278833665]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.9299556439602 with loss 32.26198755111545 in episode 447
Report: 
rewardSum:-477.9299556439602
height:-0.2665962621478526
loss:32.26198755111545
qAverage:[-47.47830185961367]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -480.7130060823726 with loss 30.636950046755373 in episode 448
Report: 
rewardSum:-480.7130060823726
height:-0.2777805707781671
loss:30.636950046755373
qAverage:[-45.59535887732578]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -468.1213545243651 with loss 33.96470137219876 in episode 449
Report: 
rewardSum:-468.1213545243651
height:-0.16865534494800505
loss:33.96470137219876
qAverage:[-44.704233482702456]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.1444506419897 with loss 27.111798968166113 in episode 450
Report: 
rewardSum:-479.1444506419897
height:-0.2665962621478526
loss:27.111798968166113
qAverage:[-49.32427104156796]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -471.17673777533406 with loss 29.999437622725964 in episode 451
Report: 
rewardSum:-471.17673777533406
height:-0.227233687314599
loss:29.999437622725964
qAverage:[-45.969600967506864]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -475.9675739193027 with loss 32.99194813054055 in episode 452
Report: 
rewardSum:-475.9675739193027
height:-0.2665962621478526
loss:32.99194813054055
qAverage:[-50.13654452651294]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -480.6351019913513 with loss 33.10601507127285 in episode 453
Report: 
rewardSum:-480.6351019913513
height:-0.2665962621478526
loss:33.10601507127285
qAverage:[-46.05525021195412]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -474.0578210170542 with loss 31.126850810833275 in episode 454
Report: 
rewardSum:-474.0578210170542
height:-0.22912606907737693
loss:31.126850810833275
qAverage:[-45.75903478699114]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -478.82609312465456 with loss 35.065406805835664 in episode 455
Report: 
rewardSum:-478.82609312465456
height:-0.2537824973987477
loss:35.065406805835664
qAverage:[-49.83027447752691]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -476.88202594610874 with loss 31.27999950479716 in episode 456
Report: 
rewardSum:-476.88202594610874
height:-0.26577936934814644
loss:31.27999950479716
qAverage:[-49.902368593455556]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.2650006415508 with loss 28.323986366391182 in episode 457
Report: 
rewardSum:-479.2650006415508
height:-0.2784233043373287
loss:28.323986366391182
qAverage:[-49.84922371288337]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -475.2329938281084 with loss 29.531099097803235 in episode 458
Report: 
rewardSum:-475.2329938281084
height:-0.24862520399031868
loss:29.531099097803235
qAverage:[-49.76950464154234]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -476.0979137606897 with loss 37.57811482809484 in episode 459
Report: 
rewardSum:-476.0979137606897
height:-0.2792663937111483
loss:37.57811482809484
qAverage:[-46.22400356934528]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -472.6681219359773 with loss 33.06526763457805 in episode 460
Report: 
rewardSum:-472.6681219359773
height:-0.23162835605670107
loss:33.06526763457805
qAverage:[-47.06369621362259]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -475.5232225281319 with loss 40.897015841677785 in episode 461
Report: 
rewardSum:-475.5232225281319
height:-0.2698930251150685
loss:40.897015841677785
qAverage:[-50.89194078445435]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -480.0817426926073 with loss 32.707147556357086 in episode 462
Report: 
rewardSum:-480.0817426926073
height:-0.2678780686913883
loss:32.707147556357086
qAverage:[-47.27865616954974]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -480.3708603512377 with loss 33.103002209216356 in episode 463
Report: 
rewardSum:-480.3708603512377
height:-0.2665962621478526
loss:33.103002209216356
qAverage:[-46.719601934552195]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -474.28395639247645 with loss 32.02934658434242 in episode 464
Report: 
rewardSum:-474.28395639247645
height:-0.2665962621478526
loss:32.02934658434242
qAverage:[-50.91286740444674]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 33.10638839844614 in episode 465
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:33.10638839844614
qAverage:[-50.85637120109293]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.32642367575284 with loss 32.8466208903119 in episode 466
Report: 
rewardSum:-477.32642367575284
height:-0.2665962621478526
loss:32.8466208903119
qAverage:[-50.99261810755966]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 28.177608381956816 in episode 467
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:28.177608381956816
qAverage:[-50.827866615332994]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -480.5308172570543 with loss 30.80336104799062 in episode 468
Report: 
rewardSum:-480.5308172570543
height:-0.2665962621478526
loss:30.80336104799062
qAverage:[-47.003302134091584]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 32.206588671542704 in episode 469
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:32.206588671542704
qAverage:[-50.85823991145994]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 32.8844102518633 in episode 470
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:32.8844102518633
qAverage:[-50.86244894378814]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.2532571931278 with loss 46.72995691280812 in episode 471
Report: 
rewardSum:-479.2532571931278
height:-0.2665962621478526
loss:46.72995691280812
qAverage:[-51.49918191739828]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 36.63122492842376 in episode 472
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:36.63122492842376
qAverage:[-51.549883480729726]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -478.8487936349168 with loss 36.045009578578174 in episode 473
Report: 
rewardSum:-478.8487936349168
height:-0.24748125870427654
loss:36.045009578578174
qAverage:[-47.793149570920576]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -471.42811875713556 with loss 35.142464072443545 in episode 474
Report: 
rewardSum:-471.42811875713556
height:-0.21161193384687604
loss:35.142464072443545
qAverage:[-47.69756390431419]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -480.2319024647587 with loss 33.279068790376186 in episode 475
Report: 
rewardSum:-480.2319024647587
height:-0.2665962621478526
loss:33.279068790376186
qAverage:[-47.6038824147846]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -468.0266580158955 with loss 35.4848369192332 in episode 476
Report: 
rewardSum:-468.0266580158955
height:-0.13341736032956267
loss:35.4848369192332
qAverage:[-50.466067984552666]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.28576722369525 with loss 32.36272555869073 in episode 477
Report: 
rewardSum:-477.28576722369525
height:-0.26631720437932127
loss:32.36272555869073
qAverage:[-51.51366132290209]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.2817961923716 with loss 27.287248061969876 in episode 478
Report: 
rewardSum:-477.2817961923716
height:-0.2665962621478526
loss:27.287248061969876
qAverage:[-47.64246292019365]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.3334384609186 with loss 30.51238378416747 in episode 479
Report: 
rewardSum:-479.3334384609186
height:-0.2789921274406046
loss:30.51238378416747
qAverage:[-51.461199793322336]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -474.1063182137575 with loss 32.75525077339262 in episode 480
Report: 
rewardSum:-474.1063182137575
height:-0.2537824973987477
loss:32.75525077339262
qAverage:[-47.44754972361555]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -475.78054858368125 with loss 36.859289159066975 in episode 481
Report: 
rewardSum:-475.78054858368125
height:-0.2665962621478526
loss:36.859289159066975
qAverage:[-52.44548263925637]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 34.34384508617222 in episode 482
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:34.34384508617222
qAverage:[-52.508715418172]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -468.69018448396855 with loss 38.56224770564586 in episode 483
Report: 
rewardSum:-468.69018448396855
height:-0.20869806048518633
loss:38.56224770564586
qAverage:[-47.756997888684275]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -476.8766971534045 with loss 36.07800767198205 in episode 484
Report: 
rewardSum:-476.8766971534045
height:-0.29330794933225485
loss:36.07800767198205
qAverage:[-52.50863102775308]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.15229018001907 with loss 30.794345904141665 in episode 485
Report: 
rewardSum:-479.15229018001907
height:-0.27449669247730135
loss:30.794345904141665
qAverage:[-52.390624032091736]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -478.2834607429305 with loss 36.46544280741364 in episode 486
Report: 
rewardSum:-478.2834607429305
height:-0.26945827150329765
loss:36.46544280741364
qAverage:[-52.43769039154053]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -478.9985340409281 with loss 32.54969091992825 in episode 487
Report: 
rewardSum:-478.9985340409281
height:-0.2665962621478526
loss:32.54969091992825
qAverage:[-52.51416541332036]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -478.9041869242508 with loss 37.19314652495086 in episode 488
Report: 
rewardSum:-478.9041869242508
height:-0.2665962621478526
loss:37.19314652495086
qAverage:[-52.511867637634275]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -476.16744068403864 with loss 37.57988066505641 in episode 489
Report: 
rewardSum:-476.16744068403864
height:-0.3176894993710815
loss:37.57988066505641
qAverage:[-52.57496477952644]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.6044347345133 with loss 37.24831451009959 in episode 490
Report: 
rewardSum:-477.6044347345133
height:-0.24283583323909314
loss:37.24831451009959
qAverage:[-48.653583988104714]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -478.3782740140455 with loss 43.12195252906531 in episode 491
Report: 
rewardSum:-478.3782740140455
height:-0.2776148474425203
loss:43.12195252906531
qAverage:[-53.012901135345004]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -475.0102740171966 with loss 31.161960400640965 in episode 492
Report: 
rewardSum:-475.0102740171966
height:-0.2665962621478526
loss:31.161960400640965
qAverage:[-53.17375378538235]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.32642367575284 with loss 41.57072165701538 in episode 493
Report: 
rewardSum:-477.32642367575284
height:-0.2665962621478526
loss:41.57072165701538
qAverage:[-53.23260321405721]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.08601093024885 with loss 33.05062963068485 in episode 494
Report: 
rewardSum:-479.08601093024885
height:-0.27943046817206457
loss:33.05062963068485
qAverage:[-48.79107021562981]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -478.668051826984 with loss 42.666796235367656 in episode 495
Report: 
rewardSum:-478.668051826984
height:-0.26138901291647026
loss:42.666796235367656
qAverage:[-48.846105082711176]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.32138541503906 with loss 35.78153005428612 in episode 496
Report: 
rewardSum:-477.32138541503906
height:-0.2928409864524248
loss:35.78153005428612
qAverage:[-49.417379208803176]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -475.29061179547415 with loss 34.60417008027434 in episode 497
Report: 
rewardSum:-475.29061179547415
height:-0.2790123814208796
loss:34.60417008027434
qAverage:[-53.17594810072424]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -476.59514800395726 with loss 32.76613796688616 in episode 498
Report: 
rewardSum:-476.59514800395726
height:-0.2665962621478526
loss:32.76613796688616
qAverage:[-53.23675071077394]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -471.5810626268364 with loss 40.30513946060091 in episode 499
Report: 
rewardSum:-471.5810626268364
height:-0.18723078635688906
loss:40.30513946060091
qAverage:[-48.16567703873957]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -475.8644442627043 with loss 40.49538696091622 in episode 500
Report: 
rewardSum:-475.8644442627043
height:-0.2665962621478526
loss:40.49538696091622
qAverage:[-53.06748611148041]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.07845979548495 with loss 44.036192541942 in episode 501
Report: 
rewardSum:-479.07845979548495
height:-0.25898833314420106
loss:44.036192541942
qAverage:[-49.41068733983965]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.3560041373158 with loss 35.145383298862725 in episode 502
Report: 
rewardSum:-479.3560041373158
height:-0.25378844996620575
loss:35.145383298862725
qAverage:[-49.51831134200096]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 40.06029041856527 in episode 503
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:40.06029041856527
qAverage:[-53.72409279358211]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 42.04629808757454 in episode 504
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:42.04629808757454
qAverage:[-53.73185111736429]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -460.0106599851209 with loss 41.30891662836075 in episode 505
Report: 
rewardSum:-460.0106599851209
height:-0.16491864037153567
loss:41.30891662836075
qAverage:[-48.88635739669278]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -473.7235410394967 with loss 37.937956074252725 in episode 506
Report: 
rewardSum:-473.7235410394967
height:-0.2665962621478526
loss:37.937956074252725
qAverage:[-49.67101719111204]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -474.46140011120656 with loss 40.61971232108772 in episode 507
Report: 
rewardSum:-474.46140011120656
height:-0.2621689610416371
loss:40.61971232108772
qAverage:[-53.826786211221524]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -468.04423433822757 with loss 34.32378496415913 in episode 508
Report: 
rewardSum:-468.04423433822757
height:-0.11507716474487907
loss:34.32378496415913
qAverage:[-49.150553898895204]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -480.15655568341634 with loss 39.34738530404866 in episode 509
Report: 
rewardSum:-480.15655568341634
height:-0.27276551494325607
loss:39.34738530404866
qAverage:[-49.15931971648231]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.59379679161196 with loss 37.40163536928594 in episode 510
Report: 
rewardSum:-477.59379679161196
height:-0.24063634136725703
loss:37.40163536928594
qAverage:[-49.31339623460818]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -475.10613646587893 with loss 43.38063233625144 in episode 511
Report: 
rewardSum:-475.10613646587893
height:-0.3295153867941129
loss:43.38063233625144
qAverage:[-50.313734618401284]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -463.68260720385194 with loss 37.41082345508039 in episode 512
Report: 
rewardSum:-463.68260720385194
height:-0.11317455169328282
loss:37.41082345508039
qAverage:[-50.65348057184607]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -469.31629139610004 with loss 40.134600354358554 in episode 513
Report: 
rewardSum:-469.31629139610004
height:-0.21074079720214914
loss:40.134600354358554
qAverage:[-50.211285837215954]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -475.7315516251588 with loss 36.447480795904994 in episode 514
Report: 
rewardSum:-475.7315516251588
height:-0.2665962621478526
loss:36.447480795904994
qAverage:[-55.18137374802015]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -450.86312230454786 with loss 31.695529870688915 in episode 515
Report: 
rewardSum:-450.86312230454786
height:-0.11995262054496693
loss:31.695529870688915
qAverage:[-54.42414027246936]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -475.1961168761315 with loss 43.68834486696869 in episode 516
Report: 
rewardSum:-475.1961168761315
height:-0.2702964415408579
loss:43.68834486696869
qAverage:[-50.103780578405875]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 37.591431982815266 in episode 517
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:37.591431982815266
qAverage:[-55.119155094541355]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -478.5407219076553 with loss 37.409402053803205 in episode 518
Report: 
rewardSum:-478.5407219076553
height:-0.2587494583354625
loss:37.409402053803205
qAverage:[-49.60205985472934]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -480.2661239197091 with loss 38.3999468581751 in episode 519
Report: 
rewardSum:-480.2661239197091
height:-0.2580440672988092
loss:38.3999468581751
qAverage:[-50.47031967438276]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -472.50166383999533 with loss 40.02295192703605 in episode 520
Report: 
rewardSum:-472.50166383999533
height:-0.24654721029339915
loss:40.02295192703605
qAverage:[-50.31360825641671]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -480.5888655280891 with loss 45.770404513925314 in episode 521
Report: 
rewardSum:-480.5888655280891
height:-0.27599185672172055
loss:45.770404513925314
qAverage:[-50.79373008400202]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -467.29640972072366 with loss 43.80229620821774 in episode 522
Report: 
rewardSum:-467.29640972072366
height:-0.11600464434672886
loss:43.80229620821774
qAverage:[-54.71427094110168]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -475.7284481624273 with loss 43.321182244457304 in episode 523
Report: 
rewardSum:-475.7284481624273
height:-0.3320072920903294
loss:43.321182244457304
qAverage:[-50.89285524016117]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -476.067116189854 with loss 49.23222086392343 in episode 524
Report: 
rewardSum:-476.067116189854
height:-0.23974486737370532
loss:49.23222086392343
qAverage:[-50.76119561553001]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -475.5736879450991 with loss 46.77732957806438 in episode 525
Report: 
rewardSum:-475.5736879450991
height:-0.30475575702627183
loss:46.77732957806438
qAverage:[-55.83751073049669]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 40.983924098312855 in episode 526
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:40.983924098312855
qAverage:[-55.676738645055615]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -480.96861475679555 with loss 50.720448047854006 in episode 527
Report: 
rewardSum:-480.96861475679555
height:-0.2665962621478526
loss:50.720448047854006
qAverage:[-50.540386978405806]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.8696252472482 with loss 42.5008485391736 in episode 528
Report: 
rewardSum:-479.8696252472482
height:-0.2538212069975906
loss:42.5008485391736
qAverage:[-50.89336850038215]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -467.8600943206502 with loss 42.24908086284995 in episode 529
Report: 
rewardSum:-467.8600943206502
height:-0.1736417434933213
loss:42.24908086284995
qAverage:[-55.029130489672]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -478.39060895589057 with loss 41.890246654860675 in episode 530
Report: 
rewardSum:-478.39060895589057
height:-0.2525488290620382
loss:41.890246654860675
qAverage:[-50.54802368470092]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -476.4694395523663 with loss 44.68973944708705 in episode 531
Report: 
rewardSum:-476.4694395523663
height:-0.27287482583592837
loss:44.68973944708705
qAverage:[-56.76255817224484]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -467.6024080088481 with loss 39.73487487901002 in episode 532
Report: 
rewardSum:-467.6024080088481
height:-0.11872726208557036
loss:39.73487487901002
qAverage:[-55.378663015837716]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -480.61595658872784 with loss 46.4569143243134 in episode 533
Report: 
rewardSum:-480.61595658872784
height:-0.2665962621478526
loss:46.4569143243134
qAverage:[-51.66004751224329]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -460.10413515060293 with loss 48.07275461126119 in episode 534
Report: 
rewardSum:-460.10413515060293
height:-0.2712972817420148
loss:48.07275461126119
qAverage:[-52.091996606309614]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.4032285332796 with loss 40.67358507961035 in episode 535
Report: 
rewardSum:-477.4032285332796
height:-0.2706462035303838
loss:40.67358507961035
qAverage:[-56.68966499177536]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.4821945178459 with loss 34.27427649870515 in episode 536
Report: 
rewardSum:-477.4821945178459
height:-0.32506174735724375
loss:34.27427649870515
qAverage:[-52.059199891090394]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -466.7523832123755 with loss 41.31211131904274 in episode 537
Report: 
rewardSum:-466.7523832123755
height:-0.10125421464993295
loss:41.31211131904274
qAverage:[-51.14240932123578]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -466.05076511967906 with loss 43.82419003173709 in episode 538
Report: 
rewardSum:-466.05076511967906
height:-0.2665962621478526
loss:43.82419003173709
qAverage:[-56.898507996360856]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -475.2786372646195 with loss 45.35133838187903 in episode 539
Report: 
rewardSum:-475.2786372646195
height:-0.22749244721740017
loss:45.35133838187903
qAverage:[-51.143444160423655]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -467.58346519201 with loss 46.58446020819247 in episode 540
Report: 
rewardSum:-467.58346519201
height:-0.2721042598988143
loss:46.58446020819247
qAverage:[-51.777941599563135]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.9031869242508 with loss 41.71442366577685 in episode 541
Report: 
rewardSum:-479.9031869242508
height:-0.2665962621478526
loss:41.71442366577685
qAverage:[-57.704695050079046]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -476.67807265088175 with loss 46.75936586037278 in episode 542
Report: 
rewardSum:-476.67807265088175
height:-0.2665962621478526
loss:46.75936586037278
qAverage:[-57.78359159782751]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.191481824515 with loss 48.148191545158625 in episode 543
Report: 
rewardSum:-479.191481824515
height:-0.25745988010608767
loss:48.148191545158625
qAverage:[-52.192477326896324]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -475.4323814020958 with loss 45.61951871402562 in episode 544
Report: 
rewardSum:-475.4323814020958
height:-0.25869522267717304
loss:45.61951871402562
qAverage:[-57.68327866739302]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -467.24907233122985 with loss 53.21808454860002 in episode 545
Report: 
rewardSum:-467.24907233122985
height:-0.1475254941893036
loss:53.21808454860002
qAverage:[-52.41927613914013]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -475.8782476015106 with loss 44.86879051476717 in episode 546
Report: 
rewardSum:-475.8782476015106
height:-0.23700574658583376
loss:44.86879051476717
qAverage:[-51.58937544683319]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -465.4553459443973 with loss 48.478368455544114 in episode 547
Report: 
rewardSum:-465.4553459443973
height:-0.2451640754274109
loss:48.478368455544114
qAverage:[-51.98493881685899]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.36656293008554 with loss 45.69383464381099 in episode 548
Report: 
rewardSum:-479.36656293008554
height:-0.25141747658091923
loss:45.69383464381099
qAverage:[-52.252815842029435]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.67269673386176 with loss 46.398070154711604 in episode 549
Report: 
rewardSum:-479.67269673386176
height:-0.2665962621478526
loss:46.398070154711604
qAverage:[-54.39807530794994]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -474.5280627672845 with loss 42.02132760826498 in episode 550
Report: 
rewardSum:-474.5280627672845
height:-0.2665962621478526
loss:42.02132760826498
qAverage:[-57.66055603782729]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -462.1658737604775 with loss 43.90791832096875 in episode 551
Report: 
rewardSum:-462.1658737604775
height:-0.07075931297391555
loss:43.90791832096875
qAverage:[-51.61570970849557]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -481.19570652228236 with loss 38.12673229817301 in episode 552
Report: 
rewardSum:-481.19570652228236
height:-0.28138307049775124
loss:38.12673229817301
qAverage:[-53.18904385590317]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -467.510584738211 with loss 47.43793370574713 in episode 553
Report: 
rewardSum:-467.510584738211
height:-0.16388245809042187
loss:47.43793370574713
qAverage:[-53.03460317909128]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -469.7873008408943 with loss 42.37549837119877 in episode 554
Report: 
rewardSum:-469.7873008408943
height:-0.28327349919574774
loss:42.37549837119877
qAverage:[-52.787892669735854]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -481.1386467667447 with loss 46.26579503715038 in episode 555
Report: 
rewardSum:-481.1386467667447
height:-0.2777805707781671
loss:46.26579503715038
qAverage:[-53.063949254289945]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -473.1137072967201 with loss 43.267627803608775 in episode 556
Report: 
rewardSum:-473.1137072967201
height:-0.279390127589399
loss:43.267627803608775
qAverage:[-52.90806110996512]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -458.4417318547469 with loss 39.621502198278904 in episode 557
Report: 
rewardSum:-458.4417318547469
height:-0.1655344184750575
loss:39.621502198278904
qAverage:[-52.58473898878145]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -476.48507898836436 with loss 46.117823229171336 in episode 558
Report: 
rewardSum:-476.48507898836436
height:-0.2806498959288623
loss:46.117823229171336
qAverage:[-56.02527285520755]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -464.83994186534375 with loss 43.30957852303982 in episode 559
Report: 
rewardSum:-464.83994186534375
height:-0.2539909751662011
loss:43.30957852303982
qAverage:[-53.07290925778965]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -473.22741425717743 with loss 39.925794527865946 in episode 560
Report: 
rewardSum:-473.22741425717743
height:-0.2853712422092148
loss:39.925794527865946
qAverage:[-52.885320835113525]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -466.3770310196683 with loss 47.80793766397983 in episode 561
Report: 
rewardSum:-466.3770310196683
height:-0.18792449835494082
loss:47.80793766397983
qAverage:[-53.19750836595374]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.72543533760194 with loss 44.09661838412285 in episode 562
Report: 
rewardSum:-479.72543533760194
height:-0.2759779461910254
loss:44.09661838412285
qAverage:[-54.13897866633401]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -469.7043800186949 with loss 40.87242998089641 in episode 563
Report: 
rewardSum:-469.7043800186949
height:-0.27449639966046685
loss:40.87242998089641
qAverage:[-59.02193065699685]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.15950727570794 with loss 48.55498391203582 in episode 564
Report: 
rewardSum:-479.15950727570794
height:-0.23694605597599636
loss:48.55498391203582
qAverage:[-54.07657341614808]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -472.94530961134797 with loss 42.23688363470137 in episode 565
Report: 
rewardSum:-472.94530961134797
height:-0.015700170095186716
loss:42.23688363470137
qAverage:[-55.29120961584226]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -466.2298692355351 with loss 41.16590492427349 in episode 566
Report: 
rewardSum:-466.2298692355351
height:-0.19246808931548764
loss:41.16590492427349
qAverage:[-54.699951802149855]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -466.9139080902346 with loss 52.0284425932914 in episode 567
Report: 
rewardSum:-466.9139080902346
height:-0.2665962621478526
loss:52.0284425932914
qAverage:[-56.02191388067887]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -466.66422107280755 with loss 46.2937627453357 in episode 568
Report: 
rewardSum:-466.66422107280755
height:-0.2800285154271878
loss:46.2937627453357
qAverage:[-54.40680513475916]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -480.9443528452296 with loss 42.62281843088567 in episode 569
Report: 
rewardSum:-480.9443528452296
height:-0.27388029645734796
loss:42.62281843088567
qAverage:[-56.884746094741445]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -463.1967583492691 with loss 48.893778802827 in episode 570
Report: 
rewardSum:-463.1967583492691
height:-0.10895880441011906
loss:48.893778802827
qAverage:[-55.78234539807436]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -459.6008877337081 with loss 52.84877244848758 in episode 571
Report: 
rewardSum:-459.6008877337081
height:-0.28029184965952786
loss:52.84877244848758
qAverage:[-54.4434891885519]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -465.56033310452864 with loss 45.0937853557989 in episode 572
Report: 
rewardSum:-465.56033310452864
height:-0.23895529209481364
loss:45.0937853557989
qAverage:[-53.68379107117653]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -468.1536532953993 with loss 46.00247760862112 in episode 573
Report: 
rewardSum:-468.1536532953993
height:-0.27966892458410647
loss:46.00247760862112
qAverage:[-56.67481126832725]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -472.512828927813 with loss 44.32040474098176 in episode 574
Report: 
rewardSum:-472.512828927813
height:-0.28331462315198624
loss:44.32040474098176
qAverage:[-60.074581468876325]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -466.72706364252895 with loss 52.83203807100654 in episode 575
Report: 
rewardSum:-466.72706364252895
height:-0.28126100520589736
loss:52.83203807100654
qAverage:[-55.61885794758797]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -451.73594047129495 with loss 54.28524579014629 in episode 576
Report: 
rewardSum:-451.73594047129495
height:-0.06755856494552434
loss:54.28524579014629
qAverage:[-58.860865578722596]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -471.7758310471361 with loss 47.71762312948704 in episode 577
Report: 
rewardSum:-471.7758310471361
height:-0.2811789020275239
loss:47.71762312948704
qAverage:[-56.13254108381032]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -460.1750383880552 with loss 52.229014329612255 in episode 578
Report: 
rewardSum:-460.1750383880552
height:-0.2570042162222122
loss:52.229014329612255
qAverage:[-54.681636239042376]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -459.7144996275866 with loss 39.53826189506799 in episode 579
Report: 
rewardSum:-459.7144996275866
height:-0.2355524293826507
loss:39.53826189506799
qAverage:[-54.37116113927827]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -442.60597194975554 with loss 38.660977992229164 in episode 580
Report: 
rewardSum:-442.60597194975554
height:-0.27992682296120325
loss:38.660977992229164
qAverage:[-54.50895669790778]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -471.69949427922325 with loss 52.13441277015954 in episode 581
Report: 
rewardSum:-471.69949427922325
height:-0.2777805707781671
loss:52.13441277015954
qAverage:[-54.59952383724289]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.175534073112 with loss 46.13330619223416 in episode 582
Report: 
rewardSum:-477.175534073112
height:-0.2902845664115183
loss:46.13330619223416
qAverage:[-55.26142973710995]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -466.6242762262957 with loss 49.79594375472516 in episode 583
Report: 
rewardSum:-466.6242762262957
height:-0.2665962621478526
loss:49.79594375472516
qAverage:[-60.59325204981435]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.5395888003064 with loss 45.556598579511046 in episode 584
Report: 
rewardSum:-477.5395888003064
height:-0.2520108178535167
loss:45.556598579511046
qAverage:[-55.21615692136323]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.25987535827517 with loss 58.05083426833153 in episode 585
Report: 
rewardSum:-477.25987535827517
height:-0.2634800828907166
loss:58.05083426833153
qAverage:[-54.26261710235388]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -464.06723109598454 with loss 49.81026527658105 in episode 586
Report: 
rewardSum:-464.06723109598454
height:-0.2207684138424847
loss:49.81026527658105
qAverage:[-55.023498970060494]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -459.40447425807986 with loss 40.14889167249203 in episode 587
Report: 
rewardSum:-459.40447425807986
height:-0.2946387847493092
loss:40.14889167249203
qAverage:[-60.69655046463013]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -469.6429493077298 with loss 48.491836342960596 in episode 588
Report: 
rewardSum:-469.6429493077298
height:-0.2990672574796427
loss:48.491836342960596
qAverage:[-54.987324137699424]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.72946451896104 with loss 44.884712686762214 in episode 589
Report: 
rewardSum:-477.72946451896104
height:-0.2866496999246701
loss:44.884712686762214
qAverage:[-60.64960715794327]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -471.5117105982258 with loss 51.14780548308045 in episode 590
Report: 
rewardSum:-471.5117105982258
height:-0.2758471846362871
loss:51.14780548308045
qAverage:[-54.1705315309763]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -471.0911326919921 with loss 55.14832291472703 in episode 591
Report: 
rewardSum:-471.0911326919921
height:-0.2473054376940209
loss:55.14832291472703
qAverage:[-55.34450325859127]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -464.01630421926563 with loss 43.10706572048366 in episode 592
Report: 
rewardSum:-464.01630421926563
height:-0.25617834900660164
loss:43.10706572048366
qAverage:[-55.253107902289614]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -471.5645773781928 with loss 35.85300026461482 in episode 593
Report: 
rewardSum:-471.5645773781928
height:-0.2495365269728106
loss:35.85300026461482
qAverage:[-55.24565029719678]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -461.9233973540077 with loss 44.8273846404627 in episode 594
Report: 
rewardSum:-461.9233973540077
height:-0.16702667706535634
loss:44.8273846404627
qAverage:[-54.53627605313685]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -474.1297361919311 with loss 49.550584874115884 in episode 595
Report: 
rewardSum:-474.1297361919311
height:-0.3459092403823209
loss:49.550584874115884
qAverage:[-61.282789671949566]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -469.57575741405293 with loss 45.79775839019567 in episode 596
Report: 
rewardSum:-469.57575741405293
height:-0.15946694435351913
loss:45.79775839019567
qAverage:[-59.00571419725466]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -452.37894123749487 with loss 47.64566996600479 in episode 597
Report: 
rewardSum:-452.37894123749487
height:0.03223398719908355
loss:47.64566996600479
qAverage:[-53.2913503006323]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -445.7420866593445 with loss 51.83508758340031 in episode 598
Report: 
rewardSum:-445.7420866593445
height:-0.09110147415811745
loss:51.83508758340031
qAverage:[-54.52097007265232]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -474.8469005865208 with loss 49.15574047900736 in episode 599
Report: 
rewardSum:-474.8469005865208
height:-0.07597446669009962
loss:49.15574047900736
qAverage:[-54.122148043396486]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -474.07796471625187 with loss 46.49328160379082 in episode 600
Report: 
rewardSum:-474.07796471625187
height:-0.07387002679089041
loss:46.49328160379082
qAverage:[-53.592196383910014]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -450.56711253445707 with loss 52.57889698818326 in episode 601
Report: 
rewardSum:-450.56711253445707
height:0.07893023031815685
loss:52.57889698818326
qAverage:[-60.690705328276664]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -462.7450933624078 with loss 41.851101539097726 in episode 602
Report: 
rewardSum:-462.7450933624078
height:-0.136886626016976
loss:41.851101539097726
qAverage:[-57.374173743724825]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -476.6643947504586 with loss 54.49285718705505 in episode 603
Report: 
rewardSum:-476.6643947504586
height:-0.2777805707781671
loss:54.49285718705505
qAverage:[-57.59297210627263]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -465.05856289132646 with loss 48.51015135738999 in episode 604
Report: 
rewardSum:-465.05856289132646
height:-0.1526794951389695
loss:48.51015135738999
qAverage:[-55.98207227033169]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -464.8642698144773 with loss 53.4884508382529 in episode 605
Report: 
rewardSum:-464.8642698144773
height:-0.27208424117870067
loss:53.4884508382529
qAverage:[-59.06955553880378]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -448.1872454502858 with loss 50.01992671750486 in episode 606
Report: 
rewardSum:-448.1872454502858
height:-0.12400065956250905
loss:50.01992671750486
qAverage:[-55.41201909631491]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -467.4122139068998 with loss 44.90943365171552 in episode 607
Report: 
rewardSum:-467.4122139068998
height:-0.16052358826032448
loss:44.90943365171552
qAverage:[-55.93796484895272]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.0958768188461 with loss 55.684216341003776 in episode 608
Report: 
rewardSum:-479.0958768188461
height:-0.12002036617722264
loss:55.684216341003776
qAverage:[-55.27203399654645]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -475.8152984814187 with loss 55.23576295748353 in episode 609
Report: 
rewardSum:-475.8152984814187
height:-0.24699465721095798
loss:55.23576295748353
qAverage:[-55.888372203213486]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -465.2523465369897 with loss 48.049257590435445 in episode 610
Report: 
rewardSum:-465.2523465369897
height:-0.14210970373670098
loss:48.049257590435445
qAverage:[-55.815051957304256]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -481.7361762974464 with loss 53.363760055974126 in episode 611
Report: 
rewardSum:-481.7361762974464
height:-0.053832951183215086
loss:53.363760055974126
qAverage:[-59.5202383686654]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -466.7543733864374 with loss 45.07928073685616 in episode 612
Report: 
rewardSum:-466.7543733864374
height:0.04603940353905389
loss:45.07928073685616
qAverage:[-58.8091696729801]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -471.29520064202023 with loss 48.6423007491976 in episode 613
Report: 
rewardSum:-471.29520064202023
height:-0.21947799437397575
loss:48.6423007491976
qAverage:[-62.34196703304798]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -468.4699587228127 with loss 49.94552480150014 in episode 614
Report: 
rewardSum:-468.4699587228127
height:0.135994642916739
loss:49.94552480150014
qAverage:[-59.35418352352575]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -473.2685431641289 with loss 41.04582432517782 in episode 615
Report: 
rewardSum:-473.2685431641289
height:-0.20796005577605617
loss:41.04582432517782
qAverage:[-55.28281840874781]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -475.8783186373829 with loss 53.80068403761834 in episode 616
Report: 
rewardSum:-475.8783186373829
height:-0.28228189329027165
loss:53.80068403761834
qAverage:[-56.3321333683183]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -472.64326384405274 with loss 62.78007506299764 in episode 617
Report: 
rewardSum:-472.64326384405274
height:-0.2302908146937032
loss:62.78007506299764
qAverage:[-56.066067175699935]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -467.50833910789595 with loss 54.47917057108134 in episode 618
Report: 
rewardSum:-467.50833910789595
height:-0.28143609446812584
loss:54.47917057108134
qAverage:[-55.576378374243504]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -460.87380889808463 with loss 48.528316990472376 in episode 619
Report: 
rewardSum:-460.87380889808463
height:-0.059531649058077105
loss:48.528316990472376
qAverage:[-55.1719112621611]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -463.05099435003507 with loss 53.43647145293653 in episode 620
Report: 
rewardSum:-463.05099435003507
height:-0.2012213723688168
loss:53.43647145293653
qAverage:[-62.27738202851394]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -481.8066692057309 with loss 53.71054819226265 in episode 621
Report: 
rewardSum:-481.8066692057309
height:-0.28286134019006826
loss:53.71054819226265
qAverage:[-56.923716734349725]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -466.31535738207276 with loss 54.35634178202599 in episode 622
Report: 
rewardSum:-466.31535738207276
height:-0.20880152856780493
loss:54.35634178202599
qAverage:[-57.61307901263237]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -447.89274071286263 with loss 46.60613599419594 in episode 623
Report: 
rewardSum:-447.89274071286263
height:0.043264314720318345
loss:46.60613599419594
qAverage:[-55.66234876692295]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -463.6133008811417 with loss 52.76550799701363 in episode 624
Report: 
rewardSum:-463.6133008811417
height:-0.12032359610922824
loss:52.76550799701363
qAverage:[-56.51898151397705]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -464.81426822483223 with loss 59.8697437858209 in episode 625
Report: 
rewardSum:-464.81426822483223
height:-0.07563199846753614
loss:59.8697437858209
qAverage:[-55.89196016480081]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.0800511355332 with loss 52.33992567192763 in episode 626
Report: 
rewardSum:-477.0800511355332
height:-0.250519470795274
loss:52.33992567192763
qAverage:[-57.15889683410303]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -461.7320852023391 with loss 48.82697749324143 in episode 627
Report: 
rewardSum:-461.7320852023391
height:-0.10890942073850929
loss:48.82697749324143
qAverage:[-55.47131068255771]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -480.760787579208 with loss 45.5830272147432 in episode 628
Report: 
rewardSum:-480.760787579208
height:-0.27581960280331774
loss:45.5830272147432
qAverage:[-57.23969991608421]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -476.98955865257886 with loss 60.92860150523484 in episode 629
Report: 
rewardSum:-476.98955865257886
height:-0.2712414816020832
loss:60.92860150523484
qAverage:[-57.88155716507878]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -461.1798436166955 with loss 58.20724479202181 in episode 630
Report: 
rewardSum:-461.1798436166955
height:0.06727510882951658
loss:58.20724479202181
qAverage:[-58.13854219871967]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -457.20737040573727 with loss 58.33998912293464 in episode 631
Report: 
rewardSum:-457.20737040573727
height:-0.1853122693924283
loss:58.33998912293464
qAverage:[-57.9258841586113]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -473.16423845759664 with loss 51.48622950632125 in episode 632
Report: 
rewardSum:-473.16423845759664
height:-0.2874605897032065
loss:51.48622950632125
qAverage:[-64.47405031411955]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -468.63163900328124 with loss 45.48364984150976 in episode 633
Report: 
rewardSum:-468.63163900328124
height:-0.24640253932887918
loss:45.48364984150976
qAverage:[-57.66448194791775]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -473.6364504672593 with loss 47.23654908314347 in episode 634
Report: 
rewardSum:-473.6364504672593
height:-0.25582767898449293
loss:47.23654908314347
qAverage:[-64.41947517584805]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -463.23005737578455 with loss 61.64047277532518 in episode 635
Report: 
rewardSum:-463.23005737578455
height:0.0013103732160677445
loss:61.64047277532518
qAverage:[-55.844092644053724]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -447.3426628299836 with loss 50.728682646527886 in episode 636
Report: 
rewardSum:-447.3426628299836
height:-0.12057301200269814
loss:50.728682646527886
qAverage:[-56.923739119390746]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -463.44833196275675 with loss 52.29904786963016 in episode 637
Report: 
rewardSum:-463.44833196275675
height:-0.15225167714259683
loss:52.29904786963016
qAverage:[-60.2284854221344]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -464.7251028772315 with loss 46.82233685813844 in episode 638
Report: 
rewardSum:-464.7251028772315
height:-0.14703528456190795
loss:46.82233685813844
qAverage:[-57.23664936490694]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -454.37807159801275 with loss 46.46911535598338 in episode 639
Report: 
rewardSum:-454.37807159801275
height:-0.20233494410300273
loss:46.46911535598338
qAverage:[-57.5955958351567]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -470.7522169528714 with loss 55.84431484527886 in episode 640
Report: 
rewardSum:-470.7522169528714
height:-0.183341499745359
loss:55.84431484527886
qAverage:[-56.891932470798494]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -335.99033593907143 with loss 42.86689649708569 in episode 641
Report: 
rewardSum:-335.99033593907143
height:0.5040574674069916
loss:42.86689649708569
qAverage:[-63.027952483206086]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -469.65882851939455 with loss 54.800926893018186 in episode 642
Report: 
rewardSum:-469.65882851939455
height:-0.2665962621478526
loss:54.800926893018186
qAverage:[-58.104392522661556]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -471.6980978477271 with loss 49.28353631310165 in episode 643
Report: 
rewardSum:-471.6980978477271
height:-0.2907561899088536
loss:49.28353631310165
qAverage:[-58.75355406187067]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -458.4540195898015 with loss 65.63577838614583 in episode 644
Report: 
rewardSum:-458.4540195898015
height:-0.2665962621478526
loss:65.63577838614583
qAverage:[-64.99918463242113]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -474.4847572825812 with loss 53.13577947579324 in episode 645
Report: 
rewardSum:-474.4847572825812
height:-0.2665962621478526
loss:53.13577947579324
qAverage:[-60.678382743597034]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -456.5181826736555 with loss 53.101095002144575 in episode 646
Report: 
rewardSum:-456.5181826736555
height:-0.20099703093312846
loss:53.101095002144575
qAverage:[-57.88790353749058]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -476.68729997224614 with loss 51.616861030459404 in episode 647
Report: 
rewardSum:-476.68729997224614
height:-0.21095124666007373
loss:51.616861030459404
qAverage:[-57.69601600820368]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.73334081472507 with loss 47.840578318573534 in episode 648
Report: 
rewardSum:-479.73334081472507
height:-0.2727454087168991
loss:47.840578318573534
qAverage:[-58.454578485151735]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -473.16282419642704 with loss 54.80577730759978 in episode 649
Report: 
rewardSum:-473.16282419642704
height:-0.21686104860865899
loss:54.80577730759978
qAverage:[-57.92247290658479]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -469.44876591348174 with loss 53.388179674744606 in episode 650
Report: 
rewardSum:-469.44876591348174
height:-0.19964995896087306
loss:53.388179674744606
qAverage:[-58.21306149995149]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -473.698704627963 with loss 51.98167738877237 in episode 651
Report: 
rewardSum:-473.698704627963
height:-0.12109058405521442
loss:51.98167738877237
qAverage:[-60.64542729765324]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -463.8811436530206 with loss 62.009800775907934 in episode 652
Report: 
rewardSum:-463.8811436530206
height:-0.07093739357917886
loss:62.009800775907934
qAverage:[-60.543727431978496]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.7543596852822 with loss 54.47839515469968 in episode 653
Report: 
rewardSum:-477.7543596852822
height:-0.2775447878769616
loss:54.47839515469968
qAverage:[-59.06319923364911]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -465.9539110850397 with loss 52.97729664854705 in episode 654
Report: 
rewardSum:-465.9539110850397
height:0.020972559864134425
loss:52.97729664854705
qAverage:[-61.331946003025976]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.7929185550774 with loss 56.56987649016082 in episode 655
Report: 
rewardSum:-477.7929185550774
height:-0.30054707170588835
loss:56.56987649016082
qAverage:[-59.17283226304979]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -467.15421558439925 with loss 51.738892344757915 in episode 656
Report: 
rewardSum:-467.15421558439925
height:0.06252429582547739
loss:51.738892344757915
qAverage:[-61.429407822318595]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -456.874973557311 with loss 55.76656718365848 in episode 657
Report: 
rewardSum:-456.874973557311
height:-0.10062246129190011
loss:55.76656718365848
qAverage:[-57.32065256379882]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -456.3462327650806 with loss 58.47536737378687 in episode 658
Report: 
rewardSum:-456.3462327650806
height:-0.32982221844782034
loss:58.47536737378687
qAverage:[-63.290996847481566]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -428.53407400476067 with loss 48.4862628178671 in episode 659
Report: 
rewardSum:-428.53407400476067
height:-0.15223122230958727
loss:48.4862628178671
qAverage:[-61.80622093713106]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -473.642824966589 with loss 53.94962769281119 in episode 660
Report: 
rewardSum:-473.642824966589
height:0.01193638554737458
loss:53.94962769281119
qAverage:[-61.604392462437694]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -457.6575437499855 with loss 48.25040053576231 in episode 661
Report: 
rewardSum:-457.6575437499855
height:-0.04436799225002677
loss:48.25040053576231
qAverage:[-61.29364877316489]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -449.18646644454964 with loss 61.335665229707956 in episode 662
Report: 
rewardSum:-449.18646644454964
height:-0.14042155911938578
loss:61.335665229707956
qAverage:[-59.51203744393764]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -461.9510468165147 with loss 53.387794859707355 in episode 663
Report: 
rewardSum:-461.9510468165147
height:-0.2439500634809082
loss:53.387794859707355
qAverage:[-63.20332061846067]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -450.78426971174895 with loss 57.586284016259015 in episode 664
Report: 
rewardSum:-450.78426971174895
height:-0.224096368106982
loss:57.586284016259015
qAverage:[-62.17049509257519]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -464.4124613171748 with loss 53.689860155805945 in episode 665
Report: 
rewardSum:-464.4124613171748
height:-0.1687217174891737
loss:53.689860155805945
qAverage:[-59.05529524922371]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -463.23654752704493 with loss 42.25183944683522 in episode 666
Report: 
rewardSum:-463.23654752704493
height:-0.2074426603233548
loss:42.25183944683522
qAverage:[-63.76303143143654]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -455.90451090289014 with loss 55.12100795377046 in episode 667
Report: 
rewardSum:-455.90451090289014
height:-0.09840419140682277
loss:55.12100795377046
qAverage:[-58.58901973456145]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -467.2008914568897 with loss 49.936881062574685 in episode 668
Report: 
rewardSum:-467.2008914568897
height:-0.05240172068718809
loss:49.936881062574685
qAverage:[-61.51820684487547]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -466.9026196465115 with loss 53.325685617513955 in episode 669
Report: 
rewardSum:-466.9026196465115
height:-0.197215499712111
loss:53.325685617513955
qAverage:[-64.35983563673616]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -480.7972238154728 with loss 57.584923109039664 in episode 670
Report: 
rewardSum:-480.7972238154728
height:-0.3108865436639108
loss:57.584923109039664
qAverage:[-59.30909762229069]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -465.8861176312911 with loss 61.18338535539806 in episode 671
Report: 
rewardSum:-465.8861176312911
height:-0.33189811611526326
loss:61.18338535539806
qAverage:[-60.47964434807573]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -450.4096390651643 with loss 53.3875398254022 in episode 672
Report: 
rewardSum:-450.4096390651643
height:-0.25316608185120176
loss:53.3875398254022
qAverage:[-65.11523032406032]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -458.25412410078593 with loss 50.816102277487516 in episode 673
Report: 
rewardSum:-458.25412410078593
height:-0.16211814049495168
loss:50.816102277487516
qAverage:[-59.79768125491567]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -462.2079215962675 with loss 51.78908134251833 in episode 674
Report: 
rewardSum:-462.2079215962675
height:-0.2459044810699775
loss:51.78908134251833
qAverage:[-60.35229047483737]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -433.1849494812047 with loss 49.34536582697183 in episode 675
Report: 
rewardSum:-433.1849494812047
height:-0.2665962621478526
loss:49.34536582697183
qAverage:[-62.91310058766274]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -460.11946698084324 with loss 58.920886986888945 in episode 676
Report: 
rewardSum:-460.11946698084324
height:-0.21292986539049044
loss:58.920886986888945
qAverage:[-64.04334020378566]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -451.98763122378153 with loss 53.9240819234401 in episode 677
Report: 
rewardSum:-451.98763122378153
height:-0.17733309680522463
loss:53.9240819234401
qAverage:[-63.3245833791276]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -466.95354299886117 with loss 41.93445133510977 in episode 678
Report: 
rewardSum:-466.95354299886117
height:-0.2779299653770176
loss:41.93445133510977
qAverage:[-62.331319343984426]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -459.11243657717546 with loss 50.24166037607938 in episode 679
Report: 
rewardSum:-459.11243657717546
height:0.2825785101166311
loss:50.24166037607938
qAverage:[-63.48745409568938]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -468.2914113824715 with loss 44.23896389827132 in episode 680
Report: 
rewardSum:-468.2914113824715
height:-0.195254669257353
loss:44.23896389827132
qAverage:[-59.90064250968397]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -445.59851178212097 with loss 51.82651389949024 in episode 681
Report: 
rewardSum:-445.59851178212097
height:-0.0003247637230547242
loss:51.82651389949024
qAverage:[-64.16011288519857]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -470.4419809411248 with loss 55.76964773051441 in episode 682
Report: 
rewardSum:-470.4419809411248
height:-0.27680490791064577
loss:55.76964773051441
qAverage:[-64.51326407455093]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -465.02399423651565 with loss 55.37180954776704 in episode 683
Report: 
rewardSum:-465.02399423651565
height:0.02118485421225308
loss:55.37180954776704
qAverage:[-64.73514333530446]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -436.9634261708395 with loss 49.89965198934078 in episode 684
Report: 
rewardSum:-436.9634261708395
height:0.14583486399002765
loss:49.89965198934078
qAverage:[-65.7027321741059]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -455.70398700088407 with loss 52.34554929472506 in episode 685
Report: 
rewardSum:-455.70398700088407
height:-0.19849136791642769
loss:52.34554929472506
qAverage:[-64.56151340723038]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -468.0865751834387 with loss 51.22401964850724 in episode 686
Report: 
rewardSum:-468.0865751834387
height:-0.20855472132352992
loss:51.22401964850724
qAverage:[-60.83449662153167]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -479.3275859743001 with loss 52.14997763931751 in episode 687
Report: 
rewardSum:-479.3275859743001
height:-0.040592700174504504
loss:52.14997763931751
qAverage:[-64.63632010455107]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -450.89977633513956 with loss 59.55115593224764 in episode 688
Report: 
rewardSum:-450.89977633513956
height:-0.12593493112710333
loss:59.55115593224764
qAverage:[-64.76366703664486]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -446.5412952078 with loss 50.51582996081561 in episode 689
Report: 
rewardSum:-446.5412952078
height:-0.10280955653499961
loss:50.51582996081561
qAverage:[-65.4406155359627]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -455.3481627213331 with loss 41.56411753408611 in episode 690
Report: 
rewardSum:-455.3481627213331
height:0.13854191253630788
loss:41.56411753408611
qAverage:[-64.93911551372172]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -452.3573085887946 with loss 58.592982180416584 in episode 691
Report: 
rewardSum:-452.3573085887946
height:-0.22548001512978538
loss:58.592982180416584
qAverage:[-66.98458784759634]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -373.02465010377534 with loss 45.22487303055823 in episode 692
Report: 
rewardSum:-373.02465010377534
height:0.511911323063729
loss:45.22487303055823
qAverage:[-68.04699986207419]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -447.9669290596715 with loss 61.0806232355535 in episode 693
Report: 
rewardSum:-447.9669290596715
height:-0.20079013122481276
loss:61.0806232355535
qAverage:[-61.22532601475716]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -463.3756101820341 with loss 64.19027909822762 in episode 694
Report: 
rewardSum:-463.3756101820341
height:-0.20199846691877274
loss:64.19027909822762
qAverage:[-65.94756553963859]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -435.369592156385 with loss 56.50278489291668 in episode 695
Report: 
rewardSum:-435.369592156385
height:-0.3049852849251235
loss:56.50278489291668
qAverage:[-65.66983933409153]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -470.27985810716785 with loss 63.793594216927886 in episode 696
Report: 
rewardSum:-470.27985810716785
height:-0.125437440750615
loss:63.793594216927886
qAverage:[-65.45974632163546]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -428.0371317156702 with loss 52.00622527487576 in episode 697
Report: 
rewardSum:-428.0371317156702
height:-0.16067453232182133
loss:52.00622527487576
qAverage:[-64.67261838912964]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -430.95903177359054 with loss 50.2712018750608 in episode 698
Report: 
rewardSum:-430.95903177359054
height:-0.22840268965608163
loss:50.2712018750608
qAverage:[-65.6352768871025]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -455.65807310498377 with loss 41.03168161120266 in episode 699
Report: 
rewardSum:-455.65807310498377
height:-0.07955479251120505
loss:41.03168161120266
qAverage:[-64.5203163398439]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -433.047294263885 with loss 58.20361502468586 in episode 700
Report: 
rewardSum:-433.047294263885
height:-0.1296390768964262
loss:58.20361502468586
qAverage:[-63.54466501149264]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -447.199102242105 with loss 49.08596825506538 in episode 701
Report: 
rewardSum:-447.199102242105
height:-0.2229853863021512
loss:49.08596825506538
qAverage:[-61.81082397452233]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -442.48567947324216 with loss 56.30759074911475 in episode 702
Report: 
rewardSum:-442.48567947324216
height:-0.09966509593785133
loss:56.30759074911475
qAverage:[-66.96246797318506]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -471.38007533494914 with loss 56.53372373059392 in episode 703
Report: 
rewardSum:-471.38007533494914
height:-0.28383146001267856
loss:56.53372373059392
qAverage:[-62.54135227562794]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -456.0811995879498 with loss 53.67626506462693 in episode 704
Report: 
rewardSum:-456.0811995879498
height:0.14127334440069486
loss:53.67626506462693
qAverage:[-65.93336414464629]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -354.3393642573678 with loss 38.41610127314925 in episode 705
Report: 
rewardSum:-354.3393642573678
height:0.513791495211146
loss:38.41610127314925
qAverage:[-68.58770879899492]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -460.7697499250613 with loss 56.032570367679 in episode 706
Report: 
rewardSum:-460.7697499250613
height:-0.25191826675333295
loss:56.032570367679
qAverage:[-62.21332074126395]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -445.31550672992944 with loss 46.22299898043275 in episode 707
Report: 
rewardSum:-445.31550672992944
height:-0.04250997541148839
loss:46.22299898043275
qAverage:[-65.45072917938232]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -446.1225842547677 with loss 60.56980578787625 in episode 708
Report: 
rewardSum:-446.1225842547677
height:-0.27390819281205164
loss:60.56980578787625
qAverage:[-67.19994714319706]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -444.9532664818597 with loss 62.04673204664141 in episode 709
Report: 
rewardSum:-444.9532664818597
height:-0.10339362415376588
loss:62.04673204664141
qAverage:[-65.68326755873676]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -442.08221280812717 with loss 50.47302043624222 in episode 710
Report: 
rewardSum:-442.08221280812717
height:-0.09288419550365686
loss:50.47302043624222
qAverage:[-67.20489902652089]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -428.4224670969321 with loss 61.0024174656719 in episode 711
Report: 
rewardSum:-428.4224670969321
height:-0.2397155210267722
loss:61.0024174656719
qAverage:[-68.11085683166391]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -466.4031129165988 with loss 53.04974292218685 in episode 712
Report: 
rewardSum:-466.4031129165988
height:-0.27366204638686037
loss:53.04974292218685
qAverage:[-62.74081282794476]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -466.154872887436 with loss 45.49132293090224 in episode 713
Report: 
rewardSum:-466.154872887436
height:-0.2244035545496119
loss:45.49132293090224
qAverage:[-66.71423959376207]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -461.82403144523585 with loss 58.97563091292977 in episode 714
Report: 
rewardSum:-461.82403144523585
height:0.04769778385745755
loss:58.97563091292977
qAverage:[-66.22593570585867]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -452.99401625886173 with loss 65.67146314121783 in episode 715
Report: 
rewardSum:-452.99401625886173
height:0.032423230015013786
loss:65.67146314121783
qAverage:[-66.62781822681427]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -468.00761406413193 with loss 52.90139230154455 in episode 716
Report: 
rewardSum:-468.00761406413193
height:-0.1025423061950931
loss:52.90139230154455
qAverage:[-65.49924671590625]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -451.08170792231886 with loss 52.74642278254032 in episode 717
Report: 
rewardSum:-451.08170792231886
height:-0.29452788391605716
loss:52.74642278254032
qAverage:[-67.61227281176629]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -453.7868678611565 with loss 55.33589677140117 in episode 718
Report: 
rewardSum:-453.7868678611565
height:-0.17146476461042442
loss:55.33589677140117
qAverage:[-65.87166922458327]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -455.5933738967469 with loss 50.26424425840378 in episode 719
Report: 
rewardSum:-455.5933738967469
height:0.23120893332984846
loss:50.26424425840378
qAverage:[-65.84619709745569]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -430.9561932118984 with loss 58.598072873428464 in episode 720
Report: 
rewardSum:-430.9561932118984
height:-0.26466922691154626
loss:58.598072873428464
qAverage:[-67.0747411466471]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -434.3651185597315 with loss 62.70449412614107 in episode 721
Report: 
rewardSum:-434.3651185597315
height:0.04565011611263387
loss:62.70449412614107
qAverage:[-67.00669426467289]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -440.3474283076402 with loss 53.85908100940287 in episode 722
Report: 
rewardSum:-440.3474283076402
height:-0.023932318038881625
loss:53.85908100940287
qAverage:[-66.47343998970372]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -438.38152316858253 with loss 53.60155416466296 in episode 723
Report: 
rewardSum:-438.38152316858253
height:-0.05626769659423578
loss:53.60155416466296
qAverage:[-61.92941457272774]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -443.6295908500103 with loss 61.27389268390834 in episode 724
Report: 
rewardSum:-443.6295908500103
height:0.11361091037036099
loss:61.27389268390834
qAverage:[-64.77954550287616]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -455.36807481055223 with loss 53.72065119445324 in episode 725
Report: 
rewardSum:-455.36807481055223
height:-0.20036225767193588
loss:53.72065119445324
qAverage:[-66.38535621569524]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.14076718701557 with loss 45.60452010668814 in episode 726
Report: 
rewardSum:-477.14076718701557
height:0.14193572494647547
loss:45.60452010668814
qAverage:[-66.61023342844301]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -253.99171320173622 with loss 25.235590735450387 in episode 727
Report: 
rewardSum:-253.99171320173622
height:0.5187203414094449
loss:25.235590735450387
qAverage:[-70.80196793611384]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -469.6481624917056 with loss 57.474605679512024 in episode 728
Report: 
rewardSum:-469.6481624917056
height:0.007311220047764012
loss:57.474605679512024
qAverage:[-66.37215767284431]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -472.4857309035667 with loss 54.44486944377422 in episode 729
Report: 
rewardSum:-472.4857309035667
height:-0.1590145208517835
loss:54.44486944377422
qAverage:[-66.31898095949211]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -454.9464089320601 with loss 54.69845114182681 in episode 730
Report: 
rewardSum:-454.9464089320601
height:-0.04380187457637001
loss:54.69845114182681
qAverage:[-67.53073351958702]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -464.87073004750624 with loss 53.43069165479392 in episode 731
Report: 
rewardSum:-464.87073004750624
height:0.07067977241443717
loss:53.43069165479392
qAverage:[-67.34159797518124]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -456.6448466573892 with loss 59.026291854679585 in episode 732
Report: 
rewardSum:-456.6448466573892
height:0.202343165585494
loss:59.026291854679585
qAverage:[-67.4951339259301]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -399.6365192514535 with loss 46.30225935578346 in episode 733
Report: 
rewardSum:-399.6365192514535
height:0.5024945159600998
loss:46.30225935578346
qAverage:[-67.9904005444465]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -437.94221804750913 with loss 53.79766957089305 in episode 734
Report: 
rewardSum:-437.94221804750913
height:-0.0021350152097886596
loss:53.79766957089305
qAverage:[-67.59235460434726]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -474.7999159057042 with loss 51.8435976523906 in episode 735
Report: 
rewardSum:-474.7999159057042
height:-0.030458577537001968
loss:51.8435976523906
qAverage:[-65.94779599775183]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -477.36177927015194 with loss 58.3297671508044 in episode 736
Report: 
rewardSum:-477.36177927015194
height:-0.07898757634141257
loss:58.3297671508044
qAverage:[-68.10243301580448]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -444.9989143360857 with loss 47.87493065558374 in episode 737
Report: 
rewardSum:-444.9989143360857
height:0.05912039751721633
loss:47.87493065558374
qAverage:[-67.4301715954184]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -411.2008302576909 with loss 46.65693338215351 in episode 738
Report: 
rewardSum:-411.2008302576909
height:0.5001515670436656
loss:46.65693338215351
qAverage:[-68.30783842057745]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -454.7463675376993 with loss 50.93558397702873 in episode 739
Report: 
rewardSum:-454.7463675376993
height:-0.17255074122340672
loss:50.93558397702873
qAverage:[-71.73723076129782]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -460.54963403033577 with loss 45.319983612746 in episode 740
Report: 
rewardSum:-460.54963403033577
height:-0.0592307388735361
loss:45.319983612746
qAverage:[-67.60256828345689]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -460.056212017529 with loss 49.7117915879935 in episode 741
Report: 
rewardSum:-460.056212017529
height:-0.08715640087956913
loss:49.7117915879935
qAverage:[-69.33488865752719]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -458.49592585999693 with loss 48.758983908221126 in episode 742
Report: 
rewardSum:-458.49592585999693
height:0.213050852327592
loss:48.758983908221126
qAverage:[-66.20512210732639]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -456.3717850630905 with loss 58.74319018796086 in episode 743
Report: 
rewardSum:-456.3717850630905
height:-0.12672186277349776
loss:58.74319018796086
qAverage:[-62.55296339087225]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -438.14002041290354 with loss 50.67293422482908 in episode 744
Report: 
rewardSum:-438.14002041290354
height:-0.17368905377275826
loss:50.67293422482908
qAverage:[-63.8434812931141]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -435.43712029003166 with loss 53.63693887554109 in episode 745
Report: 
rewardSum:-435.43712029003166
height:-0.23309477919122557
loss:53.63693887554109
qAverage:[-69.60664819592806]
libpng warning: iCCP: known incorrect sRGB profile
now epsilon is 0.01, the reward is -443.19650974247475 with loss 54.75808715354651 in episode 746
Report: 
rewardSum:-443.19650974247475
height:-0.27191836984016043
loss:54.75808715354651
qAverage:[-69.67627136230469]
libpng warning: iCCP: known incorrect sRGB profile
pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)
Hello from the pygame community. https://www.pygame.org/contribute.html
2021-03-25 11:06:37.148929: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Traceback (most recent call last):
  File "agentRunner.py", line 18, in <module>
    from tensorflow_agents.deep_sea_graphical_pwddqn import DeepSeaTreasureGraphicalPDDQN
ModuleNotFoundError: No module named 'tensorflow_agents.deep_sea_graphical_pwddqn'
pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)
Hello from the pygame community. https://www.pygame.org/contribute.html
2021-03-25 11:08:34.549387: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Traceback (most recent call last):
  File "agentRunner.py", line 18, in <module>
    from tensorflow_agents.deep_sea_graphical_pwddqn import MultiObjectiveDeepSeaW
ModuleNotFoundError: No module named 'tensorflow_agents.deep_sea_graphical_pwddqn'
pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)
Hello from the pygame community. https://www.pygame.org/contribute.html
2021-03-25 11:09:14.090797: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Traceback (most recent call last):
  File "agentRunner.py", line 18, in <module>
    from tensorflow_agents.deep_sea_graphical_wpddqn import MultiObjectiveDeepSeaW
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 10, in <module>
    import psutil
ModuleNotFoundError: No module named 'psutil'
pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)
Hello from the pygame community. https://www.pygame.org/contribute.html
2021-03-25 11:09:40.204926: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Traceback (most recent call last):
  File "agentRunner.py", line 53, in <module>
    agent = MultiObjectiveDeepSeaW(2000)
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 224, in __init__
    self.env = DeepSeaTreasure(width=5, speed=1e8, graphical_state=True,
TypeError: __init__() got an unexpected keyword argument 'random_starts'
pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)
Hello from the pygame community. https://www.pygame.org/contribute.html
2021-03-25 11:10:44.497765: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33-		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		605		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[605.3333333333334, -10]
[1000, -12]
2021-03-25 11:10:46.738784: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 11:10:46.895491: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-25 11:10:47.214243: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2021-03-25 11:10:47.214298: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ml-vm): /proc/driver/nvidia/version does not exist
2021-03-25 11:10:47.215149: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
Traceback (most recent call last):
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/framework/ops.py", line 1853, in _create_c_op
    c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Negative dimension size caused by subtracting 8 from 2 for '{{node conv2d/Conv2D}} = Conv2D[T=DT_FLOAT, data_format="NCHW", dilations=[1, 1, 1, 1], explicit_paddings=[], padding="VALID", strides=[1, 1, 4, 4], use_cudnn_on_gpu=true](Placeholder, conv2d/Conv2D/ReadVariableOp)' with input shapes: [?,84,84,2], [8,8,84,32].

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "agentRunner.py", line 53, in <module>
    agent = MultiObjectiveDeepSeaW(2000)
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 226, in __init__
    self.agent = DQNAgent(stateShape=(84,84,2), actionSpace=self.env.get_action_space(), numPicks=32, memorySize=10000, numRewards=self.numRewards)
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 56, in __init__
    self.train_network = self.createNetwork(stateShape, len(actionSpace), self.alpha)
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 76, in createNetwork
    model.add(keras.layers.Conv2D(32, kernel_size=8, strides=4, activation='relu', data_format='channels_first'))
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py", line 517, in _method_wrapper
    result = method(self, *args, **kwargs)
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py", line 223, in add
    output_tensor = layer(self.outputs[0])
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py", line 951, in __call__
    return self._functional_construction_call(inputs, args, kwargs,
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py", line 1090, in _functional_construction_call
    outputs = self._keras_tensor_symbolic_call(
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py", line 822, in _keras_tensor_symbolic_call
    return self._infer_output_signature(inputs, args, kwargs, input_masks)
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py", line 863, in _infer_output_signature
    outputs = call_fn(inputs, *args, **kwargs)
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/keras/layers/convolutional.py", line 248, in call
    outputs = self._convolution_op(inputs, self.kernel)
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py", line 201, in wrapper
    return target(*args, **kwargs)
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py", line 1013, in convolution_v2
    return convolution_internal(
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py", line 1143, in convolution_internal
    return op(
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py", line 2597, in _conv2d_expanded_batch
    return gen_nn_ops.conv2d(
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py", line 969, in conv2d
    _, _, _op, _outputs = _op_def_library._apply_op_helper(
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py", line 748, in _apply_op_helper
    op = g._create_op_internal(op_type_name, inputs, dtypes=None,
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py", line 590, in _create_op_internal
    return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/framework/ops.py", line 3528, in _create_op_internal
    ret = Operation(
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/framework/ops.py", line 2015, in __init__
    self._c_op = _create_c_op(self._graph, node_def, inputs,
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/framework/ops.py", line 1856, in _create_c_op
    raise ValueError(str(e))
ValueError: Negative dimension size caused by subtracting 8 from 2 for '{{node conv2d/Conv2D}} = Conv2D[T=DT_FLOAT, data_format="NCHW", dilations=[1, 1, 1, 1], explicit_paddings=[], padding="VALID", strides=[1, 1, 4, 4], use_cudnn_on_gpu=true](Placeholder, conv2d/Conv2D/ReadVariableOp)' with input shapes: [?,84,84,2], [8,8,84,32].
pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)
Hello from the pygame community. https://www.pygame.org/contribute.html
2021-03-25 11:11:48.353662: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
############# STATE ###############
0*		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37x		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		490		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[490.21, -10]
[1000, -13]
2021-03-25 11:11:50.359490: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 11:11:50.360349: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-25 11:11:50.491084: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2021-03-25 11:11:50.491147: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ml-vm): /proc/driver/nvidia/version does not exist
2021-03-25 11:11:50.492122: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
Traceback (most recent call last):
  File "agentRunner.py", line 53, in <module>
    agent = MultiObjectiveDeepSeaW(2000)
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 226, in __init__
    self.agent = DQNAgent(stateShape=(2,84,84), actionSpace=self.env.get_action_space(), numPicks=32, memorySize=10000, numRewards=self.numRewards)
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 60, in __init__
    self.policy_train_weights = [deepcopy(self.train_network.get_weights())] * self.numRewards
AttributeError: 'DQNAgent' object has no attribute 'numRewards'
pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)
Hello from the pygame community. https://www.pygame.org/contribute.html
2021-03-25 11:13:19.685150: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
2021-03-25 11:13:21.683419: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 11:13:21.684288: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-25 11:13:21.818767: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2021-03-25 11:13:21.818823: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ml-vm): /proc/driver/nvidia/version does not exist
2021-03-25 11:13:21.819613: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
Traceback (most recent call last):
  File "agentRunner.py", line 57, in <module>
    agent.train()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 231, in train
    self.episode()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 261, in episode
    self.agent.addMemory(state, action, policy, reward, nextState, done)
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 197, in addMemory
    self.replayMemory[i].add(state, action, policy, reward[i], nextState, done)
  File "/home/luke/MODRL/replay_buffer.py", line 103, in add
    super().add(*args, **kwargs)
TypeError: add() takes 6 positional arguments but 7 were given
pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)
Hello from the pygame community. https://www.pygame.org/contribute.html
2021-03-25 11:20:06.995377: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
2021-03-25 11:20:08.935701: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 11:20:08.936567: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-25 11:20:09.072137: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2021-03-25 11:20:09.072196: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ml-vm): /proc/driver/nvidia/version does not exist
2021-03-25 11:20:09.072966: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.9982513119532618, the reward is 56.6875 with loss [0, 0] in episode 0
Traceback (most recent call last):
  File "agentRunner.py", line 57, in <module>
    agent.train()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 231, in train
    self.episode()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 276, in episode
    self.plot()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 297, in plot
    self.ax[0].clear()
AttributeError: 'numpy.ndarray' object has no attribute 'clear'
pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)
Hello from the pygame community. https://www.pygame.org/contribute.html
2021-03-25 11:22:02.468432: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
2021-03-25 11:22:04.288899: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 11:22:04.289749: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-25 11:22:04.424668: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2021-03-25 11:22:04.424744: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ml-vm): /proc/driver/nvidia/version does not exist
2021-03-25 11:22:04.425492: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.9990003749375039, the reward is -3.0 with loss [0, 0] in episode 0
No handles with labels found to put in legend.
Report: 
rewardSum:-3.0
loss:[0, 0]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:1035.0
Traceback (most recent call last):
  File "agentRunner.py", line 57, in <module>
    agent.train()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 230, in train
    self.episode()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 261, in episode
    loss = self.agent.trainDQN()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 97, in trainDQN
    samples = self.replayMemory[i].sample(self.numPicks, beta)
  File "/home/luke/MODRL/replay_buffer_policy.py", line 167, in sample
    encoded_sample = self._encode_sample(idxes)
  File "/home/luke/MODRL/replay_buffer_policy.py", line 34, in _encode_sample
    obses_t, actions, policies, rewards, obses_tp1, dones = [], [], [], [], []
ValueError: not enough values to unpack (expected 6, got 5)
pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)
Hello from the pygame community. https://www.pygame.org/contribute.html
2021-03-25 11:22:39.790379: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
2021-03-25 11:22:41.608809: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 11:22:41.610101: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-25 11:22:41.740285: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2021-03-25 11:22:41.740340: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ml-vm): /proc/driver/nvidia/version does not exist
2021-03-25 11:22:41.741123: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.9975028106258202, the reward is 552.6875 with loss [0, 0] in episode 0
No handles with labels found to put in legend.
Report: 
rewardSum:552.6875
loss:[0, 0]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:1022.0
now epsilon is 0.996754870534042, the reward is -2.0 with loss [0, 0] in episode 1
Report: 
rewardSum:-2.0
loss:[0, 0]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:1022.0
now epsilon is 0.9962565553959545, the reward is -1.0 with loss [0, 0] in episode 2
Report: 
rewardSum:-1.0
loss:[0, 0]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:1022.0
Traceback (most recent call last):
  File "agentRunner.py", line 57, in <module>
    agent.train()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 230, in train
    self.episode()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 261, in episode
    loss = self.agent.trainDQN()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 100, in trainDQN
    currStates = np.squeeze(np.array(currStates), 1)
  File "<__array_function__ internals>", line 5, in squeeze
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/numpy/core/fromnumeric.py", line 1495, in squeeze
    return squeeze(axis=axis)
ValueError: cannot select an axis to squeeze out which has size not equal to one
pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)
Hello from the pygame community. https://www.pygame.org/contribute.html
2021-03-25 11:23:53.893376: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
2021-03-25 11:23:55.748768: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 11:23:55.750136: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-25 11:23:55.881966: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2021-03-25 11:23:55.882034: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ml-vm): /proc/driver/nvidia/version does not exist
2021-03-25 11:23:55.882991: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.9987506248437695, the reward is -4.0 with loss [0, 0] in episode 0
No handles with labels found to put in legend.
Report: 
rewardSum:-4.0
loss:[0, 0]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:1033.0
now epsilon is 0.996754870534042, the reward is -7.0 with loss [0, 0] in episode 1
Report: 
rewardSum:-7.0
loss:[0, 0]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:1033.0
Traceback (most recent call last):
  File "agentRunner.py", line 57, in <module>
    agent.train()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 230, in train
    self.episode()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 261, in episode
    loss = self.agent.trainDQN()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 112, in trainDQN
    Q_currents_all = self.train_network(currStates, training=False).numpy()
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py", line 1012, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py", line 375, in call
    return super(Sequential, self).call(inputs, training=training, mask=mask)
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py", line 424, in call
    return self._run_internal_graph(
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py", line 560, in _run_internal_graph
    outputs = node.layer(*args, **kwargs)
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py", line 1012, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/keras/layers/convolutional.py", line 248, in call
    outputs = self._convolution_op(inputs, self.kernel)
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py", line 201, in wrapper
    return target(*args, **kwargs)
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py", line 1013, in convolution_v2
    return convolution_internal(
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py", line 1143, in convolution_internal
    return op(
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py", line 2597, in _conv2d_expanded_batch
    return gen_nn_ops.conv2d(
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py", line 932, in conv2d
    _ops.raise_from_not_ok_status(e, name)
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/framework/ops.py", line 6862, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File "<string>", line 3, in raise_from
tensorflow.python.framework.errors_impl.UnimplementedError: The Conv2D op currently only supports the NHWC tensor format on the CPU. The op was given the format: NCHW [Op:Conv2D]
pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)
Hello from the pygame community. https://www.pygame.org/contribute.html
2021-03-25 11:29:34.115934: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
############# STATE ###############
0*		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
2021-03-25 11:29:36.663439: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 11:29:36.666970: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-25 11:29:36.797560: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2021-03-25 11:29:36.797617: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ml-vm): /proc/driver/nvidia/version does not exist
2021-03-25 11:29:36.798509: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
############# STATE ###############
0*		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.9995000625, the reward is -1.0 with loss [0, 0] in episode 0
No handles with labels found to put in legend.
Report: 
rewardSum:-1.0
loss:[0, 0]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:986.0
now epsilon is 0.9977522486879922, the reward is 244.25 with loss [0, 0] in episode 1
Report: 
rewardSum:244.25
loss:[0, 0]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:986.0
now epsilon is 0.9972534349231639, the reward is -1.0 with loss [0, 0] in episode 2
Report: 
rewardSum:-1.0
loss:[0, 0]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:986.0
Traceback (most recent call last):
  File "agentRunner.py", line 57, in <module>
    agent.train()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 230, in train
    self.episode()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 261, in episode
    loss = self.agent.trainDQN()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 112, in trainDQN
    Q_currents_all = self.train_network(currStates, training=False).numpy()
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py", line 1012, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py", line 375, in call
    return super(Sequential, self).call(inputs, training=training, mask=mask)
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py", line 424, in call
    return self._run_internal_graph(
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py", line 560, in _run_internal_graph
    outputs = node.layer(*args, **kwargs)
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py", line 1012, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/keras/layers/convolutional.py", line 248, in call
    outputs = self._convolution_op(inputs, self.kernel)
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py", line 201, in wrapper
    return target(*args, **kwargs)
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py", line 1013, in convolution_v2
    return convolution_internal(
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py", line 1143, in convolution_internal
    return op(
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py", line 2597, in _conv2d_expanded_batch
    return gen_nn_ops.conv2d(
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py", line 932, in conv2d
    _ops.raise_from_not_ok_status(e, name)
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/framework/ops.py", line 6862, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File "<string>", line 3, in raise_from
tensorflow.python.framework.errors_impl.UnimplementedError: The Conv2D op currently only supports the NHWC tensor format on the CPU. The op was given the format: NCHW [Op:Conv2D]
pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)
Hello from the pygame community. https://www.pygame.org/contribute.html
2021-03-25 12:30:52.126833: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Traceback (most recent call last):
  File "agentRunner.py", line 57, in <module>
    agent.train()
NameError: name 'agent' is not defined
pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)
Hello from the pygame community. https://www.pygame.org/contribute.html
2021-03-25 12:31:09.321752: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33-		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		605		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[605.3333333333334, -10]
[1000, -12]
2021-03-25 12:31:11.386634: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 12:31:11.387638: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-25 12:31:12.518065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0001:00:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 149.31GiB/s
2021-03-25 12:31:12.518145: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 12:31:12.524223: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 12:31:12.524370: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 12:31:12.525576: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-25 12:31:12.525900: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-25 12:31:12.526968: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-25 12:31:12.527868: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-25 12:31:12.528054: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 12:31:12.529330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-25 12:31:12.530072: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 12:31:12.530704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0001:00:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 149.31GiB/s
2021-03-25 12:31:12.530755: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 12:31:12.530799: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 12:31:12.530838: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 12:31:12.530875: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-25 12:31:12.530912: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-25 12:31:12.530970: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-25 12:31:12.531009: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-25 12:31:12.531047: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 12:31:12.532152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-25 12:31:12.577460: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 12:31:20.097371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-25 12:31:20.097442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-25 12:31:20.097456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-25 12:31:20.099578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7468 MB memory) -> physical GPU (device: 0, name: Tesla M60, pci bus id: 0001:00:00.0, compute capability: 5.2)
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33-		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		605		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[605.3333333333334, -10]
[1000, -12]
now epsilon is 0.9987506248437695, the reward is 193.33333333333334 with loss [0, 0] in episode 0
No handles with labels found to put in legend.
Report: 
rewardSum:193.33333333333334
loss:[0, 0]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:1586.0
now epsilon is 0.9982513119532618, the reward is -1.0 with loss [0, 0] in episode 1
Report: 
rewardSum:-1.0
loss:[0, 0]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:1586.0
2021-03-25 12:31:21.151461: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 12:31:45.415585: W tensorflow/stream_executor/gpu/asm_compiler.cc:98] *** WARNING *** You are using ptxas 9.1.108, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You do not need to update to CUDA 9.2.88; cherry-picking the ptxas binary is sufficient.
2021-03-25 12:31:45.654158: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 12:31:50.876019: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 12:31:51.451577: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-03-25 12:31:51.454814: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2596990000 Hz
Traceback (most recent call last):
  File "agentRunner.py", line 57, in <module>
    agent.train()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 230, in train
    self.episode()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 261, in episode
    loss = self.agent.trainDQN()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 151, in trainDQN
    ((self.walpha**self.delay) * (Q_currents_np - ((rewardNP * donesNP) + (self.gamma * Q_futures_np) * notDonesNP)))
AttributeError: 'DQNAgent' object has no attribute 'delay'
pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)
Hello from the pygame community. https://www.pygame.org/contribute.html
2021-03-25 12:33:18.232254: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
############# STATE ###############
0*		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25x		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37x		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		161		490		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[161.24, -7]
[490.21, -10]
[1000, -13]
2021-03-25 12:33:20.191771: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 12:33:20.192823: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-25 12:33:21.230132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0001:00:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 149.31GiB/s
2021-03-25 12:33:21.230217: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 12:33:21.235779: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 12:33:21.235875: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 12:33:21.237042: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-25 12:33:21.237349: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-25 12:33:21.238418: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-25 12:33:21.239313: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-25 12:33:21.239498: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 12:33:21.240766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-25 12:33:21.241965: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 12:33:21.242594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0001:00:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 149.31GiB/s
2021-03-25 12:33:21.242646: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 12:33:21.242691: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 12:33:21.242737: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 12:33:21.242782: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-25 12:33:21.242827: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-25 12:33:21.242872: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-25 12:33:21.242917: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-25 12:33:21.242963: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 12:33:21.244072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-25 12:33:21.244159: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 12:33:21.836509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-25 12:33:21.836589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-25 12:33:21.836602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-25 12:33:21.838668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7468 MB memory) -> physical GPU (device: 0, name: Tesla M60, pci bus id: 0001:00:00.0, compute capability: 5.2)
############# STATE ###############
0*		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25x		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37x		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		161		490		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[161.24, -7]
[490.21, -10]
[1000, -13]
now epsilon is 0.9995000625, the reward is -1.0 with loss [0, 0] in episode 0
No handles with labels found to put in legend.
Report: 
rewardSum:-1.0
loss:[0, 0]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:1587.0
now epsilon is 0.9985009371875586, the reward is 157.24 with loss [0, 0] in episode 1
Report: 
rewardSum:157.24
loss:[0, 0]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:1587.0
2021-03-25 12:33:22.817747: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 12:33:23.625504: W tensorflow/stream_executor/gpu/asm_compiler.cc:98] *** WARNING *** You are using ptxas 9.1.108, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You do not need to update to CUDA 9.2.88; cherry-picking the ptxas binary is sufficient.
2021-03-25 12:33:23.687561: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 12:33:23.923079: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 12:33:24.348216: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-03-25 12:33:24.351005: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2596990000 Hz
Traceback (most recent call last):
  File "agentRunner.py", line 57, in <module>
    agent.train()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 232, in train
    self.episode()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 263, in episode
    loss = self.agent.trainDQN()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 152, in trainDQN
    w_train[np.arange(len(inverted_policy_mask)), policiesNP] = ((1-self.walpha) * w_target) + \
ValueError: operands could not be broadcast together with shapes (32,2) (32,) 
pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)
Hello from the pygame community. https://www.pygame.org/contribute.html
2021-03-25 12:40:44.823867: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33-		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		605		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[605.3333333333334, -10]
[1000, -12]
2021-03-25 12:40:46.697354: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 12:40:46.698876: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-25 12:40:47.718727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0001:00:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 149.31GiB/s
2021-03-25 12:40:47.718801: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 12:40:47.724335: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 12:40:47.724454: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 12:40:47.725644: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-25 12:40:47.726430: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-25 12:40:47.727492: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-25 12:40:47.728368: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-25 12:40:47.728559: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 12:40:47.730284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-25 12:40:47.731444: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 12:40:47.732066: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0001:00:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 149.31GiB/s
2021-03-25 12:40:47.732117: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 12:40:47.732161: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 12:40:47.732204: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 12:40:47.732245: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-25 12:40:47.732286: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-25 12:40:47.732327: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-25 12:40:47.732369: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-25 12:40:47.732411: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 12:40:47.733526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-25 12:40:47.733598: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 12:40:48.313003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-25 12:40:48.313067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-25 12:40:48.313080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-25 12:40:48.315044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7468 MB memory) -> physical GPU (device: 0, name: Tesla M60, pci bus id: 0001:00:00.0, compute capability: 5.2)
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33-		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		605		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[605.3333333333334, -10]
[1000, -12]
now epsilon is 0.9977522486879922, the reward is 991.0 with loss [0, 0] in episode 0
No handles with labels found to put in legend.
Report: 
rewardSum:991.0
loss:[0, 0]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:1582.0
now epsilon is 0.9960074912571055, the reward is 598.3333333333334 with loss [0, 0] in episode 1
Report: 
rewardSum:598.3333333333334
loss:[0, 0]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:1582.0
2021-03-25 12:40:49.212847: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 12:40:49.996208: W tensorflow/stream_executor/gpu/asm_compiler.cc:98] *** WARNING *** You are using ptxas 9.1.108, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You do not need to update to CUDA 9.2.88; cherry-picking the ptxas binary is sufficient.
2021-03-25 12:40:50.060656: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 12:40:50.308439: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 12:40:50.761553: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-03-25 12:40:50.764094: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2596990000 Hz
Traceback (most recent call last):
  File "agentRunner.py", line 57, in <module>
    agent.train()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 228, in train
    self.episode()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 259, in episode
    loss = self.agent.trainDQN()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 149, in trainDQN
    w_train[np.arange(len(inverted_policy_mask)), policiesNP] = ((1-self.walpha) * w_train) + \
ValueError: operands could not be broadcast together with shapes (32,2) (32,) 
pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)
Hello from the pygame community. https://www.pygame.org/contribute.html
2021-03-25 12:45:08.779884: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13-		22-		31-		40-		
5x		14x		23-		32-		41-		
6x		15x		24x		33-		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		111		309		605		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[111.99999999999997, -6]
[309.33333333333337, -8]
[605.3333333333334, -10]
[1000, -12]
2021-03-25 12:45:10.667615: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 12:45:10.668439: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-25 12:45:11.657664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0001:00:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 149.31GiB/s
2021-03-25 12:45:11.657743: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 12:45:11.663389: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 12:45:11.663467: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 12:45:11.664635: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-25 12:45:11.665528: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-25 12:45:11.666630: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-25 12:45:11.667560: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-25 12:45:11.667740: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 12:45:11.668984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-25 12:45:11.669652: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 12:45:11.670271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0001:00:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 149.31GiB/s
2021-03-25 12:45:11.670313: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 12:45:11.670349: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 12:45:11.670383: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 12:45:11.670417: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-25 12:45:11.670451: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-25 12:45:11.670483: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-25 12:45:11.670517: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-25 12:45:11.670551: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 12:45:11.671645: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-25 12:45:11.671705: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 12:45:12.248366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-25 12:45:12.248429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-25 12:45:12.248443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-25 12:45:12.250346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7468 MB memory) -> physical GPU (device: 0, name: Tesla M60, pci bus id: 0001:00:00.0, compute capability: 5.2)
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13-		22-		31-		40-		
5x		14x		23-		32-		41-		
6x		15x		24x		33-		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		111		309		605		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[111.99999999999997, -6]
[309.33333333333337, -8]
[605.3333333333334, -10]
[1000, -12]
now epsilon is 0.9995000625, the reward is -1.0 with loss [0, 0] in episode 0
No handles with labels found to put in legend.
Report: 
rewardSum:-1.0
loss:[0, 0]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:1585.0
2021-03-25 12:45:13.098233: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 12:45:13.885349: W tensorflow/stream_executor/gpu/asm_compiler.cc:98] *** WARNING *** You are using ptxas 9.1.108, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You do not need to update to CUDA 9.2.88; cherry-picking the ptxas binary is sufficient.
2021-03-25 12:45:13.942815: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 12:45:14.175049: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 12:45:14.600011: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-03-25 12:45:14.602552: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2596990000 Hz
Traceback (most recent call last):
  File "agentRunner.py", line 57, in <module>
    agent.train()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 228, in train
    self.episode()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 259, in episode
    loss = self.agent.trainDQN()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 156, in trainDQN
    self.replayMemory.update_priorities(indices, prios)
AttributeError: 'list' object has no attribute 'update_priorities'
pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)
Hello from the pygame community. https://www.pygame.org/contribute.html
2021-03-25 12:45:47.937596: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13-		22-		31-		40-		
5x		14x		23-		32-		41-		
6x		15x		24x		33-		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		111		309		605		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[111.99999999999997, -6]
[309.33333333333337, -8]
[605.3333333333334, -10]
[1000, -12]
2021-03-25 12:45:49.783735: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 12:45:49.784600: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-25 12:45:50.813762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0001:00:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 149.31GiB/s
2021-03-25 12:45:50.813841: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 12:45:50.819372: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 12:45:50.819469: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 12:45:50.820630: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-25 12:45:50.821625: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-25 12:45:50.822878: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-25 12:45:50.823788: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-25 12:45:50.823980: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 12:45:50.825234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-25 12:45:50.826404: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 12:45:50.827032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0001:00:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 149.31GiB/s
2021-03-25 12:45:50.827081: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 12:45:50.827122: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 12:45:50.827163: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 12:45:50.827202: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-25 12:45:50.827240: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-25 12:45:50.827278: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-25 12:45:50.827316: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-25 12:45:50.827354: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 12:45:50.828460: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-25 12:45:50.828549: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 12:45:51.400494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-25 12:45:51.400570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-25 12:45:51.400584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-25 12:45:51.402518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7468 MB memory) -> physical GPU (device: 0, name: Tesla M60, pci bus id: 0001:00:00.0, compute capability: 5.2)
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13-		22-		31-		40-		
5x		14x		23-		32-		41-		
6x		15x		24x		33-		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		111		309		605		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[111.99999999999997, -6]
[309.33333333333337, -8]
[605.3333333333334, -10]
[1000, -12]
now epsilon is 0.9995000625, the reward is -1.0 with loss [0, 0] in episode 0
No handles with labels found to put in legend.
Report: 
rewardSum:-1.0
loss:[0, 0]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:1592.0
now epsilon is 0.9985009371875586, the reward is -3.0 with loss [0, 0] in episode 1
Report: 
rewardSum:-3.0
loss:[0, 0]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:1592.0
2021-03-25 12:45:52.327055: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 12:45:53.113237: W tensorflow/stream_executor/gpu/asm_compiler.cc:98] *** WARNING *** You are using ptxas 9.1.108, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You do not need to update to CUDA 9.2.88; cherry-picking the ptxas binary is sufficient.
2021-03-25 12:45:53.170267: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 12:45:53.402081: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 12:45:53.833924: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-03-25 12:45:53.836461: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2596990000 Hz
now epsilon is 0.9957584893842912, the reward is 298.33333333333337 with loss [2.478379726409912, 2.4726014137268066] in episode 2
Report: 
rewardSum:298.33333333333337
loss:[2.478379726409912, 2.4726014137268066]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2276.0
now epsilon is 0.995011857206411, the reward is -2.0 with loss [7.399265393614769, 14.617675304412842] in episode 3
Report: 
rewardSum:-2.0
loss:[7.399265393614769, 14.617675304412842]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2296.0
Traceback (most recent call last):
  File "agentRunner.py", line 57, in <module>
    agent.train()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 227, in train
    self.episode()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 245, in episode
    action, policy, qs, ws, random = self.agent.selectAction(state)
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 185, in selectAction
    pred = np.squeeze(self.train_network(state, training=False).numpy(), axis=0)
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py", line 1012, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py", line 375, in call
    return super(Sequential, self).call(inputs, training=training, mask=mask)
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py", line 424, in call
    return self._run_internal_graph(
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py", line 536, in _run_internal_graph
    input_t._keras_mask = mask
AttributeError: 'LazyFrames' object has no attribute '_keras_mask'
pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)
Hello from the pygame community. https://www.pygame.org/contribute.html
2021-03-25 12:47:56.829695: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
############# STATE ###############
0*		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14-		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37x		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		91		251		490		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[91.21000000000001, -6]
[251.25, -8]
[490.21, -10]
[1000, -13]
2021-03-25 12:47:58.682592: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 12:47:58.683438: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-25 12:47:59.661567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0001:00:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 149.31GiB/s
2021-03-25 12:47:59.661633: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 12:47:59.667675: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 12:47:59.667756: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 12:47:59.668925: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-25 12:47:59.669727: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-25 12:47:59.670808: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-25 12:47:59.671723: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-25 12:47:59.671899: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 12:47:59.673130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-25 12:47:59.674321: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 12:47:59.674959: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0001:00:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 149.31GiB/s
2021-03-25 12:47:59.674998: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 12:47:59.675031: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 12:47:59.675061: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 12:47:59.675090: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-25 12:47:59.675119: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-25 12:47:59.675148: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-25 12:47:59.675177: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-25 12:47:59.675205: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 12:47:59.676343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-25 12:47:59.676405: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 12:48:00.244788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-25 12:48:00.245008: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-25 12:48:00.245019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-25 12:48:00.246950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7468 MB memory) -> physical GPU (device: 0, name: Tesla M60, pci bus id: 0001:00:00.0, compute capability: 5.2)
############# STATE ###############
0*		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14-		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37x		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		91		251		490		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[91.21000000000001, -6]
[251.25, -8]
[490.21, -10]
[1000, -13]
now epsilon is 0.9987506248437695, the reward is -4.0 with loss [0, 0] in episode 0
No handles with labels found to put in legend.
Report: 
rewardSum:-4.0
loss:[0, 0]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:1593.0
now epsilon is 0.9960074912571055, the reward is 989.0 with loss [0, 0] in episode 1
Report: 
rewardSum:989.0
loss:[0, 0]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:1593.0
2021-03-25 12:48:01.145715: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 12:48:01.926184: W tensorflow/stream_executor/gpu/asm_compiler.cc:98] *** WARNING *** You are using ptxas 9.1.108, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You do not need to update to CUDA 9.2.88; cherry-picking the ptxas binary is sufficient.
2021-03-25 12:48:01.987062: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 12:48:02.215435: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 12:48:02.630480: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-03-25 12:48:02.633008: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2596990000 Hz
now epsilon is 0.992775317998912, the reward is 238.25 with loss [133.28809287771583, 166.42768014222383] in episode 2
Report: 
rewardSum:238.25
loss:[133.28809287771583, 166.42768014222383]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2290.0
now epsilon is 0.9893062451530077, the reward is 986.0 with loss [94.32036995515227, 139.2207396030426] in episode 3
Report: 
rewardSum:986.0
loss:[94.32036995515227, 139.2207396030426]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2303.0
now epsilon is 0.9873293630832584, the reward is 243.25 with loss [84.29398579895496, 140.92825873941183] in episode 4
Report: 
rewardSum:243.25
loss:[84.29398579895496, 140.92825873941183]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2303.0
now epsilon is 0.9843714443355952, the reward is 239.25 with loss [221.15764720737934, 180.21197371557355] in episode 5
Report: 
rewardSum:239.25
loss:[221.15764720737934, 180.21197371557355]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2308.0
now epsilon is 0.9814223871529224, the reward is 239.25 with loss [150.9262535572052, 201.7307030595839] in episode 6
Report: 
rewardSum:239.25
loss:[150.9262535572052, 201.7307030595839]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2308.0
Traceback (most recent call last):
  File "agentRunner.py", line 57, in <module>
    agent.train()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 229, in train
    self.episode()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 247, in episode
    action, policy, qs, ws, random = self.agent.selectAction(state)
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 187, in selectAction
    pred = np.squeeze(self.train_network(state, training=False).numpy(), axis=0)
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py", line 1012, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py", line 375, in call
    return super(Sequential, self).call(inputs, training=training, mask=mask)
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py", line 424, in call
    return self._run_internal_graph(
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py", line 560, in _run_internal_graph
    outputs = node.layer(*args, **kwargs)
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py", line 998, in __call__
    input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)
  File "/home/luke/.pyenv/versions/3.8.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/input_spec.py", line 234, in assert_input_compatibility
    raise ValueError('Input ' + str(input_index) + ' of layer ' +
ValueError: Input 0 of layer conv2d is incompatible with the layer: : expected min_ndim=4, found ndim=3. Full shape received: (2, 84, 84)
pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)
Hello from the pygame community. https://www.pygame.org/contribute.html
2021-03-25 12:50:32.736431: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
2021-03-25 12:50:34.560930: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 12:50:34.561856: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-25 12:50:35.527569: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0001:00:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 149.31GiB/s
2021-03-25 12:50:35.527641: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 12:50:35.533257: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 12:50:35.533364: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 12:50:35.534542: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-25 12:50:35.535356: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-25 12:50:35.536457: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-25 12:50:35.537401: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-25 12:50:35.537587: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 12:50:35.538838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-25 12:50:35.540010: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 12:50:35.540928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0001:00:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 149.31GiB/s
2021-03-25 12:50:35.540973: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 12:50:35.541008: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 12:50:35.541040: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 12:50:35.541086: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-25 12:50:35.541117: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-25 12:50:35.541147: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-25 12:50:35.541178: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-25 12:50:35.541209: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 12:50:35.542477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-25 12:50:35.542545: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 12:50:36.157744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-25 12:50:36.157798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-25 12:50:36.157808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-25 12:50:36.160031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7468 MB memory) -> physical GPU (device: 0, name: Tesla M60, pci bus id: 0001:00:00.0, compute capability: 5.2)
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.9960074912571055, the reward is 546.6875 with loss [0, 0] in episode 0
No handles with labels found to put in legend.
Report: 
rewardSum:546.6875
loss:[0, 0]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:1593.0
2021-03-25 12:50:37.014493: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 12:50:37.804497: W tensorflow/stream_executor/gpu/asm_compiler.cc:98] *** WARNING *** You are using ptxas 9.1.108, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You do not need to update to CUDA 9.2.88; cherry-picking the ptxas binary is sufficient.
2021-03-25 12:50:37.864381: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 12:50:38.094525: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 12:50:38.525540: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-03-25 12:50:38.528049: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2596990000 Hz
now epsilon is 0.9955095497619452, the reward is -1.0 with loss [4.508469685912132, 17.68806791305542] in episode 1
Report: 
rewardSum:-1.0
loss:[4.508469685912132, 17.68806791305542]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2289.0
now epsilon is 0.9915349691808856, the reward is 984.0 with loss [114.39025789499283, 162.25383746251464] in episode 2
Report: 
rewardSum:984.0
loss:[114.39025789499283, 162.25383746251464]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2310.0
now epsilon is 0.9898010838323561, the reward is -6.0 with loss [57.87191601097584, 53.468027114868164] in episode 3
Report: 
rewardSum:-6.0
loss:[57.87191601097584, 53.468027114868164]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2310.0
now epsilon is 0.9893062451530077, the reward is -1.0 with loss [36.70553493499756, 23.502817153930664] in episode 4
Report: 
rewardSum:-1.0
loss:[36.70553493499756, 23.502817153930664]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2310.0
now epsilon is 0.9878232129507829, the reward is 245.25 with loss [48.10954197123647, 50.09291744232178] in episode 5
Report: 
rewardSum:245.25
loss:[48.10954197123647, 50.09291744232178]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2313.0
now epsilon is 0.9870825307424875, the reward is -2.0 with loss [58.25741004943848, 24.062006399035454] in episode 6
Report: 
rewardSum:-2.0
loss:[58.25741004943848, 24.062006399035454]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2313.0
now epsilon is 0.9848638146889512, the reward is -8.0 with loss [54.07456701248884, 106.29092359542847] in episode 7
Report: 
rewardSum:-8.0
loss:[54.07456701248884, 106.29092359542847]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2313.0
now epsilon is 0.982158822130087, the reward is -10.0 with loss [71.23695348203182, 103.99829967319965] in episode 8
Report: 
rewardSum:-10.0
loss:[71.23695348203182, 103.99829967319965]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2313.0
now epsilon is 0.9811770315561342, the reward is -3.0 with loss [20.711476057767868, 43.64109992980957] in episode 9
Report: 
rewardSum:-3.0
loss:[20.711476057767868, 43.64109992980957]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2313.0
now epsilon is 0.9784821649870683, the reward is 551.6875 with loss [150.80403780937195, 110.27658049017191] in episode 10
Report: 
rewardSum:551.6875
loss:[150.80403780937195, 110.27658049017191]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2313.0
now epsilon is 0.9777484868134452, the reward is -2.0 with loss [31.869218349456787, 59.228455543518066] in episode 11
Report: 
rewardSum:-2.0
loss:[31.869218349456787, 59.228455543518066]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2313.0
now epsilon is 0.9757947000444079, the reward is 243.25 with loss [86.35862183570862, 74.56877833604813] in episode 12
Report: 
rewardSum:243.25
loss:[86.35862183570862, 74.56877833604813]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2313.0
now epsilon is 0.9753068636815545, the reward is -1.0 with loss [22.068451404571533, 8.396808505058289] in episode 13
Report: 
rewardSum:-1.0
loss:[22.068451404571533, 8.396808505058289]
policies:[0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2313.0
Traceback (most recent call last):
  File "agentRunner.py", line 57, in <module>
    agent.train()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 229, in train
    self.episode()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 251, in episode
    wSums = [wSums[i] + ws[i] for i in range(len(policies))]
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 251, in <listcomp>
    wSums = [wSums[i] + ws[i] for i in range(len(policies))]
IndexError: list index out of range
pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)
Hello from the pygame community. https://www.pygame.org/contribute.html
2021-03-25 13:02:38.068010: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33-		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		605		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[605.3333333333334, -10]
[1000, -12]
2021-03-25 13:02:39.908407: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 13:02:39.909282: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-25 13:02:40.876318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0001:00:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 149.31GiB/s
2021-03-25 13:02:40.876396: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 13:02:40.882027: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 13:02:40.882110: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 13:02:40.883275: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-25 13:02:40.883572: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-25 13:02:40.884686: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-25 13:02:40.885594: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-25 13:02:40.885781: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 13:02:40.886980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-25 13:02:40.888128: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 13:02:40.888953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0001:00:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 149.31GiB/s
2021-03-25 13:02:40.889003: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 13:02:40.889045: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 13:02:40.889083: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 13:02:40.889120: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-25 13:02:40.889156: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-25 13:02:40.889193: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-25 13:02:40.889230: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-25 13:02:40.889268: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 13:02:40.890412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-25 13:02:40.890484: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 13:02:41.462631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-25 13:02:41.462701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-25 13:02:41.462714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-25 13:02:41.465069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7468 MB memory) -> physical GPU (device: 0, name: Tesla M60, pci bus id: 0001:00:00.0, compute capability: 5.2)
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33-		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		605		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[605.3333333333334, -10]
[1000, -12]
Traceback (most recent call last):
  File "agentRunner.py", line 57, in <module>
    agent.train()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 231, in train
    self.episode()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 264, in episode
    lossSums = [lossSums[i] + loss[i][0] for i in range(len(policies))]
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 264, in <listcomp>
    lossSums = [lossSums[i] + loss[i][0] for i in range(len(policies))]
IndexError: list index out of range
pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)
Hello from the pygame community. https://www.pygame.org/contribute.html
2021-03-25 13:04:45.415468: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33-		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		605		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[605.3333333333334, -10]
[1000, -12]
2021-03-25 13:04:47.298618: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 13:04:47.299490: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-25 13:04:48.266104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0001:00:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 149.31GiB/s
2021-03-25 13:04:48.266181: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 13:04:48.271857: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 13:04:48.271941: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 13:04:48.273128: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-25 13:04:48.273944: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-25 13:04:48.275025: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-25 13:04:48.276033: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-25 13:04:48.276218: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 13:04:48.277497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-25 13:04:48.278193: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 13:04:48.278869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0001:00:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 149.31GiB/s
2021-03-25 13:04:48.278932: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 13:04:48.278976: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 13:04:48.279017: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 13:04:48.279057: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-25 13:04:48.279097: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-25 13:04:48.279137: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-25 13:04:48.279177: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-25 13:04:48.279218: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 13:04:48.280364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-25 13:04:48.280433: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 13:04:48.858263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-25 13:04:48.858329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-25 13:04:48.858342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-25 13:04:48.860269: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7468 MB memory) -> physical GPU (device: 0, name: Tesla M60, pci bus id: 0001:00:00.0, compute capability: 5.2)
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33-		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		605		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[605.3333333333334, -10]
[1000, -12]
now epsilon is 0.9972534349231639, the reward is 594.3333333333334 with loss [0, 0] in episode 0
No handles with labels found to put in legend.
Report: 
rewardSum:594.3333333333334
loss:[0, 0]
policies:[0, 0, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:1601.0
2021-03-25 13:04:49.668505: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 13:04:50.464051: W tensorflow/stream_executor/gpu/asm_compiler.cc:98] *** WARNING *** You are using ptxas 9.1.108, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You do not need to update to CUDA 9.2.88; cherry-picking the ptxas binary is sufficient.
2021-03-25 13:04:50.524412: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 13:04:50.776822: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 13:04:51.237653: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-03-25 13:04:51.240196: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2596990000 Hz
now epsilon is 0.9930235738923852, the reward is 588.3333333333334 with loss [90.25479647517204, 90.24128409102559] in episode 1
Report: 
rewardSum:588.3333333333334
loss:[90.25479647517204, 90.24128409102559]
policies:[0, 0, 17]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2302.0
now epsilon is 0.9873293630832584, the reward is -22.0 with loss [152.02932204306126, 246.5195467248559] in episode 2
Report: 
rewardSum:-22.0
loss:[152.02932204306126, 246.5195467248559]
policies:[0, 1, 22]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2309.0
now epsilon is 0.986342403906982, the reward is -3.0 with loss [28.53161795809865, 52.11332416534424] in episode 3
Report: 
rewardSum:-3.0
loss:[28.53161795809865, 52.11332416534424]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2309.0
now epsilon is 0.9853564313198341, the reward is -3.0 with loss [28.557554244995117, 19.05933900922537] in episode 4
Report: 
rewardSum:-3.0
loss:[28.557554244995117, 19.05933900922537]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2309.0
now epsilon is 0.9838793201366427, the reward is 192.33333333333334 with loss [34.774425465613604, 75.83151149749756] in episode 5
Report: 
rewardSum:192.33333333333334
loss:[34.774425465613604, 75.83151149749756]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2309.0
now epsilon is 0.9809317372982452, the reward is 988.0 with loss [77.32721639052033, 121.50302314758301] in episode 6
Report: 
rewardSum:988.0
loss:[77.32721639052033, 121.50302314758301]
policies:[0, 0, 12]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2309.0
now epsilon is 0.9789715895961421, the reward is 42.33333333333334 with loss [73.63224017620087, 92.70220828056335] in episode 7
Report: 
rewardSum:42.33333333333334
loss:[73.63224017620087, 92.70220828056335]
policies:[0, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2312.0
now epsilon is 0.976771104921209, the reward is 596.3333333333334 with loss [105.48825120925903, 78.67880648374557] in episode 8
Report: 
rewardSum:596.3333333333334
loss:[105.48825120925903, 78.67880648374557]
policies:[0, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2318.0
now epsilon is 0.9760387097218384, the reward is -2.0 with loss [27.14988386631012, 28.768788814544678] in episode 9
Report: 
rewardSum:-2.0
loss:[27.14988386631012, 28.768788814544678]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2318.0
now epsilon is 0.9753068636815545, the reward is -2.0 with loss [24.45782244205475, 24.528490275144577] in episode 10
Report: 
rewardSum:-2.0
loss:[24.45782244205475, 24.528490275144577]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2318.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38*		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33-		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		605		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[605.3333333333334, -10]
[1000, -12]
now epsilon is 0.9726281199105681, the reward is 989.0 with loss [82.82440725713968, 147.82465362548828] in episode 11
Report: 
rewardSum:989.0
loss:[82.82440725713968, 147.82465362548828]
policies:[0, 0, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2319.0
now epsilon is 0.9721418666398703, the reward is -1.0 with loss [41.18641662597656, 28.33928871154785] in episode 12
Report: 
rewardSum:-1.0
loss:[41.18641662597656, 28.33928871154785]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2319.0
now epsilon is 0.9704418937779434, the reward is 43.33333333333334 with loss [73.93758830428123, 109.18655776977539] in episode 13
Report: 
rewardSum:43.33333333333334
loss:[73.93758830428123, 109.18655776977539]
policies:[0, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2319.0
now epsilon is 0.968987140423346, the reward is 192.33333333333334 with loss [74.2302131652832, 65.76282787322998] in episode 14
Report: 
rewardSum:192.33333333333334
loss:[74.2302131652832, 65.76282787322998]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2319.0
now epsilon is 0.9668090983071408, the reward is -8.0 with loss [75.08662904426455, 118.27706480026245] in episode 15
Report: 
rewardSum:-8.0
loss:[75.08662904426455, 118.27706480026245]
policies:[0, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2319.0
now epsilon is 0.9663257541835559, the reward is -1.0 with loss [10.339849948883057, 19.690570831298828] in episode 16
Report: 
rewardSum:-1.0
loss:[10.339849948883057, 19.690570831298828]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2319.0
now epsilon is 0.9648771711807551, the reward is 192.33333333333334 with loss [57.42493391036987, 50.5766396522522] in episode 17
Report: 
rewardSum:192.33333333333334
loss:[57.42493391036987, 50.5766396522522]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2319.0
now epsilon is 0.964153694201763, the reward is -2.0 with loss [23.98214340209961, 25.616201877593994] in episode 18
Report: 
rewardSum:-2.0
loss:[23.98214340209961, 25.616201877593994]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2319.0
now epsilon is 0.9574273471977809, the reward is 972.0 with loss [199.198666036129, 302.61539156734943] in episode 19
Report: 
rewardSum:972.0
loss:[199.198666036129, 302.61539156734943]
policies:[0, 1, 27]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2319.0
now epsilon is 0.9569486933633913, the reward is -1.0 with loss [17.654730796813965, 22.375815887004137] in episode 20
Report: 
rewardSum:-1.0
loss:[17.654730796813965, 22.375815887004137]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2319.0
now epsilon is 0.9540817914089998, the reward is 593.3333333333334 with loss [104.85338667035103, 92.76405191421509] in episode 21
Report: 
rewardSum:593.3333333333334
loss:[104.85338667035103, 92.76405191421509]
policies:[0, 0, 12]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2319.0
now epsilon is 0.9536048101434074, the reward is -1.0 with loss [11.096198797225952, 9.436562895774841] in episode 22
Report: 
rewardSum:-1.0
loss:[11.096198797225952, 9.436562895774841]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2319.0
now epsilon is 0.9528897853218017, the reward is -2.0 with loss [39.20193147659302, 19.31758689880371] in episode 23
Report: 
rewardSum:-2.0
loss:[39.20193147659302, 19.31758689880371]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2319.0
now epsilon is 0.949560085217993, the reward is 986.0 with loss [153.9845080897212, 91.24966789409518] in episode 24
Report: 
rewardSum:986.0
loss:[153.9845080897212, 91.24966789409518]
policies:[0, 0, 14]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2325.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11*		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33-		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		605		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[605.3333333333334, -10]
[1000, -12]
now epsilon is 0.948136635006064, the reward is 44.33333333333334 with loss [69.61852791160345, 67.5555393062532] in episode 25
Report: 
rewardSum:44.33333333333334
loss:[69.61852791160345, 67.5555393062532]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2325.0
now epsilon is 0.9455325160362875, the reward is 39.33333333333334 with loss [124.21030747890472, 89.59764304757118] in episode 26
Report: 
rewardSum:39.33333333333334
loss:[124.21030747890472, 89.59764304757118]
policies:[1, 0, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2326.0
now epsilon is 0.9419929674411818, the reward is -14.0 with loss [154.89412421360612, 184.87823700904846] in episode 27
Report: 
rewardSum:-14.0
loss:[154.89412421360612, 184.87823700904846]
policies:[0, 1, 14]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2326.0
now epsilon is 0.9412866493245637, the reward is -2.0 with loss [45.463356018066406, 70.51912498474121] in episode 28
Report: 
rewardSum:-2.0
loss:[45.463356018066406, 70.51912498474121]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2326.0
now epsilon is 0.940345715598906, the reward is -3.0 with loss [40.99299716949463, 35.31134734302759] in episode 29
Report: 
rewardSum:-3.0
loss:[40.99299716949463, 35.31134734302759]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2326.0
now epsilon is 0.9389360783058134, the reward is -5.0 with loss [65.27924823760986, 84.41857528686523] in episode 30
Report: 
rewardSum:-5.0
loss:[65.27924823760986, 84.41857528686523]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2326.0
now epsilon is 0.9365913771080342, the reward is 595.3333333333334 with loss [106.0763581842184, 133.9855102300644] in episode 31
Report: 
rewardSum:595.3333333333334
loss:[106.0763581842184, 133.9855102300644]
policies:[0, 0, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2326.0
now epsilon is 0.9361231399564413, the reward is -1.0 with loss [22.768747329711914, 29.867021560668945] in episode 32
Report: 
rewardSum:-1.0
loss:[22.768747329711914, 29.867021560668945]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2326.0
now epsilon is 0.9356551368941594, the reward is -1.0 with loss [25.555357605218887, 3.5787540674209595] in episode 33
Report: 
rewardSum:-1.0
loss:[25.555357605218887, 3.5787540674209595]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2326.0
now epsilon is 0.9351873678041583, the reward is -1.0 with loss [34.52023506164551, 22.0754976272583] in episode 34
Report: 
rewardSum:-1.0
loss:[34.52023506164551, 22.0754976272583]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2326.0
now epsilon is 0.9335520168326188, the reward is -6.0 with loss [41.78950643539429, 60.19038753956556] in episode 35
Report: 
rewardSum:-6.0
loss:[41.78950643539429, 60.19038753956556]
policies:[0, 2, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2326.0
now epsilon is 0.9319195255797753, the reward is -6.0 with loss [65.83174693584442, 55.207042809575796] in episode 36
Report: 
rewardSum:-6.0
loss:[65.83174693584442, 55.207042809575796]
policies:[0, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2326.0
now epsilon is 0.93075520847691, the reward is 193.33333333333334 with loss [45.65083825588226, 35.10818064212799] in episode 37
Report: 
rewardSum:193.33333333333334
loss:[45.65083825588226, 35.10818064212799]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2326.0
now epsilon is 0.9302898890448721, the reward is -1.0 with loss [17.665993690490723, 16.492698669433594] in episode 38
Report: 
rewardSum:-1.0
loss:[17.665993690490723, 16.492698669433594]
policies:[1, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2326.0
now epsilon is 0.928198828726224, the reward is 41.33333333333334 with loss [100.92554593086243, 69.71976137906313] in episode 39
Report: 
rewardSum:41.33333333333334
loss:[100.92554593086243, 69.71976137906313]
policies:[1, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2326.0
now epsilon is 0.9249554066810175, the reward is 36.33333333333334 with loss [74.05201565101743, 164.02595138549805] in episode 40
Report: 
rewardSum:36.33333333333334
loss:[74.05201565101743, 164.02595138549805]
policies:[0, 0, 14]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2327.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11*		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33-		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		605		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[605.3333333333334, -10]
[1000, -12]
now epsilon is 0.9208019404548967, the reward is -17.0 with loss [203.6410569548607, 204.50907875597477] in episode 41
Report: 
rewardSum:-17.0
loss:[203.6410569548607, 204.50907875597477]
policies:[0, 1, 17]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2334.0
now epsilon is 0.9203415970347906, the reward is -1.0 with loss [20.46316623687744, 18.522703051567078] in episode 42
Report: 
rewardSum:-1.0
loss:[20.46316623687744, 18.522703051567078]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2334.0
now epsilon is 0.9187322066851401, the reward is 993.0 with loss [54.725659273564816, 69.54937708377838] in episode 43
Report: 
rewardSum:993.0
loss:[54.725659273564816, 69.54937708377838]
policies:[1, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2334.0
now epsilon is 0.9182728980025605, the reward is -1.0 with loss [8.318322330713272, 2.4468325078487396] in episode 44
Report: 
rewardSum:-1.0
loss:[8.318322330713272, 2.4468325078487396]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2334.0
now epsilon is 0.9178138189456154, the reward is -1.0 with loss [2.042158305644989, 21.972068540751934] in episode 45
Report: 
rewardSum:-1.0
loss:[2.042158305644989, 21.972068540751934]
policies:[1, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2333.0
now epsilon is 0.9168963492494921, the reward is 194.33333333333334 with loss [39.683479964733124, 53.7661714553833] in episode 46
Report: 
rewardSum:194.33333333333334
loss:[39.683479964733124, 53.7661714553833]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2333.0
now epsilon is 0.9157508017299016, the reward is -4.0 with loss [58.15711344778538, 68.01224514469504] in episode 47
Report: 
rewardSum:-4.0
loss:[58.15711344778538, 68.01224514469504]
policies:[0, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2333.0
now epsilon is 0.9148353942774917, the reward is -3.0 with loss [48.75466775894165, 44.39465417712927] in episode 48
Report: 
rewardSum:-3.0
loss:[48.75466775894165, 44.39465417712927]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2339.0
now epsilon is 0.9127790718197335, the reward is -8.0 with loss [93.47443237155676, 57.91242042928934] in episode 49
Report: 
rewardSum:-8.0
loss:[93.47443237155676, 57.91242042928934]
policies:[0, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2339.0
now epsilon is 0.9123227393325157, the reward is -1.0 with loss [6.780059635639191, 27.487239837646484] in episode 50
Report: 
rewardSum:-1.0
loss:[6.780059635639191, 27.487239837646484]
policies:[1, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2339.0
now epsilon is 0.9118666349830207, the reward is -1.0 with loss [26.66174054145813, 45.85503387451172] in episode 51
Report: 
rewardSum:-1.0
loss:[26.66174054145813, 45.85503387451172]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2339.0
now epsilon is 0.910044496682032, the reward is 190.33333333333334 with loss [89.85527256131172, 84.51282903552055] in episode 52
Report: 
rewardSum:190.33333333333334
loss:[89.85527256131172, 84.51282903552055]
policies:[0, 1, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2339.0
now epsilon is 0.909589531311472, the reward is -1.0 with loss [26.700644969940186, 17.292768955230713] in episode 53
Report: 
rewardSum:-1.0
loss:[26.700644969940186, 17.292768955230713]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2339.0
now epsilon is 0.9086802828193891, the reward is -3.0 with loss [68.85759782791138, 26.81625521183014] in episode 54
Report: 
rewardSum:-3.0
loss:[68.85759782791138, 26.81625521183014]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2339.0
now epsilon is 0.9073181139990159, the reward is -5.0 with loss [43.253976941108704, 45.71075773239136] in episode 55
Report: 
rewardSum:-5.0
loss:[43.253976941108704, 45.71075773239136]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2352.0
now epsilon is 0.9052786885178662, the reward is -8.0 with loss [50.49376890063286, 65.68762273341417] in episode 56
Report: 
rewardSum:-8.0
loss:[50.49376890063286, 65.68762273341417]
policies:[1, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2352.0
now epsilon is 0.9012135796155052, the reward is 587.3333333333334 with loss [158.62009359896183, 119.20562497526407] in episode 57
Report: 
rewardSum:587.3333333333334
loss:[158.62009359896183, 119.20562497526407]
policies:[0, 0, 18]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2352.0
now epsilon is 0.9003127039346598, the reward is -3.0 with loss [23.760480165481567, 33.50701737776399] in episode 58
Report: 
rewardSum:-3.0
loss:[23.760480165481567, 33.50701737776399]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2352.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40*		
5x		14x		23x		32-		41-		
6x		15x		24x		33-		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		605		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[605.3333333333334, -10]
[1000, -12]
now epsilon is 0.8980644526169551, the reward is 595.3333333333334 with loss [66.72360797226429, 96.77006390690804] in episode 59
Report: 
rewardSum:595.3333333333334
loss:[66.72360797226429, 96.77006390690804]
policies:[1, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2352.0
now epsilon is 0.8964940180434633, the reward is 43.33333333333334 with loss [55.161095559597015, 88.54341101646423] in episode 60
Report: 
rewardSum:43.33333333333334
loss:[55.161095559597015, 88.54341101646423]
policies:[1, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2353.0
now epsilon is 0.8944789224381992, the reward is 991.0 with loss [77.27345385029912, 111.81765234470367] in episode 61
Report: 
rewardSum:991.0
loss:[77.27345385029912, 111.81765234470367]
policies:[1, 1, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2358.0
now epsilon is 0.8940317388819128, the reward is -1.0 with loss [28.55620765686035, 28.570049334317446] in episode 62
Report: 
rewardSum:-1.0
loss:[28.55620765686035, 28.570049334317446]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2358.0
now epsilon is 0.8920221778678644, the reward is 991.0 with loss [128.73943096399307, 86.15929093956947] in episode 63
Report: 
rewardSum:991.0
loss:[128.73943096399307, 86.15929093956947]
policies:[0, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2358.0
now epsilon is 0.8904623093480015, the reward is 993.0 with loss [138.74588203430176, 99.57543063163757] in episode 64
Report: 
rewardSum:993.0
loss:[138.74588203430176, 99.57543063163757]
policies:[0, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2358.0
now epsilon is 0.8900171338472219, the reward is -1.0 with loss [28.223301887512207, 14.199211120605469] in episode 65
Report: 
rewardSum:-1.0
loss:[28.223301887512207, 14.199211120605469]
policies:[1, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2358.0
now epsilon is 0.8866854040100134, the reward is 183.33333333333334 with loss [129.78957153856754, 129.76332235336304] in episode 66
Report: 
rewardSum:183.33333333333334
loss:[129.78957153856754, 129.76332235336304]
policies:[0, 0, 15]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2358.0
now epsilon is 0.8844711826408793, the reward is 188.33333333333334 with loss [71.1566004678607, 53.30626634508371] in episode 67
Report: 
rewardSum:188.33333333333334
loss:[71.1566004678607, 53.30626634508371]
policies:[0, 0, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2358.0
now epsilon is 0.8816009591443008, the reward is 987.0 with loss [121.24029815196991, 116.07971075177193] in episode 68
Report: 
rewardSum:987.0
loss:[121.24029815196991, 116.07971075177193]
policies:[0, 1, 12]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2358.0
now epsilon is 0.8807196887304196, the reward is -3.0 with loss [27.04508563876152, 34.860425382852554] in episode 69
Report: 
rewardSum:-3.0
loss:[27.04508563876152, 34.860425382852554]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2358.0
now epsilon is 0.880279383931035, the reward is -1.0 with loss [43.93033790588379, 29.06191699206829] in episode 70
Report: 
rewardSum:-1.0
loss:[43.93033790588379, 29.06191699206829]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2358.0
now epsilon is 0.879399434596859, the reward is -3.0 with loss [53.92413341999054, 38.05457925796509] in episode 71
Report: 
rewardSum:-3.0
loss:[53.92413341999054, 38.05457925796509]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2358.0
Traceback (most recent call last):
  File "agentRunner.py", line 57, in <module>
    agent.train()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 231, in train
    self.episode()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 249, in episode
    action, policy, qs, ws, random = self.agent.selectAction(state)
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 182, in selectAction
    self.w_train_network.set_weights(self.wnet_weights[i])
AttributeError: 'DQNAgent' object has no attribute 'wnet_weights'
pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)
Hello from the pygame community. https://www.pygame.org/contribute.html
2021-03-25 13:08:37.013101: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
2021-03-25 13:08:38.847965: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 13:08:38.848811: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-25 13:08:39.827899: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0001:00:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 149.31GiB/s
2021-03-25 13:08:39.827980: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 13:08:39.833574: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 13:08:39.833668: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 13:08:39.835119: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-25 13:08:39.835951: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-25 13:08:39.837422: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-25 13:08:39.838360: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-25 13:08:39.838555: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 13:08:39.839802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-25 13:08:39.840993: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 13:08:39.841621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0001:00:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 149.31GiB/s
2021-03-25 13:08:39.841670: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 13:08:39.841712: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 13:08:39.841752: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 13:08:39.841792: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-25 13:08:39.841832: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-25 13:08:39.841871: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-25 13:08:39.841910: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-25 13:08:39.841951: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 13:08:39.843059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-25 13:08:39.843129: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 13:08:40.413790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-25 13:08:40.413855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-25 13:08:40.413868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-25 13:08:40.415781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7468 MB memory) -> physical GPU (device: 0, name: Tesla M60, pci bus id: 0001:00:00.0, compute capability: 5.2)
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.9977522486879922, the reward is 54.6875 with loss [0, 0] in episode 0
No handles with labels found to put in legend.
Report: 
rewardSum:54.6875
loss:[0, 0]
policies:[0, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:1599.0
now epsilon is 0.9972534349231639, the reward is -1.0 with loss [0, 0] in episode 1
Report: 
rewardSum:-1.0
loss:[0, 0]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:1599.0
2021-03-25 13:08:41.277281: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 13:08:42.069168: W tensorflow/stream_executor/gpu/asm_compiler.cc:98] *** WARNING *** You are using ptxas 9.1.108, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You do not need to update to CUDA 9.2.88; cherry-picking the ptxas binary is sufficient.
2021-03-25 13:08:42.127722: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 13:08:42.355776: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 13:08:42.781713: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-03-25 13:08:42.784242: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2596990000 Hz
now epsilon is 0.9940172184164668, the reward is 50.6875 with loss [5.792417585849762, 4.348054792732] in episode 2
Report: 
rewardSum:50.6875
loss:[5.792417585849762, 4.348054792732]
policies:[0, 0, 13]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2301.0
now epsilon is 0.989553633561398, the reward is 233.25 with loss [14.475537434220314, 17.982476122677326] in episode 3
Report: 
rewardSum:233.25
loss:[14.475537434220314, 17.982476122677326]
policies:[0, 0, 18]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2304.0
now epsilon is 0.9868357601098019, the reward is 551.6875 with loss [28.405895352363586, 22.03008827753365] in episode 4
Report: 
rewardSum:551.6875
loss:[28.405895352363586, 22.03008827753365]
policies:[0, 0, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2316.0
now epsilon is 0.9853564313198341, the reward is -5.0 with loss [21.23980214819312, 29.57526659965515] in episode 5
Report: 
rewardSum:-5.0
loss:[21.23980214819312, 29.57526659965515]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2317.0
now epsilon is 0.9811770315561342, the reward is 545.6875 with loss [54.52372617460787, 97.5542377345264] in episode 6
Report: 
rewardSum:545.6875
loss:[54.52372617460787, 97.5542377345264]
policies:[0, 0, 17]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2318.0
now epsilon is 0.9779929850597101, the reward is 50.6875 with loss [66.52622084319592, 85.59562830254436] in episode 7
Report: 
rewardSum:50.6875
loss:[66.52622084319592, 85.59562830254436]
policies:[0, 0, 13]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2317.0
now epsilon is 0.9762827804169426, the reward is -6.0 with loss [38.398665726184845, 29.135054171085358] in episode 8
Report: 
rewardSum:-6.0
loss:[38.398665726184845, 29.135054171085358]
policies:[0, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2318.0
now epsilon is 0.9743319224969941, the reward is 55.6875 with loss [36.01409474760294, 46.73980528116226] in episode 9
Report: 
rewardSum:55.6875
loss:[36.01409474760294, 46.73980528116226]
policies:[0, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2318.0
############# STATE ###############
0-		8-		16*		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.9706845649191732, the reward is 48.6875 with loss [59.75055155903101, 87.58366563916206] in episode 10
Report: 
rewardSum:48.6875
loss:[59.75055155903101, 87.58366563916206]
policies:[0, 0, 15]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2318.0
now epsilon is 0.970199283304499, the reward is -1.0 with loss [16.210806369781494, 2.0458658933639526] in episode 11
Report: 
rewardSum:-1.0
loss:[16.210806369781494, 2.0458658933639526]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2318.0
now epsilon is 0.9670508610223963, the reward is 238.25 with loss [48.86439389362931, 51.297179982066154] in episode 12
Report: 
rewardSum:238.25
loss:[48.86439389362931, 51.297179982066154]
policies:[0, 1, 12]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2318.0
now epsilon is 0.96608417274501, the reward is -3.0 with loss [22.65615701675415, 23.130719996988773] in episode 13
Report: 
rewardSum:-3.0
loss:[22.65615701675415, 23.130719996988773]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2318.0
now epsilon is 0.9622270732389537, the reward is -15.0 with loss [51.090529818087816, 79.92555373907089] in episode 14
Report: 
rewardSum:-15.0
loss:[51.090529818087816, 79.92555373907089]
policies:[0, 1, 15]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2320.0
now epsilon is 0.9603043021481686, the reward is 55.6875 with loss [27.694779694080353, 35.54872977733612] in episode 15
Report: 
rewardSum:55.6875
loss:[27.694779694080353, 35.54872977733612]
policies:[0, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2326.0
now epsilon is 0.9598242100161135, the reward is -1.0 with loss [3.0440609492361546, 4.47064895182848] in episode 16
Report: 
rewardSum:-1.0
loss:[3.0440609492361546, 4.47064895182848]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2326.0
now epsilon is 0.9579062404488652, the reward is 992.0 with loss [54.550007820129395, 32.587637547403574] in episode 17
Report: 
rewardSum:992.0
loss:[54.550007820129395, 32.587637547403574]
policies:[0, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2325.0
now epsilon is 0.9567094561900504, the reward is -4.0 with loss [11.933676641434431, 51.958611488342285] in episode 18
Report: 
rewardSum:-4.0
loss:[11.933676641434431, 51.958611488342285]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2326.0
now epsilon is 0.9540817914089998, the reward is 240.25 with loss [82.56821727380157, 89.92654550820589] in episode 19
Report: 
rewardSum:240.25
loss:[82.56821727380157, 89.92654550820589]
policies:[0, 0, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2326.0
now epsilon is 0.9519372528105975, the reward is 553.6875 with loss [61.508124232292175, 54.13823015987873] in episode 20
Report: 
rewardSum:553.6875
loss:[61.508124232292175, 54.13823015987873]
policies:[0, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2326.0
now epsilon is 0.9502726115153627, the reward is 56.6875 with loss [37.483933210372925, 69.19349336624146] in episode 21
Report: 
rewardSum:56.6875
loss:[37.483933210372925, 69.19349336624146]
policies:[1, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2326.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20*		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.9483737284381736, the reward is 55.6875 with loss [20.857004769146442, 48.7213049530983] in episode 22
Report: 
rewardSum:55.6875
loss:[20.857004769146442, 48.7213049530983]
policies:[0, 1, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2326.0
Traceback (most recent call last):
  File "agentRunner.py", line 57, in <module>
    agent.train()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 231, in train
    self.episode()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 252, in episode
    qSums = [qSums[i] + qs[i] for i in range(len(qs))]
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 252, in <listcomp>
    qSums = [qSums[i] + qs[i] for i in range(len(qs))]
IndexError: list index out of range
pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)
Hello from the pygame community. https://www.pygame.org/contribute.html
2021-03-25 13:16:14.412672: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
2021-03-25 13:16:16.259378: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 13:16:16.260257: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-25 13:16:17.231881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0001:00:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 149.31GiB/s
2021-03-25 13:16:17.231958: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 13:16:17.237479: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 13:16:17.237557: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 13:16:17.238712: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-25 13:16:17.239516: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-25 13:16:17.240617: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-25 13:16:17.241543: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-25 13:16:17.241730: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 13:16:17.242960: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-25 13:16:17.243617: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 13:16:17.244243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0001:00:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 149.31GiB/s
2021-03-25 13:16:17.244289: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 13:16:17.244328: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 13:16:17.244368: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 13:16:17.244406: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-25 13:16:17.244447: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-25 13:16:17.244485: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-25 13:16:17.244523: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-25 13:16:17.244574: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 13:16:17.245851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-25 13:16:17.245928: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 13:16:17.813521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-25 13:16:17.813584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-25 13:16:17.813596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-25 13:16:17.815514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7468 MB memory) -> physical GPU (device: 0, name: Tesla M60, pci bus id: 0001:00:00.0, compute capability: 5.2)
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.9985009371875586, the reward is 245.25 with loss [0, 0] in episode 0
No handles with labels found to put in legend.
Report: 
rewardSum:245.25
loss:[0, 0]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:1605.0
now epsilon is 0.9970041215644331, the reward is 556.6875 with loss [0, 0] in episode 1
Report: 
rewardSum:556.6875
loss:[0, 0]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:1605.0
2021-03-25 13:16:18.638891: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 13:16:19.417690: W tensorflow/stream_executor/gpu/asm_compiler.cc:98] *** WARNING *** You are using ptxas 9.1.108, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You do not need to update to CUDA 9.2.88; cherry-picking the ptxas binary is sufficient.
2021-03-25 13:16:19.476153: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 13:16:19.709622: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 13:16:20.126737: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-03-25 13:16:20.129288: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2596990000 Hz
now epsilon is 0.9957584893842912, the reward is 246.25 with loss [10.323500633239746, 8.362476348876953] in episode 2
Report: 
rewardSum:246.25
loss:[10.323500633239746, 8.362476348876953]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2292.0
now epsilon is 0.9932718918653515, the reward is 241.25 with loss [76.69231629371643, 73.69569201767445] in episode 3
Report: 
rewardSum:241.25
loss:[76.69231629371643, 73.69569201767445]
policies:[0, 0, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2299.0
now epsilon is 0.992775317998912, the reward is -1.0 with loss [18.67123794555664, 20.62640953063965] in episode 4
Report: 
rewardSum:-1.0
loss:[18.67123794555664, 20.62640953063965]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2299.0
now epsilon is 0.9912870854385903, the reward is 556.6875 with loss [60.412601947784424, 48.647406339645386] in episode 5
Report: 
rewardSum:556.6875
loss:[60.412601947784424, 48.647406339645386]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2299.0
now epsilon is 0.9902961700238574, the reward is -3.0 with loss [46.587913513183594, 27.533061742782593] in episode 6
Report: 
rewardSum:-3.0
loss:[46.587913513183594, 27.533061742782593]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2299.0
now epsilon is 0.9893062451530077, the reward is 59.6875 with loss [38.792866706848145, 40.24562311172485] in episode 7
Report: 
rewardSum:59.6875
loss:[38.792866706848145, 40.24562311172485]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2299.0
now epsilon is 0.9888116538620716, the reward is -1.0 with loss [10.868420839309692, 10.850402355194092] in episode 8
Report: 
rewardSum:-1.0
loss:[10.868420839309692, 10.850402355194092]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2299.0
now epsilon is 0.9878232129507829, the reward is -3.0 with loss [29.50067377090454, 28.028223514556885] in episode 9
Report: 
rewardSum:-3.0
loss:[29.50067377090454, 28.028223514556885]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2299.0
now epsilon is 0.9858492943514289, the reward is 55.6875 with loss [70.27387142181396, 37.98855859041214] in episode 10
Report: 
rewardSum:55.6875
loss:[70.27387142181396, 37.98855859041214]
policies:[0, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2313.0
now epsilon is 0.984617598735279, the reward is -4.0 with loss [28.57015436887741, 29.550976276397705] in episode 11
Report: 
rewardSum:-4.0
loss:[28.57015436887741, 29.550976276397705]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2313.0
now epsilon is 0.9831415951085397, the reward is -5.0 with loss [21.761379957199097, 30.631701231002808] in episode 12
Report: 
rewardSum:-5.0
loss:[21.761379957199097, 30.631701231002808]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2313.0
now epsilon is 0.9826500857573353, the reward is -1.0 with loss [9.855080127716064, 4.473583269864321] in episode 13
Report: 
rewardSum:-1.0
loss:[9.855080127716064, 4.473583269864321]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2313.0
now epsilon is 0.9816678041039484, the reward is -3.0 with loss [23.149199068546295, 22.67731100320816] in episode 14
Report: 
rewardSum:-3.0
loss:[23.149199068546295, 22.67731100320816]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2313.0
now epsilon is 0.9757947000444079, the reward is 976.0 with loss [161.92306059598923, 153.16242430359125] in episode 15
Report: 
rewardSum:976.0
loss:[161.92306059598923, 153.16242430359125]
policies:[0, 1, 23]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2341.0
############# STATE ###############
0-		8-		16-		24-		32*		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.970927296743359, the reward is 43.6875 with loss [121.07858064770699, 160.17065458744764] in episode 16
Report: 
rewardSum:43.6875
loss:[121.07858064770699, 160.17065458744764]
policies:[1, 0, 19]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2329.0
now epsilon is 0.9704418937779434, the reward is -1.0 with loss [6.943443734198809, 11.82157826423645] in episode 17
Report: 
rewardSum:-1.0
loss:[6.943443734198809, 11.82157826423645]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2329.0
now epsilon is 0.9685027074148307, the reward is 243.25 with loss [71.69707775115967, 37.51766900718212] in episode 18
Report: 
rewardSum:243.25
loss:[71.69707775115967, 37.51766900718212]
policies:[1, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2329.0
now epsilon is 0.9680185165925426, the reward is -1.0 with loss [24.038466453552246, 9.373234272003174] in episode 19
Report: 
rewardSum:-1.0
loss:[24.038466453552246, 9.373234272003174]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2329.0
now epsilon is 0.9663257541835559, the reward is 555.6875 with loss [49.69191485643387, 64.34688079357147] in episode 20
Report: 
rewardSum:555.6875
loss:[49.69191485643387, 64.34688079357147]
policies:[1, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2329.0
now epsilon is 0.964153694201763, the reward is -8.0 with loss [92.74355828016996, 79.03770768642426] in episode 21
Report: 
rewardSum:-8.0
loss:[92.74355828016996, 79.03770768642426]
policies:[0, 1, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2329.0
now epsilon is 0.9634307596948646, the reward is -2.0 with loss [11.879872560501099, 19.195229530334473] in episode 22
Report: 
rewardSum:-2.0
loss:[11.879872560501099, 19.195229530334473]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2329.0
now epsilon is 0.9617460198415264, the reward is 555.6875 with loss [30.18905371427536, 60.03145456314087] in episode 23
Report: 
rewardSum:555.6875
loss:[30.18905371427536, 60.03145456314087]
policies:[0, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2329.0
now epsilon is 0.9586250294937709, the reward is 50.6875 with loss [78.3445029258728, 109.11700419336557] in episode 24
Report: 
rewardSum:50.6875
loss:[78.3445029258728, 109.11700419336557]
policies:[2, 0, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2329.0
now epsilon is 0.9555141671637559, the reward is 987.0 with loss [114.9816147685051, 91.4965351484716] in episode 25
Report: 
rewardSum:987.0
loss:[114.9816147685051, 91.4965351484716]
policies:[1, 0, 12]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2329.0
now epsilon is 0.9526515628754713, the reward is 550.6875 with loss [105.57198015600443, 113.40275359153748] in episode 26
Report: 
rewardSum:550.6875
loss:[105.57198015600443, 113.40275359153748]
policies:[0, 0, 12]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2334.0
now epsilon is 0.9505102390751314, the reward is 242.25 with loss [83.56573828309774, 93.32059741020203] in episode 27
Report: 
rewardSum:242.25
loss:[83.56573828309774, 93.32059741020203]
policies:[2, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2334.0
now epsilon is 0.9493226951966884, the reward is -4.0 with loss [57.4438910484314, 43.32060706615448] in episode 28
Report: 
rewardSum:-4.0
loss:[57.4438910484314, 43.32060706615448]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2334.0
now epsilon is 0.9488480931817586, the reward is -1.0 with loss [21.624619960784912, 18.657920360565186] in episode 29
Report: 
rewardSum:-1.0
loss:[21.624619960784912, 18.657920360565186]
policies:[1, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2334.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20*		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.947425710290614, the reward is 57.6875 with loss [33.543043315410614, 36.89983696117997] in episode 30
Report: 
rewardSum:57.6875
loss:[33.543043315410614, 36.89983696117997]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2350.0
now epsilon is 0.9452961329072784, the reward is 54.6875 with loss [95.22426402568817, 88.36070036888123] in episode 31
Report: 
rewardSum:54.6875
loss:[95.22426402568817, 88.36070036888123]
policies:[0, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2360.0
now epsilon is 0.9429355494487293, the reward is 241.25 with loss [55.23744824901223, 78.74069094657898] in episode 32
Report: 
rewardSum:241.25
loss:[55.23744824901223, 78.74069094657898]
policies:[1, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2360.0
now epsilon is 0.9417574691993215, the reward is 246.25 with loss [38.42558479309082, 50.53125810623169] in episode 33
Report: 
rewardSum:246.25
loss:[38.42558479309082, 50.53125810623169]
policies:[0, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2360.0
now epsilon is 0.9410513276622327, the reward is -2.0 with loss [37.78495121002197, 15.314866065979004] in episode 34
Report: 
rewardSum:-2.0
loss:[37.78495121002197, 15.314866065979004]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2360.0
now epsilon is 0.940345715598906, the reward is -2.0 with loss [11.86574288457632, 30.932805061340332] in episode 35
Report: 
rewardSum:-2.0
loss:[11.86574288457632, 30.932805061340332]
policies:[0, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2364.0
now epsilon is 0.9384666689501654, the reward is 243.25 with loss [72.66658584028482, 43.919591426849365] in episode 36
Report: 
rewardSum:243.25
loss:[72.66658584028482, 43.919591426849365]
policies:[1, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2364.0
now epsilon is 0.9363572292637572, the reward is 553.6875 with loss [87.90909884124994, 101.15520837157965] in episode 37
Report: 
rewardSum:553.6875
loss:[87.90909884124994, 101.15520837157965]
policies:[0, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2364.0
now epsilon is 0.9354212231099358, the reward is -3.0 with loss [28.052982926368713, 32.52488422393799] in episode 38
Report: 
rewardSum:-3.0
loss:[28.052982926368713, 32.52488422393799]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2364.0
now epsilon is 0.9330852991712035, the reward is 53.6875 with loss [67.94979846477509, 72.86466383934021] in episode 39
Report: 
rewardSum:53.6875
loss:[67.94979846477509, 72.86466383934021]
policies:[1, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2364.0
now epsilon is 0.9316865456983804, the reward is 556.6875 with loss [50.108426332473755, 47.192944794893265] in episode 40
Report: 
rewardSum:556.6875
loss:[50.108426332473755, 47.192944794893265]
policies:[0, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2364.0
now epsilon is 0.9305225196747907, the reward is -4.0 with loss [46.1980242729187, 50.12124490737915] in episode 41
Report: 
rewardSum:-4.0
loss:[46.1980242729187, 50.12124490737915]
policies:[1, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2364.0
now epsilon is 0.9288953260674148, the reward is 244.25 with loss [76.0840744972229, 40.37313098832965] in episode 42
Report: 
rewardSum:244.25
loss:[76.0840744972229, 40.37313098832965]
policies:[0, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2364.0
now epsilon is 0.928198828726224, the reward is -2.0 with loss [14.80373501777649, 26.53739833831787] in episode 43
Report: 
rewardSum:-2.0
loss:[14.80373501777649, 26.53739833831787]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2364.0
now epsilon is 0.9272709779140498, the reward is -3.0 with loss [30.005231857299805, 33.48031151294708] in episode 44
Report: 
rewardSum:-3.0
loss:[30.005231857299805, 33.48031151294708]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2364.0
now epsilon is 0.9249554066810175, the reward is 53.6875 with loss [76.23132061585784, 55.250660829246044] in episode 45
Report: 
rewardSum:53.6875
loss:[76.23132061585784, 55.250660829246044]
policies:[0, 0, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2365.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18*		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.9217233181844409, the reward is -13.0 with loss [126.7629227489233, 142.94962787628174] in episode 46
Report: 
rewardSum:-13.0
loss:[126.7629227489233, 142.94962787628174]
policies:[1, 0, 13]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2365.0
now epsilon is 0.9208019404548967, the reward is 247.25 with loss [23.64370369911194, 28.05370283126831] in episode 47
Report: 
rewardSum:247.25
loss:[23.64370369911194, 28.05370283126831]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2365.0
now epsilon is 0.9196515133866837, the reward is -4.0 with loss [36.417144656181335, 32.00467824935913] in episode 48
Report: 
rewardSum:-4.0
loss:[36.417144656181335, 32.00467824935913]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2365.0
now epsilon is 0.9182728980025605, the reward is 245.25 with loss [57.98796844482422, 52.56067419052124] in episode 49
Report: 
rewardSum:245.25
loss:[57.98796844482422, 52.56067419052124]
policies:[1, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2366.0
now epsilon is 0.9155218640294692, the reward is -11.0 with loss [80.76436764001846, 58.69587295129895] in episode 50
Report: 
rewardSum:-11.0
loss:[80.76436764001846, 58.69587295129895]
policies:[1, 0, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2366.0
now epsilon is 0.9148353942774917, the reward is -2.0 with loss [32.43501281738281, 27.036089420318604] in episode 51
Report: 
rewardSum:-2.0
loss:[32.43501281738281, 27.036089420318604]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2366.0
now epsilon is 0.9123227393325157, the reward is 240.25 with loss [98.75190770626068, 91.84426784515381] in episode 52
Report: 
rewardSum:240.25
loss:[98.75190770626068, 91.84426784515381]
policies:[0, 0, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2366.0
now epsilon is 0.9114107586571939, the reward is -3.0 with loss [29.540714263916016, 45.1750693321228] in episode 53
Report: 
rewardSum:-3.0
loss:[29.540714263916016, 45.1750693321228]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2366.0
now epsilon is 0.909589531311472, the reward is -7.0 with loss [76.58210599422455, 70.21212943270802] in episode 54
Report: 
rewardSum:-7.0
loss:[76.58210599422455, 70.21212943270802]
policies:[0, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2366.0
now epsilon is 0.9077719432348869, the reward is 554.6875 with loss [66.32293462753296, 84.39158153533936] in episode 55
Report: 
rewardSum:554.6875
loss:[66.32293462753296, 84.39158153533936]
policies:[0, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2362.0
now epsilon is 0.9073181139990159, the reward is -1.0 with loss [20.09520149230957, 15.22776460647583] in episode 56
Report: 
rewardSum:-1.0
loss:[20.09520149230957, 15.22776460647583]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2362.0
now epsilon is 0.9066377955214862, the reward is -2.0 with loss [19.22940492630005, 17.33136996254325] in episode 57
Report: 
rewardSum:-2.0
loss:[19.22940492630005, 17.33136996254325]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2347.0
now epsilon is 0.9043737492522802, the reward is -9.0 with loss [93.76125361397862, 63.064959451556206] in episode 58
Report: 
rewardSum:-9.0
loss:[93.76125361397862, 63.064959451556206]
policies:[0, 0, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2347.0
now epsilon is 0.9036956384962882, the reward is -2.0 with loss [25.508424282073975, 24.063172578811646] in episode 59
Report: 
rewardSum:-2.0
loss:[25.508424282073975, 24.063172578811646]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2347.0
now epsilon is 0.902340941970853, the reward is 245.25 with loss [52.9982213973999, 33.555997252464294] in episode 60
Report: 
rewardSum:245.25
loss:[52.9982213973999, 33.555997252464294]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2347.0
now epsilon is 0.8991878756095252, the reward is 548.6875 with loss [89.58882141113281, 110.0062744319439] in episode 61
Report: 
rewardSum:548.6875
loss:[89.58882141113281, 110.0062744319439]
policies:[0, 1, 13]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2351.0
now epsilon is 0.8987383378709627, the reward is -1.0 with loss [12.357164144515991, 4.469949722290039] in episode 62
Report: 
rewardSum:-1.0
loss:[12.357164144515991, 4.469949722290039]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2351.0
now epsilon is 0.8982890248731734, the reward is -1.0 with loss [20.048975944519043, 18.18880271911621] in episode 63
Report: 
rewardSum:-1.0
loss:[20.048975944519043, 18.18880271911621]
policies:[1, 1, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2351.0
############# STATE ###############
0-		8-		16*		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.8960458270653178, the reward is 990.0 with loss [69.34117019176483, 61.197928071022034] in episode 64
Report: 
rewardSum:990.0
loss:[69.34117019176483, 61.197928071022034]
policies:[0, 1, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2352.0
now epsilon is 0.8955978601546494, the reward is -1.0 with loss [26.469209671020508, 8.889062099158764] in episode 65
Report: 
rewardSum:-1.0
loss:[26.469209671020508, 8.889062099158764]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2352.0
now epsilon is 0.8951501171994384, the reward is -1.0 with loss [20.638575315475464, 21.141109466552734] in episode 66
Report: 
rewardSum:-1.0
loss:[20.638575315475464, 21.141109466552734]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2352.0
now epsilon is 0.8944789224381992, the reward is -2.0 with loss [25.07132625579834, 12.8430655002594] in episode 67
Report: 
rewardSum:-2.0
loss:[25.07132625579834, 12.8430655002594]
policies:[0, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2351.0
now epsilon is 0.8922452391776589, the reward is 53.6875 with loss [59.75051153451204, 100.71352285146713] in episode 68
Report: 
rewardSum:53.6875
loss:[59.75051153451204, 100.71352285146713]
policies:[0, 0, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2352.0
now epsilon is 0.8913533284746841, the reward is -3.0 with loss [24.317538149654865, 37.915242217481136] in episode 69
Report: 
rewardSum:-3.0
loss:[24.317538149654865, 37.915242217481136]
policies:[2, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2352.0
now epsilon is 0.8904623093480015, the reward is -3.0 with loss [17.31453776359558, 14.069838523864746] in episode 70
Report: 
rewardSum:-3.0
loss:[17.31453776359558, 14.069838523864746]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2
memory used:2352.0
Traceback (most recent call last):
  File "agentRunner.py", line 57, in <module>
    agent.train()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 231, in train
    self.episode()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 252, in episode
    qSums[policy]+= qs[i]
NameError: name 'i' is not defined
pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)
Hello from the pygame community. https://www.pygame.org/contribute.html
2021-03-25 13:17:49.947888: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
2021-03-25 13:17:51.748467: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 13:17:51.749386: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-25 13:17:52.709154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0001:00:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 149.31GiB/s
2021-03-25 13:17:52.709235: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 13:17:52.714881: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 13:17:52.714983: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 13:17:52.716165: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-25 13:17:52.717006: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-25 13:17:52.718111: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-25 13:17:52.719049: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-25 13:17:52.719246: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 13:17:52.720458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-25 13:17:52.721621: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 13:17:52.722249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0001:00:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 149.31GiB/s
2021-03-25 13:17:52.722298: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 13:17:52.722339: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 13:17:52.722379: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 13:17:52.722418: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-25 13:17:52.722457: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-25 13:17:52.722495: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-25 13:17:52.722534: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-25 13:17:52.722574: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 13:17:52.723681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-25 13:17:52.723755: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 13:17:53.294724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-25 13:17:53.294793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-25 13:17:53.294806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-25 13:17:53.296850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7468 MB memory) -> physical GPU (device: 0, name: Tesla M60, pci bus id: 0001:00:00.0, compute capability: 5.2)
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.9970041215644331, the reward is 550.6875 with loss [0, 0] in episode 0
No handles with labels found to put in legend.
Report: 
rewardSum:550.6875
loss:[0, 0]
policies:[0, 0, 12]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:24
memory used:1602.0
now epsilon is 0.9960074912571055, the reward is -3.0 with loss [0, 0] in episode 1
Report: 
rewardSum:-3.0
loss:[0, 0]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:32
memory used:1603.0
2021-03-25 13:17:54.192233: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 13:17:54.963701: W tensorflow/stream_executor/gpu/asm_compiler.cc:98] *** WARNING *** You are using ptxas 9.1.108, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You do not need to update to CUDA 9.2.88; cherry-picking the ptxas binary is sufficient.
2021-03-25 13:17:55.021459: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 13:17:55.251473: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 13:17:55.657114: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-03-25 13:17:55.659815: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2596990000 Hz
now epsilon is 0.994514413466049, the reward is 57.6875 with loss [16.42477723583579, 145.74925231933594] in episode 2
Report: 
rewardSum:57.6875
loss:[16.42477723583579, 145.74925231933594]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:44
memory used:2292.0
now epsilon is 0.992775317998912, the reward is -6.0 with loss [23.274795725941658, 45.189828395843506] in episode 3
Report: 
rewardSum:-6.0
loss:[23.274795725941658, 45.189828395843506]
policies:[0, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:58
memory used:2311.0
now epsilon is 0.990791503851314, the reward is 554.6875 with loss [68.64291475713253, 48.14617085456848] in episode 4
Report: 
rewardSum:554.6875
loss:[68.64291475713253, 48.14617085456848]
policies:[0, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:74
memory used:2311.0
now epsilon is 0.9885644509486061, the reward is 54.6875 with loss [57.50643577426672, 79.46370697021484] in episode 5
Report: 
rewardSum:54.6875
loss:[57.50643577426672, 79.46370697021484]
policies:[0, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:92
memory used:2311.0
now epsilon is 0.9873293630832584, the reward is 58.6875 with loss [25.61326053366065, 52.949822425842285] in episode 6
Report: 
rewardSum:58.6875
loss:[25.61326053366065, 52.949822425842285]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:102
memory used:2311.0
now epsilon is 0.9851100922120042, the reward is 991.0 with loss [109.7650055885315, 68.80284786224365] in episode 7
Report: 
rewardSum:991.0
loss:[109.7650055885315, 68.80284786224365]
policies:[0, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:120
memory used:2312.0
now epsilon is 0.9841253514745113, the reward is 59.6875 with loss [28.007691383361816, 14.812242865562439] in episode 8
Report: 
rewardSum:59.6875
loss:[28.007691383361816, 14.812242865562439]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:128
memory used:2318.0
now epsilon is 0.9814223871529224, the reward is 551.6875 with loss [121.5762884169817, 152.8611815571785] in episode 9
Report: 
rewardSum:551.6875
loss:[121.5762884169817, 152.8611815571785]
policies:[0, 0, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:150
memory used:2324.0
now epsilon is 0.9809317372982452, the reward is -1.0 with loss [9.369403444230556, 35.25093746185303] in episode 10
Report: 
rewardSum:-1.0
loss:[9.369403444230556, 35.25093746185303]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:154
memory used:2324.0
now epsilon is 0.9789715895961421, the reward is -7.0 with loss [79.54717391729355, 84.83680403232574] in episode 11
Report: 
rewardSum:-7.0
loss:[79.54717391729355, 84.83680403232574]
policies:[0, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:170
memory used:2324.0
now epsilon is 0.9782375444458216, the reward is -2.0 with loss [20.633115112781525, 38.68940353393555] in episode 12
Report: 
rewardSum:-2.0
loss:[20.633115112781525, 38.68940353393555]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:176
memory used:2324.0
now epsilon is 0.9765269121449788, the reward is 56.6875 with loss [66.21665423735976, 45.6990984082222] in episode 13
Report: 
rewardSum:56.6875
loss:[66.21665423735976, 45.6990984082222]
policies:[0, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:190
memory used:2324.0
now epsilon is 0.9750630369656341, the reward is 556.6875 with loss [60.79085350036621, 64.21950298547745] in episode 14
Report: 
rewardSum:556.6875
loss:[60.79085350036621, 64.21950298547745]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:202
memory used:2325.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28*		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.9723849628805905, the reward is 240.25 with loss [85.89757245779037, 83.45863622054458] in episode 15
Report: 
rewardSum:240.25
loss:[85.89757245779037, 83.45863622054458]
policies:[0, 0, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:224
memory used:2325.0
now epsilon is 0.9718988311732104, the reward is -1.0 with loss [23.53508424758911, 14.73900032043457] in episode 16
Report: 
rewardSum:-1.0
loss:[23.53508424758911, 14.73900032043457]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:228
memory used:2325.0
now epsilon is 0.9714129425013007, the reward is -1.0 with loss [21.098369598388672, 22.544492721557617] in episode 17
Report: 
rewardSum:-1.0
loss:[21.098369598388672, 22.544492721557617]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:232
memory used:2325.0
now epsilon is 0.9668090983071408, the reward is 44.6875 with loss [170.8113152384758, 151.795735899359] in episode 18
Report: 
rewardSum:44.6875
loss:[170.8113152384758, 151.795735899359]
policies:[0, 1, 18]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:270
memory used:2326.0
now epsilon is 0.964153694201763, the reward is 551.6875 with loss [56.17278319597244, 100.1184644587338] in episode 19
Report: 
rewardSum:551.6875
loss:[56.17278319597244, 100.1184644587338]
policies:[0, 0, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:292
memory used:2326.0
now epsilon is 0.9631899020049409, the reward is -3.0 with loss [28.526300255209208, 39.77483654022217] in episode 20
Report: 
rewardSum:-3.0
loss:[28.526300255209208, 39.77483654022217]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:300
memory used:2326.0
now epsilon is 0.961986516470644, the reward is -4.0 with loss [41.79270428419113, 44.18497037887573] in episode 21
Report: 
rewardSum:-4.0
loss:[41.79270428419113, 44.18497037887573]
policies:[1, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:310
memory used:2326.0
now epsilon is 0.9598242100161135, the reward is -8.0 with loss [92.74539983272552, 76.11159497499466] in episode 22
Report: 
rewardSum:-8.0
loss:[92.74539983272552, 76.11159497499466]
policies:[0, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:328
memory used:2327.0
now epsilon is 0.9574273471977809, the reward is 990.0 with loss [84.00600874423981, 41.488385550677776] in episode 23
Report: 
rewardSum:990.0
loss:[84.00600874423981, 41.488385550677776]
policies:[0, 0, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:348
memory used:2327.0
now epsilon is 0.9562311612562965, the reward is -4.0 with loss [46.11041308939457, 77.34239387512207] in episode 24
Report: 
rewardSum:-4.0
loss:[46.11041308939457, 77.34239387512207]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:358
memory used:2327.0
now epsilon is 0.9550364697998095, the reward is -4.0 with loss [64.70733165740967, 81.8062424659729] in episode 25
Report: 
rewardSum:-4.0
loss:[64.70733165740967, 81.8062424659729]
policies:[0, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:368
memory used:2327.0
now epsilon is 0.9538432709611476, the reward is -4.0 with loss [78.83283120393753, 52.46742385625839] in episode 26
Report: 
rewardSum:-4.0
loss:[78.83283120393753, 52.46742385625839]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:378
memory used:2327.0
now epsilon is 0.9514613436802705, the reward is 990.0 with loss [80.55167876556516, 104.98754471540451] in episode 27
Report: 
rewardSum:990.0
loss:[80.55167876556516, 104.98754471540451]
policies:[0, 0, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:398
memory used:2333.0
now epsilon is 0.9497975346016433, the reward is 56.6875 with loss [87.29960012435913, 80.93251568078995] in episode 28
Report: 
rewardSum:56.6875
loss:[87.29960012435913, 80.93251568078995]
policies:[1, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:412
memory used:2333.0
now epsilon is 0.9490853645228893, the reward is -2.0 with loss [46.087181091308594, 45.56911611557007] in episode 29
Report: 
rewardSum:-2.0
loss:[46.087181091308594, 45.56911611557007]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:418
memory used:2333.0
now epsilon is 0.9476626259471007, the reward is 57.6875 with loss [57.37974637746811, 27.090655747801065] in episode 30
Report: 
rewardSum:57.6875
loss:[57.37974637746811, 27.090655747801065]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:430
memory used:2333.0
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.946242020145803, the reward is -5.0 with loss [53.0964674949646, 63.34857987612486] in episode 31
Report: 
rewardSum:-5.0
loss:[53.0964674949646, 63.34857987612486]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:442
memory used:2334.0
now epsilon is 0.9445873380358527, the reward is 555.6875 with loss [56.480857253074646, 43.34230774641037] in episode 32
Report: 
rewardSum:555.6875
loss:[56.480857253074646, 43.34230774641037]
policies:[0, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:456
memory used:2335.0
now epsilon is 0.9426998155613671, the reward is 554.6875 with loss [76.51525058597326, 109.71496224403381] in episode 33
Report: 
rewardSum:554.6875
loss:[76.51525058597326, 109.71496224403381]
policies:[0, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:472
memory used:2335.0
now epsilon is 0.9401106291700063, the reward is 52.6875 with loss [100.13541361689568, 134.30416100099683] in episode 34
Report: 
rewardSum:52.6875
loss:[100.13541361689568, 134.30416100099683]
policies:[0, 0, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:494
memory used:2334.0
now epsilon is 0.9396406326123358, the reward is -1.0 with loss [24.988176345825195, 38.15383720397949] in episode 35
Report: 
rewardSum:-1.0
loss:[24.988176345825195, 38.15383720397949]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:498
memory used:2334.0
now epsilon is 0.9361231399564413, the reward is -14.0 with loss [142.89500069618225, 184.36168760061264] in episode 36
Report: 
rewardSum:-14.0
loss:[142.89500069618225, 184.36168760061264]
policies:[0, 0, 15]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:528
memory used:2334.0
now epsilon is 0.9347198325694668, the reward is 994.0 with loss [32.051579892635345, 61.8009441383183] in episode 37
Report: 
rewardSum:994.0
loss:[32.051579892635345, 61.8009441383183]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:540
memory used:2334.0
now epsilon is 0.9342525310731716, the reward is -1.0 with loss [4.971646010875702, 4.473975643515587] in episode 38
Report: 
rewardSum:-1.0
loss:[4.971646010875702, 4.473975643515587]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:544
memory used:2334.0
now epsilon is 0.9319195255797753, the reward is -9.0 with loss [99.0881889462471, 91.2969114780426] in episode 39
Report: 
rewardSum:-9.0
loss:[99.0881889462471, 91.2969114780426]
policies:[0, 1, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:564
memory used:2334.0
now epsilon is 0.9295923460429069, the reward is 552.6875 with loss [120.59976243972778, 76.66359461843967] in episode 40
Report: 
rewardSum:552.6875
loss:[120.59976243972778, 76.66359461843967]
policies:[1, 1, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:584
memory used:2334.0
now epsilon is 0.9279667790190425, the reward is -6.0 with loss [74.5166261382401, 85.28600311279297] in episode 41
Report: 
rewardSum:-6.0
loss:[74.5166261382401, 85.28600311279297]
policies:[1, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:598
memory used:2335.0
now epsilon is 0.9270391601695713, the reward is 59.6875 with loss [15.871995873749256, 42.68931394070387] in episode 42
Report: 
rewardSum:59.6875
loss:[15.871995873749256, 42.68931394070387]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:606
memory used:2341.0
now epsilon is 0.9249554066810175, the reward is 54.6875 with loss [64.38359304890037, 84.95254409313202] in episode 43
Report: 
rewardSum:54.6875
loss:[64.38359304890037, 84.95254409313202]
policies:[0, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:624
memory used:2341.0
now epsilon is 0.92449298678739, the reward is -1.0 with loss [17.187994956970215, 40.633174896240234] in episode 44
Report: 
rewardSum:-1.0
loss:[17.187994956970215, 40.633174896240234]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:628
memory used:2341.0
now epsilon is 0.9228763369521017, the reward is -6.0 with loss [65.28845791704953, 75.04120349884033] in episode 45
Report: 
rewardSum:-6.0
loss:[65.28845791704953, 75.04120349884033]
policies:[0, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:642
memory used:2341.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2*		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.9221843527242809, the reward is -2.0 with loss [38.74375605583191, 29.474472522735596] in episode 46
Report: 
rewardSum:-2.0
loss:[38.74375605583191, 29.474472522735596]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:648
memory used:2341.0
now epsilon is 0.9194216005083371, the reward is 51.6875 with loss [85.58754235506058, 173.95256280899048] in episode 47
Report: 
rewardSum:51.6875
loss:[85.58754235506058, 173.95256280899048]
policies:[0, 0, 12]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:672
memory used:2341.0
now epsilon is 0.9180433297780599, the reward is 57.6875 with loss [36.957128811627626, 77.9732757806778] in episode 48
Report: 
rewardSum:57.6875
loss:[36.957128811627626, 77.9732757806778]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:684
memory used:2342.0
now epsilon is 0.9171256306571565, the reward is -3.0 with loss [25.560090839862823, 34.836652517318726] in episode 49
Report: 
rewardSum:-3.0
loss:[25.560090839862823, 34.836652517318726]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:692
memory used:2342.0
now epsilon is 0.9164379583808894, the reward is -2.0 with loss [26.022725582122803, 29.93454122543335] in episode 50
Report: 
rewardSum:-2.0
loss:[26.022725582122803, 29.93454122543335]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:698
memory used:2342.0
now epsilon is 0.9146066854289224, the reward is 243.25 with loss [108.77427530288696, 60.977061688899994] in episode 51
Report: 
rewardSum:243.25
loss:[108.77427530288696, 60.977061688899994]
policies:[1, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:714
memory used:2342.0
now epsilon is 0.9118666349830207, the reward is 988.0 with loss [161.71292202174664, 130.0032085776329] in episode 52
Report: 
rewardSum:988.0
loss:[161.71292202174664, 130.0032085776329]
policies:[0, 0, 12]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:738
memory used:2347.0
now epsilon is 0.909589531311472, the reward is -9.0 with loss [84.03373392671347, 83.96860575675964] in episode 53
Report: 
rewardSum:-9.0
loss:[84.03373392671347, 83.96860575675964]
policies:[0, 1, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:758
memory used:2348.0
now epsilon is 0.9064111360726058, the reward is -13.0 with loss [183.36227560043335, 149.57275700569153] in episode 54
Report: 
rewardSum:-13.0
loss:[183.36227560043335, 149.57275700569153]
policies:[1, 2, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:786
memory used:2348.0
now epsilon is 0.9057314976584768, the reward is -2.0 with loss [22.595898628234863, 24.520965576171875] in episode 55
Report: 
rewardSum:-2.0
loss:[22.595898628234863, 24.520965576171875]
policies:[0, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:792
memory used:2348.0
now epsilon is 0.9036956384962882, the reward is 242.25 with loss [74.16958926618099, 106.82444586977363] in episode 56
Report: 
rewardSum:242.25
loss:[74.16958926618099, 106.82444586977363]
policies:[0, 2, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:810
memory used:2349.0
now epsilon is 0.902340941970853, the reward is 556.6875 with loss [78.9149866104126, 33.06831666082144] in episode 57
Report: 
rewardSum:556.6875
loss:[78.9149866104126, 33.06831666082144]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:822
memory used:2349.0
now epsilon is 0.9005378383942583, the reward is 55.6875 with loss [83.43555495142937, 95.09883184731007] in episode 58
Report: 
rewardSum:55.6875
loss:[83.43555495142937, 95.09883184731007]
policies:[0, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:838
memory used:2349.0
now epsilon is 0.8994127287917231, the reward is -4.0 with loss [58.86166763305664, 42.28191018104553] in episode 59
Report: 
rewardSum:-4.0
loss:[58.86166763305664, 42.28191018104553]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:848
memory used:2348.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34*		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.8971667248823825, the reward is -9.0 with loss [91.3053959608078, 124.12404704093933] in episode 60
Report: 
rewardSum:-9.0
loss:[91.3053959608078, 124.12404704093933]
policies:[0, 0, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:868
memory used:2348.0
now epsilon is 0.8967181975928615, the reward is -1.0 with loss [29.863014221191406, 19.112332820892334] in episode 61
Report: 
rewardSum:-1.0
loss:[29.863014221191406, 19.112332820892334]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:872
memory used:2348.0
now epsilon is 0.8958218156085515, the reward is -3.0 with loss [39.31971096992493, 50.08202838897705] in episode 62
Report: 
rewardSum:-3.0
loss:[39.31971096992493, 50.08202838897705]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:880
memory used:2348.0
now epsilon is 0.8949263296701385, the reward is 59.6875 with loss [20.705000042915344, 40.770824909210205] in episode 63
Report: 
rewardSum:59.6875
loss:[20.705000042915344, 40.770824909210205]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:888
memory used:2348.0
now epsilon is 0.8944789224381992, the reward is -1.0 with loss [8.399893522262573, 4.551891505718231] in episode 64
Report: 
rewardSum:-1.0
loss:[8.399893522262573, 4.551891505718231]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:892
memory used:2348.0
now epsilon is 0.8931380423490596, the reward is 245.25 with loss [65.60558915138245, 78.87175369262695] in episode 65
Report: 
rewardSum:245.25
loss:[65.60558915138245, 78.87175369262695]
policies:[1, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:904
memory used:2353.0
now epsilon is 0.8917991723233974, the reward is 57.6875 with loss [50.62885934114456, 57.63458567857742] in episode 66
Report: 
rewardSum:57.6875
loss:[50.62885934114456, 57.63458567857742]
policies:[1, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:916
memory used:2355.0
now epsilon is 0.8897946295637601, the reward is 242.25 with loss [58.71378964185715, 61.21361689269543] in episode 67
Report: 
rewardSum:242.25
loss:[58.71378964185715, 61.21361689269543]
policies:[0, 1, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:934
memory used:2355.0
now epsilon is 0.8886829422594359, the reward is -4.0 with loss [47.107191413640976, 28.048585057258606] in episode 68
Report: 
rewardSum:-4.0
loss:[47.107191413640976, 28.048585057258606]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:944
memory used:2355.0
now epsilon is 0.8877945925177406, the reward is -3.0 with loss [34.84811890125275, 17.444321990013123] in episode 69
Report: 
rewardSum:-3.0
loss:[34.84811890125275, 17.444321990013123]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:952
memory used:2355.0
now epsilon is 0.8855776012948512, the reward is 552.6875 with loss [111.7865361571312, 91.73842465877533] in episode 70
Report: 
rewardSum:552.6875
loss:[111.7865361571312, 91.73842465877533]
policies:[0, 0, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:972
memory used:2355.0
now epsilon is 0.8840290023290079, the reward is 56.6875 with loss [58.76947903633118, 56.50444169342518] in episode 71
Report: 
rewardSum:56.6875
loss:[58.76947903633118, 56.50444169342518]
policies:[0, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:986
memory used:2356.0
now epsilon is 0.8831453047823065, the reward is 247.25 with loss [22.0997531414032, 69.09876823425293] in episode 72
Report: 
rewardSum:247.25
loss:[22.0997531414032, 69.09876823425293]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:994
memory used:2356.0
now epsilon is 0.88204192497917, the reward is -4.0 with loss [27.477014809846878, 24.187011182308197] in episode 73
Report: 
rewardSum:-4.0
loss:[27.477014809846878, 24.187011182308197]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1004
memory used:2356.0
now epsilon is 0.880279383931035, the reward is 55.6875 with loss [42.669609889388084, 73.41689026355743] in episode 74
Report: 
rewardSum:55.6875
loss:[42.669609889388084, 73.41689026355743]
policies:[2, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1020
memory used:2356.0
now epsilon is 0.8778616393172712, the reward is 52.6875 with loss [96.64951272308826, 61.89494226872921] in episode 75
Report: 
rewardSum:52.6875
loss:[96.64951272308826, 61.89494226872921]
policies:[0, 1, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1042
memory used:2356.0
now epsilon is 0.8754505351964283, the reward is 240.25 with loss [68.69187873601913, 60.59302565455437] in episode 76
Report: 
rewardSum:240.25
loss:[68.69187873601913, 60.59302565455437]
policies:[2, 1, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1064
memory used:2356.0
now epsilon is 0.8745754129004705, the reward is 247.25 with loss [3.191679373383522, 40.862120151519775] in episode 77
Report: 
rewardSum:247.25
loss:[3.191679373383522, 40.862120151519775]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1072
memory used:2362.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34*		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.8732643694223159, the reward is 245.25 with loss [37.625112414360046, 51.824222803115845] in episode 78
Report: 
rewardSum:245.25
loss:[37.625112414360046, 51.824222803115845]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1084
memory used:2361.0
now epsilon is 0.870212905857161, the reward is 49.6875 with loss [56.60854232311249, 108.7204068750143] in episode 79
Report: 
rewardSum:49.6875
loss:[56.60854232311249, 108.7204068750143]
policies:[2, 0, 12]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1112
memory used:2361.0
now epsilon is 0.8697778537925391, the reward is -1.0 with loss [8.552401065826416, 25.55714225769043] in episode 80
Report: 
rewardSum:-1.0
loss:[8.552401065826416, 25.55714225769043]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1116
memory used:2361.0
now epsilon is 0.8689084020510842, the reward is 59.6875 with loss [36.16557002067566, 30.140058994293213] in episode 81
Report: 
rewardSum:59.6875
loss:[36.16557002067566, 30.140058994293213]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1124
memory used:2361.0
now epsilon is 0.8682568836562946, the reward is -2.0 with loss [25.038053274154663, 25.01929020881653] in episode 82
Report: 
rewardSum:-2.0
loss:[25.038053274154663, 25.01929020881653]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1130
memory used:2361.0
now epsilon is 0.8665218885790411, the reward is 992.0 with loss [44.22300034761429, 80.05970107018948] in episode 83
Report: 
rewardSum:992.0
loss:[44.22300034761429, 80.05970107018948]
policies:[0, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1146
memory used:2362.0
now epsilon is 0.8658721596219217, the reward is -2.0 with loss [25.218398213386536, 18.249816417694092] in episode 84
Report: 
rewardSum:-2.0
loss:[25.218398213386536, 18.249816417694092]
policies:[1, 0, 2]
qAverage:[-0.28813454508781433, 0.0]
ws:[-1.093596339225769, -2.8621742725372314]
memory len:1152
memory used:2362.0
now epsilon is 0.8654392776591207, the reward is -1.0 with loss [12.996101140975952, 13.351293921470642] in episode 85
Report: 
rewardSum:-1.0
loss:[12.996101140975952, 13.351293921470642]
policies:[1, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1156
memory used:2362.0
now epsilon is 0.8634939853872996, the reward is 242.25 with loss [58.57627773284912, 55.04909147694707] in episode 86
Report: 
rewardSum:242.25
loss:[58.57627773284912, 55.04909147694707]
policies:[1, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1174
memory used:2362.0
now epsilon is 0.8628465267898894, the reward is -2.0 with loss [17.89717385172844, 29.362900733947754] in episode 87
Report: 
rewardSum:-2.0
loss:[17.89717385172844, 29.362900733947754]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1180
memory used:2362.0
now epsilon is 0.8609070623772361, the reward is 54.6875 with loss [98.60332727432251, 54.109491646289825] in episode 88
Report: 
rewardSum:54.6875
loss:[98.60332727432251, 54.109491646289825]
policies:[0, 1, 8]
qAverage:[0.0, 1.3050537109375]
ws:[-3.3116893768310547, -2.334719181060791]
memory len:1198
memory used:2362.0
now epsilon is 0.8594016044879046, the reward is -6.0 with loss [53.62896180152893, 58.38942728936672] in episode 89
Report: 
rewardSum:-6.0
loss:[53.62896180152893, 58.38942728936672]
policies:[3, 0, 4]
qAverage:[1.1626912752787273, 0.0]
ws:[-3.9823387463887534, -4.376063505808513]
memory len:1212
memory used:2362.0
now epsilon is 0.8585425251053093, the reward is -3.0 with loss [35.70231342315674, 21.905250668525696] in episode 90
Report: 
rewardSum:-3.0
loss:[35.70231342315674, 21.905250668525696]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1220
memory used:2362.0
now epsilon is 0.8566127350180894, the reward is 991.0 with loss [89.05611425638199, 43.16398164629936] in episode 91
Report: 
rewardSum:991.0
loss:[89.05611425638199, 43.16398164629936]
policies:[0, 1, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1238
memory used:2362.0
now epsilon is 0.8551147865676797, the reward is -6.0 with loss [61.50793141126633, 111.68701648712158] in episode 92
Report: 
rewardSum:-6.0
loss:[61.50793141126633, 111.68701648712158]
policies:[1, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1252
memory used:2362.0
now epsilon is 0.8544736107984153, the reward is -2.0 with loss [17.019066393375397, 33.067893981933594] in episode 93
Report: 
rewardSum:-2.0
loss:[17.019066393375397, 33.067893981933594]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1258
memory used:2362.0
now epsilon is 0.8531927011842549, the reward is 245.25 with loss [28.52541634440422, 25.8473279774189] in episode 94
Report: 
rewardSum:245.25
loss:[28.52541634440422, 25.8473279774189]
policies:[0, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1270
memory used:2362.0
now epsilon is 0.8517007333061297, the reward is 244.25 with loss [59.563421219587326, 65.34669780731201] in episode 95
Report: 
rewardSum:244.25
loss:[59.563421219587326, 65.34669780731201]
policies:[0, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1284
memory used:2362.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18*		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.8495738752849367, the reward is 53.6875 with loss [69.67307758331299, 107.0424337387085] in episode 96
Report: 
rewardSum:53.6875
loss:[69.67307758331299, 107.0424337387085]
policies:[0, 1, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1304
memory used:2362.0
now epsilon is 0.8478762135455039, the reward is 55.6875 with loss [65.4412317276001, 89.16026824712753] in episode 97
Report: 
rewardSum:55.6875
loss:[65.4412317276001, 89.16026824712753]
policies:[0, 1, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1320
memory used:2361.0
now epsilon is 0.8472404653488868, the reward is -2.0 with loss [7.2195820808410645, 25.486186504364014] in episode 98
Report: 
rewardSum:-2.0
loss:[7.2195820808410645, 25.486186504364014]
policies:[0, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1326
memory used:2361.0
now epsilon is 0.8457589060744183, the reward is 244.25 with loss [47.47900342941284, 80.1564450263977] in episode 99
Report: 
rewardSum:244.25
loss:[47.47900342941284, 80.1564450263977]
policies:[1, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1340
memory used:2362.0
now epsilon is 0.8442799375849436, the reward is 56.6875 with loss [40.69256943464279, 33.51040840148926] in episode 100
Report: 
rewardSum:56.6875
loss:[40.69256943464279, 33.51040840148926]
policies:[1, 2, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1354
memory used:2362.0
now epsilon is 0.8434359741995711, the reward is 247.25 with loss [66.20379161834717, 33.239813804626465] in episode 101
Report: 
rewardSum:247.25
loss:[66.20379161834717, 33.239813804626465]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1362
memory used:2362.0
now epsilon is 0.841329754846999, the reward is 552.6875 with loss [57.869610488414764, 89.47315102815628] in episode 102
Report: 
rewardSum:552.6875
loss:[57.869610488414764, 89.47315102815628]
policies:[0, 0, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1382
memory used:2362.0
now epsilon is 0.8409091425526852, the reward is -1.0 with loss [16.011447429656982, 9.141754627227783] in episode 103
Report: 
rewardSum:-1.0
loss:[16.011447429656982, 9.141754627227783]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1386
memory used:2362.0
now epsilon is 0.8400685486985076, the reward is 247.25 with loss [16.756949424743652, 54.950973868370056] in episode 104
Report: 
rewardSum:247.25
loss:[16.756949424743652, 54.950973868370056]
policies:[1, 0, 3]
qAverage:[2.163090467453003, 0.0]
ws:[2.165572166442871, -0.2774047553539276]
memory len:1394
memory used:2362.0
now epsilon is 0.8396485669284426, the reward is -1.0 with loss [7.466393709182739, 19.141023635864258] in episode 105
Report: 
rewardSum:-1.0
loss:[7.466393709182739, 19.141023635864258]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1398
memory used:2362.0
now epsilon is 0.8388092331772521, the reward is -3.0 with loss [33.83002948760986, 22.224475383758545] in episode 106
Report: 
rewardSum:-3.0
loss:[33.83002948760986, 22.224475383758545]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1406
memory used:2362.0
now epsilon is 0.837970738445115, the reward is 59.6875 with loss [38.03802275657654, 34.286590576171875] in episode 107
Report: 
rewardSum:59.6875
loss:[38.03802275657654, 34.286590576171875]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1414
memory used:2362.0
now epsilon is 0.8367145676731981, the reward is 245.25 with loss [29.508005499839783, 64.10785102844238] in episode 108
Report: 
rewardSum:245.25
loss:[29.508005499839783, 64.10785102844238]
policies:[0, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1426
memory used:2362.0
now epsilon is 0.8358781668211965, the reward is 59.6875 with loss [17.999634742736816, 35.26245903968811] in episode 109
Report: 
rewardSum:59.6875
loss:[17.999634742736816, 35.26245903968811]
policies:[1, 1, 2]
qAverage:[3.4159412384033203, 0.0]
ws:[-0.5561591386795044, -0.5606146454811096]
memory len:1434
memory used:2362.0
now epsilon is 0.8342078725431813, the reward is 243.25 with loss [83.37837171554565, 55.970826745033264] in episode 110
Report: 
rewardSum:243.25
loss:[83.37837171554565, 55.970826745033264]
policies:[1, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1450
memory used:2362.0
now epsilon is 0.8323327807031853, the reward is 553.6875 with loss [62.63889339566231, 50.12372589111328] in episode 111
Report: 
rewardSum:553.6875
loss:[62.63889339566231, 50.12372589111328]
policies:[1, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1468
memory used:2362.0
now epsilon is 0.8317086871670492, the reward is -2.0 with loss [7.290169835090637, 9.740457952022552] in episode 112
Report: 
rewardSum:-2.0
loss:[7.290169835090637, 9.740457952022552]
policies:[1, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1474
memory used:2362.0
now epsilon is 0.8283880829619933, the reward is 47.6875 with loss [88.57398164272308, 153.93594074249268] in episode 113
Report: 
rewardSum:47.6875
loss:[88.57398164272308, 153.93594074249268]
policies:[2, 0, 14]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1506
memory used:2368.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20*		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.8269394906232573, the reward is 555.6875 with loss [56.92501926422119, 37.141409516334534] in episode 114
Report: 
rewardSum:555.6875
loss:[56.92501926422119, 37.141409516334534]
policies:[0, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1520
memory used:2368.0
now epsilon is 0.8263194410435236, the reward is -2.0 with loss [12.480057895183563, 18.764588713645935] in episode 115
Report: 
rewardSum:-2.0
loss:[12.480057895183563, 18.764588713645935]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1526
memory used:2374.0
now epsilon is 0.8252870580627737, the reward is -4.0 with loss [33.85716116428375, 31.260581493377686] in episode 116
Report: 
rewardSum:-4.0
loss:[33.85716116428375, 31.260581493377686]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1536
memory used:2374.0
now epsilon is 0.8242559649156717, the reward is 246.25 with loss [14.158154666423798, 14.949054151773453] in episode 117
Report: 
rewardSum:246.25
loss:[14.158154666423798, 14.949054151773453]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1546
memory used:2374.0
now epsilon is 0.8224032424891022, the reward is -8.0 with loss [67.63888192176819, 91.27422595024109] in episode 118
Report: 
rewardSum:-8.0
loss:[67.63888192176819, 91.27422595024109]
policies:[1, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1564
memory used:2374.0
now epsilon is 0.8219920922680604, the reward is -1.0 with loss [19.446237325668335, 4.870661973953247] in episode 119
Report: 
rewardSum:-1.0
loss:[19.446237325668335, 4.870661973953247]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1568
memory used:2374.0
now epsilon is 0.8211704083714556, the reward is -3.0 with loss [25.03084945678711, 21.992100477218628] in episode 120
Report: 
rewardSum:-3.0
loss:[25.03084945678711, 21.992100477218628]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1576
memory used:2374.0
now epsilon is 0.8205546845217979, the reward is -2.0 with loss [5.358496785163879, 10.963094055652618] in episode 121
Report: 
rewardSum:-2.0
loss:[5.358496785163879, 10.963094055652618]
policies:[1, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1582
memory used:2375.0
now epsilon is 0.8193246215086568, the reward is 245.25 with loss [29.46080881357193, 24.430146872997284] in episode 122
Report: 
rewardSum:245.25
loss:[29.46080881357193, 24.430146872997284]
policies:[1, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1594
memory used:2376.0
now epsilon is 0.8172786127698216, the reward is 241.25 with loss [44.392214357852936, 74.82700848579407] in episode 123
Report: 
rewardSum:241.25
loss:[44.392214357852936, 74.82700848579407]
policies:[0, 0, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1614
memory used:2376.0
now epsilon is 0.8154415736956924, the reward is 54.6875 with loss [69.51680105924606, 124.9232479929924] in episode 124
Report: 
rewardSum:54.6875
loss:[69.51680105924606, 124.9232479929924]
policies:[1, 1, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1632
memory used:2376.0
now epsilon is 0.814626437861625, the reward is 59.6875 with loss [42.853246986866, 20.471194863319397] in episode 125
Report: 
rewardSum:59.6875
loss:[42.853246986866, 20.471194863319397]
policies:[0, 2, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1640
memory used:2376.0
now epsilon is 0.8136086638285522, the reward is -4.0 with loss [31.359662652015686, 30.08876860141754] in episode 126
Report: 
rewardSum:-4.0
loss:[31.359662652015686, 30.08876860141754]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1650
memory used:2376.0
now epsilon is 0.8117798738869707, the reward is 242.25 with loss [54.793556571006775, 61.04843547940254] in episode 127
Report: 
rewardSum:242.25
loss:[54.793556571006775, 61.04843547940254]
policies:[0, 1, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1668
memory used:2376.0
now epsilon is 0.8107656562802086, the reward is -4.0 with loss [25.20403826236725, 11.689669847488403] in episode 128
Report: 
rewardSum:-4.0
loss:[25.20403826236725, 11.689669847488403]
policies:[0, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1678
memory used:2376.0
now epsilon is 0.8097527058117273, the reward is -4.0 with loss [23.424544274806976, 18.34093600511551] in episode 129
Report: 
rewardSum:-4.0
loss:[23.424544274806976, 18.34093600511551]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1688
memory used:2376.0
now epsilon is 0.8085388356431713, the reward is 57.6875 with loss [31.017116904258728, 28.18221613764763] in episode 130
Report: 
rewardSum:57.6875
loss:[31.017116904258728, 28.18221613764763]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1700
memory used:2376.0
now epsilon is 0.808134616759027, the reward is -1.0 with loss [15.489220142364502, 15.627639055252075] in episode 131
Report: 
rewardSum:-1.0
loss:[15.489220142364502, 15.627639055252075]
policies:[1, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1704
memory used:2376.0
now epsilon is 0.8065197610541917, the reward is -7.0 with loss [52.60207176208496, 57.62190389633179] in episode 132
Report: 
rewardSum:-7.0
loss:[52.60207176208496, 57.62190389633179]
policies:[1, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1720
memory used:2375.0
now epsilon is 0.8053107372728964, the reward is 245.25 with loss [19.404219806194305, 43.93605637550354] in episode 133
Report: 
rewardSum:245.25
loss:[19.404219806194305, 43.93605637550354]
policies:[1, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1732
memory used:2375.0
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.8037015243877141, the reward is 55.6875 with loss [61.14216983318329, 37.096094369888306] in episode 134
Report: 
rewardSum:55.6875
loss:[61.14216983318329, 37.096094369888306]
policies:[2, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1748
memory used:2375.0
now epsilon is 0.8024967253202021, the reward is -5.0 with loss [39.699758648872375, 39.73016142845154] in episode 135
Report: 
rewardSum:-5.0
loss:[39.699758648872375, 39.73016142845154]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1760
memory used:2375.0
now epsilon is 0.8020955271135873, the reward is -1.0 with loss [4.914025783538818, 6.650459349155426] in episode 136
Report: 
rewardSum:-1.0
loss:[4.914025783538818, 6.650459349155426]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1764
memory used:2375.0
now epsilon is 0.8012937323221686, the reward is 59.6875 with loss [18.621262073516846, 8.300845623016357] in episode 137
Report: 
rewardSum:59.6875
loss:[18.621262073516846, 8.300845623016357]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1772
memory used:2375.0
now epsilon is 0.8004927390249184, the reward is 59.6875 with loss [19.25612336397171, 27.98270344734192] in episode 138
Report: 
rewardSum:59.6875
loss:[19.25612336397171, 27.98270344734192]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1780
memory used:2375.0
now epsilon is 0.7992927501282168, the reward is 57.6875 with loss [33.34137672185898, 39.20868122577667] in episode 139
Report: 
rewardSum:57.6875
loss:[33.34137672185898, 39.20868122577667]
policies:[0, 2, 4]
qAverage:[0.0, 20.220365524291992]
ws:[1.0942554473876953, 1.8006551265716553]
memory len:1792
memory used:2375.0
now epsilon is 0.7984937570629174, the reward is 247.25 with loss [23.822781026363373, 16.87881100177765] in episode 140
Report: 
rewardSum:247.25
loss:[23.822781026363373, 16.87881100177765]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1800
memory used:2375.0
now epsilon is 0.7961015668351977, the reward is 51.6875 with loss [92.07580780982971, 45.45836102962494] in episode 141
Report: 
rewardSum:51.6875
loss:[92.07580780982971, 45.45836102962494]
policies:[3, 1, 8]
qAverage:[0.0, 24.46337127685547]
ws:[2.3027679920196533, 2.77767014503479]
memory len:1824
memory used:2375.0
now epsilon is 0.7943121284938526, the reward is 54.6875 with loss [43.741913199424744, 44.96470618247986] in episode 142
Report: 
rewardSum:54.6875
loss:[43.741913199424744, 44.96470618247986]
policies:[1, 1, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1842
memory used:2375.0
now epsilon is 0.7929231243693764, the reward is 244.25 with loss [68.06801199913025, 58.55945563316345] in episode 143
Report: 
rewardSum:244.25
loss:[68.06801199913025, 58.55945563316345]
policies:[1, 2, 4]
qAverage:[0.0, 26.245441436767578]
ws:[5.9831647872924805, 6.294295787811279]
memory len:1856
memory used:2381.0
now epsilon is 0.7915365491798094, the reward is 56.6875 with loss [49.72136205434799, 35.103815257549286] in episode 144
Report: 
rewardSum:56.6875
loss:[49.72136205434799, 35.103815257549286]
policies:[0, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1870
memory used:2381.0
now epsilon is 0.789954860578033, the reward is 55.6875 with loss [46.040143728256226, 35.112691432237625] in episode 145
Report: 
rewardSum:55.6875
loss:[46.040143728256226, 35.112691432237625]
policies:[1, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1886
memory used:2382.0
now epsilon is 0.7887706686230332, the reward is 57.6875 with loss [31.98471224308014, 22.286739587783813] in episode 146
Report: 
rewardSum:57.6875
loss:[31.98471224308014, 22.286739587783813]
policies:[1, 2, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1898
memory used:2382.0
now epsilon is 0.7879821936941159, the reward is 59.6875 with loss [24.44832706451416, 29.40235185623169] in episode 147
Report: 
rewardSum:59.6875
loss:[24.44832706451416, 29.40235185623169]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1906
memory used:2382.0
now epsilon is 0.787588251846156, the reward is -1.0 with loss [14.130064010620117, 8.824655532836914] in episode 148
Report: 
rewardSum:-1.0
loss:[14.130064010620117, 8.824655532836914]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1910
memory used:2382.0
now epsilon is 0.7864076075862979, the reward is 245.25 with loss [25.590295553207397, 20.747275352478027] in episode 149
Report: 
rewardSum:245.25
loss:[25.590295553207397, 20.747275352478027]
policies:[1, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1922
memory used:2382.0
now epsilon is 0.7850324260030478, the reward is 56.6875 with loss [52.68106955289841, 66.56932210922241] in episode 150
Report: 
rewardSum:56.6875
loss:[52.68106955289841, 66.56932210922241]
policies:[2, 2, 3]
qAverage:[22.181884288787842, 11.595993041992188]
ws:[5.490079164505005, 5.408079981803894]
memory len:1936
memory used:2382.0
now epsilon is 0.7836596491833945, the reward is 56.6875 with loss [28.544696927070618, 28.639343857765198] in episode 151
Report: 
rewardSum:56.6875
loss:[28.544696927070618, 28.639343857765198]
policies:[4, 0, 3]
qAverage:[39.60798009236654, 0.0]
ws:[6.351810614267985, 5.587986469268799]
memory len:1950
memory used:2382.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2*		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.7826805642867645, the reward is -4.0 with loss [15.963561177253723, 27.8750638961792] in episode 152
Report: 
rewardSum:-4.0
loss:[15.963561177253723, 27.8750638961792]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1960
memory used:2382.0
now epsilon is 0.7822892729221563, the reward is -1.0 with loss [12.422686040401459, 3.7808055877685547] in episode 153
Report: 
rewardSum:-1.0
loss:[12.422686040401459, 3.7808055877685547]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1964
memory used:2382.0
now epsilon is 0.7818981771787749, the reward is -1.0 with loss [12.098093032836914, 5.269598722457886] in episode 154
Report: 
rewardSum:-1.0
loss:[12.098093032836914, 5.269598722457886]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1968
memory used:2382.0
now epsilon is 0.7807260626982507, the reward is -5.0 with loss [38.143611431121826, 33.394914507865906] in episode 155
Report: 
rewardSum:-5.0
loss:[38.143611431121826, 33.394914507865906]
policies:[0, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1980
memory used:2382.0
now epsilon is 0.7789711846665023, the reward is 242.25 with loss [54.61636555194855, 50.24666512012482] in episode 156
Report: 
rewardSum:242.25
loss:[54.61636555194855, 50.24666512012482]
policies:[2, 1, 6]
qAverage:[28.547216415405273, 0.0]
ws:[0.06265656153361003, -0.38851459821065265]
memory len:1998
memory used:2382.0
now epsilon is 0.7778034579316053, the reward is 57.6875 with loss [58.942495465278625, 26.782495498657227] in episode 157
Report: 
rewardSum:57.6875
loss:[58.942495465278625, 26.782495498657227]
policies:[1, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2010
memory used:2382.0
now epsilon is 0.7770259461013608, the reward is 247.25 with loss [18.78617238998413, 20.4958958029747] in episode 158
Report: 
rewardSum:247.25
loss:[18.78617238998413, 20.4958958029747]
policies:[1, 2, 1]
qAverage:[0.0, 19.461069107055664]
ws:[2.9242448806762695, 2.97530460357666]
memory len:2018
memory used:2382.0
now epsilon is 0.7750855651652945, the reward is 53.6875 with loss [50.119618356227875, 53.08451932668686] in episode 159
Report: 
rewardSum:53.6875
loss:[50.119618356227875, 53.08451932668686]
policies:[2, 1, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2038
memory used:2382.0
now epsilon is 0.7739236632180954, the reward is -5.0 with loss [46.94485169649124, 26.406817197799683] in episode 160
Report: 
rewardSum:-5.0
loss:[46.94485169649124, 26.406817197799683]
policies:[0, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2050
memory used:2382.0
now epsilon is 0.7731500297278838, the reward is 247.25 with loss [36.27151441574097, 16.194558024406433] in episode 161
Report: 
rewardSum:247.25
loss:[36.27151441574097, 16.194558024406433]
policies:[1, 0, 3]
qAverage:[19.086986541748047, 0.0]
ws:[2.293268918991089, 2.1601879596710205]
memory len:2058
memory used:2382.0
now epsilon is 0.7716050820046852, the reward is 243.25 with loss [46.04997479915619, 31.109345078468323] in episode 162
Report: 
rewardSum:243.25
loss:[46.04997479915619, 31.109345078468323]
policies:[0, 1, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2074
memory used:2382.0
now epsilon is 0.7712193276890005, the reward is -1.0 with loss [13.74878478050232, 11.073183298110962] in episode 163
Report: 
rewardSum:-1.0
loss:[13.74878478050232, 11.073183298110962]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2078
memory used:2382.0
now epsilon is 0.7700632214746259, the reward is 245.25 with loss [37.15570020675659, 21.39671391248703] in episode 164
Report: 
rewardSum:245.25
loss:[37.15570020675659, 21.39671391248703]
policies:[1, 2, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2090
memory used:2388.0
now epsilon is 0.7685244419687195, the reward is -7.0 with loss [29.556621551513672, 32.696470737457275] in episode 165
Report: 
rewardSum:-7.0
loss:[29.556621551513672, 32.696470737457275]
policies:[1, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2106
memory used:2400.0
now epsilon is 0.7656475132933545, the reward is 547.6875 with loss [69.1742535829544, 106.04880809783936] in episode 166
Report: 
rewardSum:547.6875
loss:[69.1742535829544, 106.04880809783936]
policies:[1, 4, 10]
qAverage:[0.0, 21.319849014282227]
ws:[-0.01638517528772354, 0.7958700060844421]
memory len:2136
memory used:2400.0
now epsilon is 0.7648821528500287, the reward is 59.6875 with loss [16.000018894672394, 20.55624830722809] in episode 167
Report: 
rewardSum:59.6875
loss:[16.000018894672394, 20.55624830722809]
policies:[2, 1, 1]
qAverage:[0.0, 24.150405883789062]
ws:[4.098635673522949, 4.573976993560791]
memory len:2144
memory used:2401.0
now epsilon is 0.7637355464587912, the reward is 556.6875 with loss [33.92950481176376, 13.392800033092499] in episode 168
Report: 
rewardSum:556.6875
loss:[33.92950481176376, 13.392800033092499]
policies:[1, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2156
memory used:2401.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26*		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.761828354167491, the reward is 53.6875 with loss [27.054409742355347, 89.22163927555084] in episode 169
Report: 
rewardSum:53.6875
loss:[27.054409742355347, 89.22163927555084]
policies:[0, 2, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2176
memory used:2400.0
now epsilon is 0.7604961540308922, the reward is -6.0 with loss [30.800933092832565, 26.37882661819458] in episode 170
Report: 
rewardSum:-6.0
loss:[30.800933092832565, 26.37882661819458]
policies:[1, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2190
memory used:2400.0
now epsilon is 0.7580282455526989, the reward is 238.25 with loss [66.21210670471191, 41.26914197206497] in episode 171
Report: 
rewardSum:238.25
loss:[66.21210670471191, 41.26914197206497]
policies:[0, 2, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2216
memory used:2400.0
now epsilon is 0.7568919135990108, the reward is -5.0 with loss [22.06268334388733, 42.06388771533966] in episode 172
Report: 
rewardSum:-5.0
loss:[22.06268334388733, 42.06388771533966]
policies:[1, 1, 4]
qAverage:[23.003820419311523, 0.0]
ws:[2.4018592834472656, 2.175222635269165]
memory len:2228
memory used:2401.0
now epsilon is 0.7559462716462085, the reward is -4.0 with loss [39.57615280151367, 25.379450291395187] in episode 173
Report: 
rewardSum:-4.0
loss:[39.57615280151367, 25.379450291395187]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2238
memory used:2401.0
now epsilon is 0.7544357013476453, the reward is 243.25 with loss [25.064987182617188, 38.40173149108887] in episode 174
Report: 
rewardSum:243.25
loss:[25.064987182617188, 38.40173149108887]
policies:[1, 3, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2254
memory used:2402.0
now epsilon is 0.753304754843377, the reward is 556.6875 with loss [38.187254190444946, 38.20611500740051] in episode 175
Report: 
rewardSum:556.6875
loss:[38.187254190444946, 38.20611500740051]
policies:[1, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2266
memory used:2402.0
now epsilon is 0.7525517325307381, the reward is 59.6875 with loss [26.136062383651733, 14.246199607849121] in episode 176
Report: 
rewardSum:59.6875
loss:[26.136062383651733, 14.246199607849121]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2274
memory used:2402.0
now epsilon is 0.7504848002236568, the reward is 240.25 with loss [48.88653355836868, 39.107285261154175] in episode 177
Report: 
rewardSum:240.25
loss:[48.88653355836868, 39.107285261154175]
policies:[2, 1, 8]
qAverage:[0.0, 30.287307739257812]
ws:[-1.4622350931167603, -1.169908046722412]
memory len:2296
memory used:2402.0
now epsilon is 0.7480493798846974, the reward is 549.6875 with loss [70.39191430807114, 104.46068865060806] in episode 178
Report: 
rewardSum:549.6875
loss:[70.39191430807114, 104.46068865060806]
policies:[1, 1, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2322
memory used:2402.0
now epsilon is 0.7473016109765801, the reward is 59.6875 with loss [19.9493488073349, 13.56566059589386] in episode 179
Report: 
rewardSum:59.6875
loss:[19.9493488073349, 13.56566059589386]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2330
memory used:2402.0
now epsilon is 0.7452490984700083, the reward is 52.6875 with loss [43.62914949655533, 56.784561932086945] in episode 180
Report: 
rewardSum:52.6875
loss:[43.62914949655533, 56.784561932086945]
policies:[1, 2, 8]
qAverage:[0.0, 24.328954696655273]
ws:[0.6965253353118896, 1.071471929550171]
memory len:2352
memory used:2402.0
now epsilon is 0.7445041287933751, the reward is -3.0 with loss [27.63089656829834, 18.669081687927246] in episode 181
Report: 
rewardSum:-3.0
loss:[27.63089656829834, 18.669081687927246]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2360
memory used:2402.0
now epsilon is 0.7430164227667764, the reward is 55.6875 with loss [49.19513666629791, 38.52623808383942] in episode 182
Report: 
rewardSum:55.6875
loss:[49.19513666629791, 38.52623808383942]
policies:[1, 2, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2376
memory used:2402.0
############# STATE ###############
0-		8*		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.7404200869699955, the reward is 548.6875 with loss [64.28104370832443, 85.99832117557526] in episode 183
Report: 
rewardSum:548.6875
loss:[64.28104370832443, 85.99832117557526]
policies:[0, 3, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2404
memory used:2402.0
now epsilon is 0.7383864750169357, the reward is 240.25 with loss [56.490007519721985, 33.63224485516548] in episode 184
Report: 
rewardSum:240.25
loss:[56.490007519721985, 33.63224485516548]
policies:[0, 1, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2426
memory used:2409.0
now epsilon is 0.737279587311028, the reward is 245.25 with loss [26.97313743829727, 13.086487531661987] in episode 185
Report: 
rewardSum:245.25
loss:[26.97313743829727, 13.086487531661987]
policies:[1, 2, 3]
qAverage:[0.0, 38.29385757446289]
ws:[3.049611210823059, 3.709583600362142]
memory len:2438
memory used:2415.0
now epsilon is 0.7356223661513334, the reward is 553.6875 with loss [47.125457644462585, 58.84752058982849] in episode 186
Report: 
rewardSum:553.6875
loss:[47.125457644462585, 58.84752058982849]
policies:[2, 2, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2456
memory used:2415.0
now epsilon is 0.7326853874230054, the reward is 984.0 with loss [116.00767290592194, 50.41919243335724] in episode 187
Report: 
rewardSum:984.0
loss:[116.00767290592194, 50.41919243335724]
policies:[0, 1, 15]
qAverage:[0.0, 26.736547470092773]
ws:[1.3696422576904297, 1.9154154062271118]
memory len:2488
memory used:2415.0
now epsilon is 0.731404149243999, the reward is 244.25 with loss [41.15383553504944, 71.02244520187378] in episode 188
Report: 
rewardSum:244.25
loss:[41.15383553504944, 71.02244520187378]
policies:[1, 2, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2502
memory used:2415.0
now epsilon is 0.7292129513255947, the reward is 239.25 with loss [81.6775530576706, 77.36844229698181] in episode 189
Report: 
rewardSum:239.25
loss:[81.6775530576706, 77.36844229698181]
policies:[0, 3, 9]
qAverage:[0.0, 21.815237045288086]
ws:[2.014714002609253, 2.7574057579040527]
memory len:2526
memory used:2415.0
now epsilon is 0.7277558009077464, the reward is 55.6875 with loss [35.89119052886963, 40.21683168411255] in episode 190
Report: 
rewardSum:55.6875
loss:[35.89119052886963, 40.21683168411255]
policies:[2, 0, 6]
qAverage:[33.87553278605143, 0.0]
ws:[3.303139845530192, 2.4167824586232505]
memory len:2542
memory used:2415.0
now epsilon is 0.7257569722405083, the reward is 551.6875 with loss [54.84597761929035, 38.90674531459808] in episode 191
Report: 
rewardSum:551.6875
loss:[54.84597761929035, 38.90674531459808]
policies:[0, 1, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2564
memory used:2415.0
now epsilon is 0.7248502295099299, the reward is 58.6875 with loss [25.500022172927856, 19.466796398162842] in episode 192
Report: 
rewardSum:58.6875
loss:[25.500022172927856, 19.466796398162842]
policies:[0, 1, 4]
qAverage:[0.0, 25.553861618041992]
ws:[3.778907060623169, 4.475743770599365]
memory len:2574
memory used:2415.0
now epsilon is 0.7237636334862821, the reward is 57.6875 with loss [47.85800242424011, 57.093157172203064] in episode 193
Report: 
rewardSum:57.6875
loss:[47.85800242424011, 57.093157172203064]
policies:[2, 1, 3]
qAverage:[28.15736961364746, 0.0]
ws:[1.2364808320999146, 0.8141294121742249]
memory len:2586
memory used:2415.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18*		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.7226786663383253, the reward is 57.6875 with loss [43.82237255573273, 19.476789355278015] in episode 194
Report: 
rewardSum:57.6875
loss:[43.82237255573273, 19.476789355278015]
policies:[0, 3, 3]
qAverage:[0.0, 25.57349395751953]
ws:[2.0465424060821533, 2.2693076133728027]
memory len:2598
memory used:2414.0
now epsilon is 0.7215953256242728, the reward is 57.6875 with loss [29.201391339302063, 49.527854919433594] in episode 195
Report: 
rewardSum:57.6875
loss:[29.201391339302063, 49.527854919433594]
policies:[0, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2610
memory used:2414.0
now epsilon is 0.7203334805037716, the reward is 244.25 with loss [45.69946336746216, 39.34391212463379] in episode 196
Report: 
rewardSum:244.25
loss:[45.69946336746216, 39.34391212463379]
policies:[1, 2, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2624
memory used:2414.0
now epsilon is 0.7188940734962602, the reward is 243.25 with loss [47.79830491542816, 63.05787491798401] in episode 197
Report: 
rewardSum:243.25
loss:[47.79830491542816, 63.05787491798401]
policies:[1, 2, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2640
memory used:2415.0
now epsilon is 0.7181754489631136, the reward is 247.25 with loss [19.601019263267517, 19.514485359191895] in episode 198
Report: 
rewardSum:247.25
loss:[19.601019263267517, 19.514485359191895]
policies:[2, 2, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2648
memory used:2415.0
now epsilon is 0.7162029333559505, the reward is 52.6875 with loss [38.618919253349304, 66.15595707297325] in episode 199
Report: 
rewardSum:52.6875
loss:[38.618919253349304, 66.15595707297325]
policies:[1, 1, 9]
qAverage:[0.0, 27.146682739257812]
ws:[1.2084391117095947, 1.4466148614883423]
memory len:2670
memory used:2416.0
now epsilon is 0.7151293001723952, the reward is 57.6875 with loss [21.70215207338333, 30.270455479621887] in episode 200
Report: 
rewardSum:57.6875
loss:[21.70215207338333, 30.270455479621887]
policies:[1, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2682
memory used:2416.0
now epsilon is 0.7137002924227832, the reward is -7.0 with loss [44.7399787902832, 40.02145013213158] in episode 201
Report: 
rewardSum:-7.0
loss:[44.7399787902832, 40.02145013213158]
policies:[1, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2698
memory used:2416.0
now epsilon is 0.7133434868828401, the reward is -1.0 with loss [11.261017322540283, 3.687657594680786] in episode 202
Report: 
rewardSum:-1.0
loss:[11.261017322540283, 3.687657594680786]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2702
memory used:2416.0
now epsilon is 0.7126304108551839, the reward is 247.25 with loss [26.171711206436157, 13.228356838226318] in episode 203
Report: 
rewardSum:247.25
loss:[26.171711206436157, 13.228356838226318]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2710
memory used:2416.0
now epsilon is 0.7119180476361961, the reward is 59.6875 with loss [17.649834275245667, 22.205261707305908] in episode 204
Report: 
rewardSum:59.6875
loss:[17.649834275245667, 22.205261707305908]
policies:[0, 2, 2]
qAverage:[0.0, 24.94935417175293]
ws:[2.559542417526245, 2.7911994457244873]
memory len:2718
memory used:2417.0
now epsilon is 0.7115621331072561, the reward is -1.0 with loss [17.949103832244873, 12.65410041809082] in episode 205
Report: 
rewardSum:-1.0
loss:[17.949103832244873, 12.65410041809082]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2722
memory used:2417.0
now epsilon is 0.71031783291058, the reward is 244.25 with loss [31.563650608062744, 40.99680107831955] in episode 206
Report: 
rewardSum:244.25
loss:[31.563650608062744, 40.99680107831955]
policies:[2, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2736
memory used:2417.0
now epsilon is 0.7096077814024648, the reward is 247.25 with loss [20.697861552238464, 22.817195773124695] in episode 207
Report: 
rewardSum:247.25
loss:[20.697861552238464, 22.817195773124695]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2744
memory used:2417.0
now epsilon is 0.7088984396796328, the reward is 247.25 with loss [24.67733943462372, 20.760078370571136] in episode 208
Report: 
rewardSum:247.25
loss:[24.67733943462372, 20.760078370571136]
policies:[0, 2, 2]
qAverage:[0.0, 27.522836685180664]
ws:[4.019567489624023, 4.178206920623779]
memory len:2752
memory used:2417.0
now epsilon is 0.7074818827524506, the reward is 55.6875 with loss [45.51710033416748, 52.74232041835785] in episode 209
Report: 
rewardSum:55.6875
loss:[45.51710033416748, 52.74232041835785]
policies:[2, 1, 5]
qAverage:[0.0, 27.427043914794922]
ws:[-0.6494220495223999, -0.034658852964639664]
memory len:2768
memory used:2417.0
now epsilon is 0.7071281860286921, the reward is -1.0 with loss [10.862690448760986, 2.1915847659111023] in episode 210
Report: 
rewardSum:-1.0
loss:[10.862690448760986, 2.1915847659111023]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2772
memory used:2417.0
now epsilon is 0.7064213229715406, the reward is 247.25 with loss [20.29170036315918, 22.710102796554565] in episode 211
Report: 
rewardSum:247.25
loss:[20.29170036315918, 22.710102796554565]
policies:[2, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2780
memory used:2417.0
now epsilon is 0.7060681564613875, the reward is -1.0 with loss [9.139655590057373, 5.995393455028534] in episode 212
Report: 
rewardSum:-1.0
loss:[9.139655590057373, 5.995393455028534]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2784
memory used:2417.0
now epsilon is 0.7044810908363346, the reward is 242.25 with loss [35.65839743614197, 37.41505587100983] in episode 213
Report: 
rewardSum:242.25
loss:[35.65839743614197, 37.41505587100983]
policies:[0, 5, 4]
qAverage:[0.0, 30.95151646931966]
ws:[2.8020393451054892, 3.4025111198425293]
memory len:2802
memory used:2417.0
now epsilon is 0.70377687388188, the reward is 247.25 with loss [27.17969161272049, 15.078471660614014] in episode 214
Report: 
rewardSum:247.25
loss:[27.17969161272049, 15.078471660614014]
policies:[2, 2, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2810
memory used:2417.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10*		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.701843904898191, the reward is 52.6875 with loss [49.633372724056244, 35.728055238723755] in episode 215
Report: 
rewardSum:52.6875
loss:[49.633372724056244, 35.728055238723755]
policies:[3, 1, 7]
qAverage:[37.260613759358726, 0.0]
ws:[3.6188623905181885, 3.199822028477987]
memory len:2832
memory used:2417.0
now epsilon is 0.7002663343401315, the reward is 54.6875 with loss [26.472120583057404, 54.16992402076721] in episode 216
Report: 
rewardSum:54.6875
loss:[26.472120583057404, 54.16992402076721]
policies:[2, 0, 7]
qAverage:[34.412353515625, 0.0]
ws:[2.8592031002044678, 2.6099841594696045]
memory len:2850
memory used:2417.0
now epsilon is 0.6992165911195176, the reward is 57.6875 with loss [21.634007215499878, 18.094031929969788] in episode 217
Report: 
rewardSum:57.6875
loss:[21.634007215499878, 18.094031929969788]
policies:[2, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2862
memory used:2417.0
now epsilon is 0.6978193809546899, the reward is 243.25 with loss [53.84617619216442, 47.74635946750641] in episode 218
Report: 
rewardSum:243.25
loss:[53.84617619216442, 47.74635946750641]
policies:[1, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2878
memory used:2417.0
now epsilon is 0.6967733058708998, the reward is 57.6875 with loss [16.702914223074913, 34.92530053853989] in episode 219
Report: 
rewardSum:57.6875
loss:[16.702914223074913, 34.92530053853989]
policies:[2, 0, 4]
qAverage:[25.334699630737305, 0.0]
ws:[-0.0838010385632515, -0.4456433057785034]
memory len:2890
memory used:2417.0
now epsilon is 0.695033330975267, the reward is 241.25 with loss [34.10393297672272, 44.04012171924114] in episode 220
Report: 
rewardSum:241.25
loss:[34.10393297672272, 44.04012171924114]
policies:[3, 0, 7]
qAverage:[35.341898600260414, 0.0]
ws:[3.01930832862854, 2.835123856862386]
memory len:2910
memory used:2417.0
now epsilon is 0.6934710688936784, the reward is -8.0 with loss [54.32046175003052, 31.64425228536129] in episode 221
Report: 
rewardSum:-8.0
loss:[54.32046175003052, 31.64425228536129]
policies:[3, 0, 6]
qAverage:[37.47763252258301, 0.0]
ws:[4.074304103851318, 3.49537193775177]
memory len:2928
memory used:2417.0
now epsilon is 0.6922584043247452, the reward is 244.25 with loss [35.43515831232071, 28.094352662563324] in episode 222
Report: 
rewardSum:244.25
loss:[35.43515831232071, 28.094352662563324]
policies:[0, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2942
memory used:2417.0
now epsilon is 0.6893221829539882, the reward is 234.25 with loss [99.75321841239929, 88.0122018456459] in episode 223
Report: 
rewardSum:234.25
loss:[99.75321841239929, 88.0122018456459]
policies:[3, 0, 14]
qAverage:[37.55803426106771, 0.0]
ws:[0.3577222029368083, 0.1546205679575602]
memory len:2976
memory used:2417.0
now epsilon is 0.688288845703731, the reward is -5.0 with loss [45.64990699291229, 16.247725546360016] in episode 224
Report: 
rewardSum:-5.0
loss:[45.64990699291229, 16.247725546360016]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2988
memory used:2417.0
now epsilon is 0.6876008149233291, the reward is 247.25 with loss [11.561380982398987, 9.265197694301605] in episode 225
Report: 
rewardSum:247.25
loss:[11.561380982398987, 9.265197694301605]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2996
memory used:2429.0
now epsilon is 0.6869134719157391, the reward is 247.25 with loss [17.6690419241786, 9.676653981208801] in episode 226
Report: 
rewardSum:247.25
loss:[17.6690419241786, 9.676653981208801]
policies:[1, 1, 2]
qAverage:[0.0, 33.47713088989258]
ws:[-0.6360873579978943, -0.46172112226486206]
memory len:3004
memory used:2429.0
now epsilon is 0.6858837454746252, the reward is 245.25 with loss [30.845589339733124, 50.057435512542725] in episode 227
Report: 
rewardSum:245.25
loss:[30.845589339733124, 50.057435512542725]
policies:[3, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3016
memory used:2429.0
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.6834870494309833, the reward is 237.25 with loss [99.12547147274017, 45.509979486465454] in episode 228
Report: 
rewardSum:237.25
loss:[99.12547147274017, 45.509979486465454]
policies:[2, 2, 10]
qAverage:[22.318553924560547, 0.0]
ws:[2.1966652870178223, 1.9073110818862915]
memory len:3044
memory used:2429.0
now epsilon is 0.6828038186464805, the reward is 247.25 with loss [24.033472627401352, 22.160633087158203] in episode 229
Report: 
rewardSum:247.25
loss:[24.033472627401352, 22.160633087158203]
policies:[1, 1, 2]
qAverage:[0.0, 33.29301452636719]
ws:[-1.9199156761169434, -1.5952385663986206]
memory len:3052
memory used:2442.0
now epsilon is 0.6821212708365935, the reward is 247.25 with loss [21.29913604259491, 12.77084732055664] in episode 230
Report: 
rewardSum:247.25
loss:[21.29913604259491, 12.77084732055664]
policies:[0, 1, 3]
qAverage:[0.0, 33.51887512207031]
ws:[-1.9587669372558594, -1.2947818040847778]
memory len:3060
memory used:2442.0
now epsilon is 0.6814394053186036, the reward is 247.25 with loss [8.726104080677032, 17.66178822517395] in episode 231
Report: 
rewardSum:247.25
loss:[8.726104080677032, 17.66178822517395]
policies:[0, 2, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3068
memory used:2442.0
now epsilon is 0.6795677876459754, the reward is 551.6875 with loss [61.96865892410278, 42.85133221745491] in episode 232
Report: 
rewardSum:551.6875
loss:[61.96865892410278, 42.85133221745491]
policies:[1, 1, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3090
memory used:2430.0
now epsilon is 0.678209840719876, the reward is 243.25 with loss [31.205159604549408, 40.36114740371704] in episode 233
Report: 
rewardSum:243.25
loss:[31.205159604549408, 40.36114740371704]
policies:[1, 2, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3106
memory used:2430.0
now epsilon is 0.6773625021941698, the reward is 246.25 with loss [19.538575887680054, 25.92712664604187] in episode 234
Report: 
rewardSum:246.25
loss:[19.538575887680054, 25.92712664604187]
policies:[1, 2, 2]
qAverage:[29.474037170410156, 0.0]
ws:[3.0502960681915283, 2.9430503845214844]
memory len:3116
memory used:2430.0
now epsilon is 0.6763470932565885, the reward is 245.25 with loss [29.740470364689827, 25.266288220882416] in episode 235
Report: 
rewardSum:245.25
loss:[29.740470364689827, 25.266288220882416]
policies:[0, 2, 4]
qAverage:[0.0, 33.153446197509766]
ws:[-2.8326878547668457, -2.097078561782837]
memory len:3128
memory used:2430.0
now epsilon is 0.6753332064807848, the reward is 245.25 with loss [38.76857542991638, 28.31920623779297] in episode 236
Report: 
rewardSum:245.25
loss:[38.76857542991638, 28.31920623779297]
policies:[0, 3, 3]
qAverage:[0.0, 33.125465393066406]
ws:[-1.0273003578186035, -0.04363645613193512]
memory len:3140
memory used:2429.0
now epsilon is 0.6739837213102028, the reward is 992.0 with loss [31.669101238250732, 34.88232308626175] in episode 237
Report: 
rewardSum:992.0
loss:[31.669101238250732, 34.88232308626175]
policies:[2, 2, 4]
qAverage:[20.569374084472656, 16.352929433186848]
ws:[4.668636004130046, 4.665759881337483]
memory len:3156
memory used:2429.0
now epsilon is 0.6729733773773958, the reward is 245.25 with loss [39.469226717948914, 37.90408992767334] in episode 238
Report: 
rewardSum:245.25
loss:[39.469226717948914, 37.90408992767334]
policies:[1, 2, 3]
qAverage:[0.0, 32.304744720458984]
ws:[2.475402275721232, 3.7998029390970864]
memory len:3168
memory used:2429.0
now epsilon is 0.6719645480136065, the reward is -5.0 with loss [47.20576572418213, 23.832108110189438] in episode 239
Report: 
rewardSum:-5.0
loss:[47.20576572418213, 23.832108110189438]
policies:[1, 0, 5]
qAverage:[27.09244155883789, 0.0]
ws:[4.451834201812744, 4.250252723693848]
memory len:3180
memory used:2430.0
now epsilon is 0.6709572309484003, the reward is 245.25 with loss [41.013288259506226, 41.14055681228638] in episode 240
Report: 
rewardSum:245.25
loss:[41.013288259506226, 41.14055681228638]
policies:[2, 2, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3192
memory used:2430.0
now epsilon is 0.6689471246490145, the reward is 51.6875 with loss [43.17587625980377, 98.91849362850189] in episode 241
Report: 
rewardSum:51.6875
loss:[43.17587625980377, 98.91849362850189]
policies:[3, 2, 7]
qAverage:[14.418540000915527, 23.979801177978516]
ws:[4.9564409255981445, 4.91496342420578]
memory len:3216
memory used:2430.0
now epsilon is 0.6686126928958853, the reward is -1.0 with loss [20.113011598587036, 18.909972190856934] in episode 242
Report: 
rewardSum:-1.0
loss:[20.113011598587036, 18.909972190856934]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3220
memory used:2430.0
now epsilon is 0.667276636997453, the reward is 55.6875 with loss [39.86176609992981, 23.310250759124756] in episode 243
Report: 
rewardSum:55.6875
loss:[39.86176609992981, 23.310250759124756]
policies:[1, 2, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3236
memory used:2430.0
now epsilon is 0.6666096105474922, the reward is 247.25 with loss [32.516119956970215, 16.85537099838257] in episode 244
Report: 
rewardSum:247.25
loss:[32.516119956970215, 16.85537099838257]
policies:[1, 2, 1]
qAverage:[33.52374267578125, 0.0]
ws:[1.5336053371429443, 1.1776584386825562]
memory len:3244
memory used:2430.0
############# STATE ###############
0-		8-		16-		24-		32-		
1*		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.6659432508738884, the reward is 59.6875 with loss [13.565544754266739, 17.54626441001892] in episode 245
Report: 
rewardSum:59.6875
loss:[13.565544754266739, 17.54626441001892]
policies:[1, 1, 2]
qAverage:[27.95124626159668, 0.0]
ws:[3.079737901687622, 2.5652318000793457]
memory len:3252
memory used:2430.0
now epsilon is 0.6641141943978833, the reward is 240.25 with loss [39.95780235528946, 62.453203558921814] in episode 246
Report: 
rewardSum:240.25
loss:[39.95780235528946, 62.453203558921814]
policies:[4, 0, 7]
qAverage:[30.22553825378418, 0.0]
ws:[0.716421365737915, 0.6259158253669739]
memory len:3274
memory used:2435.0
now epsilon is 0.6634503292048038, the reward is 247.25 with loss [8.484444618225098, 36.00642955303192] in episode 247
Report: 
rewardSum:247.25
loss:[8.484444618225098, 36.00642955303192]
policies:[0, 2, 2]
qAverage:[0.0, 29.803064346313477]
ws:[2.265071392059326, 2.409632682800293]
memory len:3282
memory used:2436.0
now epsilon is 0.6622901615445188, the reward is 993.0 with loss [30.897153347730637, 19.32548588514328] in episode 248
Report: 
rewardSum:993.0
loss:[30.897153347730637, 19.32548588514328]
policies:[3, 0, 4]
qAverage:[32.68620936075846, 0.0]
ws:[3.754242420196533, 3.4036070505777993]
memory len:3296
memory used:2436.0
now epsilon is 0.6604711385160885, the reward is 240.25 with loss [57.67134007811546, 72.89212822914124] in episode 249
Report: 
rewardSum:240.25
loss:[57.67134007811546, 72.89212822914124]
policies:[2, 1, 8]
qAverage:[22.650360107421875, 19.112468719482422]
ws:[-0.05987548828125, -0.13386758168538412]
memory len:3318
memory used:2436.0
now epsilon is 0.6596459622842192, the reward is -4.0 with loss [28.413793206214905, 14.369220614433289] in episode 250
Report: 
rewardSum:-4.0
loss:[28.413793206214905, 14.369220614433289]
policies:[1, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3328
memory used:2436.0
now epsilon is 0.658327824163075, the reward is 243.25 with loss [28.77281092107296, 42.750627875328064] in episode 251
Report: 
rewardSum:243.25
loss:[28.77281092107296, 42.750627875328064]
policies:[0, 1, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3344
memory used:2436.0
now epsilon is 0.6556994445694134, the reward is 235.25 with loss [76.50206273794174, 93.73146475851536] in episode 252
Report: 
rewardSum:235.25
loss:[76.50206273794174, 93.73146475851536]
policies:[1, 4, 11]
qAverage:[0.0, 38.23555119832357]
ws:[0.9110714991887411, 1.4151514371236165]
memory len:3376
memory used:2437.0
now epsilon is 0.654716509915921, the reward is 57.6875 with loss [35.23951768875122, 33.856014370918274] in episode 253
Report: 
rewardSum:57.6875
loss:[35.23951768875122, 33.856014370918274]
policies:[1, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3388
memory used:2437.0
now epsilon is 0.6537350487432148, the reward is 245.25 with loss [33.134748458862305, 28.791331887245178] in episode 254
Report: 
rewardSum:245.25
loss:[33.134748458862305, 28.791331887245178]
policies:[1, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3400
memory used:2437.0
now epsilon is 0.6527550588424543, the reward is 57.6875 with loss [17.69626522064209, 39.19254523515701] in episode 255
Report: 
rewardSum:57.6875
loss:[17.69626522064209, 39.19254523515701]
policies:[1, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3412
memory used:2437.0
now epsilon is 0.6517765380081108, the reward is 245.25 with loss [25.331975400447845, 21.590425610542297] in episode 256
Report: 
rewardSum:245.25
loss:[25.331975400447845, 21.590425610542297]
policies:[1, 2, 3]
qAverage:[0.0, 28.600034713745117]
ws:[2.2227697372436523, 3.398859739303589]
memory len:3424
memory used:2437.0
now epsilon is 0.6506367841669516, the reward is 244.25 with loss [26.89109218120575, 19.28959584236145] in episode 257
Report: 
rewardSum:244.25
loss:[26.89109218120575, 19.28959584236145]
policies:[1, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3438
memory used:2437.0
now epsilon is 0.6493366486438608, the reward is 243.25 with loss [24.823779821395874, 61.32328748703003] in episode 258
Report: 
rewardSum:243.25
loss:[24.823779821395874, 61.32328748703003]
policies:[1, 4, 3]
qAverage:[0.0, 43.23131561279297]
ws:[0.05051851272583008, 1.5204532941182454]
memory len:3454
memory used:2437.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19*		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.6486875554558792, the reward is 59.6875 with loss [10.218804895877838, 8.864353954792023] in episode 259
Report: 
rewardSum:59.6875
loss:[10.218804895877838, 8.864353954792023]
policies:[0, 2, 2]
qAverage:[0.0, 25.09992027282715]
ws:[1.0921810865402222, 2.0872628688812256]
memory len:3462
memory used:2437.0
now epsilon is 0.646905892870286, the reward is -10.0 with loss [84.63393598794937, 40.4780530333519] in episode 260
Report: 
rewardSum:-10.0
loss:[84.63393598794937, 40.4780530333519]
policies:[2, 1, 8]
qAverage:[0.0, 28.187240600585938]
ws:[1.4758312702178955, 2.368772506713867]
memory len:3484
memory used:2437.0
now epsilon is 0.6464208347353804, the reward is -2.0 with loss [10.391424536705017, 9.968988299369812] in episode 261
Report: 
rewardSum:-2.0
loss:[10.391424536705017, 9.968988299369812]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3490
memory used:2437.0
now epsilon is 0.6451291237369292, the reward is 243.25 with loss [51.59364700317383, 51.93485927581787] in episode 262
Report: 
rewardSum:243.25
loss:[51.59364700317383, 51.93485927581787]
policies:[1, 3, 4]
qAverage:[0.0, 28.704418182373047]
ws:[1.8439339399337769, 2.239180326461792]
memory len:3506
memory used:2437.0
now epsilon is 0.6435181141441596, the reward is 552.6875 with loss [54.0777006149292, 51.64920938014984] in episode 263
Report: 
rewardSum:552.6875
loss:[54.0777006149292, 51.64920938014984]
policies:[1, 1, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3526
memory used:2437.0
now epsilon is 0.6427141185997638, the reward is -4.0 with loss [35.80844706296921, 23.8979549407959] in episode 264
Report: 
rewardSum:-4.0
loss:[35.80844706296921, 23.8979549407959]
policies:[1, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3536
memory used:2437.0
now epsilon is 0.6420716454587914, the reward is 59.6875 with loss [21.197999715805054, 13.453406929969788] in episode 265
Report: 
rewardSum:59.6875
loss:[21.197999715805054, 13.453406929969788]
policies:[0, 3, 1]
qAverage:[0.0, 40.782169342041016]
ws:[3.9269126256306968, 5.213813463846843]
memory len:3544
memory used:2437.0
now epsilon is 0.641109139732161, the reward is 57.6875 with loss [21.108226001262665, 8.923415668308735] in episode 266
Report: 
rewardSum:57.6875
loss:[21.108226001262665, 8.923415668308735]
policies:[2, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3556
memory used:2437.0
now epsilon is 0.6406284280753083, the reward is -2.0 with loss [18.5585595369339, 27.169793486595154] in episode 267
Report: 
rewardSum:-2.0
loss:[18.5585595369339, 27.169793486595154]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3562
memory used:2437.0
now epsilon is 0.639348291758532, the reward is 554.6875 with loss [59.76527452468872, 32.801171123981476] in episode 268
Report: 
rewardSum:554.6875
loss:[59.76527452468872, 32.801171123981476]
policies:[0, 3, 5]
qAverage:[0.0, 40.41055170694987]
ws:[2.185183842976888, 3.2189180850982666]
memory len:3578
memory used:2437.0
now epsilon is 0.6377517179979526, the reward is 241.25 with loss [38.252361595630646, 38.11819840967655] in episode 269
Report: 
rewardSum:241.25
loss:[38.252361595630646, 38.11819840967655]
policies:[1, 4, 5]
qAverage:[0.0, 40.79548772176107]
ws:[2.9448070526123047, 4.413348197937012]
memory len:3598
memory used:2437.0
now epsilon is 0.6361591311844034, the reward is 552.6875 with loss [39.776615142822266, 54.18718349933624] in episode 270
Report: 
rewardSum:552.6875
loss:[39.776615142822266, 54.18718349933624]
policies:[1, 1, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3618
memory used:2437.0
now epsilon is 0.6352054886880499, the reward is 245.25 with loss [41.459490060806274, 38.97885298728943] in episode 271
Report: 
rewardSum:245.25
loss:[41.459490060806274, 38.97885298728943]
policies:[2, 2, 2]
qAverage:[0.0, 36.90769958496094]
ws:[-0.685073971748352, -0.15055714547634125]
memory len:3630
memory used:2443.0
now epsilon is 0.6340947124427588, the reward is 244.25 with loss [60.02151143550873, 41.02252125740051] in episode 272
Report: 
rewardSum:244.25
loss:[60.02151143550873, 41.02252125740051]
policies:[1, 2, 4]
qAverage:[0.0, 37.742427825927734]
ws:[-1.670627236366272, -1.1455535888671875]
memory len:3644
memory used:2443.0
now epsilon is 0.6331441646397704, the reward is 57.6875 with loss [27.841194808483124, 31.41957116127014] in episode 273
Report: 
rewardSum:57.6875
loss:[27.841194808483124, 31.41957116127014]
policies:[0, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3656
memory used:2443.0
now epsilon is 0.6325112578646233, the reward is 247.25 with loss [19.670259714126587, 13.971874952316284] in episode 274
Report: 
rewardSum:247.25
loss:[19.670259714126587, 13.971874952316284]
policies:[2, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3664
memory used:2443.0
now epsilon is 0.6317210140130112, the reward is -4.0 with loss [23.15442442893982, 40.78327703475952] in episode 275
Report: 
rewardSum:-4.0
loss:[23.15442442893982, 40.78327703475952]
policies:[0, 1, 4]
qAverage:[0.0, 28.394990921020508]
ws:[3.4589569568634033, 3.75431752204895]
memory len:3674
memory used:2443.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2*		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.6310895298548985, the reward is 247.25 with loss [30.07935667037964, 23.8085994720459] in episode 276
Report: 
rewardSum:247.25
loss:[30.07935667037964, 23.8085994720459]
policies:[1, 1, 2]
qAverage:[0.0, 26.59053611755371]
ws:[2.3246335983276367, 2.6222410202026367]
memory len:3682
memory used:2443.0
now epsilon is 0.6304586769441768, the reward is 247.25 with loss [19.818431593477726, 12.163589417934418] in episode 277
Report: 
rewardSum:247.25
loss:[19.818431593477726, 12.163589417934418]
policies:[2, 2, 0]
qAverage:[0.0, 33.1987419128418]
ws:[2.51455020904541, 3.0779550075531006]
memory len:3690
memory used:2443.0
now epsilon is 0.6299859511376197, the reward is -2.0 with loss [17.611457109451294, 30.445935249328613] in episode 278
Report: 
rewardSum:-2.0
loss:[17.611457109451294, 30.445935249328613]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3696
memory used:2443.0
now epsilon is 0.6290415626259088, the reward is 245.25 with loss [41.52266162633896, 21.849650859832764] in episode 279
Report: 
rewardSum:245.25
loss:[41.52266162633896, 21.849650859832764]
policies:[1, 3, 2]
qAverage:[0.0, 41.493595123291016]
ws:[3.4663674036661782, 4.577683130900065]
memory len:3708
memory used:2444.0
now epsilon is 0.6280985898118965, the reward is 57.6875 with loss [25.543890714645386, 28.356841802597046] in episode 280
Report: 
rewardSum:57.6875
loss:[25.543890714645386, 28.356841802597046]
policies:[1, 3, 2]
qAverage:[0.0, 41.479084968566895]
ws:[4.688490152359009, 5.851167440414429]
memory len:3720
memory used:2443.0
now epsilon is 0.6273138590381222, the reward is 58.6875 with loss [31.099023044109344, 35.06024885177612] in episode 281
Report: 
rewardSum:58.6875
loss:[31.099023044109344, 35.06024885177612]
policies:[0, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3730
memory used:2443.0
now epsilon is 0.6266867803825766, the reward is 59.6875 with loss [20.321017265319824, 14.72908866405487] in episode 282
Report: 
rewardSum:59.6875
loss:[20.321017265319824, 14.72908866405487]
policies:[1, 1, 2]
qAverage:[0.0, 33.10268783569336]
ws:[2.16221284866333, 2.8503193855285645]
memory len:3738
memory used:2443.0
now epsilon is 0.6251218248136664, the reward is 53.6875 with loss [44.5287002325058, 50.33307747542858] in episode 283
Report: 
rewardSum:53.6875
loss:[44.5287002325058, 50.33307747542858]
policies:[0, 0, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3758
memory used:2443.0
now epsilon is 0.6244969373704693, the reward is 247.25 with loss [18.46155881881714, 28.492464542388916] in episode 284
Report: 
rewardSum:247.25
loss:[18.46155881881714, 28.492464542388916]
policies:[2, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3766
memory used:2443.0
now epsilon is 0.6235607772351738, the reward is 245.25 with loss [25.165491819381714, 29.046839356422424] in episode 285
Report: 
rewardSum:245.25
loss:[25.165491819381714, 29.046839356422424]
policies:[2, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3778
memory used:2444.0
now epsilon is 0.6229374502542601, the reward is 247.25 with loss [13.90766191482544, 22.03069496154785] in episode 286
Report: 
rewardSum:247.25
loss:[13.90766191482544, 22.03069496154785]
policies:[0, 3, 1]
qAverage:[0.0, 32.97296142578125]
ws:[3.76719331741333, 3.863245964050293]
memory len:3786
memory used:2444.0
now epsilon is 0.6223147463666187, the reward is 59.6875 with loss [24.868757486343384, 15.773399516940117] in episode 287
Report: 
rewardSum:59.6875
loss:[24.868757486343384, 15.773399516940117]
policies:[0, 3, 1]
qAverage:[0.0, 34.669219970703125]
ws:[3.0292768478393555, 3.26545786857605]
memory len:3794
memory used:2445.0
now epsilon is 0.622003627888107, the reward is -1.0 with loss [5.2066139578819275, 8.50200867652893] in episode 288
Report: 
rewardSum:-1.0
loss:[5.2066139578819275, 8.50200867652893]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3798
memory used:2445.0
now epsilon is 0.6212265120083386, the reward is 246.25 with loss [11.420536875724792, 18.394752144813538] in episode 289
Report: 
rewardSum:246.25
loss:[11.420536875724792, 18.394752144813538]
policies:[2, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3808
memory used:2445.0
now epsilon is 0.620605518417448, the reward is 247.25 with loss [18.79420393705368, 18.38824987411499] in episode 290
Report: 
rewardSum:247.25
loss:[18.79420393705368, 18.38824987411499]
policies:[0, 2, 2]
qAverage:[0.0, 32.93648147583008]
ws:[3.2962913513183594, 3.690089702606201]
memory len:3816
memory used:2445.0
now epsilon is 0.6193653928974104, the reward is 243.25 with loss [49.372092604637146, 44.533706948161125] in episode 291
Report: 
rewardSum:243.25
loss:[49.372092604637146, 44.533706948161125]
policies:[1, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3832
memory used:2445.0
now epsilon is 0.618436925269605, the reward is 245.25 with loss [46.493758752942085, 44.80604445934296] in episode 292
Report: 
rewardSum:245.25
loss:[46.493758752942085, 44.80604445934296]
policies:[2, 2, 2]
qAverage:[0.0, 33.34223175048828]
ws:[1.3772648572921753, 2.205662488937378]
memory len:3844
memory used:2445.0
now epsilon is 0.6170468328594363, the reward is 553.6875 with loss [55.010456681251526, 52.29560708999634] in episode 293
Report: 
rewardSum:553.6875
loss:[55.010456681251526, 52.29560708999634]
policies:[0, 1, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3862
memory used:2445.0
now epsilon is 0.6164300173805761, the reward is 59.6875 with loss [22.979522585868835, 13.738883972167969] in episode 294
Report: 
rewardSum:59.6875
loss:[22.979522585868835, 13.738883972167969]
policies:[2, 1, 1]
qAverage:[0.0, 25.60256576538086]
ws:[2.1150755882263184, 2.9591381549835205]
memory len:3870
memory used:2450.0
now epsilon is 0.6156598650313063, the reward is 246.25 with loss [40.335853934288025, 50.159419775009155] in episode 295
Report: 
rewardSum:246.25
loss:[40.335853934288025, 50.159419775009155]
policies:[0, 2, 3]
qAverage:[0.0, 42.31936772664388]
ws:[5.483914693196614, 6.651078224182129]
memory len:3880
memory used:2450.0
now epsilon is 0.6153520735775323, the reward is -1.0 with loss [8.400622367858887, 6.424478828907013] in episode 296
Report: 
rewardSum:-1.0
loss:[8.400622367858887, 6.424478828907013]
policies:[0, 1, 1]
qAverage:[0.0, 24.913604736328125]
ws:[1.9339808225631714, 2.868143320083618]
memory len:3884
memory used:2450.0
now epsilon is 0.6145832679844697, the reward is 58.6875 with loss [18.039861917495728, 21.779760360717773] in episode 297
Report: 
rewardSum:58.6875
loss:[18.039861917495728, 21.779760360717773]
policies:[2, 1, 2]
qAverage:[0.0, 32.70809555053711]
ws:[2.798497200012207, 3.5475518703460693]
memory len:3894
memory used:2452.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18*		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.6139689151468019, the reward is 247.25 with loss [30.22463297843933, 37.71591091156006] in episode 298
Report: 
rewardSum:247.25
loss:[30.22463297843933, 37.71591091156006]
policies:[1, 2, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3902
memory used:2452.0
now epsilon is 0.6136619690622857, the reward is -1.0 with loss [14.245991945266724, 5.584179997444153] in episode 299
Report: 
rewardSum:-1.0
loss:[14.245991945266724, 5.584179997444153]
policies:[0, 1, 1]
qAverage:[0.0, 28.04817008972168]
ws:[0.8156623244285583, 1.8921812772750854]
memory len:3906
memory used:2452.0
now epsilon is 0.6133551764316276, the reward is -1.0 with loss [5.43055272102356, 12.718895435333252] in episode 300
Report: 
rewardSum:-1.0
loss:[5.43055272102356, 12.718895435333252]
policies:[1, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3910
memory used:2456.0
now epsilon is 0.6119765065290768, the reward is 242.25 with loss [28.749690294265747, 49.065112829208374] in episode 301
Report: 
rewardSum:242.25
loss:[28.749690294265747, 49.065112829208374]
policies:[1, 5, 3]
qAverage:[0.0, 52.214742279052736]
ws:[5.272162866592407, 6.374754333496094]
memory len:3928
memory used:2456.0
now epsilon is 0.610753623939593, the reward is 55.6875 with loss [34.80037087202072, 40.13150668144226] in episode 302
Report: 
rewardSum:55.6875
loss:[34.80037087202072, 40.13150668144226]
policies:[0, 2, 6]
qAverage:[0.0, 27.045169830322266]
ws:[2.511857032775879, 2.9688897132873535]
memory len:3944
memory used:2456.0
now epsilon is 0.6095331849763135, the reward is 243.25 with loss [34.11100643873215, 51.011889934539795] in episode 303
Report: 
rewardSum:243.25
loss:[34.11100643873215, 51.011889934539795]
policies:[2, 2, 4]
qAverage:[0.0, 40.137359619140625]
ws:[2.000581661860148, 3.1061981519063315]
memory len:3960
memory used:2456.0
now epsilon is 0.60831518475626, the reward is 243.25 with loss [29.066080331802368, 27.4367915391922] in episode 304
Report: 
rewardSum:243.25
loss:[29.066080331802368, 27.4367915391922]
policies:[1, 1, 6]
qAverage:[0.0, 36.338401794433594]
ws:[2.5265109539031982, 3.6971933841705322]
memory len:3976
memory used:2456.0
now epsilon is 0.6070996184062114, the reward is 243.25 with loss [26.803555130958557, 28.680985376238823] in episode 305
Report: 
rewardSum:243.25
loss:[26.803555130958557, 28.680985376238823]
policies:[0, 5, 3]
qAverage:[0.0, 51.59849262237549]
ws:[3.738217096310109, 5.090614527463913]
memory len:3992
memory used:2456.0
now epsilon is 0.605583575690059, the reward is 241.25 with loss [46.89402365684509, 42.76501244306564] in episode 306
Report: 
rewardSum:241.25
loss:[46.89402365684509, 42.76501244306564]
policies:[2, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4012
memory used:2458.0
now epsilon is 0.6046757678719169, the reward is 245.25 with loss [48.308457374572754, 27.71126027405262] in episode 307
Report: 
rewardSum:245.25
loss:[48.308457374572754, 27.71126027405262]
policies:[2, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4024
memory used:2458.0
now epsilon is 0.6028642327784397, the reward is 51.6875 with loss [70.21506249904633, 68.72371339797974] in episode 308
Report: 
rewardSum:51.6875
loss:[70.21506249904633, 68.72371339797974]
policies:[1, 4, 7]
qAverage:[0.0, 46.37828063964844]
ws:[2.226063519716263, 3.095411936442057]
memory len:4048
memory used:2458.0
now epsilon is 0.6013587666222724, the reward is 241.25 with loss [46.8088344335556, 64.98887085914612] in episode 309
Report: 
rewardSum:241.25
loss:[46.8088344335556, 64.98887085914612]
policies:[0, 4, 6]
qAverage:[0.0, 54.16801071166992]
ws:[3.3705541789531708, 4.315230190753937]
memory len:4068
memory used:2470.0
now epsilon is 0.6000070616656099, the reward is 991.0 with loss [46.42497360706329, 43.05564880371094] in episode 310
Report: 
rewardSum:991.0
loss:[46.42497360706329, 43.05564880371094]
policies:[1, 1, 7]
qAverage:[0.0, 27.615867614746094]
ws:[-9.417499542236328, -9.01159381866455]
memory len:4086
memory used:2470.0
now epsilon is 0.5983591032241842, the reward is 551.6875 with loss [71.4873548746109, 57.127241015434265] in episode 311
Report: 
rewardSum:551.6875
loss:[71.4873548746109, 57.127241015434265]
policies:[0, 3, 8]
qAverage:[0.0, 28.146682739257812]
ws:[2.5703139305114746, 3.3640923500061035]
memory len:4108
memory used:2470.0
now epsilon is 0.598059961070016, the reward is -1.0 with loss [6.80851149559021, 12.691454410552979] in episode 312
Report: 
rewardSum:-1.0
loss:[6.80851149559021, 12.691454410552979]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4112
memory used:2470.0
############# STATE ###############
0-		8*		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.5971634316227658, the reward is 245.25 with loss [23.638498067855835, 20.246670603752136] in episode 313
Report: 
rewardSum:245.25
loss:[23.638498067855835, 20.246670603752136]
policies:[0, 2, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4124
memory used:2475.0
now epsilon is 0.5962682461294703, the reward is 245.25 with loss [29.913851261138916, 33.50372076034546] in episode 314
Report: 
rewardSum:245.25
loss:[29.913851261138916, 33.50372076034546]
policies:[1, 3, 2]
qAverage:[0.0, 42.45123291015625]
ws:[0.896306037902832, 1.434648036956787]
memory len:4136
memory used:2475.0
now epsilon is 0.5956722014466688, the reward is 247.25 with loss [9.496210098266602, 10.067138671875] in episode 315
Report: 
rewardSum:247.25
loss:[9.496210098266602, 10.067138671875]
policies:[2, 1, 1]
qAverage:[0.0, 35.50517654418945]
ws:[2.8866660594940186, 3.352403163909912]
memory len:4144
memory used:2475.0
now epsilon is 0.5946305565882248, the reward is 56.6875 with loss [36.90772771835327, 35.54940354824066] in episode 316
Report: 
rewardSum:56.6875
loss:[36.90772771835327, 35.54940354824066]
policies:[0, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4158
memory used:2475.0
now epsilon is 0.5924045900397641, the reward is 48.6875 with loss [69.7628538608551, 81.9844936132431] in episode 317
Report: 
rewardSum:48.6875
loss:[69.7628538608551, 81.9844936132431]
policies:[2, 2, 11]
qAverage:[0.0, 31.994441986083984]
ws:[3.0965709686279297, 3.5615267753601074]
memory len:4188
memory used:2475.0
now epsilon is 0.5913686592143288, the reward is 244.25 with loss [46.6047140955925, 28.04292106628418] in episode 318
Report: 
rewardSum:244.25
loss:[46.6047140955925, 28.04292106628418]
policies:[1, 1, 5]
qAverage:[0.0, 34.71321487426758]
ws:[4.260293006896973, 4.696506977081299]
memory len:4202
memory used:2475.0
now epsilon is 0.5910730118452628, the reward is -1.0 with loss [11.196934700012207, 7.1877007484436035] in episode 319
Report: 
rewardSum:-1.0
loss:[11.196934700012207, 7.1877007484436035]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4206
memory used:2487.0
now epsilon is 0.5904821604488573, the reward is 247.25 with loss [25.1063032746315, 8.07169246673584] in episode 320
Report: 
rewardSum:247.25
loss:[25.1063032746315, 8.07169246673584]
policies:[0, 3, 1]
qAverage:[0.0, 46.795284271240234]
ws:[2.189496914545695, 3.03021502494812]
memory len:4214
memory used:2487.0
now epsilon is 0.5895969906007185, the reward is 245.25 with loss [44.56655675172806, 43.86006420850754] in episode 321
Report: 
rewardSum:245.25
loss:[44.56655675172806, 43.86006420850754]
policies:[0, 3, 3]
qAverage:[0.0, 55.68498229980469]
ws:[4.222638249397278, 5.055663764476776]
memory len:4226
memory used:2488.0
now epsilon is 0.5888603627684736, the reward is -4.0 with loss [19.247893691062927, 26.003111600875854] in episode 322
Report: 
rewardSum:-4.0
loss:[19.247893691062927, 26.003111600875854]
policies:[0, 2, 3]
qAverage:[0.0, 27.98859405517578]
ws:[1.5099366903305054, 2.153041124343872]
memory len:4236
memory used:2454.0
now epsilon is 0.5882717231915396, the reward is 59.6875 with loss [22.765619456768036, 21.327483981847763] in episode 323
Report: 
rewardSum:59.6875
loss:[22.765619456768036, 21.327483981847763]
policies:[1, 2, 1]
qAverage:[0.0, 30.99431610107422]
ws:[4.343468189239502, 4.837352275848389]
memory len:4244
memory used:2454.0
now epsilon is 0.5879776240969266, the reward is -1.0 with loss [22.390722036361694, 3.4991527795791626] in episode 324
Report: 
rewardSum:-1.0
loss:[22.390722036361694, 3.4991527795791626]
policies:[0, 1, 1]
qAverage:[0.0, 29.03370475769043]
ws:[2.6740312576293945, 3.091749429702759]
memory len:4248
memory used:2454.0
now epsilon is 0.5875367511154713, the reward is -2.0 with loss [7.67880117893219, 33.11997985839844] in episode 325
Report: 
rewardSum:-2.0
loss:[7.67880117893219, 33.11997985839844]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4254
memory used:2454.0
now epsilon is 0.5869494346539188, the reward is -3.0 with loss [13.567367672920227, 23.147023558616638] in episode 326
Report: 
rewardSum:-3.0
loss:[13.567367672920227, 23.147023558616638]
policies:[0, 2, 2]
qAverage:[0.0, 31.35410499572754]
ws:[2.88782000541687, 3.4446969032287598]
memory len:4262
memory used:2461.0
now epsilon is 0.5860695605836458, the reward is 245.25 with loss [38.917159736156464, 48.20217728614807] in episode 327
Report: 
rewardSum:245.25
loss:[38.917159736156464, 48.20217728614807]
policies:[2, 3, 1]
qAverage:[0.0, 29.38667106628418]
ws:[1.5927661657333374, 2.126404047012329]
memory len:4274
memory used:2461.0
now epsilon is 0.5854837107625204, the reward is 247.25 with loss [21.000062465667725, 14.057309709489346] in episode 328
Report: 
rewardSum:247.25
loss:[21.000062465667725, 14.057309709489346]
policies:[0, 3, 1]
qAverage:[0.0, 47.37697092692057]
ws:[4.961948871612549, 5.478940010070801]
memory len:4282
memory used:2461.0
now epsilon is 0.584898446571559, the reward is 247.25 with loss [17.491317629814148, 23.46093726158142] in episode 329
Report: 
rewardSum:247.25
loss:[17.491317629814148, 23.46093726158142]
policies:[0, 2, 2]
qAverage:[0.0, 39.52743657430013]
ws:[2.2675509452819824, 3.15990940729777]
memory len:4290
memory used:2461.0
now epsilon is 0.5835837403208864, the reward is 242.25 with loss [27.22260195016861, 46.66832947731018] in episode 330
Report: 
rewardSum:242.25
loss:[27.22260195016861, 46.66832947731018]
policies:[0, 4, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4308
memory used:2461.0
now epsilon is 0.5821264212056131, the reward is 241.25 with loss [54.45561987161636, 73.01412463188171] in episode 331
Report: 
rewardSum:241.25
loss:[54.45561987161636, 73.01412463188171]
policies:[0, 3, 7]
qAverage:[0.0, 56.879112243652344]
ws:[2.536058135330677, 3.322270765900612]
memory len:4328
memory used:2467.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18*		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.5815445130454349, the reward is 247.25 with loss [16.666769981384277, 29.606497168540955] in episode 332
Report: 
rewardSum:247.25
loss:[16.666769981384277, 29.606497168540955]
policies:[2, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4336
memory used:2467.0
now epsilon is 0.5809631865752376, the reward is 247.25 with loss [21.00829553604126, 24.816426038742065] in episode 333
Report: 
rewardSum:247.25
loss:[21.00829553604126, 24.816426038742065]
policies:[0, 4, 0]
qAverage:[0.0, 49.57915115356445]
ws:[2.1200665831565857, 3.433919370174408]
memory len:4344
memory used:2467.0
now epsilon is 0.5806727412921492, the reward is -1.0 with loss [8.670530796051025, 8.741438150405884] in episode 334
Report: 
rewardSum:-1.0
loss:[8.670530796051025, 8.741438150405884]
policies:[0, 1, 1]
qAverage:[0.0, 29.316831588745117]
ws:[1.3942283391952515, 2.4662375450134277]
memory len:4348
memory used:2467.0
now epsilon is 0.5793675333760627, the reward is 242.25 with loss [41.94316387176514, 94.76273536682129] in episode 335
Report: 
rewardSum:242.25
loss:[41.94316387176514, 94.76273536682129]
policies:[2, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4366
memory used:2475.0
now epsilon is 0.5786436859735363, the reward is 246.25 with loss [17.87324285507202, 39.46294450759888] in episode 336
Report: 
rewardSum:246.25
loss:[17.87324285507202, 39.46294450759888]
policies:[1, 2, 2]
qAverage:[0.0, 28.766921997070312]
ws:[2.421400785446167, 3.753265380859375]
memory len:4376
memory used:2500.0
now epsilon is 0.5777762627422395, the reward is 245.25 with loss [37.32307577133179, 61.97556495666504] in episode 337
Report: 
rewardSum:245.25
loss:[37.32307577133179, 61.97556495666504]
policies:[2, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4388
memory used:2462.0
now epsilon is 0.5763334459982663, the reward is 552.6875 with loss [72.42143988609314, 39.375163197517395] in episode 338
Report: 
rewardSum:552.6875
loss:[72.42143988609314, 39.375163197517395]
policies:[1, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4408
memory used:2462.0
now epsilon is 0.5757573286412918, the reward is 247.25 with loss [18.13527536392212, 15.952967882156372] in episode 339
Report: 
rewardSum:247.25
loss:[18.13527536392212, 15.952967882156372]
policies:[1, 2, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4416
memory used:2462.0
now epsilon is 0.5750379917388698, the reward is 58.6875 with loss [36.8188591003418, 18.46283048391342] in episode 340
Report: 
rewardSum:58.6875
loss:[36.8188591003418, 18.46283048391342]
policies:[0, 2, 3]
qAverage:[0.0, 35.75046157836914]
ws:[4.656702041625977, 5.63510274887085]
memory len:4426
memory used:2462.0
now epsilon is 0.5738889215688768, the reward is 554.6875 with loss [41.12937891483307, 44.27700638771057] in episode 341
Report: 
rewardSum:554.6875
loss:[41.12937891483307, 44.27700638771057]
policies:[0, 2, 6]
qAverage:[0.0, 49.854199727376304]
ws:[6.603862762451172, 7.475139617919922]
memory len:4442
memory used:2463.0
now epsilon is 0.5730286260280809, the reward is 994.0 with loss [33.90749645233154, 43.955687910318375] in episode 342
Report: 
rewardSum:994.0
loss:[33.90749645233154, 43.955687910318375]
policies:[1, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4454
memory used:2469.0
now epsilon is 0.5723126982989126, the reward is 58.6875 with loss [51.97952365875244, 30.060626983642578] in episode 343
Report: 
rewardSum:58.6875
loss:[51.97952365875244, 30.060626983642578]
policies:[0, 2, 3]
qAverage:[0.0, 46.277844746907554]
ws:[3.9260760148366294, 5.1519473393758135]
memory len:4464
memory used:2469.0
now epsilon is 0.5720265777193069, the reward is -1.0 with loss [8.196484327316284, 6.55492639541626] in episode 344
Report: 
rewardSum:-1.0
loss:[8.196484327316284, 6.55492639541626]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4468
memory used:2469.0
now epsilon is 0.5711690739489199, the reward is 57.6875 with loss [64.16754055023193, 26.144304037094116] in episode 345
Report: 
rewardSum:57.6875
loss:[64.16754055023193, 26.144304037094116]
policies:[0, 2, 4]
qAverage:[0.0, 34.489173889160156]
ws:[1.7888466119766235, 3.49501895904541]
memory len:4480
memory used:2469.0
now epsilon is 0.5700277348472846, the reward is 243.25 with loss [54.24161219596863, 38.69645380973816] in episode 346
Report: 
rewardSum:243.25
loss:[54.24161219596863, 38.69645380973816]
policies:[1, 1, 6]
qAverage:[0.0, 41.705989837646484]
ws:[1.381154179573059, 2.783735990524292]
memory len:4496
memory used:2469.0
now epsilon is 0.5694579208372135, the reward is -3.0 with loss [20.410516500473022, 12.888710975646973] in episode 347
Report: 
rewardSum:-3.0
loss:[20.410516500473022, 12.888710975646973]
policies:[0, 2, 2]
qAverage:[0.0, 40.13221867879232]
ws:[1.824587643146515, 4.15396515528361]
memory len:4504
memory used:2469.0
now epsilon is 0.5688886764275076, the reward is 247.25 with loss [27.055721759796143, 20.83076846599579] in episode 348
Report: 
rewardSum:247.25
loss:[27.055721759796143, 20.83076846599579]
policies:[1, 1, 2]
qAverage:[0.0, 41.15643310546875]
ws:[3.087648391723633, 4.478519916534424]
memory len:4512
memory used:2481.0
now epsilon is 0.5683200010487806, the reward is 247.25 with loss [27.348450660705566, 23.02664852142334] in episode 349
Report: 
rewardSum:247.25
loss:[27.348450660705566, 23.02664852142334]
policies:[0, 1, 3]
qAverage:[0.0, 48.395137786865234]
ws:[1.189947247505188, 3.086026906967163]
memory len:4520
memory used:2481.0
now epsilon is 0.5674680536696417, the reward is 245.25 with loss [44.14835298061371, 13.558613240718842] in episode 350
Report: 
rewardSum:245.25
loss:[44.14835298061371, 13.558613240718842]
policies:[0, 2, 4]
qAverage:[0.0, 38.74274826049805]
ws:[4.345177173614502, 5.492213726043701]
memory len:4532
memory used:2481.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21*		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.5660509784758316, the reward is 241.25 with loss [55.14932656288147, 38.112989753484726] in episode 351
Report: 
rewardSum:241.25
loss:[55.14932656288147, 38.112989753484726]
policies:[1, 3, 6]
qAverage:[0.0, 46.42480723063151]
ws:[7.954058011372884, 9.249382019042969]
memory len:4552
memory used:2481.0
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.5654851397310968, the reward is 247.25 with loss [22.02897596359253, 16.890387058258057] in episode 352
Report: 
rewardSum:247.25
loss:[22.02897596359253, 16.890387058258057]
policies:[0, 3, 1]
qAverage:[0.0, 40.792301177978516]
ws:[3.134413480758667, 3.9508371353149414]
memory len:4560
memory used:2481.0
now epsilon is 0.5649198666129526, the reward is 247.25 with loss [28.716927528381348, 11.603507280349731] in episode 353
Report: 
rewardSum:247.25
loss:[28.716927528381348, 11.603507280349731]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4568
memory used:2481.0
now epsilon is 0.5642140697663454, the reward is 246.25 with loss [19.318603038787842, 44.182138442993164] in episode 354
Report: 
rewardSum:246.25
loss:[19.318603038787842, 44.182138442993164]
policies:[0, 2, 3]
qAverage:[0.0, 33.74013137817383]
ws:[5.207320690155029, 5.612794399261475]
memory len:4578
memory used:2481.0
now epsilon is 0.563650067241594, the reward is 247.25 with loss [16.454803705215454, 26.791892290115356] in episode 355
Report: 
rewardSum:247.25
loss:[16.454803705215454, 26.791892290115356]
policies:[4, 0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4586
memory used:2481.0
now epsilon is 0.5626644191064656, the reward is 56.6875 with loss [28.64254668354988, 41.93605589866638] in episode 356
Report: 
rewardSum:56.6875
loss:[28.64254668354988, 41.93605589866638]
policies:[2, 2, 3]
qAverage:[0.0, 30.058561325073242]
ws:[1.325926661491394, 1.8612715005874634]
memory len:4600
memory used:2482.0
now epsilon is 0.5616804945624492, the reward is 244.25 with loss [24.76909774541855, 35.816742062568665] in episode 357
Report: 
rewardSum:244.25
loss:[24.76909774541855, 35.816742062568665]
policies:[1, 2, 4]
qAverage:[0.0, 42.26942825317383]
ws:[3.607910633087158, 4.179116249084473]
memory len:4614
memory used:2482.0
now epsilon is 0.5604179764938675, the reward is 991.0 with loss [25.45939666032791, 43.45833384990692] in episode 358
Report: 
rewardSum:991.0
loss:[25.45939666032791, 43.45833384990692]
policies:[2, 2, 5]
qAverage:[0.0, 31.415679931640625]
ws:[-13.686803817749023, -12.73790168762207]
memory len:4632
memory used:2482.0
now epsilon is 0.5595778747458818, the reward is 245.25 with loss [40.08559012413025, 27.231714963912964] in episode 359
Report: 
rewardSum:245.25
loss:[40.08559012413025, 27.231714963912964]
policies:[2, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4644
memory used:2482.0
now epsilon is 0.5584596977681932, the reward is 55.6875 with loss [34.14956732094288, 28.74299630522728] in episode 360
Report: 
rewardSum:55.6875
loss:[34.14956732094288, 28.74299630522728]
policies:[0, 1, 7]
qAverage:[0.0, 41.151519775390625]
ws:[1.5021569728851318, 2.3719708919525146]
memory len:4660
memory used:2483.0
now epsilon is 0.557065118145019, the reward is 53.6875 with loss [48.434325218200684, 59.164107233285904] in episode 361
Report: 
rewardSum:53.6875
loss:[48.434325218200684, 59.164107233285904]
policies:[1, 3, 6]
qAverage:[0.0, 41.063804626464844]
ws:[2.178349256515503, 3.121030330657959]
memory len:4680
memory used:2483.0
now epsilon is 0.5553962187703618, the reward is 239.25 with loss [56.19192838668823, 43.624948024749756] in episode 362
Report: 
rewardSum:239.25
loss:[56.19192838668823, 43.624948024749756]
policies:[1, 6, 5]
qAverage:[0.0, 58.142643737792966]
ws:[4.568403100967407, 6.049344444274903]
memory len:4704
memory used:2483.0
now epsilon is 0.5547023205327658, the reward is 246.25 with loss [26.235971450805664, 23.225589752197266] in episode 363
Report: 
rewardSum:246.25
loss:[26.235971450805664, 23.225589752197266]
policies:[0, 2, 3]
qAverage:[0.0, 54.06279754638672]
ws:[3.518987019856771, 5.148913542429606]
memory len:4714
memory used:2484.0
now epsilon is 0.5538707869120801, the reward is 245.25 with loss [36.04458141326904, 25.640244364738464] in episode 364
Report: 
rewardSum:245.25
loss:[36.04458141326904, 25.640244364738464]
policies:[1, 3, 2]
qAverage:[0.0, 44.37545267740885]
ws:[3.937366167704264, 5.303539276123047]
memory len:4726
memory used:2484.0
now epsilon is 0.5534554876640145, the reward is -2.0 with loss [14.247124195098877, 14.715074241161346] in episode 365
Report: 
rewardSum:-2.0
loss:[14.247124195098877, 14.715074241161346]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4732
memory used:2484.0
now epsilon is 0.5527640141276478, the reward is 58.6875 with loss [23.337438583374023, 22.513142108917236] in episode 366
Report: 
rewardSum:58.6875
loss:[23.337438583374023, 22.513142108917236]
policies:[1, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4742
memory used:2484.0
now epsilon is 0.5515215380896618, the reward is 242.25 with loss [41.53790640830994, 28.817873001098633] in episode 367
Report: 
rewardSum:242.25
loss:[41.53790640830994, 28.817873001098633]
policies:[0, 5, 4]
qAverage:[0.0, 43.5948232014974]
ws:[3.009364128112793, 4.969197909037272]
memory len:4760
memory used:2483.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2*		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.5505570989684857, the reward is -6.0 with loss [43.771589279174805, 34.77193641662598] in episode 368
Report: 
rewardSum:-6.0
loss:[43.771589279174805, 34.77193641662598]
policies:[0, 3, 4]
qAverage:[0.0, 34.58615493774414]
ws:[5.428275108337402, 6.603153228759766]
memory len:4774
memory used:2484.0
now epsilon is 0.5497317792952965, the reward is 245.25 with loss [25.805577278137207, 39.14866495132446] in episode 369
Report: 
rewardSum:245.25
loss:[25.805577278137207, 39.14866495132446]
policies:[1, 3, 2]
qAverage:[0.0, 60.31379508972168]
ws:[2.994814157485962, 5.547016739845276]
memory len:4786
memory used:2484.0
now epsilon is 0.5487704699039307, the reward is 244.25 with loss [27.045719519257545, 19.388555884361267] in episode 370
Report: 
rewardSum:244.25
loss:[27.045719519257545, 19.388555884361267]
policies:[0, 1, 6]
qAverage:[0.0, 35.402618408203125]
ws:[3.687814474105835, 5.490297317504883]
memory len:4800
memory used:2483.0
now epsilon is 0.5478108415428068, the reward is -6.0 with loss [29.712064057588577, 28.17158842086792] in episode 371
Report: 
rewardSum:-6.0
loss:[29.712064057588577, 28.17158842086792]
policies:[0, 3, 4]
qAverage:[0.0, 51.00338172912598]
ws:[2.8043686747550964, 5.665661096572876]
memory len:4814
memory used:2483.0
now epsilon is 0.5472632360960935, the reward is 247.25 with loss [17.291759252548218, 14.29125428199768] in episode 372
Report: 
rewardSum:247.25
loss:[17.291759252548218, 14.29125428199768]
policies:[0, 4, 0]
qAverage:[0.0, 53.333875020345054]
ws:[4.171664237976074, 6.875200271606445]
memory len:4822
memory used:2483.0
now epsilon is 0.5467161780495091, the reward is 247.25 with loss [13.684489965438843, 21.455926179885864] in episode 373
Report: 
rewardSum:247.25
loss:[13.684489965438843, 21.455926179885864]
policies:[1, 2, 1]
qAverage:[0.0, 50.693379720052086]
ws:[4.340081850687663, 6.985550880432129]
memory len:4830
memory used:2483.0
now epsilon is 0.5454872960430026, the reward is 242.25 with loss [35.7000030875206, 34.74309003353119] in episode 374
Report: 
rewardSum:242.25
loss:[35.7000030875206, 34.74309003353119]
policies:[0, 4, 5]
qAverage:[0.0, 58.775471687316895]
ws:[4.055647969245911, 6.547565460205078]
memory len:4848
memory used:2483.0
now epsilon is 0.543309433244413, the reward is 984.0 with loss [81.1970522403717, 62.35065722465515] in episode 375
Report: 
rewardSum:984.0
loss:[81.1970522403717, 62.35065722465515]
policies:[1, 6, 9]
qAverage:[0.0, 58.127262878417966]
ws:[3.0763338088989256, 5.362428665161133]
memory len:4880
memory used:2487.0
now epsilon is 0.5427663275182514, the reward is 59.6875 with loss [11.109740734100342, 22.350686073303223] in episode 376
Report: 
rewardSum:59.6875
loss:[11.109740734100342, 22.350686073303223]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4888
memory used:2487.0
now epsilon is 0.5419526867008235, the reward is 57.6875 with loss [47.942700028419495, 58.51936674118042] in episode 377
Report: 
rewardSum:57.6875
loss:[47.942700028419495, 58.51936674118042]
policies:[1, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4900
memory used:2487.0
now epsilon is 0.5408697292705633, the reward is 243.25 with loss [35.67285752296448, 43.75237810611725] in episode 378
Report: 
rewardSum:243.25
loss:[35.67285752296448, 43.75237810611725]
policies:[1, 3, 4]
qAverage:[0.0, 47.45655822753906]
ws:[3.9174509048461914, 6.604479471842448]
memory len:4916
memory used:2487.0
now epsilon is 0.5403290623336391, the reward is 247.25 with loss [20.19279670715332, 28.16700553894043] in episode 379
Report: 
rewardSum:247.25
loss:[20.19279670715332, 28.16700553894043]
policies:[1, 2, 1]
qAverage:[0.0, 65.76173655192058]
ws:[6.030157089233398, 8.616572062174479]
memory len:4924
memory used:2487.0
now epsilon is 0.539384195361031, the reward is 244.25 with loss [31.708099246025085, 60.33957886695862] in episode 380
Report: 
rewardSum:244.25
loss:[31.708099246025085, 60.33957886695862]
policies:[1, 3, 3]
qAverage:[0.0, 33.18636703491211]
ws:[3.5559864044189453, 5.294116497039795]
memory len:4938
memory used:2492.0
now epsilon is 0.5383063704208372, the reward is 55.6875 with loss [30.082438826560974, 38.60795545578003] in episode 381
Report: 
rewardSum:55.6875
loss:[30.082438826560974, 38.60795545578003]
policies:[1, 2, 5]
qAverage:[0.0, 33.60505294799805]
ws:[2.3176043033599854, 3.5569326877593994]
memory len:4954
memory used:2492.0
now epsilon is 0.5373650405053992, the reward is 56.6875 with loss [61.328906178474426, 26.524304807186127] in episode 382
Report: 
rewardSum:56.6875
loss:[61.328906178474426, 26.524304807186127]
policies:[1, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4968
memory used:2492.0
now epsilon is 0.5368278769432009, the reward is 247.25 with loss [15.525705337524414, 20.919970512390137] in episode 383
Report: 
rewardSum:247.25
loss:[15.525705337524414, 20.919970512390137]
policies:[0, 3, 1]
qAverage:[0.0, 47.79464975992838]
ws:[3.4779974619547525, 5.013894240061442]
memory len:4976
memory used:2492.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34*		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.5358891324516345, the reward is 244.25 with loss [48.82938712835312, 39.94407272338867] in episode 384
Report: 
rewardSum:244.25
loss:[48.82938712835312, 39.94407272338867]
policies:[0, 3, 4]
qAverage:[0.0, 32.27794647216797]
ws:[3.711130380630493, 4.933927536010742]
memory len:4990
memory used:2492.0
now epsilon is 0.5346845869510757, the reward is 242.25 with loss [59.23069167137146, 36.901872515678406] in episode 385
Report: 
rewardSum:242.25
loss:[59.23069167137146, 36.901872515678406]
policies:[2, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5008
memory used:2493.0
now epsilon is 0.5337495904050993, the reward is 244.25 with loss [67.03341770172119, 43.887837290763855] in episode 386
Report: 
rewardSum:244.25
loss:[67.03341770172119, 43.887837290763855]
policies:[0, 4, 3]
qAverage:[0.0, 55.91008504231771]
ws:[1.9098443190256755, 3.9480040868123374]
memory len:5022
memory used:2493.0
now epsilon is 0.532416716599467, the reward is 241.25 with loss [58.47095036506653, 55.07846164703369] in episode 387
Report: 
rewardSum:241.25
loss:[58.47095036506653, 55.07846164703369]
policies:[3, 3, 4]
qAverage:[0.0, 65.07118479410808]
ws:[0.8028993606567383, 4.351683934529622]
memory len:5042
memory used:2493.0
now epsilon is 0.5318844995058624, the reward is 59.6875 with loss [11.622088551521301, 3.8677192628383636] in episode 388
Report: 
rewardSum:59.6875
loss:[11.622088551521301, 3.8677192628383636]
policies:[0, 1, 3]
qAverage:[0.0, 39.41849899291992]
ws:[2.855576992034912, 5.2845540046691895]
memory len:5050
memory used:2493.0
now epsilon is 0.5312199762261958, the reward is 58.6875 with loss [18.9642733335495, 19.182574033737183] in episode 389
Report: 
rewardSum:58.6875
loss:[18.9642733335495, 19.182574033737183]
policies:[1, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5060
memory used:2493.0
now epsilon is 0.5302910382035806, the reward is 244.25 with loss [27.914328575134277, 33.223660588264465] in episode 390
Report: 
rewardSum:244.25
loss:[27.914328575134277, 33.223660588264465]
policies:[0, 3, 4]
qAverage:[0.0, 64.42032432556152]
ws:[4.797358751296997, 7.775114059448242]
memory len:5074
memory used:2492.0
now epsilon is 0.5294960986284387, the reward is 57.6875 with loss [23.849678099155426, 45.60807943344116] in episode 391
Report: 
rewardSum:57.6875
loss:[23.849678099155426, 45.60807943344116]
policies:[0, 3, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5086
memory used:2493.0
now epsilon is 0.5284380325861905, the reward is 243.25 with loss [47.5372793674469, 44.08246147632599] in episode 392
Report: 
rewardSum:243.25
loss:[47.5372793674469, 44.08246147632599]
policies:[0, 5, 3]
qAverage:[0.0, 73.29496256510417]
ws:[5.1868630250295, 8.599135637283325]
memory len:5102
memory used:2493.0
now epsilon is 0.5279097926848412, the reward is 59.6875 with loss [21.524553060531616, 16.327808737754822] in episode 393
Report: 
rewardSum:59.6875
loss:[21.524553060531616, 16.327808737754822]
policies:[2, 1, 1]
qAverage:[0.0, 42.51636505126953]
ws:[5.08465576171875, 7.574633598327637]
memory len:5110
memory used:2493.0
now epsilon is 0.5273820808253363, the reward is 247.25 with loss [38.6041316986084, 25.61250901222229] in episode 394
Report: 
rewardSum:247.25
loss:[38.6041316986084, 25.61250901222229]
policies:[3, 1, 0]
qAverage:[0.0, 49.02482986450195]
ws:[4.094868183135986, 6.171370983123779]
memory len:5118
memory used:2493.0
now epsilon is 0.5267231827557122, the reward is 246.25 with loss [24.06600832939148, 19.639291167259216] in episode 395
Report: 
rewardSum:246.25
loss:[24.06600832939148, 19.639291167259216]
policies:[1, 1, 3]
qAverage:[0.0, 57.056121826171875]
ws:[3.4114139080047607, 5.507286071777344]
memory len:5128
memory used:2493.0
now epsilon is 0.5260651078969666, the reward is 246.25 with loss [28.263612508773804, 29.087628841400146] in episode 396
Report: 
rewardSum:246.25
loss:[28.263612508773804, 29.087628841400146]
policies:[0, 4, 1]
qAverage:[0.0, 76.28159637451172]
ws:[1.9596927881240844, 5.303275012969971]
memory len:5138
memory used:2493.0
now epsilon is 0.5251451841309811, the reward is 244.25 with loss [36.467540979385376, 33.62695288658142] in episode 397
Report: 
rewardSum:244.25
loss:[36.467540979385376, 33.62695288658142]
policies:[2, 2, 3]
qAverage:[0.0, 63.75873565673828]
ws:[2.786983847618103, 5.679470380147298]
memory len:5152
memory used:2493.0
now epsilon is 0.5242268690246892, the reward is -6.0 with loss [38.93316435813904, 36.259874045848846] in episode 398
Report: 
rewardSum:-6.0
loss:[38.93316435813904, 36.259874045848846]
policies:[0, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5166
memory used:2494.0
now epsilon is 0.5231793322251056, the reward is -7.0 with loss [58.07280790805817, 22.34845380485058] in episode 399
Report: 
rewardSum:-7.0
loss:[58.07280790805817, 22.34845380485058]
policies:[1, 1, 6]
qAverage:[0.0, 36.41134262084961]
ws:[1.5123286247253418, 3.4009015560150146]
memory len:5182
memory used:2493.0
now epsilon is 0.5226563490524334, the reward is -3.0 with loss [13.521315693855286, 12.85636591911316] in episode 400
Report: 
rewardSum:-3.0
loss:[13.521315693855286, 12.85636591911316]
policies:[0, 2, 2]
qAverage:[0.0, 37.004188537597656]
ws:[4.981955528259277, 6.879234790802002]
memory len:5190
memory used:2493.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28*		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.521611950545758, the reward is 55.6875 with loss [40.0242919921875, 27.191713571548462] in episode 401
Report: 
rewardSum:55.6875
loss:[40.0242919921875, 27.191713571548462]
policies:[1, 3, 4]
qAverage:[0.0, 65.09801737467448]
ws:[6.718820889790853, 8.942724227905273]
memory len:5206
memory used:2493.0
now epsilon is 0.5208300214681698, the reward is 245.25 with loss [45.18426012992859, 38.18232321739197] in episode 402
Report: 
rewardSum:245.25
loss:[45.18426012992859, 38.18232321739197]
policies:[2, 2, 2]
qAverage:[0.0, 56.02324676513672]
ws:[3.135981718699137, 6.2063093185424805]
memory len:5218
memory used:2493.0
now epsilon is 0.5203093867254099, the reward is 247.25 with loss [22.72257125377655, 27.982133626937866] in episode 403
Report: 
rewardSum:247.25
loss:[22.72257125377655, 27.982133626937866]
policies:[1, 1, 2]
qAverage:[0.0, 45.51527404785156]
ws:[2.3354663848876953, 4.960662364959717]
memory len:5226
memory used:2493.0
now epsilon is 0.5197892724221872, the reward is 59.6875 with loss [26.93185806274414, 15.548263311386108] in episode 404
Report: 
rewardSum:59.6875
loss:[26.93185806274414, 15.548263311386108]
policies:[0, 1, 3]
qAverage:[0.0, 43.727935791015625]
ws:[4.444350242614746, 6.705556392669678]
memory len:5234
memory used:2493.0
now epsilon is 0.5187506030538962, the reward is 55.6875 with loss [43.313499331474304, 38.06700897216797] in episode 405
Report: 
rewardSum:55.6875
loss:[43.313499331474304, 38.06700897216797]
policies:[1, 2, 5]
qAverage:[0.0, 43.81880569458008]
ws:[2.7639050483703613, 4.783999443054199]
memory len:5250
memory used:2493.0
now epsilon is 0.5181024889381611, the reward is 58.6875 with loss [23.207319259643555, 26.99379301071167] in episode 406
Report: 
rewardSum:58.6875
loss:[23.207319259643555, 26.99379301071167]
policies:[2, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5260
memory used:2493.0
now epsilon is 0.5178434700750976, the reward is -1.0 with loss [7.240344047546387, 14.368926286697388] in episode 407
Report: 
rewardSum:-1.0
loss:[7.240344047546387, 14.368926286697388]
policies:[0, 1, 1]
qAverage:[0.0, 38.211631774902344]
ws:[2.2140424251556396, 4.330428123474121]
memory len:5264
memory used:2493.0
now epsilon is 0.5173258207639606, the reward is 59.6875 with loss [14.829532980918884, 29.535671710968018] in episode 408
Report: 
rewardSum:59.6875
loss:[14.829532980918884, 29.535671710968018]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5272
memory used:2499.0
now epsilon is 0.5165503168641377, the reward is 245.25 with loss [25.398294687271118, 27.730931043624878] in episode 409
Report: 
rewardSum:245.25
loss:[25.398294687271118, 27.730931043624878]
policies:[0, 3, 3]
qAverage:[0.0, 75.32759857177734]
ws:[8.85457468032837, 11.164782285690308]
memory len:5284
memory used:2505.0
now epsilon is 0.5159049517313046, the reward is 246.25 with loss [22.570424675941467, 24.914562702178955] in episode 410
Report: 
rewardSum:246.25
loss:[22.570424675941467, 24.914562702178955]
policies:[0, 2, 3]
qAverage:[0.0, 65.72631581624348]
ws:[8.456872940063477, 9.83199659983317]
memory len:5294
memory used:2511.0
now epsilon is 0.5153892402116882, the reward is 247.25 with loss [17.44590973854065, 13.84864616394043] in episode 411
Report: 
rewardSum:247.25
loss:[17.44590973854065, 13.84864616394043]
policies:[0, 3, 1]
qAverage:[0.0, 76.93262481689453]
ws:[7.913542747497559, 8.807818094889322]
memory len:5302
memory used:2513.0
now epsilon is 0.5143593632116106, the reward is 243.25 with loss [47.689104199409485, 41.63006353378296] in episode 412
Report: 
rewardSum:243.25
loss:[47.689104199409485, 41.63006353378296]
policies:[2, 1, 5]
qAverage:[0.0, 45.628814697265625]
ws:[3.348513603210449, 4.215235233306885]
memory len:5318
memory used:2520.0
now epsilon is 0.5133315441641494, the reward is 55.6875 with loss [35.44255208969116, 30.720043301582336] in episode 413
Report: 
rewardSum:55.6875
loss:[35.44255208969116, 30.720043301582336]
policies:[0, 3, 5]
qAverage:[0.0, 56.970654805501304]
ws:[5.385537624359131, 6.917787233988444]
memory len:5334
memory used:2525.0
now epsilon is 0.5128184050872331, the reward is 247.25 with loss [13.24864375591278, 25.721235513687134] in episode 414
Report: 
rewardSum:247.25
loss:[13.24864375591278, 25.721235513687134]
policies:[2, 2, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5342
memory used:2525.0
now epsilon is 0.5119216456721097, the reward is 244.25 with loss [34.18941670656204, 62.639580488204956] in episode 415
Report: 
rewardSum:244.25
loss:[34.18941670656204, 62.639580488204956]
policies:[1, 4, 2]
qAverage:[0.0, 69.88035774230957]
ws:[7.097411155700684, 8.117094993591309]
memory len:5356
memory used:2525.0
now epsilon is 0.5110264544094564, the reward is 244.25 with loss [38.234498023986816, 36.92449617385864] in episode 416
Report: 
rewardSum:244.25
loss:[38.234498023986816, 36.92449617385864]
policies:[0, 4, 3]
qAverage:[0.0, 81.4112319946289]
ws:[7.256641685962677, 8.7194983959198]
memory len:5370
memory used:2532.0
now epsilon is 0.5102603936554775, the reward is 57.6875 with loss [17.14826039969921, 29.85396957397461] in episode 417
Report: 
rewardSum:57.6875
loss:[17.14826039969921, 29.85396957397461]
policies:[0, 2, 4]
qAverage:[0.0, 46.95924758911133]
ws:[3.738048791885376, 4.60825777053833]
memory len:5382
memory used:2538.0
now epsilon is 0.5093681074043683, the reward is -6.0 with loss [48.644009828567505, 38.90977191925049] in episode 418
Report: 
rewardSum:-6.0
loss:[48.644009828567505, 38.90977191925049]
policies:[0, 1, 6]
qAverage:[0.0, 46.5592155456543]
ws:[5.85824728012085, 7.0304388999938965]
memory len:5396
memory used:2538.0
now epsilon is 0.5086045326167149, the reward is 245.25 with loss [18.523329973220825, 38.59177494049072] in episode 419
Report: 
rewardSum:245.25
loss:[18.523329973220825, 38.59177494049072]
policies:[2, 3, 1]
qAverage:[0.0, 47.7048225402832]
ws:[5.665962219238281, 6.618880748748779]
memory len:5408
memory used:2538.0
now epsilon is 0.5080961187790122, the reward is 59.6875 with loss [20.757744789123535, 22.023547649383545] in episode 420
Report: 
rewardSum:59.6875
loss:[20.757744789123535, 22.023547649383545]
policies:[1, 1, 2]
qAverage:[0.0, 58.83524703979492]
ws:[3.7535994052886963, 4.563040256500244]
memory len:5416
memory used:2539.0
############# STATE ###############
0-		8*		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.5068273065501353, the reward is 241.25 with loss [48.74948428571224, 51.10232996940613] in episode 421
Report: 
rewardSum:241.25
loss:[48.74948428571224, 51.10232996940613]
policies:[1, 3, 6]
qAverage:[0.0, 79.33504104614258]
ws:[8.657629132270813, 9.82290005683899]
memory len:5436
memory used:2538.0
now epsilon is 0.5060675405825563, the reward is 57.6875 with loss [34.390809774398804, 24.899878978729248] in episode 422
Report: 
rewardSum:57.6875
loss:[34.390809774398804, 24.899878978729248]
policies:[1, 3, 2]
qAverage:[0.0, 76.18724060058594]
ws:[4.836725115776062, 5.927234649658203]
memory len:5448
memory used:2538.0
now epsilon is 0.5055616627856743, the reward is 59.6875 with loss [20.7740638256073, 29.42600393295288] in episode 423
Report: 
rewardSum:59.6875
loss:[20.7740638256073, 29.42600393295288]
policies:[0, 2, 2]
qAverage:[0.0, 67.79305013020833]
ws:[5.112893104553223, 6.4891510009765625]
memory len:5456
memory used:2538.0
now epsilon is 0.5048037940975962, the reward is 245.25 with loss [24.1230206489563, 32.591598987579346] in episode 424
Report: 
rewardSum:245.25
loss:[24.1230206489563, 32.591598987579346]
policies:[2, 1, 3]
qAverage:[0.0, 42.67299270629883]
ws:[3.069031000137329, 4.425283908843994]
memory len:5468
memory used:2538.0
now epsilon is 0.5042991795733732, the reward is -3.0 with loss [18.621924877166748, 16.535784393548965] in episode 425
Report: 
rewardSum:-3.0
loss:[18.621924877166748, 16.535784393548965]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5476
memory used:2538.0
now epsilon is 0.5031656404308428, the reward is 242.25 with loss [32.230364724993706, 46.107529640197754] in episode 426
Report: 
rewardSum:242.25
loss:[32.230364724993706, 46.107529640197754]
policies:[3, 2, 4]
qAverage:[0.0, 50.25038528442383]
ws:[3.989603281021118, 4.754428863525391]
memory len:5494
memory used:2538.0
now epsilon is 0.5026626634460813, the reward is 247.25 with loss [32.49197381734848, 13.486363530158997] in episode 427
Report: 
rewardSum:247.25
loss:[32.49197381734848, 13.486363530158997]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5502
memory used:2505.0
now epsilon is 0.5019091405401067, the reward is 245.25 with loss [29.697144985198975, 19.687637209892273] in episode 428
Report: 
rewardSum:245.25
loss:[29.697144985198975, 19.687637209892273]
policies:[0, 5, 1]
qAverage:[0.0, 90.00149230957031]
ws:[9.671357727050781, 11.967578887939453]
memory len:5514
memory used:2505.0
now epsilon is 0.5014074195841269, the reward is -3.0 with loss [19.746586084365845, 29.173755645751953] in episode 429
Report: 
rewardSum:-3.0
loss:[19.746586084365845, 29.173755645751953]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5522
memory used:2505.0
now epsilon is 0.49990526390836165, the reward is 51.6875 with loss [56.251654863357544, 51.09720718860626] in episode 430
Report: 
rewardSum:51.6875
loss:[56.251654863357544, 51.09720718860626]
policies:[1, 3, 8]
qAverage:[0.0, 77.38951110839844]
ws:[7.503218650817871, 9.065590858459473]
memory len:5546
memory used:2518.0
now epsilon is 0.499155874517493, the reward is 57.6875 with loss [39.10428011417389, 22.315518379211426] in episode 431
Report: 
rewardSum:57.6875
loss:[39.10428011417389, 22.315518379211426]
policies:[0, 2, 4]
qAverage:[0.0, 60.99930191040039]
ws:[3.4098637104034424, 4.993705749511719]
memory len:5558
memory used:2518.0
now epsilon is 0.49766046419703946, the reward is 239.25 with loss [57.645536720752716, 49.97847293317318] in episode 432
Report: 
rewardSum:239.25
loss:[57.645536720752716, 49.97847293317318]
policies:[1, 3, 8]
qAverage:[0.0, 74.51773834228516]
ws:[8.283787727355957, 10.809383392333984]
memory len:5582
memory used:2518.0
now epsilon is 0.4971629903244147, the reward is 247.25 with loss [9.098031878471375, 28.138594388961792] in episode 433
Report: 
rewardSum:247.25
loss:[9.098031878471375, 28.138594388961792]
policies:[0, 2, 2]
qAverage:[0.0, 72.77496337890625]
ws:[11.601054191589355, 13.574043273925781]
memory len:5590
memory used:2518.0
now epsilon is 0.49641771177389726, the reward is 245.25 with loss [23.479116797447205, 37.153515219688416] in episode 434
Report: 
rewardSum:245.25
loss:[23.479116797447205, 37.153515219688416]
policies:[0, 3, 3]
qAverage:[0.0, 59.959049224853516]
ws:[3.7372779846191406, 5.345142841339111]
memory len:5602
memory used:2518.0
now epsilon is 0.49567355044273986, the reward is 245.25 with loss [32.395575284957886, 39.167356967926025] in episode 435
Report: 
rewardSum:245.25
loss:[32.395575284957886, 39.167356967926025]
policies:[0, 2, 4]
qAverage:[0.0, 74.18678283691406]
ws:[4.744669278462728, 7.524607022603353]
memory len:5614
memory used:2518.0
now epsilon is 0.4949305046561604, the reward is 245.25 with loss [26.481220722198486, 20.610029935836792] in episode 436
Report: 
rewardSum:245.25
loss:[26.481220722198486, 20.610029935836792]
policies:[1, 1, 4]
qAverage:[0.0, 72.77005004882812]
ws:[11.848361015319824, 14.433853149414062]
memory len:5626
memory used:2518.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19*		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.4938180239649669, the reward is 242.25 with loss [50.857574582099915, 42.683035135269165] in episode 437
Report: 
rewardSum:242.25
loss:[50.857574582099915, 42.683035135269165]
policies:[1, 3, 5]
qAverage:[0.0, 84.2373046875]
ws:[9.517415046691895, 13.038492838541666]
memory len:5644
memory used:2518.0
now epsilon is 0.49357114581661093, the reward is -1.0 with loss [9.696058988571167, 11.716821670532227] in episode 438
Report: 
rewardSum:-1.0
loss:[9.696058988571167, 11.716821670532227]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5648
memory used:2518.0
now epsilon is 0.4929544902891955, the reward is 246.25 with loss [37.240739822387695, 26.274770617485046] in episode 439
Report: 
rewardSum:246.25
loss:[37.240739822387695, 26.274770617485046]
policies:[1, 2, 2]
qAverage:[0.0, 73.09713236490886]
ws:[4.845739523569743, 7.107181866963704]
memory len:5658
memory used:2518.0
now epsilon is 0.492338605195876, the reward is 246.25 with loss [26.645968437194824, 34.9125018119812] in episode 440
Report: 
rewardSum:246.25
loss:[26.645968437194824, 34.9125018119812]
policies:[2, 2, 1]
qAverage:[0.0, 65.90094757080078]
ws:[3.467249790827433, 5.582449277242024]
memory len:5668
memory used:2518.0
now epsilon is 0.4918464511868879, the reward is 247.25 with loss [16.812448978424072, 20.355116367340088] in episode 441
Report: 
rewardSum:247.25
loss:[16.812448978424072, 20.355116367340088]
policies:[2, 2, 0]
qAverage:[0.0, 84.65434010823567]
ws:[11.86056900024414, 14.936121622721354]
memory len:5676
memory used:2518.0
now epsilon is 0.49135478914738173, the reward is 59.6875 with loss [27.204657077789307, 17.68019998073578] in episode 442
Report: 
rewardSum:59.6875
loss:[27.204657077789307, 17.68019998073578]
policies:[0, 2, 2]
qAverage:[0.0, 66.67862192789714]
ws:[6.68972380956014, 9.106673240661621]
memory len:5684
memory used:2519.0
now epsilon is 0.4893930508558478, the reward is 47.6875 with loss [50.408092617988586, 98.9766606092453] in episode 443
Report: 
rewardSum:47.6875
loss:[50.408092617988586, 98.9766606092453]
policies:[2, 3, 11]
qAverage:[0.0, 73.1572748819987]
ws:[6.968500296274821, 9.231661796569824]
memory len:5716
memory used:2519.0
now epsilon is 0.48878161533647674, the reward is 246.25 with loss [35.4198260307312, 20.441170692443848] in episode 444
Report: 
rewardSum:246.25
loss:[35.4198260307312, 20.441170692443848]
policies:[0, 2, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5726
memory used:2519.0
now epsilon is 0.4885372550776595, the reward is -1.0 with loss [18.991914749145508, 14.418001413345337] in episode 445
Report: 
rewardSum:-1.0
loss:[18.991914749145508, 14.418001413345337]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5730
memory used:2519.0
now epsilon is 0.4880489009935209, the reward is 247.25 with loss [16.936939001083374, 25.05827569961548] in episode 446
Report: 
rewardSum:247.25
loss:[16.936939001083374, 25.05827569961548]
policies:[1, 3, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5738
memory used:2519.0
now epsilon is 0.4873172850353886, the reward is 245.25 with loss [21.066863894462585, 29.854680061340332] in episode 447
Report: 
rewardSum:245.25
loss:[21.066863894462585, 29.854680061340332]
policies:[0, 2, 4]
qAverage:[0.0, 75.19018809000652]
ws:[2.151817719141642, 4.8035515149434405]
memory len:5750
memory used:2519.0
now epsilon is 0.48707365685020126, the reward is -1.0 with loss [17.511191368103027, 8.811198472976685] in episode 448
Report: 
rewardSum:-1.0
loss:[17.511191368103027, 8.811198472976685]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5754
memory used:2519.0
now epsilon is 0.48646511912407836, the reward is 246.25 with loss [19.488430976867676, 25.16756582260132] in episode 449
Report: 
rewardSum:246.25
loss:[19.488430976867676, 25.16756582260132]
policies:[1, 3, 1]
qAverage:[0.0, 54.50701904296875]
ws:[5.270066261291504, 7.740230560302734]
memory len:5764
memory used:2519.0
now epsilon is 0.4859788363989718, the reward is 247.25 with loss [30.421112537384033, 37.90235638618469] in episode 450
Report: 
rewardSum:247.25
loss:[30.421112537384033, 37.90235638618469]
policies:[0, 4, 0]
qAverage:[0.0, 86.73094431559245]
ws:[14.67650286356608, 17.620521545410156]
memory len:5772
memory used:2518.0
now epsilon is 0.48525032359769266, the reward is 994.0 with loss [19.3978054523468, 37.27308511734009] in episode 451
Report: 
rewardSum:994.0
loss:[19.3978054523468, 37.27308511734009]
policies:[0, 2, 4]
qAverage:[0.0, 54.859012603759766]
ws:[3.3267159461975098, 5.381756782531738]
memory len:5784
memory used:2518.0
now epsilon is 0.4847652552126401, the reward is 247.25 with loss [24.37071418762207, 19.546695470809937] in episode 452
Report: 
rewardSum:247.25
loss:[24.37071418762207, 19.546695470809937]
policies:[1, 1, 2]
qAverage:[0.0, 54.30716323852539]
ws:[2.2511236667633057, 4.548764705657959]
memory len:5792
memory used:2518.0
now epsilon is 0.48367562347422016, the reward is 242.25 with loss [78.8984022140503, 39.92288780212402] in episode 453
Report: 
rewardSum:242.25
loss:[78.8984022140503, 39.92288780212402]
policies:[1, 3, 5]
qAverage:[0.0, 84.43452962239583]
ws:[5.329229195912679, 7.848430633544922]
memory len:5810
memory used:2518.0
now epsilon is 0.483192129198877, the reward is 247.25 with loss [22.268858671188354, 32.58304524421692] in episode 454
Report: 
rewardSum:247.25
loss:[22.268858671188354, 32.58304524421692]
policies:[2, 2, 0]
qAverage:[0.0, 52.90880584716797]
ws:[0.4837793707847595, 2.3901729583740234]
memory len:5818
memory used:2518.0
now epsilon is 0.4825884409569699, the reward is 246.25 with loss [35.85771632194519, 23.770347952842712] in episode 455
Report: 
rewardSum:246.25
loss:[35.85771632194519, 23.770347952842712]
policies:[0, 3, 2]
qAverage:[0.0, 77.47397104899089]
ws:[4.491130193074544, 6.116403102874756]
memory len:5828
memory used:2518.0
now epsilon is 0.48198550694815434, the reward is 246.25 with loss [25.320470094680786, 15.023199915885925] in episode 456
Report: 
rewardSum:246.25
loss:[25.320470094680786, 15.023199915885925]
policies:[1, 3, 1]
qAverage:[0.0, 65.93890380859375]
ws:[4.029008865356445, 5.343640327453613]
memory len:5838
memory used:2519.0
now epsilon is 0.4811426646534531, the reward is 244.25 with loss [41.19204235076904, 31.024715185165405] in episode 457
Report: 
rewardSum:244.25
loss:[41.19204235076904, 31.024715185165405]
policies:[0, 3, 4]
qAverage:[0.0, 83.33898671468098]
ws:[8.160783131917318, 9.02707322438558]
memory len:5852
memory used:2530.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2*		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.48066170238722944, the reward is -3.0 with loss [13.429888486862183, 24.60498356819153] in episode 458
Report: 
rewardSum:-3.0
loss:[13.429888486862183, 24.60498356819153]
policies:[0, 1, 3]
qAverage:[0.0, 50.363609313964844]
ws:[3.079887628555298, 4.167873859405518]
memory len:5860
memory used:2530.0
now epsilon is 0.479941160303816, the reward is 57.6875 with loss [40.8023464679718, 38.847386598587036] in episode 459
Report: 
rewardSum:57.6875
loss:[40.8023464679718, 38.847386598587036]
policies:[0, 3, 3]
qAverage:[0.0, 85.48098754882812]
ws:[4.83778440952301, 6.914726257324219]
memory len:5872
memory used:2530.0
now epsilon is 0.4788623719310566, the reward is 242.25 with loss [42.849066972732544, 31.064953207969666] in episode 460
Report: 
rewardSum:242.25
loss:[42.849066972732544, 31.064953207969666]
policies:[1, 3, 5]
qAverage:[0.0, 67.87122344970703]
ws:[9.09628677368164, 9.945497512817383]
memory len:5890
memory used:2531.0
now epsilon is 0.47802499102522816, the reward is 56.6875 with loss [30.439407110214233, 37.37757587432861] in episode 461
Report: 
rewardSum:56.6875
loss:[30.439407110214233, 37.37757587432861]
policies:[0, 3, 4]
qAverage:[0.0, 59.861976623535156]
ws:[2.2751810550689697, 4.103270530700684]
memory len:5904
memory used:2531.0
now epsilon is 0.4775471452636999, the reward is 247.25 with loss [13.20240867137909, 21.540193557739258] in episode 462
Report: 
rewardSum:247.25
loss:[13.20240867137909, 21.540193557739258]
policies:[0, 2, 2]
qAverage:[0.0, 62.30341720581055]
ws:[5.096311569213867, 6.823938369750977]
memory len:5912
memory used:2531.0
now epsilon is 0.4765928862629536, the reward is 243.25 with loss [30.668792009353638, 45.02006983757019] in episode 463
Report: 
rewardSum:243.25
loss:[30.668792009353638, 45.02006983757019]
policies:[2, 3, 3]
qAverage:[0.0, 84.2569580078125]
ws:[8.0387601852417, 10.369871457417807]
memory len:5928
memory used:2531.0
now epsilon is 0.4758784435904827, the reward is 245.25 with loss [19.45311164855957, 31.90304946899414] in episode 464
Report: 
rewardSum:245.25
loss:[19.45311164855957, 31.90304946899414]
policies:[1, 2, 3]
qAverage:[0.0, 51.66897964477539]
ws:[-0.15165820717811584, 1.8820767402648926]
memory len:5940
memory used:2531.0
now epsilon is 0.47469008499774734, the reward is 990.0 with loss [51.72232228517532, 51.85141146183014] in episode 465
Report: 
rewardSum:990.0
loss:[51.72232228517532, 51.85141146183014]
policies:[2, 2, 6]
qAverage:[0.0, 74.1121317545573]
ws:[7.940711657206218, 8.90008290608724]
memory len:5960
memory used:2539.0
now epsilon is 0.4739784947438926, the reward is 994.0 with loss [37.79122495651245, 33.643027901649475] in episode 466
Report: 
rewardSum:994.0
loss:[37.79122495651245, 33.643027901649475]
policies:[0, 2, 4]
qAverage:[0.0, 75.66603342692058]
ws:[-6.483981450398763, -4.967641194661458]
memory len:5972
memory used:2539.0
now epsilon is 0.4737415351201766, the reward is -1.0 with loss [6.778912782669067, 4.116419315338135] in episode 467
Report: 
rewardSum:-1.0
loss:[6.778912782669067, 4.116419315338135]
policies:[0, 1, 1]
qAverage:[0.0, 51.248722076416016]
ws:[0.6179749369621277, 1.6010323762893677]
memory len:5976
memory used:2539.0
now epsilon is 0.4732679712085251, the reward is 247.25 with loss [26.33429765701294, 14.238447070121765] in episode 468
Report: 
rewardSum:247.25
loss:[26.33429765701294, 14.238447070121765]
policies:[0, 3, 1]
qAverage:[0.0, 95.50562477111816]
ws:[12.009871482849121, 13.646911859512329]
memory len:5984
memory used:2539.0
now epsilon is 0.47232226307107766, the reward is 55.6875 with loss [30.85260009765625, 41.34509778022766] in episode 469
Report: 
rewardSum:55.6875
loss:[30.85260009765625, 41.34509778022766]
policies:[0, 2, 6]
qAverage:[0.0, 67.19318389892578]
ws:[6.965360641479492, 8.339238166809082]
memory len:6000
memory used:2538.0
now epsilon is 0.4716142223310198, the reward is 57.6875 with loss [46.690881967544556, 26.74208950996399] in episode 470
Report: 
rewardSum:57.6875
loss:[46.690881967544556, 26.74208950996399]
policies:[1, 2, 3]
qAverage:[0.0, 78.50062306722005]
ws:[3.234772046407064, 5.483376185099284]
memory len:6012
memory used:2538.0
now epsilon is 0.470907242988505, the reward is 245.25 with loss [43.2986536026001, 33.42202514410019] in episode 471
Report: 
rewardSum:245.25
loss:[43.2986536026001, 33.42202514410019]
policies:[1, 2, 3]
qAverage:[0.0, 56.12436294555664]
ws:[6.013296127319336, 7.78074312210083]
memory len:6024
memory used:2538.0
now epsilon is 0.47067181879871345, the reward is -1.0 with loss [10.425965547561646, 4.62977409362793] in episode 472
Report: 
rewardSum:-1.0
loss:[10.425965547561646, 4.62977409362793]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6028
memory used:2538.0
now epsilon is 0.4699662521782882, the reward is 245.25 with loss [18.915257692337036, 42.173635482788086] in episode 473
Report: 
rewardSum:245.25
loss:[18.915257692337036, 42.173635482788086]
policies:[0, 3, 3]
qAverage:[0.0, 82.02239990234375]
ws:[11.498006184895834, 14.128923734029135]
memory len:6040
memory used:2538.0
now epsilon is 0.4697312984250898, the reward is -1.0 with loss [7.831241130828857, 18.780477046966553] in episode 474
Report: 
rewardSum:-1.0
loss:[7.831241130828857, 18.780477046966553]
policies:[0, 1, 1]
qAverage:[0.0, 51.122344970703125]
ws:[-0.3301489055156708, 1.2208657264709473]
memory len:6044
memory used:2538.0
now epsilon is 0.4684411508453386, the reward is 989.0 with loss [49.305803179740906, 80.54606986045837] in episode 475
Report: 
rewardSum:989.0
loss:[49.305803179740906, 80.54606986045837]
policies:[1, 4, 6]
qAverage:[0.0, 86.72872619628906]
ws:[-0.944419002532959, 1.544684600830078]
memory len:6066
memory used:2544.0
now epsilon is 0.4679728853306491, the reward is 247.25 with loss [17.304628610610962, 24.431585788726807] in episode 476
Report: 
rewardSum:247.25
loss:[17.304628610610962, 24.431585788726807]
policies:[1, 2, 1]
qAverage:[0.0, 92.27656555175781]
ws:[13.051831563313803, 14.80438232421875]
memory len:6074
memory used:2544.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3*		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.4677389281362892, the reward is -1.0 with loss [17.648435592651367, 16.98425030708313] in episode 477
Report: 
rewardSum:-1.0
loss:[17.648435592651367, 16.98425030708313]
policies:[0, 1, 1]
qAverage:[0.0, 49.59235382080078]
ws:[1.52114999294281, 2.6852729320526123]
memory len:6078
memory used:2544.0
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.4664542527311933, the reward is 240.25 with loss [70.73466634750366, 72.05740928649902] in episode 478
Report: 
rewardSum:240.25
loss:[70.73466634750366, 72.05740928649902]
policies:[0, 5, 6]
qAverage:[0.0, 90.4363037109375]
ws:[5.8573668003082275, 7.834347248077393]
memory len:6100
memory used:2544.0
now epsilon is 0.46598797336965536, the reward is -3.0 with loss [14.275044918060303, 16.359604120254517] in episode 479
Report: 
rewardSum:-3.0
loss:[14.275044918060303, 16.359604120254517]
policies:[0, 1, 3]
qAverage:[0.0, 52.07109069824219]
ws:[1.0255422592163086, 1.8861638307571411]
memory len:6108
memory used:2544.0
now epsilon is 0.46528942812773205, the reward is 245.25 with loss [35.51360750198364, 33.41985368728638] in episode 480
Report: 
rewardSum:245.25
loss:[35.51360750198364, 33.41985368728638]
policies:[0, 1, 5]
qAverage:[0.0, 57.992210388183594]
ws:[2.965000629425049, 4.097946643829346]
memory len:6120
memory used:2544.0
now epsilon is 0.4641275123118934, the reward is 53.6875 with loss [42.468839049339294, 51.67882704734802] in episode 481
Report: 
rewardSum:53.6875
loss:[42.468839049339294, 51.67882704734802]
policies:[1, 2, 7]
qAverage:[0.0, 59.23500061035156]
ws:[4.246250152587891, 5.185858249664307]
memory len:6140
memory used:2544.0
now epsilon is 0.46296849801988366, the reward is 53.6875 with loss [32.17883571982384, 64.17728066444397] in episode 482
Report: 
rewardSum:53.6875
loss:[32.17883571982384, 64.17728066444397]
policies:[0, 2, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6160
memory used:2545.0
now epsilon is 0.46227447916117026, the reward is 245.25 with loss [36.28865411877632, 15.297826290130615] in episode 483
Report: 
rewardSum:245.25
loss:[36.28865411877632, 15.297826290130615]
policies:[2, 1, 3]
qAverage:[0.0, 60.082088470458984]
ws:[3.55973482131958, 4.488663673400879]
memory len:6172
memory used:2546.0
now epsilon is 0.46123540109412803, the reward is 242.25 with loss [25.653017938137054, 37.97550141811371] in episode 484
Report: 
rewardSum:242.25
loss:[25.653017938137054, 37.97550141811371]
policies:[2, 3, 4]
qAverage:[0.0, 100.1488265991211]
ws:[11.393950045108795, 13.40253472328186]
memory len:6190
memory used:2546.0
now epsilon is 0.460774338627484, the reward is 59.6875 with loss [46.23835754394531, 15.14728045463562] in episode 485
Report: 
rewardSum:59.6875
loss:[46.23835754394531, 15.14728045463562]
policies:[1, 1, 2]
qAverage:[0.0, 66.91429138183594]
ws:[4.947556495666504, 6.34915828704834]
memory len:6198
memory used:2545.0
now epsilon is 0.4597386325032946, the reward is 242.25 with loss [48.781104654073715, 58.25182747840881] in episode 486
Report: 
rewardSum:242.25
loss:[48.781104654073715, 58.25182747840881]
policies:[3, 3, 3]
qAverage:[0.0, 86.80385080973308]
ws:[4.863611380259196, 6.530577341715495]
memory len:6216
memory used:2545.0
now epsilon is 0.45927906624404674, the reward is 247.25 with loss [20.03397762775421, 23.347344636917114] in episode 487
Report: 
rewardSum:247.25
loss:[20.03397762775421, 23.347344636917114]
policies:[1, 1, 2]
qAverage:[0.0, 77.39169311523438]
ws:[19.123477935791016, 20.725139617919922]
memory len:6224
memory used:2545.0
now epsilon is 0.458246721120319, the reward is 991.0 with loss [63.073490142822266, 68.94191074371338] in episode 488
Report: 
rewardSum:991.0
loss:[63.073490142822266, 68.94191074371338]
policies:[1, 2, 6]
qAverage:[0.0, 71.55413818359375]
ws:[6.888638973236084, 7.649115085601807]
memory len:6242
memory used:2545.0
now epsilon is 0.4573310292089999, the reward is 554.6875 with loss [28.719845175743103, 39.09257388114929] in episode 489
Report: 
rewardSum:554.6875
loss:[28.719845175743103, 39.09257388114929]
policies:[3, 1, 4]
qAverage:[0.0, 61.417850494384766]
ws:[2.7646727561950684, 4.240889072418213]
memory len:6258
memory used:2545.0
now epsilon is 0.4566454612701373, the reward is 57.6875 with loss [26.06319534778595, 33.59483742713928] in episode 490
Report: 
rewardSum:57.6875
loss:[26.06319534778595, 33.59483742713928]
policies:[2, 1, 3]
qAverage:[0.0, 54.09297561645508]
ws:[0.9658650159835815, 2.5324313640594482]
memory len:6270
memory used:2545.0
now epsilon is 0.4559609210406772, the reward is 245.25 with loss [35.50964641571045, 31.210598707199097] in episode 491
Report: 
rewardSum:245.25
loss:[35.50964641571045, 31.210598707199097]
policies:[0, 4, 2]
qAverage:[0.0, 103.14106369018555]
ws:[16.352009534835815, 19.005139350891113]
memory len:6282
memory used:2545.0
now epsilon is 0.45550513107648616, the reward is 247.25 with loss [15.90182077884674, 25.751351594924927] in episode 492
Report: 
rewardSum:247.25
loss:[15.90182077884674, 25.751351594924927]
policies:[1, 1, 2]
qAverage:[0.0, 59.35715866088867]
ws:[1.8046915531158447, 3.6104156970977783]
memory len:6290
memory used:2545.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17*		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.4550497967313666, the reward is 247.25 with loss [28.696285247802734, 19.203333139419556] in episode 493
Report: 
rewardSum:247.25
loss:[28.696285247802734, 19.203333139419556]
policies:[0, 3, 1]
qAverage:[0.0, 76.85943603515625]
ws:[18.185686111450195, 20.407621383666992]
memory len:6298
memory used:2545.0
now epsilon is 0.4541404930770041, the reward is 55.6875 with loss [39.548465967178345, 30.290648579597473] in episode 494
Report: 
rewardSum:55.6875
loss:[39.548465967178345, 30.290648579597473]
policies:[0, 4, 4]
qAverage:[0.0, 80.35046132405598]
ws:[4.387001196543376, 6.328318277994792]
memory len:6314
memory used:2545.0
now epsilon is 0.4534597079522087, the reward is 245.25 with loss [24.082277536392212, 33.56895399093628] in episode 495
Report: 
rewardSum:245.25
loss:[24.082277536392212, 33.56895399093628]
policies:[1, 3, 2]
qAverage:[0.0, 101.23247337341309]
ws:[11.088164925575256, 14.287210464477539]
memory len:6326
memory used:2545.0
now epsilon is 0.4530064182633075, the reward is 59.6875 with loss [26.145355224609375, 21.716968297958374] in episode 496
Report: 
rewardSum:59.6875
loss:[26.145355224609375, 21.716968297958374]
policies:[1, 2, 1]
qAverage:[0.0, 79.77601877848308]
ws:[3.863901217778524, 6.766056219736735]
memory len:6334
memory used:2545.0
now epsilon is 0.45221425135459475, the reward is 244.25 with loss [48.51510310173035, 34.329803705215454] in episode 497
Report: 
rewardSum:244.25
loss:[48.51510310173035, 34.329803705215454]
policies:[1, 3, 3]
qAverage:[0.0, 90.1116886138916]
ws:[5.841817617416382, 8.701107740402222]
memory len:6348
memory used:2545.0
now epsilon is 0.45176220665532285, the reward is 247.25 with loss [13.285885572433472, 26.83086848258972] in episode 498
Report: 
rewardSum:247.25
loss:[13.285885572433472, 26.83086848258972]
policies:[0, 3, 1]
qAverage:[0.0, 86.86134084065755]
ws:[10.706826657056808, 13.603526751200357]
memory len:6356
memory used:2545.0
now epsilon is 0.4513106138312617, the reward is 247.25 with loss [21.971582025289536, 27.206783533096313] in episode 499
Report: 
rewardSum:247.25
loss:[21.971582025289536, 27.206783533096313]
policies:[1, 2, 1]
qAverage:[0.0, 73.86782582600911]
ws:[5.247443536917369, 7.8527177174886065]
memory len:6364
memory used:2545.0
now epsilon is 0.45040878200240003, the reward is 243.25 with loss [52.8703498840332, 40.3391609787941] in episode 500
Report: 
rewardSum:243.25
loss:[52.8703498840332, 40.3391609787941]
policies:[1, 4, 3]
qAverage:[0.0, 89.98531150817871]
ws:[3.33558252453804, 6.701110005378723]
memory len:6380
memory used:2546.0
now epsilon is 0.44995854209554215, the reward is 247.25 with loss [11.27540111541748, 10.067407846450806] in episode 501
Report: 
rewardSum:247.25
loss:[11.27540111541748, 10.067407846450806]
policies:[1, 1, 2]
qAverage:[0.0, 66.21621704101562]
ws:[5.113417625427246, 7.48850154876709]
memory len:6388
memory used:2545.0
now epsilon is 0.4495087522597793, the reward is 247.25 with loss [13.359395384788513, 25.50867509841919] in episode 502
Report: 
rewardSum:247.25
loss:[13.359395384788513, 25.50867509841919]
policies:[0, 4, 0]
qAverage:[0.0, 102.11164474487305]
ws:[11.690883040428162, 15.867165088653564]
memory len:6396
memory used:2545.0
now epsilon is 0.4489471471921978, the reward is -4.0 with loss [22.629138946533203, 11.616516292095184] in episode 503
Report: 
rewardSum:-4.0
loss:[22.629138946533203, 11.616516292095184]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6406
memory used:2545.0
now epsilon is 0.4479380256530744, the reward is 242.25 with loss [55.6217645406723, 44.6023588180542] in episode 504
Report: 
rewardSum:242.25
loss:[55.6217645406723, 44.6023588180542]
policies:[0, 5, 4]
qAverage:[0.0, 92.38846397399902]
ws:[10.680970907211304, 14.795186042785645]
memory len:6424
memory used:2545.0
now epsilon is 0.4471547217819353, the reward is 244.25 with loss [22.56631910800934, 35.14759373664856] in episode 505
Report: 
rewardSum:244.25
loss:[22.56631910800934, 35.14759373664856]
policies:[1, 4, 2]
qAverage:[0.0, 98.51870727539062]
ws:[13.592817902565002, 16.18573522567749]
memory len:6438
memory used:2545.0
now epsilon is 0.4467077347152287, the reward is -3.0 with loss [29.493937611579895, 24.032720685005188] in episode 506
Report: 
rewardSum:-3.0
loss:[29.493937611579895, 24.032720685005188]
policies:[1, 1, 2]
qAverage:[0.0, 59.03158187866211]
ws:[7.162740230560303, 8.601927757263184]
memory len:6446
memory used:2545.0
now epsilon is 0.4460380917620872, the reward is 245.25 with loss [23.17616903781891, 38.58147835731506] in episode 507
Report: 
rewardSum:245.25
loss:[23.17616903781891, 38.58147835731506]
policies:[1, 4, 1]
qAverage:[0.0, 101.70812683105468]
ws:[13.011254262924194, 15.9200119972229]
memory len:6458
memory used:2545.0
now epsilon is 0.4455922209067339, the reward is 247.25 with loss [26.488228797912598, 23.514355659484863] in episode 508
Report: 
rewardSum:247.25
loss:[26.488228797912598, 23.514355659484863]
policies:[1, 3, 0]
qAverage:[0.0, 57.77379608154297]
ws:[9.30406665802002, 11.337198257446289]
memory len:6466
memory used:2545.0
now epsilon is 0.4453694526457943, the reward is -1.0 with loss [16.6153302192688, 7.256920337677002] in episode 509
Report: 
rewardSum:-1.0
loss:[16.6153302192688, 7.256920337677002]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6470
memory used:2545.0
now epsilon is 0.4447018158615357, the reward is 245.25 with loss [25.125879526138306, 34.08460831642151] in episode 510
Report: 
rewardSum:245.25
loss:[25.125879526138306, 34.08460831642151]
policies:[1, 4, 1]
qAverage:[0.0, 89.1233622233073]
ws:[17.906932830810547, 20.757845560709637]
memory len:6482
memory used:2546.0
now epsilon is 0.44425728078106297, the reward is 247.25 with loss [20.414358854293823, 17.434510588645935] in episode 511
Report: 
rewardSum:247.25
loss:[20.414358854293823, 17.434510588645935]
policies:[1, 2, 1]
qAverage:[0.0, 57.914390563964844]
ws:[2.227381706237793, 3.7037160396575928]
memory len:6490
memory used:2546.0
now epsilon is 0.4435913112122878, the reward is 556.6875 with loss [36.14825463294983, 31.973269045352936] in episode 512
Report: 
rewardSum:556.6875
loss:[36.14825463294983, 31.973269045352936]
policies:[1, 2, 3]
qAverage:[0.0, 75.71500142415364]
ws:[3.394705851872762, 4.779956658681233]
memory len:6502
memory used:2546.0
now epsilon is 0.4430370992485395, the reward is 246.25 with loss [30.792983055114746, 25.338439226150513] in episode 513
Report: 
rewardSum:246.25
loss:[30.792983055114746, 25.338439226150513]
policies:[1, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6512
memory used:2546.0
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.4423729588085242, the reward is 245.25 with loss [27.725217700004578, 25.056876838207245] in episode 514
Report: 
rewardSum:245.25
loss:[27.725217700004578, 25.056876838207245]
policies:[2, 1, 3]
qAverage:[0.0, 51.25141906738281]
ws:[0.3690240681171417, 1.1648720502853394]
memory len:6524
memory used:2546.0
now epsilon is 0.44159938650325553, the reward is 244.25 with loss [39.76821041107178, 38.16911816596985] in episode 515
Report: 
rewardSum:244.25
loss:[39.76821041107178, 38.16911816596985]
policies:[1, 4, 2]
qAverage:[0.0, 91.43901824951172]
ws:[12.059370835622152, 14.394898096720377]
memory len:6538
memory used:2546.0
now epsilon is 0.441157952688924, the reward is 247.25 with loss [42.67111873626709, 21.490384817123413] in episode 516
Report: 
rewardSum:247.25
loss:[42.67111873626709, 21.490384817123413]
policies:[3, 1, 0]
qAverage:[0.0, 53.45943069458008]
ws:[0.48062771558761597, 1.4503153562545776]
memory len:6546
memory used:2546.0
now epsilon is 0.4404966292076354, the reward is 994.0 with loss [43.04024803638458, 32.07211351394653] in episode 517
Report: 
rewardSum:994.0
loss:[43.04024803638458, 32.07211351394653]
policies:[0, 4, 2]
qAverage:[0.0, 88.10623474121094]
ws:[-1.7245941638946534, 0.1256709098815918]
memory len:6558
memory used:2546.0
now epsilon is 0.43983629709178457, the reward is 245.25 with loss [27.274236917495728, 22.65141749382019] in episode 518
Report: 
rewardSum:245.25
loss:[27.274236917495728, 22.65141749382019]
policies:[2, 3, 1]
qAverage:[0.0, 97.63886833190918]
ws:[17.598937511444092, 20.339422464370728]
memory len:6570
memory used:2547.0
now epsilon is 0.4389573938263846, the reward is 554.6875 with loss [32.79126465320587, 42.99889874458313] in episode 519
Report: 
rewardSum:554.6875
loss:[32.79126465320587, 42.99889874458313]
policies:[2, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6586
memory used:2546.0
now epsilon is 0.4385186010141478, the reward is 247.25 with loss [20.750620365142822, 16.02665138244629] in episode 520
Report: 
rewardSum:247.25
loss:[20.750620365142822, 16.02665138244629]
policies:[0, 3, 1]
qAverage:[0.0, 77.00640360514323]
ws:[1.6404868240157764, 3.4856375058492026]
memory len:6594
memory used:2546.0
now epsilon is 0.4380802468302034, the reward is 247.25 with loss [17.347963094711304, 13.068911075592041] in episode 521
Report: 
rewardSum:247.25
loss:[17.347963094711304, 13.068911075592041]
policies:[2, 1, 1]
qAverage:[0.0, 58.763179779052734]
ws:[1.8559266328811646, 3.6487576961517334]
memory len:6602
memory used:2546.0
now epsilon is 0.4376423308360875, the reward is 247.25 with loss [18.59834086894989, 20.003994464874268] in episode 522
Report: 
rewardSum:247.25
loss:[18.59834086894989, 20.003994464874268]
policies:[1, 3, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6610
memory used:2546.0
now epsilon is 0.43709555138062617, the reward is 246.25 with loss [11.884272933006287, 20.47003722190857] in episode 523
Report: 
rewardSum:246.25
loss:[11.884272933006287, 20.47003722190857]
policies:[0, 3, 2]
qAverage:[0.0, 80.71635182698567]
ws:[10.071731249491373, 12.570134798685709]
memory len:6620
memory used:2546.0
now epsilon is 0.43654945505783244, the reward is 58.6875 with loss [17.92568689584732, 42.77427291870117] in episode 524
Report: 
rewardSum:58.6875
loss:[17.92568689584732, 42.77427291870117]
policies:[0, 2, 3]
qAverage:[0.0, 80.03016153971355]
ws:[8.431567509969076, 10.302777608235678]
memory len:6630
memory used:2546.0
now epsilon is 0.4357860662439628, the reward is 244.25 with loss [32.49989080429077, 29.370694398880005] in episode 525
Report: 
rewardSum:244.25
loss:[32.49989080429077, 29.370694398880005]
policies:[2, 1, 4]
qAverage:[0.0, 64.99250793457031]
ws:[2.3641507625579834, 3.7832839488983154]
memory len:6644
memory used:2557.0
now epsilon is 0.4351327955578765, the reward is 245.25 with loss [30.96498990058899, 36.23358702659607] in episode 526
Report: 
rewardSum:245.25
loss:[30.96498990058899, 36.23358702659607]
policies:[0, 4, 2]
qAverage:[0.0, 97.05404281616211]
ws:[11.579137325286865, 14.52584195137024]
memory len:6656
memory used:2557.0
now epsilon is 0.4346978259099229, the reward is 247.25 with loss [30.965018272399902, 21.028241127729416] in episode 527
Report: 
rewardSum:247.25
loss:[30.965018272399902, 21.028241127729416]
policies:[1, 3, 0]
qAverage:[0.0, 85.92617988586426]
ws:[7.828218102455139, 10.301915645599365]
memory len:6664
memory used:2558.0
now epsilon is 0.4338291905990567, the reward is 55.6875 with loss [18.132962927222252, 34.221709966659546] in episode 528
Report: 
rewardSum:55.6875
loss:[18.132962927222252, 34.221709966659546]
policies:[0, 4, 4]
qAverage:[0.0, 95.66200103759766]
ws:[3.2381633520126343, 4.828389263153076]
memory len:6680
memory used:2558.0
now epsilon is 0.4329622910394603, the reward is 243.25 with loss [48.069921255111694, 29.148857474327087] in episode 529
Report: 
rewardSum:243.25
loss:[48.069921255111694, 29.148857474327087]
policies:[1, 3, 4]
qAverage:[0.0, 91.51702499389648]
ws:[12.05891239643097, 14.421918749809265]
memory len:6696
memory used:2558.0
now epsilon is 0.4324213587094511, the reward is -4.0 with loss [20.939988136291504, 26.899372160434723] in episode 530
Report: 
rewardSum:-4.0
loss:[20.939988136291504, 26.899372160434723]
policies:[1, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6706
memory used:2558.0
now epsilon is 0.43198909948172654, the reward is -3.0 with loss [12.29806137084961, 12.78031325340271] in episode 531
Report: 
rewardSum:-3.0
loss:[12.29806137084961, 12.78031325340271]
policies:[0, 1, 3]
qAverage:[0.0, 51.026737213134766]
ws:[1.0103095769882202, 2.005689859390259]
memory len:6714
memory used:2558.0
now epsilon is 0.43155727235115954, the reward is 247.25 with loss [26.160822868347168, 21.626188278198242] in episode 532
Report: 
rewardSum:247.25
loss:[26.160822868347168, 21.626188278198242]
policies:[1, 3, 0]
qAverage:[0.0, 91.74206924438477]
ws:[8.778185188770294, 11.304750084877014]
memory len:6722
memory used:2558.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21*		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.4309103408927393, the reward is 245.25 with loss [26.41311550140381, 58.38983917236328] in episode 533
Report: 
rewardSum:245.25
loss:[26.41311550140381, 58.38983917236328]
policies:[1, 1, 4]
qAverage:[0.0, 50.7508544921875]
ws:[0.2538898289203644, 1.26649808883667]
memory len:6734
memory used:2558.0
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.43047959211629416, the reward is 247.25 with loss [31.64901304244995, 36.86653423309326] in episode 534
Report: 
rewardSum:247.25
loss:[31.64901304244995, 36.86653423309326]
policies:[1, 1, 2]
qAverage:[0.0, 56.120849609375]
ws:[2.4292829036712646, 3.3190274238586426]
memory len:6742
memory used:2558.0
now epsilon is 0.43004927392712167, the reward is 59.6875 with loss [21.255226373672485, 27.325847387313843] in episode 535
Report: 
rewardSum:59.6875
loss:[21.255226373672485, 27.325847387313843]
policies:[2, 1, 1]
qAverage:[0.0, 57.813880920410156]
ws:[7.696463108062744, 9.109750747680664]
memory len:6750
memory used:2558.0
now epsilon is 0.42972681759919573, the reward is -2.0 with loss [17.27996325492859, 21.879834175109863] in episode 536
Report: 
rewardSum:-2.0
loss:[17.27996325492859, 21.879834175109863]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6756
memory used:2558.0
now epsilon is 0.42918992758932134, the reward is 246.25 with loss [30.533156991004944, 23.612528562545776] in episode 537
Report: 
rewardSum:246.25
loss:[30.533156991004944, 23.612528562545776]
policies:[2, 2, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6766
memory used:2558.0
now epsilon is 0.42876089858113225, the reward is 247.25 with loss [43.56659984588623, 26.39397096633911] in episode 538
Report: 
rewardSum:247.25
loss:[43.56659984588623, 26.39397096633911]
policies:[1, 2, 1]
qAverage:[0.0, 72.47897593180339]
ws:[7.34920072555542, 9.081363519032797]
memory len:6774
memory used:2564.0
now epsilon is 0.42811815906264034, the reward is 245.25 with loss [23.426688075065613, 31.77173924446106] in episode 539
Report: 
rewardSum:245.25
loss:[23.426688075065613, 31.77173924446106]
policies:[1, 4, 1]
qAverage:[0.0, 96.82259216308594]
ws:[7.8403150726109745, 10.492741680145263]
memory len:6786
memory used:2572.0
now epsilon is 0.427369513955296, the reward is 244.25 with loss [39.00009083747864, 33.30902290344238] in episode 540
Report: 
rewardSum:244.25
loss:[39.00009083747864, 33.30902290344238]
policies:[0, 4, 3]
qAverage:[0.0, 95.9322280883789]
ws:[8.52850263118744, 11.153847980499268]
memory len:6800
memory used:2572.0
now epsilon is 0.42651552245020347, the reward is 243.25 with loss [34.38195598125458, 40.575995206832886] in episode 541
Report: 
rewardSum:243.25
loss:[34.38195598125458, 40.575995206832886]
policies:[1, 4, 3]
qAverage:[0.0, 82.1379623413086]
ws:[12.32662057876587, 14.997103055318197]
memory len:6816
memory used:2572.0
now epsilon is 0.4258761488915694, the reward is 57.6875 with loss [29.754746913909912, 23.150468587875366] in episode 542
Report: 
rewardSum:57.6875
loss:[29.754746913909912, 23.150468587875366]
policies:[0, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6828
memory used:2572.0
now epsilon is 0.4254504324196181, the reward is 247.25 with loss [12.087371110916138, 12.273817777633667] in episode 543
Report: 
rewardSum:247.25
loss:[12.087371110916138, 12.273817777633667]
policies:[2, 1, 1]
qAverage:[0.0, 56.74894332885742]
ws:[1.8062843084335327, 3.2962863445281982]
memory len:6836
memory used:2572.0
now epsilon is 0.4246002757208829, the reward is 243.25 with loss [42.169896483421326, 59.774813532829285] in episode 544
Report: 
rewardSum:243.25
loss:[42.169896483421326, 59.774813532829285]
policies:[1, 2, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6852
memory used:2572.0
now epsilon is 0.4239637732373974, the reward is 245.25 with loss [24.337785243988037, 22.036057472229004] in episode 545
Report: 
rewardSum:245.25
loss:[24.337785243988037, 22.036057472229004]
policies:[0, 3, 3]
qAverage:[0.0, 78.03861490885417]
ws:[7.1854963302612305, 8.966940561930338]
memory len:6864
memory used:2572.0
now epsilon is 0.4235399684240789, the reward is 247.25 with loss [12.731017649173737, 29.048463821411133] in episode 546
Report: 
rewardSum:247.25
loss:[12.731017649173737, 29.048463821411133]
policies:[1, 3, 0]
qAverage:[0.0, 86.66680526733398]
ws:[6.413612276315689, 8.370547123253345]
memory len:6872
memory used:2572.0
now epsilon is 0.42290505540783185, the reward is 245.25 with loss [28.864030599594116, 37.15044641494751] in episode 547
Report: 
rewardSum:245.25
loss:[28.864030599594116, 37.15044641494751]
policies:[2, 1, 3]
qAverage:[0.0, 66.84517669677734]
ws:[11.409390449523926, 13.329909324645996]
memory len:6884
memory used:2572.0
now epsilon is 0.4224823089153899, the reward is 247.25 with loss [14.750800371170044, 17.057636976242065] in episode 548
Report: 
rewardSum:247.25
loss:[14.750800371170044, 17.057636976242065]
policies:[2, 1, 1]
qAverage:[0.0, 54.38634490966797]
ws:[7.6166911125183105, 9.044620513916016]
memory len:6892
memory used:2572.0
now epsilon is 0.4217435191518312, the reward is 993.0 with loss [32.36869955062866, 37.36958718299866] in episode 549
Report: 
rewardSum:993.0
loss:[32.36869955062866, 37.36958718299866]
policies:[1, 2, 4]
qAverage:[0.0, 77.99585469563802]
ws:[10.275148709615072, 12.724464098612467]
memory len:6906
memory used:2572.0
now epsilon is 0.4212166032767018, the reward is 246.25 with loss [48.75431966781616, 25.266271829605103] in episode 550
Report: 
rewardSum:246.25
loss:[48.75431966781616, 25.266271829605103]
policies:[0, 2, 3]
qAverage:[0.0, 77.37689971923828]
ws:[8.466972907384237, 10.96250162521998]
memory len:6916
memory used:2572.0
now epsilon is 0.4204800268374642, the reward is 244.25 with loss [28.666788458824158, 30.547631859779358] in episode 551
Report: 
rewardSum:244.25
loss:[28.666788458824158, 30.547631859779358]
policies:[0, 4, 3]
qAverage:[0.0, 93.92000732421874]
ws:[6.068907105922699, 9.332450771331787]
memory len:6930
memory used:2571.0
now epsilon is 0.4200597044643585, the reward is 247.25 with loss [21.004958152770996, 37.78843116760254] in episode 552
Report: 
rewardSum:247.25
loss:[21.004958152770996, 37.78843116760254]
policies:[1, 2, 1]
qAverage:[0.0, 74.84682210286458]
ws:[7.266019503275554, 9.730827649434408]
memory len:6938
memory used:2571.0
now epsilon is 0.4195348923054672, the reward is 246.25 with loss [23.339733600616455, 22.289990305900574] in episode 553
Report: 
rewardSum:246.25
loss:[23.339733600616455, 22.289990305900574]
policies:[0, 3, 2]
qAverage:[0.0, 77.21863555908203]
ws:[9.288351903359095, 11.567933003107706]
memory len:6948
memory used:2572.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2*		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.4191155147125271, the reward is -3.0 with loss [21.66541600227356, 25.322976112365723] in episode 554
Report: 
rewardSum:-3.0
loss:[21.66541600227356, 25.322976112365723]
policies:[1, 1, 2]
qAverage:[0.0, 54.16013717651367]
ws:[2.3406505584716797, 3.644120931625366]
memory len:6956
memory used:2572.0
now epsilon is 0.4181734472644492, the reward is 242.25 with loss [45.73455357551575, 38.03403162956238] in episode 555
Report: 
rewardSum:242.25
loss:[45.73455357551575, 38.03403162956238]
policies:[1, 4, 4]
qAverage:[0.0, 88.60308227539062]
ws:[9.491074132919312, 11.631164932250977]
memory len:6974
memory used:2571.0
now epsilon is 0.4177554306060933, the reward is 247.25 with loss [11.583513975143433, 29.3147234916687] in episode 556
Report: 
rewardSum:247.25
loss:[11.583513975143433, 29.3147234916687]
policies:[0, 1, 3]
qAverage:[0.0, 54.234703063964844]
ws:[1.4284839630126953, 2.586024045944214]
memory len:6982
memory used:2571.0
now epsilon is 0.41712918897537643, the reward is 245.25 with loss [33.74705505371094, 38.34931695461273] in episode 557
Report: 
rewardSum:245.25
loss:[33.74705505371094, 38.34931695461273]
policies:[2, 2, 2]
qAverage:[0.0, 70.47691090901692]
ws:[-0.6771402955055237, 0.960749348004659]
memory len:6994
memory used:2572.0
now epsilon is 0.41671221618377796, the reward is 247.25 with loss [18.202999114990234, 25.64468002319336] in episode 558
Report: 
rewardSum:247.25
loss:[18.202999114990234, 25.64468002319336]
policies:[0, 3, 1]
qAverage:[0.0, 89.61414909362793]
ws:[11.091124773025513, 13.405161380767822]
memory len:7002
memory used:2572.0
now epsilon is 0.4158795206332796, the reward is 243.25 with loss [29.990707874298096, 44.316200733184814] in episode 559
Report: 
rewardSum:243.25
loss:[29.990707874298096, 44.316200733184814]
policies:[1, 2, 5]
qAverage:[0.0, 76.86812845865886]
ws:[3.2107014656066895, 3.718383471171061]
memory len:7018
memory used:2572.0
now epsilon is 0.4154637970414758, the reward is 59.6875 with loss [13.861207485198975, 16.753058910369873] in episode 560
Report: 
rewardSum:59.6875
loss:[13.861207485198975, 16.753058910369873]
policies:[1, 2, 1]
qAverage:[0.0, 52.561519622802734]
ws:[8.80256462097168, 9.64278793334961]
memory len:7026
memory used:2572.0
now epsilon is 0.41494472689513906, the reward is 58.6875 with loss [39.5793262720108, 36.92501187324524] in episode 561
Report: 
rewardSum:58.6875
loss:[39.5793262720108, 36.92501187324524]
policies:[0, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7036
memory used:2572.0
now epsilon is 0.41452993774658414, the reward is 247.25 with loss [12.330476999282837, 26.282554864883423] in episode 562
Report: 
rewardSum:247.25
loss:[12.330476999282837, 26.282554864883423]
policies:[0, 2, 2]
qAverage:[0.0, 77.52184549967448]
ws:[8.05096717675527, 9.410946925481161]
memory len:7044
memory used:2572.0
now epsilon is 0.4137016029358818, the reward is 992.0 with loss [60.866904973983765, 36.229013085365295] in episode 563
Report: 
rewardSum:992.0
loss:[60.866904973983765, 36.229013085365295]
policies:[1, 2, 5]
qAverage:[0.0, 74.28544616699219]
ws:[7.9732035001118975, 9.040801684061686]
memory len:7060
memory used:2572.0
now epsilon is 0.41328805644519234, the reward is 247.25 with loss [18.906792879104614, 16.23357319831848] in episode 564
Report: 
rewardSum:247.25
loss:[18.906792879104614, 16.23357319831848]
policies:[2, 2, 0]
qAverage:[0.0, 51.356414794921875]
ws:[8.976848602294922, 10.002551078796387]
memory len:7068
memory used:2581.0
now epsilon is 0.41266851168894925, the reward is 245.25 with loss [27.38208758831024, 52.40564036369324] in episode 565
Report: 
rewardSum:245.25
loss:[27.38208758831024, 52.40564036369324]
policies:[1, 4, 1]
qAverage:[0.0, 68.21827952067058]
ws:[6.475571076075236, 8.128268082936605]
memory len:7080
memory used:2585.0
now epsilon is 0.4121529339026865, the reward is 246.25 with loss [34.517115354537964, 20.780948638916016] in episode 566
Report: 
rewardSum:246.25
loss:[34.517115354537964, 20.780948638916016]
policies:[0, 4, 1]
qAverage:[0.0, 85.10135841369629]
ws:[12.365159213542938, 14.737061023712158]
memory len:7090
memory used:2585.0
now epsilon is 0.41174093550037605, the reward is 247.25 with loss [31.605260372161865, 18.629429817199707] in episode 567
Report: 
rewardSum:247.25
loss:[31.605260372161865, 18.629429817199707]
policies:[0, 3, 1]
qAverage:[0.0, 83.6428705851237]
ws:[10.623003800710043, 13.035311381022135]
memory len:7098
memory used:2585.0
now epsilon is 0.41122651660475884, the reward is 246.25 with loss [20.004682540893555, 23.076378643512726] in episode 568
Report: 
rewardSum:246.25
loss:[20.004682540893555, 23.076378643512726]
policies:[1, 3, 1]
qAverage:[0.0, 58.80522918701172]
ws:[3.167959213256836, 4.435566425323486]
memory len:7108
memory used:2572.0
now epsilon is 0.41050740971067035, the reward is 56.6875 with loss [31.29317009449005, 27.03853154182434] in episode 569
Report: 
rewardSum:56.6875
loss:[31.29317009449005, 27.03853154182434]
policies:[0, 4, 3]
qAverage:[0.0, 83.46455574035645]
ws:[2.8420778326690197, 4.193139523267746]
memory len:7122
memory used:2572.0
now epsilon is 0.4100970562155833, the reward is 247.25 with loss [14.078489422798157, 17.334876477718353] in episode 570
Report: 
rewardSum:247.25
loss:[14.078489422798157, 17.334876477718353]
policies:[0, 2, 2]
qAverage:[0.0, 72.20418294270833]
ws:[2.4908430576324463, 3.52410356203715]
memory len:7130
memory used:2572.0
now epsilon is 0.4096871129201344, the reward is 247.25 with loss [13.439771085977554, 36.83578824996948] in episode 571
Report: 
rewardSum:247.25
loss:[13.439771085977554, 36.83578824996948]
policies:[3, 1, 0]
qAverage:[0.0, 54.285606384277344]
ws:[1.7025152444839478, 2.6515328884124756]
memory len:7138
memory used:2572.0
now epsilon is 0.40907296620441935, the reward is 245.25 with loss [18.56354022026062, 36.721909523010254] in episode 572
Report: 
rewardSum:245.25
loss:[18.56354022026062, 36.721909523010254]
policies:[2, 3, 1]
qAverage:[0.0, 83.71409034729004]
ws:[6.179290719330311, 8.119332164525986]
memory len:7150
memory used:2573.0
now epsilon is 0.4084597401332073, the reward is 245.25 with loss [31.36084508895874, 36.74444532394409] in episode 573
Report: 
rewardSum:245.25
loss:[31.36084508895874, 36.74444532394409]
policies:[2, 1, 3]
qAverage:[0.0, 67.406005859375]
ws:[10.491652488708496, 12.922170639038086]
memory len:7162
memory used:2573.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18*		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.40764353510019563, the reward is 243.25 with loss [44.01216697692871, 40.97902703285217] in episode 574
Report: 
rewardSum:243.25
loss:[44.01216697692871, 40.97902703285217]
policies:[1, 4, 3]
qAverage:[0.0, 83.76918601989746]
ws:[7.38095623254776, 9.405024528503418]
memory len:7178
memory used:2573.0
now epsilon is 0.407236044405945, the reward is 247.25 with loss [19.185646772384644, 16.73447012901306] in episode 575
Report: 
rewardSum:247.25
loss:[19.185646772384644, 16.73447012901306]
policies:[1, 3, 0]
qAverage:[0.0, 85.81856791178386]
ws:[10.177934169769287, 12.422048568725586]
memory len:7186
memory used:2573.0
now epsilon is 0.40672725380934277, the reward is 246.25 with loss [26.737501859664917, 18.086454391479492] in episode 576
Report: 
rewardSum:246.25
loss:[26.737501859664917, 18.086454391479492]
policies:[0, 2, 3]
qAverage:[0.0, 82.97864278157552]
ws:[12.247653007507324, 14.484410603841146]
memory len:7196
memory used:2573.0
now epsilon is 0.4057115788329408, the reward is 241.25 with loss [53.487952709198, 49.19521152973175] in episode 577
Report: 
rewardSum:241.25
loss:[53.487952709198, 49.19521152973175]
policies:[1, 2, 7]
qAverage:[0.0, 77.10883076985677]
ws:[9.940893014272055, 12.295474847157797]
memory len:7216
memory used:2572.0
now epsilon is 0.40510339169253545, the reward is 245.25 with loss [30.653285264968872, 17.44537627696991] in episode 578
Report: 
rewardSum:245.25
loss:[30.653285264968872, 17.44537627696991]
policies:[0, 4, 2]
qAverage:[0.0, 65.2677230834961]
ws:[12.311145782470703, 15.351439476013184]
memory len:7228
memory used:2585.0
now epsilon is 0.4038897511771888, the reward is 239.25 with loss [42.89295035600662, 58.32540762424469] in episode 579
Report: 
rewardSum:239.25
loss:[42.89295035600662, 58.32540762424469]
policies:[0, 3, 9]
qAverage:[0.0, 82.61632537841797]
ws:[6.805062294006348, 9.157464663187662]
memory len:7252
memory used:2587.0
now epsilon is 0.40348601285942676, the reward is 247.25 with loss [16.42286491394043, 26.01576840877533] in episode 580
Report: 
rewardSum:247.25
loss:[16.42286491394043, 26.01576840877533]
policies:[1, 3, 0]
qAverage:[0.0, 77.14028676350911]
ws:[6.140072703361511, 8.347829093535742]
memory len:7260
memory used:2585.0
now epsilon is 0.402881161982209, the reward is 245.25 with loss [20.578745424747467, 40.25004863739014] in episode 581
Report: 
rewardSum:245.25
loss:[20.578745424747467, 40.25004863739014]
policies:[0, 5, 1]
qAverage:[0.0, 95.11874643961589]
ws:[6.486224412918091, 9.04885764916738]
memory len:7272
memory used:2585.0
now epsilon is 0.4024784318754841, the reward is 247.25 with loss [20.73716977238655, 22.022382736206055] in episode 582
Report: 
rewardSum:247.25
loss:[20.73716977238655, 22.022382736206055]
policies:[3, 1, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7280
memory used:2585.0
now epsilon is 0.40187509142544986, the reward is 57.6875 with loss [34.199880599975586, 35.378541469573975] in episode 583
Report: 
rewardSum:57.6875
loss:[34.199880599975586, 35.378541469573975]
policies:[0, 3, 3]
qAverage:[0.0, 68.88772074381511]
ws:[6.722941557566325, 9.046302795410156]
memory len:7292
memory used:2585.0
now epsilon is 0.40117233725679236, the reward is 244.25 with loss [31.378167867660522, 36.119757413864136] in episode 584
Report: 
rewardSum:244.25
loss:[31.378167867660522, 36.119757413864136]
policies:[0, 3, 4]
qAverage:[0.0, 61.18309783935547]
ws:[3.3488214015960693, 5.173532485961914]
memory len:7306
memory used:2585.0
now epsilon is 0.4005709547246305, the reward is 245.25 with loss [24.838313817977905, 58.18494367599487] in episode 585
Report: 
rewardSum:245.25
loss:[24.838313817977905, 58.18494367599487]
policies:[0, 2, 4]
qAverage:[0.0, 87.32230885823567]
ws:[8.835828304290771, 12.076307614644369]
memory len:7318
memory used:2585.0
now epsilon is 0.4003706942829529, the reward is -1.0 with loss [17.81393003463745, 5.634325206279755] in episode 586
Report: 
rewardSum:-1.0
loss:[17.81393003463745, 5.634325206279755]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7322
memory used:2585.0
now epsilon is 0.39997047370265876, the reward is 59.6875 with loss [18.570226430892944, 34.62277150154114] in episode 587
Report: 
rewardSum:59.6875
loss:[18.570226430892944, 34.62277150154114]
policies:[1, 2, 1]
qAverage:[0.0, 67.79876963297527]
ws:[1.6691092252731323, 3.074726184209188]
memory len:7330
memory used:2585.0
now epsilon is 0.3995706531928872, the reward is 247.25 with loss [15.65696918964386, 24.392438650131226] in episode 588
Report: 
rewardSum:247.25
loss:[15.65696918964386, 24.392438650131226]
policies:[2, 2, 0]
qAverage:[0.0, 78.49689229329427]
ws:[6.551889697710673, 9.130699634552002]
memory len:7338
memory used:2585.0
now epsilon is 0.39897167168574293, the reward is 245.25 with loss [34.62977075576782, 42.26953125] in episode 589
Report: 
rewardSum:245.25
loss:[34.62977075576782, 42.26953125]
policies:[0, 3, 3]
qAverage:[0.0, 83.87417221069336]
ws:[5.436060616746545, 7.814255595207214]
memory len:7350
memory used:2585.0
now epsilon is 0.3983735880895013, the reward is 245.25 with loss [34.467592000961304, 38.75017213821411] in episode 590
Report: 
rewardSum:245.25
loss:[34.467592000961304, 38.75017213821411]
policies:[2, 1, 3]
qAverage:[0.0, 54.051605224609375]
ws:[8.076376914978027, 9.793325424194336]
memory len:7362
memory used:2585.0
now epsilon is 0.3979753638666106, the reward is 59.6875 with loss [22.789551258087158, 16.487253189086914] in episode 591
Report: 
rewardSum:59.6875
loss:[22.789551258087158, 16.487253189086914]
policies:[0, 2, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7370
memory used:2585.0
now epsilon is 0.3973787737983704, the reward is -5.0 with loss [34.98578327894211, 23.25322586297989] in episode 592
Report: 
rewardSum:-5.0
loss:[34.98578327894211, 23.25322586297989]
policies:[0, 3, 3]
qAverage:[0.0, 78.01007652282715]
ws:[6.553927779197693, 8.33924674987793]
memory len:7382
memory used:2585.0
now epsilon is 0.39698154401677765, the reward is 247.25 with loss [12.190877482295036, 21.600775957107544] in episode 593
Report: 
rewardSum:247.25
loss:[12.190877482295036, 21.600775957107544]
policies:[2, 2, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7390
memory used:2585.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18*		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.3965847113160301, the reward is 247.25 with loss [12.601064920425415, 29.960153818130493] in episode 594
Report: 
rewardSum:247.25
loss:[12.601064920425415, 29.960153818130493]
policies:[0, 2, 2]
qAverage:[0.0, 83.94437154134114]
ws:[11.724168459574381, 14.986162503560385]
memory len:7398
memory used:2585.0
now epsilon is 0.39618827529919587, the reward is 59.6875 with loss [14.881327152252197, 27.68560552597046] in episode 595
Report: 
rewardSum:59.6875
loss:[14.881327152252197, 27.68560552597046]
policies:[0, 2, 2]
qAverage:[0.0, 54.99333572387695]
ws:[10.633439064025879, 12.565388679504395]
memory len:7406
memory used:2585.0
now epsilon is 0.39579223556973975, the reward is 247.25 with loss [29.83255672454834, 11.268070697784424] in episode 596
Report: 
rewardSum:247.25
loss:[29.83255672454834, 11.268070697784424]
policies:[1, 2, 1]
qAverage:[0.0, 91.22390238444011]
ws:[11.26813538869222, 15.227142651875814]
memory len:7414
memory used:2585.0
now epsilon is 0.3953965917315229, the reward is 247.25 with loss [13.669654726982117, 19.591155976057053] in episode 597
Report: 
rewardSum:247.25
loss:[13.669654726982117, 19.591155976057053]
policies:[0, 3, 1]
qAverage:[0.0, 80.69315083821614]
ws:[9.502217471599579, 13.679139296213785]
memory len:7422
memory used:2585.0
now epsilon is 0.3950013433888026, the reward is 247.25 with loss [29.391611099243164, 30.47666025161743] in episode 598
Report: 
rewardSum:247.25
loss:[29.391611099243164, 30.47666025161743]
policies:[1, 3, 0]
qAverage:[0.0, 80.0770492553711]
ws:[3.6876071294148765, 6.616250991821289]
memory len:7430
memory used:2585.0
now epsilon is 0.3946064901462316, the reward is 247.25 with loss [25.8648419380188, 21.851433515548706] in episode 599
Report: 
rewardSum:247.25
loss:[25.8648419380188, 21.851433515548706]
policies:[3, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7438
memory used:2585.0
now epsilon is 0.39421203160885787, the reward is 59.6875 with loss [16.817602515220642, 38.52459096908569] in episode 600
Report: 
rewardSum:59.6875
loss:[16.817602515220642, 38.52459096908569]
policies:[1, 1, 2]
qAverage:[0.0, 65.02217864990234]
ws:[1.1707513332366943, 2.8083152770996094]
memory len:7446
memory used:2586.0
now epsilon is 0.3933259409975999, the reward is 242.25 with loss [39.6695100069046, 48.145482897758484] in episode 601
Report: 
rewardSum:242.25
loss:[39.6695100069046, 48.145482897758484]
policies:[0, 6, 3]
qAverage:[0.0, 80.80923843383789]
ws:[0.76836097240448, 2.7531738579273224]
memory len:7464
memory used:2586.0
now epsilon is 0.3929327625292489, the reward is 247.25 with loss [13.49800455570221, 23.444926261901855] in episode 602
Report: 
rewardSum:247.25
loss:[13.49800455570221, 23.444926261901855]
policies:[0, 3, 1]
qAverage:[0.0, 83.85114034016927]
ws:[14.29715379079183, 18.45896116892497]
memory len:7472
memory used:2586.0
now epsilon is 0.3922456457042423, the reward is 244.25 with loss [33.62414050102234, 37.707077741622925] in episode 603
Report: 
rewardSum:244.25
loss:[33.62414050102234, 37.707077741622925]
policies:[0, 4, 3]
qAverage:[0.0, 94.82095642089844]
ws:[11.228637981414796, 15.077876091003418]
memory len:7486
memory used:2586.0
now epsilon is 0.3918535471261414, the reward is 247.25 with loss [12.153168201446533, 17.275071144104004] in episode 604
Report: 
rewardSum:247.25
loss:[12.153168201446533, 17.275071144104004]
policies:[0, 3, 1]
qAverage:[0.0, 84.02559407552083]
ws:[12.828146616617838, 17.237016995747883]
memory len:7494
memory used:2586.0
now epsilon is 0.39126613404572136, the reward is 245.25 with loss [38.87601709365845, 33.19743275642395] in episode 605
Report: 
rewardSum:245.25
loss:[38.87601709365845, 33.19743275642395]
policies:[2, 3, 1]
qAverage:[0.0, 88.27082061767578]
ws:[2.8399413228034973, 6.014779806137085]
memory len:7506
memory used:2585.0
now epsilon is 0.39067960153440573, the reward is 245.25 with loss [31.535295367240906, 22.31196165084839] in episode 606
Report: 
rewardSum:245.25
loss:[31.535295367240906, 22.31196165084839]
policies:[1, 3, 2]
qAverage:[0.0, 87.60884602864583]
ws:[10.479598442713419, 15.162790775299072]
memory len:7518
memory used:2586.0
now epsilon is 0.39009394827216615, the reward is 245.25 with loss [37.81321465969086, 36.31596386432648] in episode 607
Report: 
rewardSum:245.25
loss:[37.81321465969086, 36.31596386432648]
policies:[0, 3, 3]
qAverage:[0.0, 64.79315948486328]
ws:[2.652547836303711, 5.050435543060303]
memory len:7530
memory used:2586.0
now epsilon is 0.3897040005847453, the reward is 247.25 with loss [12.387078523635864, 14.889471888542175] in episode 608
Report: 
rewardSum:247.25
loss:[12.387078523635864, 14.889471888542175]
policies:[1, 3, 0]
qAverage:[0.0, 73.09350840250652]
ws:[7.095337291558583, 10.498781204223633]
memory len:7538
memory used:2586.0
now epsilon is 0.38911980980960914, the reward is 245.25 with loss [36.90320563316345, 25.529485940933228] in episode 609
Report: 
rewardSum:245.25
loss:[36.90320563316345, 25.529485940933228]
policies:[2, 2, 2]
qAverage:[0.0, 92.8224385579427]
ws:[9.522948741912842, 12.968910217285156]
memory len:7550
memory used:2586.0
now epsilon is 0.3887308358954098, the reward is 247.25 with loss [35.28022861480713, 29.941115379333496] in episode 610
Report: 
rewardSum:247.25
loss:[35.28022861480713, 29.941115379333496]
policies:[0, 3, 1]
qAverage:[0.0, 89.1433588663737]
ws:[8.476301868756613, 12.425573190053305]
memory len:7558
memory used:2586.0
now epsilon is 0.38824516524658137, the reward is 58.6875 with loss [20.349058389663696, 28.055737376213074] in episode 611
Report: 
rewardSum:58.6875
loss:[20.349058389663696, 28.055737376213074]
policies:[0, 2, 3]
qAverage:[0.0, 83.00532786051433]
ws:[8.826677322387695, 12.012057940165201]
memory len:7568
memory used:2586.0
now epsilon is 0.38776010138259576, the reward is 246.25 with loss [47.03246212005615, 21.389997005462646] in episode 612
Report: 
rewardSum:246.25
loss:[47.03246212005615, 21.389997005462646]
policies:[1, 3, 1]
qAverage:[0.0, 94.81809616088867]
ws:[11.243412137031555, 15.394088983535767]
memory len:7578
memory used:2586.0
now epsilon is 0.38756624556691077, the reward is -1.0 with loss [20.5898380279541, 15.63845682144165] in episode 613
Report: 
rewardSum:-1.0
loss:[20.5898380279541, 15.63845682144165]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7582
memory used:2585.0
now epsilon is 0.3871788246344646, the reward is 247.25 with loss [9.540081977844238, 25.374597549438477] in episode 614
Report: 
rewardSum:247.25
loss:[9.540081977844238, 25.374597549438477]
policies:[1, 2, 1]
qAverage:[0.0, 75.47874196370442]
ws:[3.6611172358194985, 5.956153233846028]
memory len:7590
memory used:2586.0
now epsilon is 0.3868885131059688, the reward is -2.0 with loss [15.2958664894104, 18.28129005432129] in episode 615
Report: 
rewardSum:-2.0
loss:[15.2958664894104, 18.28129005432129]
policies:[0, 1, 2]
qAverage:[0.0, 52.857696533203125]
ws:[1.7856193780899048, 3.137791395187378]
memory len:7596
memory used:2586.0
now epsilon is 0.38650176965187627, the reward is 247.25 with loss [20.431801319122314, 19.586576223373413] in episode 616
Report: 
rewardSum:247.25
loss:[20.431801319122314, 19.586576223373413]
policies:[1, 3, 0]
qAverage:[0.0, 73.15722401936848]
ws:[2.241481383641561, 4.0738480885823565]
memory len:7604
memory used:2586.0
now epsilon is 0.3861154127962332, the reward is 247.25 with loss [37.86692476272583, 26.459819793701172] in episode 617
Report: 
rewardSum:247.25
loss:[37.86692476272583, 26.459819793701172]
policies:[0, 3, 1]
qAverage:[0.0, 72.91848754882812]
ws:[1.0893975347280502, 2.027761459350586]
memory len:7612
memory used:2586.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20*		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.3857294421525861, the reward is 247.25 with loss [16.677133202552795, 18.76832914352417] in episode 618
Report: 
rewardSum:247.25
loss:[16.677133202552795, 18.76832914352417]
policies:[0, 4, 0]
qAverage:[0.0, 98.35152282714844]
ws:[10.535068941116332, 14.565675568580627]
memory len:7620
memory used:2586.0
now epsilon is 0.38515120949019144, the reward is 245.25 with loss [29.69740867614746, 58.90567493438721] in episode 619
Report: 
rewardSum:245.25
loss:[29.69740867614746, 58.90567493438721]
policies:[1, 4, 1]
qAverage:[0.0, 97.19772491455078]
ws:[12.236608934402465, 16.231568622589112]
memory len:7632
memory used:2585.0
now epsilon is 0.3845738436348779, the reward is 245.25 with loss [14.694178104400635, 30.007150173187256] in episode 620
Report: 
rewardSum:245.25
loss:[14.694178104400635, 30.007150173187256]
policies:[0, 5, 1]
qAverage:[0.0, 102.62867863972981]
ws:[7.792855203151703, 11.230669140815735]
memory len:7644
memory used:2585.0
now epsilon is 0.3839973432872472, the reward is 245.25 with loss [34.088706851005554, 27.62780785560608] in episode 621
Report: 
rewardSum:245.25
loss:[34.088706851005554, 27.62780785560608]
policies:[0, 3, 3]
qAverage:[0.0, 90.66640663146973]
ws:[11.304942071437836, 15.83896541595459]
memory len:7656
memory used:2585.0
now epsilon is 0.38361348991896543, the reward is 247.25 with loss [20.194751739501953, 18.017997980117798] in episode 622
Report: 
rewardSum:247.25
loss:[20.194751739501953, 18.017997980117798]
policies:[0, 4, 0]
qAverage:[0.0, 98.8992431640625]
ws:[6.1848159551620485, 9.663360428810119]
memory len:7664
memory used:2585.0
now epsilon is 0.3830384292018771, the reward is 245.25 with loss [20.914170503616333, 12.071541368961334] in episode 623
Report: 
rewardSum:245.25
loss:[20.914170503616333, 12.071541368961334]
policies:[2, 3, 1]
qAverage:[0.0, 85.10269673665364]
ws:[10.168532609939575, 13.922789891560873]
memory len:7676
memory used:2585.0
now epsilon is 0.3826555343881478, the reward is 247.25 with loss [10.7808917760849, 27.841971397399902] in episode 624
Report: 
rewardSum:247.25
loss:[10.7808917760849, 27.841971397399902]
policies:[1, 3, 0]
qAverage:[0.0, 95.68020057678223]
ws:[9.262340515851974, 12.636123716831207]
memory len:7684
memory used:2585.0
now epsilon is 0.38208190970657174, the reward is 245.25 with loss [30.61290717124939, 46.44839596748352] in episode 625
Report: 
rewardSum:245.25
loss:[30.61290717124939, 46.44839596748352]
policies:[1, 4, 1]
qAverage:[0.0, 96.15727233886719]
ws:[13.57493281364441, 17.21199870109558]
memory len:7696
memory used:2585.0
now epsilon is 0.3816045460609393, the reward is 58.6875 with loss [23.321118593215942, 31.8832505941391] in episode 626
Report: 
rewardSum:58.6875
loss:[23.321118593215942, 31.8832505941391]
policies:[2, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7706
memory used:2585.0
now epsilon is 0.38112777882158616, the reward is 246.25 with loss [23.48726212978363, 18.87870955467224] in episode 627
Report: 
rewardSum:246.25
loss:[23.48726212978363, 18.87870955467224]
policies:[0, 3, 2]
qAverage:[0.0, 98.40719032287598]
ws:[11.59521996974945, 15.6337411403656]
memory len:7716
memory used:2585.0
now epsilon is 0.38036618990417337, the reward is 243.25 with loss [24.166375637054443, 52.69150447845459] in episode 628
Report: 
rewardSum:243.25
loss:[24.166375637054443, 52.69150447845459]
policies:[1, 3, 4]
qAverage:[0.0, 85.81356811523438]
ws:[4.449281215667725, 7.030420621236165]
memory len:7732
memory used:2585.0
now epsilon is 0.37989096983623705, the reward is 246.25 with loss [31.659839868545532, 21.931763648986816] in episode 629
Report: 
rewardSum:246.25
loss:[31.659839868545532, 21.931763648986816]
policies:[1, 3, 1]
qAverage:[0.0, 79.62566375732422]
ws:[3.848993937174479, 6.165216128031413]
memory len:7742
memory used:2585.0
now epsilon is 0.37941634349644743, the reward is 58.6875 with loss [23.167829155921936, 27.234097480773926] in episode 630
Report: 
rewardSum:58.6875
loss:[23.167829155921936, 27.234097480773926]
policies:[0, 1, 4]
qAverage:[0.0, 66.03620147705078]
ws:[3.8624584674835205, 5.808965682983398]
memory len:7752
memory used:2585.0
now epsilon is 0.3790370694103678, the reward is 59.6875 with loss [18.38366711139679, 39.37394666671753] in episode 631
Report: 
rewardSum:59.6875
loss:[18.38366711139679, 39.37394666671753]
policies:[2, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7760
memory used:2586.0
now epsilon is 0.37865817445617017, the reward is 247.25 with loss [22.88229274749756, 18.480859398841858] in episode 632
Report: 
rewardSum:247.25
loss:[22.88229274749756, 18.480859398841858]
policies:[1, 3, 0]
qAverage:[0.0, 97.86783218383789]
ws:[12.973462343215942, 17.135230541229248]
memory len:7768
memory used:2586.0
now epsilon is 0.37846886903507804, the reward is -1.0 with loss [8.231479167938232, 1.5466551184654236] in episode 633
Report: 
rewardSum:-1.0
loss:[8.231479167938232, 1.5466551184654236]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7772
memory used:2586.0
now epsilon is 0.3780905420682161, the reward is 247.25 with loss [21.565821647644043, 23.218899488449097] in episode 634
Report: 
rewardSum:247.25
loss:[21.565821647644043, 23.218899488449097]
policies:[1, 3, 0]
qAverage:[0.0, 58.701656341552734]
ws:[9.444676399230957, 12.122306823730469]
memory len:7780
memory used:2586.0
now epsilon is 0.3774293796567167, the reward is 244.25 with loss [34.514811992645264, 47.39227330684662] in episode 635
Report: 
rewardSum:244.25
loss:[34.514811992645264, 47.39227330684662]
policies:[3, 2, 2]
qAverage:[0.0, 72.21652730305989]
ws:[1.0722414975365002, 3.268188794453939]
memory len:7794
memory used:2586.0
now epsilon is 0.3770520917894895, the reward is 247.25 with loss [19.894344568252563, 24.901611804962158] in episode 636
Report: 
rewardSum:247.25
loss:[19.894344568252563, 24.901611804962158]
policies:[0, 2, 2]
qAverage:[0.0, 79.71749623616536]
ws:[10.471678058306376, 14.67591134707133]
memory len:7802
memory used:2586.0
now epsilon is 0.37648686702033474, the reward is 245.25 with loss [38.87085938453674, 31.070399284362793] in episode 637
Report: 
rewardSum:245.25
loss:[38.87085938453674, 31.070399284362793]
policies:[2, 3, 1]
qAverage:[0.0, 92.33529472351074]
ws:[9.297627717256546, 12.847868740558624]
memory len:7814
memory used:2586.0
now epsilon is 0.37611052131236067, the reward is 247.25 with loss [23.635772943496704, 18.863236010074615] in episode 638
Report: 
rewardSum:247.25
loss:[23.635772943496704, 18.863236010074615]
policies:[1, 1, 2]
qAverage:[0.0, 57.17982482910156]
ws:[8.417986869812012, 10.712204933166504]
memory len:7822
memory used:2586.0
now epsilon is 0.3755467080164935, the reward is 245.25 with loss [19.92847168445587, 26.550630509853363] in episode 639
Report: 
rewardSum:245.25
loss:[19.92847168445587, 26.550630509853363]
policies:[0, 3, 3]
qAverage:[0.0, 79.47253672281902]
ws:[8.530258854230246, 12.069123347600302]
memory len:7834
memory used:2586.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18*		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.37507750928949357, the reward is 246.25 with loss [20.725810050964355, 25.360103607177734] in episode 640
Report: 
rewardSum:246.25
loss:[20.725810050964355, 25.360103607177734]
policies:[2, 2, 1]
qAverage:[0.0, 87.40579223632812]
ws:[10.964946746826172, 14.712669690450033]
memory len:7844
memory used:2586.0
now epsilon is 0.37451524454353463, the reward is 245.25 with loss [32.03129422664642, 30.978350400924683] in episode 641
Report: 
rewardSum:245.25
loss:[32.03129422664642, 30.978350400924683]
policies:[1, 3, 2]
qAverage:[0.0, 94.70071411132812]
ws:[9.834638595581055, 12.885550022125244]
memory len:7856
memory used:2586.0
now epsilon is 0.3736734274112451, the reward is 242.25 with loss [51.37803626060486, 38.75756061077118] in episode 642
Report: 
rewardSum:242.25
loss:[51.37803626060486, 38.75756061077118]
policies:[2, 3, 4]
qAverage:[0.0, 91.18856430053711]
ws:[9.35235345363617, 12.3325856924057]
memory len:7874
memory used:2598.0
now epsilon is 0.37348661405212874, the reward is -1.0 with loss [15.90482473373413, 7.708093166351318] in episode 643
Report: 
rewardSum:-1.0
loss:[15.90482473373413, 7.708093166351318]
policies:[0, 1, 1]
qAverage:[0.0, 51.26491165161133]
ws:[0.23673494160175323, 1.1922285556793213]
memory len:7878
memory used:2598.0
now epsilon is 0.37311326747221546, the reward is 59.6875 with loss [31.57566785812378, 20.696802377700806] in episode 644
Report: 
rewardSum:59.6875
loss:[31.57566785812378, 20.696802377700806]
policies:[0, 2, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7886
memory used:2598.0
now epsilon is 0.3727402940989004, the reward is 247.25 with loss [21.74411404132843, 26.471718311309814] in episode 645
Report: 
rewardSum:247.25
loss:[21.74411404132843, 26.471718311309814]
policies:[2, 2, 0]
qAverage:[0.0, 57.01873016357422]
ws:[8.268365859985352, 10.001423835754395]
memory len:7894
memory used:2598.0
now epsilon is 0.3722746016357273, the reward is 246.25 with loss [37.22967481613159, 25.168222427368164] in episode 646
Report: 
rewardSum:246.25
loss:[37.22967481613159, 25.168222427368164]
policies:[0, 3, 2]
qAverage:[0.0, 90.07207489013672]
ws:[9.561888039112091, 12.371110916137695]
memory len:7904
memory used:2604.0
now epsilon is 0.3719024666138015, the reward is -3.0 with loss [46.786861419677734, 21.60651969909668] in episode 647
Report: 
rewardSum:-3.0
loss:[46.786861419677734, 21.60651969909668]
policies:[0, 1, 3]
qAverage:[0.0, 50.618751525878906]
ws:[0.5346351861953735, 1.1695294380187988]
memory len:7912
memory used:2604.0
now epsilon is 0.3715307035873703, the reward is 247.25 with loss [20.37448287010193, 17.120375156402588] in episode 648
Report: 
rewardSum:247.25
loss:[20.37448287010193, 17.120375156402588]
policies:[1, 3, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7920
memory used:2605.0
now epsilon is 0.37115931218457754, the reward is 247.25 with loss [39.16307783126831, 34.716288805007935] in episode 649
Report: 
rewardSum:247.25
loss:[39.16307783126831, 34.716288805007935]
policies:[0, 2, 2]
qAverage:[0.0, 54.29966735839844]
ws:[9.080048561096191, 11.001299858093262]
memory len:7928
memory used:2605.0
now epsilon is 0.37078829203393915, the reward is -3.0 with loss [27.329925060272217, 40.355629444122314] in episode 650
Report: 
rewardSum:-3.0
loss:[27.329925060272217, 40.355629444122314]
policies:[1, 1, 2]
qAverage:[0.0, 54.85092544555664]
ws:[2.1410861015319824, 3.1812820434570312]
memory len:7936
memory used:2605.0
now epsilon is 0.3704176427643419, the reward is 247.25 with loss [36.750224590301514, 16.22986912727356] in episode 651
Report: 
rewardSum:247.25
loss:[36.750224590301514, 16.22986912727356]
policies:[1, 2, 1]
qAverage:[0.0, 61.91877365112305]
ws:[1.1669727563858032, 2.055234909057617]
memory len:7944
memory used:2605.0
now epsilon is 0.36986236345100176, the reward is 245.25 with loss [20.698473572731018, 40.94220185279846] in episode 652
Report: 
rewardSum:245.25
loss:[20.698473572731018, 40.94220185279846]
policies:[0, 2, 4]
qAverage:[0.0, 78.1215337117513]
ws:[8.065585056940714, 10.527630488077799]
memory len:7956
memory used:2605.0
now epsilon is 0.3690310048382925, the reward is 54.6875 with loss [37.11606192588806, 67.56253933906555] in episode 653
Report: 
rewardSum:54.6875
loss:[37.11606192588806, 67.56253933906555]
policies:[0, 2, 7]
qAverage:[0.0, 67.21433766682942]
ws:[-0.5574156393607458, 0.2181147336959839]
memory len:7974
memory used:2605.0
now epsilon is 0.36829358831007314, the reward is 243.25 with loss [19.78140127658844, 52.042643547058105] in episode 654
Report: 
rewardSum:243.25
loss:[19.78140127658844, 52.042643547058105]
policies:[1, 3, 4]
qAverage:[0.0, 86.2141596476237]
ws:[11.149964809417725, 14.328994750976562]
memory len:7990
memory used:2605.0
now epsilon is 0.367741493087777, the reward is 245.25 with loss [28.74027144908905, 18.51716697216034] in episode 655
Report: 
rewardSum:245.25
loss:[28.74027144908905, 18.51716697216034]
policies:[0, 5, 1]
qAverage:[0.0, 95.07092412312825]
ws:[6.627727031707764, 8.981709043184916]
memory len:8002
memory used:2605.0
now epsilon is 0.36737388947476673, the reward is 59.6875 with loss [15.723414301872253, 20.001640677452087] in episode 656
Report: 
rewardSum:59.6875
loss:[15.723414301872253, 20.001640677452087]
policies:[0, 2, 2]
qAverage:[0.0, 74.87037150065105]
ws:[7.624633312225342, 10.225088119506836]
memory len:8010
memory used:2605.0
now epsilon is 0.3670066533275411, the reward is 247.25 with loss [20.561025619506836, 44.09050178527832] in episode 657
Report: 
rewardSum:247.25
loss:[20.561025619506836, 44.09050178527832]
policies:[1, 3, 0]
qAverage:[0.0, 89.4299430847168]
ws:[7.820796608924866, 10.664068222045898]
memory len:8018
memory used:2606.0
now epsilon is 0.36654812433270245, the reward is 246.25 with loss [24.58633542060852, 27.76619815826416] in episode 658
Report: 
rewardSum:246.25
loss:[24.58633542060852, 27.76619815826416]
policies:[1, 2, 2]
qAverage:[0.0, 54.24626922607422]
ws:[1.3843532800674438, 2.0941059589385986]
memory len:8028
memory used:2606.0
now epsilon is 0.36599864567054524, the reward is 245.25 with loss [48.67049837112427, 30.25726866722107] in episode 659
Report: 
rewardSum:245.25
loss:[48.67049837112427, 30.25726866722107]
policies:[0, 3, 3]
qAverage:[0.0, 90.42758178710938]
ws:[7.048759996891022, 9.483055859804153]
memory len:8040
memory used:2606.0
now epsilon is 0.36563278425149337, the reward is 247.25 with loss [34.69178292155266, 19.723694801330566] in episode 660
Report: 
rewardSum:247.25
loss:[34.69178292155266, 19.723694801330566]
policies:[1, 1, 2]
qAverage:[0.0, 54.83428192138672]
ws:[7.936594009399414, 9.549219131469727]
memory len:8048
memory used:2606.0
now epsilon is 0.36508467774161263, the reward is 245.25 with loss [30.29570198059082, 29.087663888931274] in episode 661
Report: 
rewardSum:245.25
loss:[30.29570198059082, 29.087663888931274]
policies:[1, 2, 3]
qAverage:[0.0, 78.37794494628906]
ws:[7.154611865679423, 10.117059667905172]
memory len:8060
memory used:2606.0
############# STATE ###############
0-		8-		16*		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.36471972994780877, the reward is 247.25 with loss [24.380020141601562, 14.83168876171112] in episode 662
Report: 
rewardSum:247.25
loss:[24.380020141601562, 14.83168876171112]
policies:[1, 3, 0]
qAverage:[0.0, 75.45328776041667]
ws:[1.1529874404271443, 2.7693869272867837]
memory len:8068
memory used:2611.0
now epsilon is 0.36435514696496624, the reward is 59.6875 with loss [19.979455530643463, 27.733027696609497] in episode 663
Report: 
rewardSum:59.6875
loss:[19.979455530643463, 27.733027696609497]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:8076
memory used:2611.0
now epsilon is 0.3638089557136294, the reward is 245.25 with loss [18.461907148361206, 25.6934854388237] in episode 664
Report: 
rewardSum:245.25
loss:[18.461907148361206, 25.6934854388237]
policies:[1, 3, 2]
qAverage:[0.0, 82.92070198059082]
ws:[3.4079447984695435, 5.132378816604614]
memory len:8088
memory used:2611.0
now epsilon is 0.36344528316353764, the reward is 247.25 with loss [18.131606101989746, 24.166274547576904] in episode 665
Report: 
rewardSum:247.25
loss:[18.131606101989746, 24.166274547576904]
policies:[0, 4, 0]
qAverage:[0.0, 81.37054443359375]
ws:[9.679848512013754, 13.925838311513266]
memory len:8096
memory used:2611.0
now epsilon is 0.3630819741496414, the reward is 247.25 with loss [37.16770696640015, 31.87667417526245] in episode 666
Report: 
rewardSum:247.25
loss:[37.16770696640015, 31.87667417526245]
policies:[1, 2, 1]
qAverage:[0.0, 59.13643264770508]
ws:[2.1251893043518066, 3.7300381660461426]
memory len:8104
memory used:2611.0
now epsilon is 0.3627190283085409, the reward is 247.25 with loss [17.06290376186371, 33.517828702926636] in episode 667
Report: 
rewardSum:247.25
loss:[17.06290376186371, 33.517828702926636]
policies:[0, 3, 1]
qAverage:[0.0, 68.31900787353516]
ws:[1.0418403049310048, 2.255817969640096]
memory len:8112
memory used:2611.0
now epsilon is 0.36235644527719957, the reward is 247.25 with loss [22.249611735343933, 24.646397352218628] in episode 668
Report: 
rewardSum:247.25
loss:[22.249611735343933, 24.646397352218628]
policies:[0, 2, 2]
qAverage:[0.0, 80.17398325602214]
ws:[9.150633295377096, 13.862935702006022]
memory len:8120
memory used:2611.0
now epsilon is 0.36199422469294346, the reward is 247.25 with loss [26.46384358406067, 16.66917324066162] in episode 669
Report: 
rewardSum:247.25
loss:[26.46384358406067, 16.66917324066162]
policies:[1, 2, 1]
qAverage:[0.0, 48.31343460083008]
ws:[0.1378999799489975, 1.0452723503112793]
memory len:8128
memory used:2611.0
now epsilon is 0.3612708694168049, the reward is 243.25 with loss [62.02871775627136, 43.97141474485397] in episode 670
Report: 
rewardSum:243.25
loss:[62.02871775627136, 43.97141474485397]
policies:[3, 2, 3]
qAverage:[0.0, 61.35299301147461]
ws:[3.0922343730926514, 5.14485502243042]
memory len:8144
memory used:2611.0
now epsilon is 0.3607293016912439, the reward is 57.6875 with loss [55.8740291595459, 41.63552474975586] in episode 671
Report: 
rewardSum:57.6875
loss:[55.8740291595459, 41.63552474975586]
policies:[0, 3, 3]
qAverage:[0.0, 81.64276313781738]
ws:[1.9757710145786405, 4.392960846424103]
memory len:8156
memory used:2611.0
now epsilon is 0.36054895958597966, the reward is -1.0 with loss [6.513784408569336, 16.344414710998535] in episode 672
Report: 
rewardSum:-1.0
loss:[6.513784408569336, 16.344414710998535]
policies:[0, 1, 1]
qAverage:[0.0, 49.572059631347656]
ws:[-1.4523279666900635, 0.16692917048931122]
memory len:8160
memory used:2611.0
now epsilon is 0.36018854580972065, the reward is 247.25 with loss [25.352487564086914, 41.012287616729736] in episode 673
Report: 
rewardSum:247.25
loss:[25.352487564086914, 41.012287616729736]
policies:[0, 3, 1]
qAverage:[0.0, 71.29377746582031]
ws:[-0.16101956367492676, 2.2725507815678916]
memory len:8168
memory used:2612.0
now epsilon is 0.3593789315333067, the reward is 242.25 with loss [58.870527267456055, 53.77015447616577] in episode 674
Report: 
rewardSum:242.25
loss:[58.870527267456055, 53.77015447616577]
policies:[1, 5, 3]
qAverage:[0.0, 90.34606018066407]
ws:[6.10792719013989, 9.643902778625488]
memory len:8186
memory used:2612.0
now epsilon is 0.359019687346413, the reward is 247.25 with loss [20.38106918334961, 17.483092308044434] in episode 675
Report: 
rewardSum:247.25
loss:[20.38106918334961, 17.483092308044434]
policies:[0, 3, 1]
qAverage:[0.0, 53.044227600097656]
ws:[2.2424356937408447, 3.4552505016326904]
memory len:8194
memory used:2612.0
now epsilon is 0.35830227594212904, the reward is 243.25 with loss [38.99705123901367, 68.0304297208786] in episode 676
Report: 
rewardSum:243.25
loss:[38.99705123901367, 68.0304297208786]
policies:[1, 2, 5]
qAverage:[0.0, 76.77047983805339]
ws:[11.06503677368164, 14.754160245259603]
memory len:8210
memory used:2612.0
now epsilon is 0.3579441080071479, the reward is 247.25 with loss [25.144236087799072, 15.641502141952515] in episode 677
Report: 
rewardSum:247.25
loss:[25.144236087799072, 15.641502141952515]
policies:[0, 4, 0]
qAverage:[0.0, 87.44689559936523]
ws:[7.4652452021837234, 9.824238151311874]
memory len:8218
memory used:2612.0
now epsilon is 0.3575862981058112, the reward is 247.25 with loss [22.61488676071167, 26.825191736221313] in episode 678
Report: 
rewardSum:247.25
loss:[22.61488676071167, 26.825191736221313]
policies:[2, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:8226
memory used:2612.0
now epsilon is 0.3572288458802195, the reward is 247.25 with loss [22.473256587982178, 24.660201907157898] in episode 679
Report: 
rewardSum:247.25
loss:[22.473256587982178, 24.660201907157898]
policies:[0, 3, 1]
qAverage:[0.0, 76.01024627685547]
ws:[9.033053750793139, 11.655871252218882]
memory len:8234
memory used:2612.0
now epsilon is 0.35687175097283114, the reward is 247.25 with loss [26.542900562286377, 13.943139791488647] in episode 680
Report: 
rewardSum:247.25
loss:[26.542900562286377, 13.943139791488647]
policies:[1, 3, 0]
qAverage:[31.685984293619793, 42.91620890299479]
ws:[10.177599986394247, 12.674299955368042]
memory len:8242
memory used:2612.0
now epsilon is 0.3565150130264619, the reward is 247.25 with loss [19.923582792282104, 15.83887505531311] in episode 681
Report: 
rewardSum:247.25
loss:[19.923582792282104, 15.83887505531311]
policies:[0, 2, 2]
qAverage:[0.0, 64.10027313232422]
ws:[13.728160858154297, 17.49671745300293]
memory len:8250
memory used:2612.0
now epsilon is 0.3561586316842845, the reward is 247.25 with loss [29.745759963989258, 34.41071557998657] in episode 682
Report: 
rewardSum:247.25
loss:[29.745759963989258, 34.41071557998657]
policies:[1, 3, 0]
qAverage:[0.0, 82.4608891805013]
ws:[10.878362655639648, 13.891661326090494]
memory len:8258
memory used:2612.0
now epsilon is 0.35580260658982865, the reward is 59.6875 with loss [18.870027780532837, 37.098044872283936] in episode 683
Report: 
rewardSum:59.6875
loss:[18.870027780532837, 37.098044872283936]
policies:[1, 2, 1]
qAverage:[0.0, 59.226993560791016]
ws:[1.6123429536819458, 2.4059245586395264]
memory len:8266
memory used:2612.0
now epsilon is 0.35544693738698, the reward is 247.25 with loss [13.15405797958374, 10.470876216888428] in episode 684
Report: 
rewardSum:247.25
loss:[13.15405797958374, 10.470876216888428]
policies:[2, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:8274
memory used:2612.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21*		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.35491410010134705, the reward is 245.25 with loss [15.966170370578766, 29.582133293151855] in episode 685
Report: 
rewardSum:245.25
loss:[15.966170370578766, 29.582133293151855]
policies:[0, 5, 1]
qAverage:[0.0, 90.58993682861328]
ws:[6.424349510669709, 9.11581892967224]
memory len:8286
memory used:2612.0
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.3545593190718525, the reward is 247.25 with loss [24.478549003601074, 16.32837224006653] in episode 686
Report: 
rewardSum:247.25
loss:[24.478549003601074, 16.32837224006653]
policies:[1, 1, 2]
qAverage:[0.0, 55.156517028808594]
ws:[3.1185569763183594, 3.7050087451934814]
memory len:8294
memory used:2612.0
now epsilon is 0.35367391730775, the reward is 241.25 with loss [69.21638035774231, 62.43883991241455] in episode 687
Report: 
rewardSum:241.25
loss:[69.21638035774231, 62.43883991241455]
policies:[1, 3, 6]
qAverage:[0.0, 81.39487075805664]
ws:[7.089584112167358, 9.161815166473389]
memory len:8314
memory used:2612.0
now epsilon is 0.35332037599605803, the reward is 247.25 with loss [33.88240957260132, 16.730665683746338] in episode 688
Report: 
rewardSum:247.25
loss:[33.88240957260132, 16.730665683746338]
policies:[0, 3, 1]
qAverage:[0.0, 67.47131601969402]
ws:[0.34608574708302814, 0.9415464003880819]
memory len:8322
memory used:2612.0
now epsilon is 0.3529671880931219, the reward is 247.25 with loss [20.438169479370117, 22.625428438186646] in episode 689
Report: 
rewardSum:247.25
loss:[20.438169479370117, 22.625428438186646]
policies:[0, 3, 1]
qAverage:[0.0, 63.390098571777344]
ws:[8.898493766784668, 13.533080101013184]
memory len:8330
memory used:2612.0
now epsilon is 0.3523499585904127, the reward is 244.25 with loss [27.55567443370819, 52.17197608947754] in episode 690
Report: 
rewardSum:244.25
loss:[27.55567443370819, 52.17197608947754]
policies:[1, 3, 3]
qAverage:[0.0, 78.55050659179688]
ws:[5.517055839300156, 8.458472862839699]
memory len:8344
memory used:2612.0
now epsilon is 0.3519977407410363, the reward is 247.25 with loss [20.91416096687317, 19.809597492218018] in episode 691
Report: 
rewardSum:247.25
loss:[20.91416096687317, 19.809597492218018]
policies:[0, 2, 2]
qAverage:[0.0, 73.6221211751302]
ws:[7.254133900006612, 11.36844245592753]
memory len:8352
memory used:2612.0
now epsilon is 0.3516458749774496, the reward is 247.25 with loss [16.992456912994385, 31.630210399627686] in episode 692
Report: 
rewardSum:247.25
loss:[16.992456912994385, 31.630210399627686]
policies:[1, 2, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:8360
memory used:2612.0
now epsilon is 0.3511187357231225, the reward is 57.6875 with loss [31.789018154144287, 36.0128607749939] in episode 693
Report: 
rewardSum:57.6875
loss:[31.789018154144287, 36.0128607749939]
policies:[1, 3, 2]
qAverage:[0.0, 67.6512451171875]
ws:[-1.4630140662193298, -0.4633695085843404]
memory len:8372
memory used:2612.0
now epsilon is 0.3505047385869778, the reward is 244.25 with loss [37.183757573366165, 38.36882519721985] in episode 694
Report: 
rewardSum:244.25
loss:[37.183757573366165, 38.36882519721985]
policies:[0, 4, 3]
qAverage:[0.0, 84.29900550842285]
ws:[3.672674983739853, 7.836601346731186]
memory len:8386
memory used:2612.0
now epsilon is 0.3498043421865006, the reward is 243.25 with loss [51.327943325042725, 26.415631651878357] in episode 695
Report: 
rewardSum:243.25
loss:[51.327943325042725, 26.415631651878357]
policies:[0, 5, 3]
qAverage:[0.0, 91.39056396484375]
ws:[7.128919859727223, 13.00623075167338]
memory len:8402
memory used:2612.0
now epsilon is 0.3492799635054983, the reward is 245.25 with loss [44.95923662185669, 31.524880707263947] in episode 696
Report: 
rewardSum:245.25
loss:[44.95923662185669, 31.524880707263947]
policies:[1, 3, 2]
qAverage:[0.0, 81.61396026611328]
ws:[5.80500864982605, 10.273922681808472]
memory len:8414
memory used:2612.0
now epsilon is 0.34875637090107636, the reward is 245.25 with loss [54.96030914783478, 20.76068878173828] in episode 697
Report: 
rewardSum:245.25
loss:[54.96030914783478, 20.76068878173828]
policies:[1, 3, 2]
qAverage:[0.0, 68.22437286376953]
ws:[8.700876553853353, 13.452879587809244]
memory len:8426
memory used:2612.0
now epsilon is 0.34788546019748473, the reward is 53.6875 with loss [61.2855863571167, 46.79069232940674] in episode 698
Report: 
rewardSum:53.6875
loss:[61.2855863571167, 46.79069232940674]
policies:[0, 5, 5]
qAverage:[0.0, 86.75830205281575]
ws:[5.761592129866282, 11.046881357828775]
memory len:8446
memory used:2612.0
now epsilon is 0.3472771170516034, the reward is 56.6875 with loss [20.977041363716125, 39.27663445472717] in episode 699
Report: 
rewardSum:56.6875
loss:[20.977041363716125, 39.27663445472717]
policies:[0, 4, 3]
qAverage:[0.0, 78.19488143920898]
ws:[5.1499181389808655, 8.55984652042389]
memory len:8460
memory used:2612.0
now epsilon is 0.3465831702486825, the reward is 243.25 with loss [49.921114683151245, 41.9274708032608] in episode 700
Report: 
rewardSum:243.25
loss:[49.921114683151245, 41.9274708032608]
policies:[2, 3, 3]
qAverage:[0.0, 75.68814086914062]
ws:[2.5522754788398743, 4.11722868680954]
memory len:8476
memory used:2618.0
now epsilon is 0.3460636203067447, the reward is 57.6875 with loss [27.21453356742859, 32.12773156166077] in episode 701
Report: 
rewardSum:57.6875
loss:[27.21453356742859, 32.12773156166077]
policies:[1, 1, 4]
qAverage:[0.0, 47.407413482666016]
ws:[-0.6065139174461365, -0.1613851636648178]
memory len:8488
memory used:2623.0
now epsilon is 0.34571768643866796, the reward is 247.25 with loss [18.71731197834015, 19.515345335006714] in episode 702
Report: 
rewardSum:247.25
loss:[18.71731197834015, 19.515345335006714]
policies:[1, 2, 1]
qAverage:[0.0, 69.57832845052083]
ws:[1.6840142806371052, 3.8332060972849527]
memory len:8496
memory used:2623.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28*		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.34519943391132457, the reward is 245.25 with loss [38.792635917663574, 49.03842830657959] in episode 703
Report: 
rewardSum:245.25
loss:[38.792635917663574, 49.03842830657959]
policies:[0, 4, 2]
qAverage:[0.0, 78.1795539855957]
ws:[0.9599176645278931, 2.4562870264053345]
memory len:8508
memory used:2623.0
now epsilon is 0.3448543639056274, the reward is 247.25 with loss [18.358130931854248, 15.266115188598633] in episode 704
Report: 
rewardSum:247.25
loss:[18.358130931854248, 15.266115188598633]
policies:[1, 2, 1]
qAverage:[0.0, 64.40348052978516]
ws:[5.643544673919678, 10.186418533325195]
memory len:8516
memory used:2623.0
now epsilon is 0.3444235114308461, the reward is 246.25 with loss [20.080130100250244, 22.55196762084961] in episode 705
Report: 
rewardSum:246.25
loss:[20.080130100250244, 22.55196762084961]
policies:[1, 3, 1]
qAverage:[0.0, 54.98429870605469]
ws:[1.0790311098098755, 2.868508815765381]
memory len:8526
memory used:2623.0
now epsilon is 0.3439071989531297, the reward is 245.25 with loss [29.12631344795227, 22.048767805099487] in episode 706
Report: 
rewardSum:245.25
loss:[29.12631344795227, 22.048767805099487]
policies:[0, 4, 2]
qAverage:[0.0, 82.79251098632812]
ws:[4.166568779945374, 7.196158361434937]
memory len:8538
memory used:2624.0
now epsilon is 0.34356342069788337, the reward is 247.25 with loss [15.351912498474121, 20.43124294281006] in episode 707
Report: 
rewardSum:247.25
loss:[15.351912498474121, 20.43124294281006]
policies:[1, 3, 0]
qAverage:[0.0, 85.83782196044922]
ws:[10.469861030578613, 14.966827392578125]
memory len:8546
memory used:2625.0
now epsilon is 0.3433916604602482, the reward is -1.0 with loss [9.896013736724854, 20.150501251220703] in episode 708
Report: 
rewardSum:-1.0
loss:[9.896013736724854, 20.150501251220703]
policies:[0, 1, 1]
qAverage:[0.0, 47.64559555053711]
ws:[2.0410890579223633, 2.5898945331573486]
memory len:8550
memory used:2625.0
now epsilon is 0.34304839755020006, the reward is 247.25 with loss [18.156532049179077, 14.989990949630737] in episode 709
Report: 
rewardSum:247.25
loss:[18.156532049179077, 14.989990949630737]
policies:[1, 2, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:8558
memory used:2625.0
now epsilon is 0.3427054777743598, the reward is 247.25 with loss [27.276030778884888, 33.73864698410034] in episode 710
Report: 
rewardSum:247.25
loss:[27.276030778884888, 33.73864698410034]
policies:[2, 2, 0]
qAverage:[23.225122451782227, 54.153594970703125]
ws:[5.070226848125458, 6.756595075130463]
memory len:8566
memory used:2625.0
now epsilon is 0.34236290078972187, the reward is 247.25 with loss [17.456356525421143, 27.241198301315308] in episode 711
Report: 
rewardSum:247.25
loss:[17.456356525421143, 27.241198301315308]
policies:[1, 1, 2]
qAverage:[0.0, 46.38410186767578]
ws:[0.02358701080083847, 0.36127927899360657]
memory len:8574
memory used:2625.0
now epsilon is 0.34202066625362365, the reward is -3.0 with loss [17.558574676513672, 22.524468421936035] in episode 712
Report: 
rewardSum:-3.0
loss:[17.558574676513672, 22.524468421936035]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:8582
memory used:2625.0
now epsilon is 0.34167877382374495, the reward is 247.25 with loss [25.12299919128418, 28.36612844467163] in episode 713
Report: 
rewardSum:247.25
loss:[25.12299919128418, 28.36612844467163]
policies:[1, 2, 1]
qAverage:[0.0, 72.59058634440105]
ws:[1.1852903366088867, 2.477841854095459]
memory len:8590
memory used:2625.0
now epsilon is 0.34125188885231833, the reward is 58.6875 with loss [20.171379804611206, 32.60997700691223] in episode 714
Report: 
rewardSum:58.6875
loss:[20.171379804611206, 32.60997700691223]
policies:[1, 1, 3]
qAverage:[0.0, 46.75870132446289]
ws:[0.6621081233024597, 1.4848400354385376]
memory len:8600
memory used:2624.0
now epsilon is 0.3407403308360644, the reward is 245.25 with loss [20.999739170074463, 34.1496000289917] in episode 715
Report: 
rewardSum:245.25
loss:[20.999739170074463, 34.1496000289917]
policies:[2, 3, 1]
qAverage:[0.0, 81.65939331054688]
ws:[5.712629497051239, 9.433668851852417]
memory len:8612
memory used:2624.0
now epsilon is 0.3403997182615575, the reward is 247.25 with loss [26.236382961273193, 12.334189176559448] in episode 716
Report: 
rewardSum:247.25
loss:[26.236382961273193, 12.334189176559448]
policies:[1, 2, 1]
qAverage:[0.0, 70.93508911132812]
ws:[6.422577937444051, 10.23513420422872]
memory len:8620
memory used:2630.0
now epsilon is 0.3400594461719167, the reward is 247.25 with loss [31.666614055633545, 12.258294820785522] in episode 717
Report: 
rewardSum:247.25
loss:[31.666614055633545, 12.258294820785522]
policies:[0, 3, 1]
qAverage:[0.0, 82.75283241271973]
ws:[6.5618332624435425, 10.175834655761719]
memory len:8628
memory used:2630.0
now epsilon is 0.33971951422678476, the reward is 247.25 with loss [15.101244926452637, 20.851430654525757] in episode 718
Report: 
rewardSum:247.25
loss:[15.101244926452637, 20.851430654525757]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:8636
memory used:2630.0
now epsilon is 0.33912545077301265, the reward is 244.25 with loss [33.12512981891632, 34.46950280666351] in episode 719
Report: 
rewardSum:244.25
loss:[33.12512981891632, 34.46950280666351]
policies:[1, 3, 3]
qAverage:[0.0, 62.7994384765625]
ws:[5.7675317625204725, 8.096253554026285]
memory len:8650
memory used:2632.0
now epsilon is 0.3387864524730897, the reward is 247.25 with loss [31.228858947753906, 13.54224157333374] in episode 720
Report: 
rewardSum:247.25
loss:[31.228858947753906, 13.54224157333374]
policies:[0, 3, 1]
qAverage:[0.0, 62.53480021158854]
ws:[4.911086718241374, 7.09609313805898]
memory len:8658
memory used:2632.0
now epsilon is 0.33844779304436345, the reward is 247.25 with loss [23.1812744140625, 17.047244429588318] in episode 721
Report: 
rewardSum:247.25
loss:[23.1812744140625, 17.047244429588318]
policies:[1, 3, 0]
qAverage:[0.0, 80.75743293762207]
ws:[4.685510676819831, 8.094350099563599]
memory len:8666
memory used:2632.0
now epsilon is 0.33827859030082835, the reward is -1.0 with loss [3.4456487894058228, 5.529994487762451] in episode 722
Report: 
rewardSum:-1.0
loss:[3.4456487894058228, 5.529994487762451]
policies:[0, 1, 1]
qAverage:[0.0, 45.3624267578125]
ws:[-0.39010924100875854, 0.029830124229192734]
memory len:8670
memory used:2632.0
now epsilon is 0.33794043854385786, the reward is 59.6875 with loss [27.6970157623291, 17.223451614379883] in episode 723
Report: 
rewardSum:59.6875
loss:[27.6970157623291, 17.223451614379883]
policies:[1, 1, 2]
qAverage:[0.0, 55.27764129638672]
ws:[1.8410091400146484, 3.73081636428833]
memory len:8678
memory used:2632.0
now epsilon is 0.33760262481185854, the reward is 247.25 with loss [13.536348462104797, 25.877421855926514] in episode 724
Report: 
rewardSum:247.25
loss:[13.536348462104797, 25.877421855926514]
policies:[1, 3, 0]
qAverage:[0.0, 77.96805826822917]
ws:[6.441274325052897, 11.298388163248697]
memory len:8686
memory used:2632.0
now epsilon is 0.33726514876693214, the reward is 247.25 with loss [23.626073837280273, 22.923113465309143] in episode 725
Report: 
rewardSum:247.25
loss:[23.626073837280273, 22.923113465309143]
policies:[1, 2, 1]
qAverage:[0.0, 45.5446891784668]
ws:[0.34210485219955444, 1.2105028629302979]
memory len:8694
memory used:2632.0
now epsilon is 0.3366753772327021, the reward is 244.25 with loss [46.76003801822662, 47.4195237159729] in episode 726
Report: 
rewardSum:244.25
loss:[46.76003801822662, 47.4195237159729]
policies:[1, 2, 4]
qAverage:[0.0, 70.48003387451172]
ws:[8.888193130493164, 14.599693298339844]
memory len:8708
memory used:2639.0
now epsilon is 0.33617067969482783, the reward is 57.6875 with loss [30.49923324584961, 27.864668309688568] in episode 727
Report: 
rewardSum:57.6875
loss:[30.49923324584961, 27.864668309688568]
policies:[0, 1, 5]
qAverage:[0.0, 44.35942459106445]
ws:[-0.01910085789859295, 0.6370757222175598]
memory len:8720
memory used:2639.0
now epsilon is 0.3358346350581286, the reward is 247.25 with loss [17.16408121585846, 15.245429992675781] in episode 728
Report: 
rewardSum:247.25
loss:[17.16408121585846, 15.245429992675781]
policies:[0, 4, 0]
qAverage:[0.0, 82.01660614013672]
ws:[5.737165236473084, 9.777951788902282]
memory len:8728
memory used:2639.0
############# STATE ###############
0-		8*		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.33533119784558324, the reward is 245.25 with loss [30.42463254928589, 30.79597520828247] in episode 729
Report: 
rewardSum:245.25
loss:[30.42463254928589, 30.79597520828247]
policies:[0, 4, 2]
qAverage:[0.0, 80.55256462097168]
ws:[5.898153066635132, 8.815647542476654]
memory len:8740
memory used:2638.0
now epsilon is 0.33516355320486035, the reward is -1.0 with loss [18.102664947509766, 12.653133869171143] in episode 730
Report: 
rewardSum:-1.0
loss:[18.102664947509766, 12.653133869171143]
policies:[0, 1, 1]
qAverage:[0.0, 43.98133087158203]
ws:[1.372199296951294, 1.874463677406311]
memory len:8744
memory used:2638.0
now epsilon is 0.3348285153170416, the reward is 247.25 with loss [13.995248317718506, 12.5928795337677] in episode 731
Report: 
rewardSum:247.25
loss:[13.995248317718506, 12.5928795337677]
policies:[0, 2, 2]
qAverage:[0.0, 54.83077621459961]
ws:[2.3514389991760254, 3.9514594078063965]
memory len:8752
memory used:2638.0
now epsilon is 0.3343265863411849, the reward is 994.0 with loss [33.60372614860535, 55.96176052093506] in episode 732
Report: 
rewardSum:994.0
loss:[33.60372614860535, 55.96176052093506]
policies:[2, 2, 2]
qAverage:[31.38941192626953, 35.233133951822914]
ws:[-6.965125719706218, -5.940892855326335]
memory len:8764
memory used:2638.0
now epsilon is 0.3339088870101429, the reward is 246.25 with loss [31.479807257652283, 38.37319564819336] in episode 733
Report: 
rewardSum:246.25
loss:[31.479807257652283, 38.37319564819336]
policies:[1, 3, 1]
qAverage:[0.0, 78.95074462890625]
ws:[4.720774173736572, 8.20669013261795]
memory len:8774
memory used:2638.0
now epsilon is 0.3332416532845959, the reward is 243.25 with loss [38.94676089286804, 37.32903051376343] in episode 734
Report: 
rewardSum:243.25
loss:[38.94676089286804, 37.32903051376343]
policies:[1, 3, 4]
qAverage:[0.0, 66.57329813639323]
ws:[8.880192438761393, 13.01260503133138]
memory len:8790
memory used:2638.0
now epsilon is 0.332908536576105, the reward is 247.25 with loss [15.177652716636658, 22.20091962814331] in episode 735
Report: 
rewardSum:247.25
loss:[15.177652716636658, 22.20091962814331]
policies:[0, 3, 1]
qAverage:[0.0, 69.16420237223308]
ws:[4.7211302518844604, 8.00013009707133]
memory len:8798
memory used:2638.0
now epsilon is 0.33240948576897955, the reward is 245.25 with loss [30.203737258911133, 30.960242748260498] in episode 736
Report: 
rewardSum:245.25
loss:[30.203737258911133, 30.960242748260498]
policies:[1, 3, 2]
qAverage:[0.0, 68.40479278564453]
ws:[4.264057695865631, 6.072326302528381]
memory len:8810
memory used:2637.0
now epsilon is 0.33174524822327434, the reward is 55.6875 with loss [50.05699062347412, 42.663020968437195] in episode 737
Report: 
rewardSum:55.6875
loss:[50.05699062347412, 42.663020968437195]
policies:[1, 3, 4]
qAverage:[0.0, 49.292728424072266]
ws:[8.288447380065918, 11.295854568481445]
memory len:8826
memory used:2637.0
now epsilon is 0.33124794125845874, the reward is 245.25 with loss [37.98646664619446, 28.68446946144104] in episode 738
Report: 
rewardSum:245.25
loss:[37.98646664619446, 28.68446946144104]
policies:[0, 2, 4]
qAverage:[0.0, 66.85607147216797]
ws:[5.594640890757243, 7.713963985443115]
memory len:8838
memory used:2637.0
now epsilon is 0.33075137978802044, the reward is 245.25 with loss [24.95432686805725, 35.484856843948364] in episode 739
Report: 
rewardSum:245.25
loss:[24.95432686805725, 35.484856843948364]
policies:[1, 4, 1]
qAverage:[0.0, 66.77377319335938]
ws:[1.3803740342458088, 3.1668418248494468]
memory len:8850
memory used:2637.0
now epsilon is 0.3302555626944166, the reward is 245.25 with loss [33.50202178955078, 50.59676480293274] in episode 740
Report: 
rewardSum:245.25
loss:[33.50202178955078, 50.59676480293274]
policies:[0, 4, 2]
qAverage:[0.0, 84.04998779296875]
ws:[5.333805227279663, 8.859175777435302]
memory len:8862
memory used:2637.0
now epsilon is 0.32992543095691856, the reward is 59.6875 with loss [18.667394161224365, 18.690667390823364] in episode 741
Report: 
rewardSum:59.6875
loss:[18.667394161224365, 18.690667390823364]
policies:[0, 3, 1]
qAverage:[0.0, 72.52287483215332]
ws:[1.8165888786315918, 2.8777905106544495]
memory len:8870
memory used:2637.0
now epsilon is 0.3295956292273793, the reward is 247.25 with loss [23.965435028076172, 23.130840063095093] in episode 742
Report: 
rewardSum:247.25
loss:[23.965435028076172, 23.130840063095093]
policies:[2, 2, 0]
qAverage:[0.0, 52.92218780517578]
ws:[2.2414727210998535, 3.465782403945923]
memory len:8878
memory used:2637.0
now epsilon is 0.32926615717591445, the reward is 247.25 with loss [29.628201484680176, 18.29541301727295] in episode 743
Report: 
rewardSum:247.25
loss:[29.628201484680176, 18.29541301727295]
policies:[1, 2, 1]
qAverage:[0.0, 64.34206899007161]
ws:[2.11455508073171, 3.0852273305257163]
memory len:8886
memory used:2637.0
now epsilon is 0.32910154467646136, the reward is -1.0 with loss [10.094089031219482, 8.24440622329712] in episode 744
Report: 
rewardSum:-1.0
loss:[10.094089031219482, 8.24440622329712]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:8890
memory used:2637.0
now epsilon is 0.32852604873912256, the reward is 993.0 with loss [26.311761617660522, 44.44904613494873] in episode 745
Report: 
rewardSum:993.0
loss:[26.311761617660522, 44.44904613494873]
policies:[0, 2, 5]
qAverage:[0.0, 44.775428771972656]
ws:[0.3875043988227844, 0.926494836807251]
memory len:8904
memory used:2637.0
now epsilon is 0.3280335675565395, the reward is 245.25 with loss [31.177679300308228, 39.071587800979614] in episode 746
Report: 
rewardSum:245.25
loss:[31.177679300308228, 39.071587800979614]
policies:[0, 4, 2]
qAverage:[0.0, 73.96177673339844]
ws:[6.4975234270095825, 9.21578311920166]
memory len:8916
memory used:2637.0
now epsilon is 0.32770565698107007, the reward is 247.25 with loss [29.322182178497314, 13.579588651657104] in episode 747
Report: 
rewardSum:247.25
loss:[29.322182178497314, 13.579588651657104]
policies:[1, 3, 0]
qAverage:[0.0, 77.2979679107666]
ws:[6.86972188949585, 11.155832648277283]
memory len:8924
memory used:2637.0
now epsilon is 0.32737807419323006, the reward is 247.25 with loss [26.184490203857422, 23.378682136535645] in episode 748
Report: 
rewardSum:247.25
loss:[26.184490203857422, 23.378682136535645]
policies:[3, 1, 0]
qAverage:[0.0, 42.18337631225586]
ws:[1.0179648399353027, 1.7397079467773438]
memory len:8932
memory used:2637.0
now epsilon is 0.32721440561726306, the reward is -1.0 with loss [11.593067646026611, 17.044244527816772] in episode 749
Report: 
rewardSum:-1.0
loss:[11.593067646026611, 17.044244527816772]
policies:[0, 1, 1]
qAverage:[0.0, 42.54066467285156]
ws:[0.8447557091712952, 1.4859871864318848]
memory len:8936
memory used:2637.0
now epsilon is 0.32680559206812415, the reward is 246.25 with loss [30.54439878463745, 21.302414178848267] in episode 750
Report: 
rewardSum:246.25
loss:[30.54439878463745, 21.302414178848267]
policies:[1, 2, 2]
qAverage:[0.0, 70.01583353678386]
ws:[6.954308191935222, 11.918455123901367]
memory len:8946
memory used:2638.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19*		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.326478909007729, the reward is 247.25 with loss [28.89955997467041, 31.288198232650757] in episode 751
Report: 
rewardSum:247.25
loss:[28.89955997467041, 31.288198232650757]
policies:[2, 1, 1]
qAverage:[0.0, 45.69643020629883]
ws:[6.5456624031066895, 9.280207633972168]
memory len:8954
memory used:2638.0
now epsilon is 0.32615255250790853, the reward is 247.25 with loss [23.923155426979065, 14.483385741710663] in episode 752
Report: 
rewardSum:247.25
loss:[23.923155426979065, 14.483385741710663]
policies:[1, 3, 0]
qAverage:[0.0, 77.29921531677246]
ws:[3.5678820759058, 7.050619423389435]
memory len:8962
memory used:2643.0
now epsilon is 0.3255008178845655, the reward is 992.0 with loss [42.37045240402222, 51.27792954444885] in episode 753
Report: 
rewardSum:992.0
loss:[42.37045240402222, 51.27792954444885]
policies:[1, 2, 5]
qAverage:[0.0, 46.041561126708984]
ws:[6.987455368041992, 9.98856258392334]
memory len:8978
memory used:2643.0
now epsilon is 0.3250941452493678, the reward is 246.25 with loss [39.40551042556763, 32.17832946777344] in episode 754
Report: 
rewardSum:246.25
loss:[39.40551042556763, 32.17832946777344]
policies:[3, 1, 1]
qAverage:[0.0, 53.85197067260742]
ws:[2.3695003986358643, 4.350132942199707]
memory len:8988
memory used:2643.0
now epsilon is 0.3247691729941059, the reward is 247.25 with loss [24.15688318014145, 26.589683771133423] in episode 755
Report: 
rewardSum:247.25
loss:[24.15688318014145, 26.589683771133423]
policies:[1, 3, 0]
qAverage:[0.0, 58.50830586751302]
ws:[1.1116475264231365, 2.838766574859619]
memory len:8996
memory used:2643.0
now epsilon is 0.3242823236042431, the reward is 245.25 with loss [27.23348045349121, 29.607422590255737] in episode 756
Report: 
rewardSum:245.25
loss:[27.23348045349121, 29.607422590255737]
policies:[0, 4, 2]
qAverage:[0.0, 76.95132255554199]
ws:[5.132953763008118, 9.347064554691315]
memory len:9008
memory used:2643.0
now epsilon is 0.32395816286624385, the reward is 247.25 with loss [26.27555799484253, 30.096606850624084] in episode 757
Report: 
rewardSum:247.25
loss:[26.27555799484253, 30.096606850624084]
policies:[0, 3, 1]
qAverage:[0.0, 79.05178833007812]
ws:[5.7845375537872314, 9.37446391582489]
memory len:9016
memory used:2643.0
now epsilon is 0.32363432616744264, the reward is 247.25 with loss [26.214417219161987, 16.15161681175232] in episode 758
Report: 
rewardSum:247.25
loss:[26.214417219161987, 16.15161681175232]
policies:[0, 4, 0]
qAverage:[0.0, 80.9107650756836]
ws:[5.872562098503113, 10.016215276718139]
memory len:9024
memory used:2643.0
now epsilon is 0.3233108131839217, the reward is 247.25 with loss [11.710036993026733, 35.89213752746582] in episode 759
Report: 
rewardSum:247.25
loss:[11.710036993026733, 35.89213752746582]
policies:[0, 4, 0]
qAverage:[0.0, 80.25938262939454]
ws:[4.420335197448731, 8.730171489715577]
memory len:9032
memory used:2643.0
now epsilon is 0.3229876235920871, the reward is 247.25 with loss [30.321881771087646, 17.76188886165619] in episode 760
Report: 
rewardSum:247.25
loss:[30.321881771087646, 17.76188886165619]
policies:[0, 3, 1]
qAverage:[0.0, 71.40255546569824]
ws:[3.865100644528866, 6.481017485260963]
memory len:9040
memory used:2643.0
now epsilon is 0.32242281899546726, the reward is 244.25 with loss [43.49470853805542, 27.49127459526062] in episode 761
Report: 
rewardSum:244.25
loss:[43.49470853805542, 27.49127459526062]
policies:[0, 5, 2]
qAverage:[0.0, 71.28264808654785]
ws:[4.71372064948082, 7.3342603743076324]
memory len:9054
memory used:2643.0
now epsilon is 0.32210051706487874, the reward is 247.25 with loss [15.909322023391724, 15.260349035263062] in episode 762
Report: 
rewardSum:247.25
loss:[15.909322023391724, 15.260349035263062]
policies:[2, 2, 0]
qAverage:[0.0, 48.315860748291016]
ws:[1.5634554624557495, 2.5934948921203613]
memory len:9062
memory used:2644.0
now epsilon is 0.32177853731537776, the reward is 247.25 with loss [31.77893304824829, 18.10896396636963] in episode 763
Report: 
rewardSum:247.25
loss:[31.77893304824829, 18.10896396636963]
policies:[1, 2, 1]
qAverage:[0.0, 52.01133346557617]
ws:[1.9315780401229858, 3.0502431392669678]
memory len:9070
memory used:2644.0
now epsilon is 0.32145687942490403, the reward is 247.25 with loss [14.950277090072632, 38.383686542510986] in episode 764
Report: 
rewardSum:247.25
loss:[14.950277090072632, 38.383686542510986]
policies:[0, 4, 0]
qAverage:[0.0, 72.32430076599121]
ws:[4.751308053731918, 7.220038592815399]
memory len:9078
memory used:2644.0
now epsilon is 0.32097499537115476, the reward is 245.25 with loss [34.84184813499451, 43.79101848602295] in episode 765
Report: 
rewardSum:245.25
loss:[34.84184813499451, 43.79101848602295]
policies:[0, 3, 3]
qAverage:[0.0, 75.74628257751465]
ws:[3.330158978700638, 6.690404057502747]
memory len:9090
memory used:2644.0
now epsilon is 0.32065414072134724, the reward is 247.25 with loss [12.24844354391098, 20.55776822566986] in episode 766
Report: 
rewardSum:247.25
loss:[12.24844354391098, 20.55776822566986]
policies:[0, 2, 2]
qAverage:[0.0, 66.91674296061198]
ws:[6.096707185109456, 8.330775101979574]
memory len:9098
memory used:2645.0
now epsilon is 0.32033360680588907, the reward is 247.25 with loss [19.333725214004517, 12.817428454756737] in episode 767
Report: 
rewardSum:247.25
loss:[19.333725214004517, 12.817428454756737]
policies:[1, 3, 0]
qAverage:[0.0, 81.59054756164551]
ws:[6.223997950553894, 10.203598856925964]
memory len:9106
memory used:2645.0
now epsilon is 0.3201734600233366, the reward is -1.0 with loss [7.04984712600708, 11.63521671295166] in episode 768
Report: 
rewardSum:-1.0
loss:[7.04984712600708, 11.63521671295166]
policies:[0, 1, 1]
qAverage:[0.0, 43.603118896484375]
ws:[0.9949402809143066, 1.4108812808990479]
memory len:9110
memory used:2645.0
now epsilon is 0.31977344325669915, the reward is 246.25 with loss [37.43685340881348, 21.46071434020996] in episode 769
Report: 
rewardSum:246.25
loss:[37.43685340881348, 21.46071434020996]
policies:[1, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:9120
memory used:2645.0
now epsilon is 0.3192940827795067, the reward is 245.25 with loss [26.275486826896667, 31.121076107025146] in episode 770
Report: 
rewardSum:245.25
loss:[26.275486826896667, 31.121076107025146]
policies:[1, 3, 2]
qAverage:[0.0, 81.03820419311523]
ws:[3.028278559446335, 6.723934769630432]
memory len:9132
memory used:2644.0
now epsilon is 0.3188154408937795, the reward is 245.25 with loss [26.93990671634674, 25.66763401031494] in episode 771
Report: 
rewardSum:245.25
loss:[26.93990671634674, 25.66763401031494]
policies:[2, 3, 1]
qAverage:[0.0, 77.78282165527344]
ws:[8.11881947517395, 13.43310546875]
memory len:9144
memory used:2644.0
now epsilon is 0.31849674498875136, the reward is 247.25 with loss [15.47446870803833, 17.437763214111328] in episode 772
Report: 
rewardSum:247.25
loss:[15.47446870803833, 17.437763214111328]
policies:[0, 3, 1]
qAverage:[0.0, 60.28652699788412]
ws:[2.7804399331410727, 3.966972827911377]
memory len:9152
memory used:2644.0
now epsilon is 0.3181783676601372, the reward is 247.25 with loss [18.75898027420044, 24.35935115814209] in episode 773
Report: 
rewardSum:247.25
loss:[18.75898027420044, 24.35935115814209]
policies:[1, 3, 0]
qAverage:[0.0, 67.3546371459961]
ws:[4.209321657816569, 5.869292577107747]
memory len:9160
memory used:2644.0
now epsilon is 0.31786030858948006, the reward is 247.25 with loss [23.9722101688385, 13.250699996948242] in episode 774
Report: 
rewardSum:247.25
loss:[23.9722101688385, 13.250699996948242]
policies:[1, 3, 0]
qAverage:[0.0, 73.27894401550293]
ws:[6.328461229801178, 9.141258001327515]
memory len:9168
memory used:2644.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28*		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.3173838160213225, the reward is 245.25 with loss [33.5027871131897, 49.833361864089966] in episode 775
Report: 
rewardSum:245.25
loss:[33.5027871131897, 49.833361864089966]
policies:[1, 4, 1]
qAverage:[0.0, 79.01117401123047]
ws:[2.4938998460769652, 3.9928213119506837]
memory len:9180
memory used:2644.0
now epsilon is 0.3167496035333338, the reward is 243.25 with loss [45.73799213767052, 48.06973314285278] in episode 776
Report: 
rewardSum:243.25
loss:[45.73799213767052, 48.06973314285278]
policies:[0, 3, 5]
qAverage:[0.0, 79.87635803222656]
ws:[2.9664888083934784, 7.063447654247284]
memory len:9196
memory used:2644.0
now epsilon is 0.31643297269110626, the reward is 247.25 with loss [20.44125247001648, 16.024148404598236] in episode 777
Report: 
rewardSum:247.25
loss:[20.44125247001648, 16.024148404598236]
policies:[0, 4, 0]
qAverage:[0.0, 81.66373901367187]
ws:[2.3376001834869387, 5.758871471881866]
memory len:9204
memory used:2644.0
now epsilon is 0.31611665836100417, the reward is 247.25 with loss [25.52490210533142, 18.509479999542236] in episode 778
Report: 
rewardSum:247.25
loss:[25.52490210533142, 18.509479999542236]
policies:[0, 4, 0]
qAverage:[0.0, 81.53588562011718]
ws:[5.041455569863319, 9.950611519813538]
memory len:9212
memory used:2644.0
now epsilon is 0.31564277963406195, the reward is 245.25 with loss [23.406338214874268, 36.59264063835144] in episode 779
Report: 
rewardSum:245.25
loss:[23.406338214874268, 36.59264063835144]
policies:[0, 5, 1]
qAverage:[0.0, 76.89742851257324]
ws:[4.024193719029427, 8.632921934127808]
memory len:9224
memory used:2650.0
now epsilon is 0.31532725520074384, the reward is 247.25 with loss [21.224465489387512, 16.819140911102295] in episode 780
Report: 
rewardSum:247.25
loss:[21.224465489387512, 16.819140911102295]
policies:[1, 3, 0]
qAverage:[0.0, 74.8671875]
ws:[5.478923161824544, 10.407822608947754]
memory len:9232
memory used:2650.0
now epsilon is 0.3150120461735571, the reward is 59.6875 with loss [25.749722719192505, 25.635302543640137] in episode 781
Report: 
rewardSum:59.6875
loss:[25.749722719192505, 25.635302543640137]
policies:[1, 2, 1]
qAverage:[0.0, 68.01338958740234]
ws:[2.993075688680013, 4.592282613118489]
memory len:9240
memory used:2650.0
now epsilon is 0.3145398233296673, the reward is 245.25 with loss [42.13579344749451, 32.63507866859436] in episode 782
Report: 
rewardSum:245.25
loss:[42.13579344749451, 32.63507866859436]
policies:[1, 3, 2]
qAverage:[0.0, 81.23425483703613]
ws:[4.520521879196167, 7.972173452377319]
memory len:9252
memory used:2650.0
now epsilon is 0.31422540143911387, the reward is 247.25 with loss [19.8160183429718, 16.819860458374023] in episode 783
Report: 
rewardSum:247.25
loss:[19.8160183429718, 16.819860458374023]
policies:[1, 3, 0]
qAverage:[0.0, 80.4505558013916]
ws:[7.042526602745056, 11.595773220062256]
memory len:9260
memory used:2650.0
now epsilon is 0.3139112938525625, the reward is 247.25 with loss [31.442573308944702, 17.92056429386139] in episode 784
Report: 
rewardSum:247.25
loss:[31.442573308944702, 17.92056429386139]
policies:[1, 3, 0]
qAverage:[0.0, 71.7238655090332]
ws:[5.206454813480377, 7.826332211494446]
memory len:9268
memory used:2651.0
now epsilon is 0.3135975002558269, the reward is 247.25 with loss [17.171278953552246, 32.67078423500061] in episode 785
Report: 
rewardSum:247.25
loss:[17.171278953552246, 32.67078423500061]
policies:[1, 3, 0]
qAverage:[0.0, 78.13116836547852]
ws:[4.773331522941589, 8.814721465110779]
memory len:9276
memory used:2651.0
now epsilon is 0.31328402033503505, the reward is 247.25 with loss [29.198044776916504, 14.63798177242279] in episode 786
Report: 
rewardSum:247.25
loss:[29.198044776916504, 14.63798177242279]
policies:[0, 4, 0]
qAverage:[0.0, 74.47682189941406]
ws:[7.351259648799896, 11.828347206115723]
memory len:9284
memory used:2651.0
now epsilon is 0.3130491160556425, the reward is -2.0 with loss [16.842830419540405, 12.008998394012451] in episode 787
Report: 
rewardSum:-2.0
loss:[16.842830419540405, 12.008998394012451]
policies:[0, 1, 2]
qAverage:[0.0, 42.609336853027344]
ws:[2.0934438705444336, 2.947265863418579]
memory len:9290
memory used:2651.0
now epsilon is 0.3127361843134411, the reward is 247.25 with loss [24.923364639282227, 28.808091163635254] in episode 788
Report: 
rewardSum:247.25
loss:[24.923364639282227, 28.808091163635254]
policies:[1, 3, 0]
qAverage:[0.0, 71.70356559753418]
ws:[3.1564176082611084, 4.1922489404678345]
memory len:9298
memory used:2651.0
now epsilon is 0.312423565385652, the reward is 247.25 with loss [28.891979932785034, 24.134907245635986] in episode 789
Report: 
rewardSum:247.25
loss:[28.891979932785034, 24.134907245635986]
policies:[0, 4, 0]
qAverage:[0.0, 78.33423042297363]
ws:[3.892952620983124, 7.0908781588077545]
memory len:9306
memory used:2651.0
now epsilon is 0.3119552228370521, the reward is 245.25 with loss [22.70466113090515, 36.850430727005005] in episode 790
Report: 
rewardSum:245.25
loss:[22.70466113090515, 36.850430727005005]
policies:[1, 3, 2]
qAverage:[0.0, 72.50523376464844]
ws:[1.0468748651910573, 2.317221313714981]
memory len:9318
memory used:2651.0
now epsilon is 0.3116433845779277, the reward is 247.25 with loss [20.40248966217041, 17.02123522758484] in episode 791
Report: 
rewardSum:247.25
loss:[20.40248966217041, 17.02123522758484]
policies:[2, 2, 0]
qAverage:[0.0, 64.57589975992839]
ws:[1.1838125238815944, 2.7532212336858115]
memory len:9326
memory used:2651.0
now epsilon is 0.3113318580401425, the reward is 59.6875 with loss [18.135732650756836, 33.259578227996826] in episode 792
Report: 
rewardSum:59.6875
loss:[18.135732650756836, 33.259578227996826]
policies:[0, 2, 2]
qAverage:[0.0, 69.10093688964844]
ws:[5.611545403798421, 8.701149940490723]
memory len:9334
memory used:2651.0
now epsilon is 0.31086515202942633, the reward is 245.25 with loss [34.58465003967285, 28.425456881523132] in episode 793
Report: 
rewardSum:245.25
loss:[34.58465003967285, 28.425456881523132]
policies:[0, 4, 2]
qAverage:[0.0, 76.76320457458496]
ws:[3.715972453355789, 6.910886466503143]
memory len:9346
memory used:2657.0
now epsilon is 0.3103991456403352, the reward is 245.25 with loss [33.397658586502075, 30.00660514831543] in episode 794
Report: 
rewardSum:245.25
loss:[33.397658586502075, 30.00660514831543]
policies:[1, 4, 1]
qAverage:[0.0, 83.92406768798828]
ws:[3.73468599319458, 6.651033353805542]
memory len:9358
memory used:2657.0
now epsilon is 0.3100888628749758, the reward is 59.6875 with loss [24.774620056152344, 17.538963079452515] in episode 795
Report: 
rewardSum:59.6875
loss:[24.774620056152344, 17.538963079452515]
policies:[0, 3, 1]
qAverage:[0.0, 71.54464912414551]
ws:[2.3617319464683533, 3.6628281474113464]
memory len:9366
memory used:2657.0
now epsilon is 0.3096240201920877, the reward is 245.25 with loss [27.399735927581787, 39.89263463020325] in episode 796
Report: 
rewardSum:245.25
loss:[27.399735927581787, 39.89263463020325]
policies:[0, 5, 1]
qAverage:[0.0, 78.50322570800782]
ws:[3.4948147773742675, 5.222835063934326]
memory len:9378
memory used:2656.0
now epsilon is 0.30931451226155293, the reward is 247.25 with loss [16.201319217681885, 10.814591765403748] in episode 797
Report: 
rewardSum:247.25
loss:[16.201319217681885, 10.814591765403748]
policies:[0, 3, 1]
qAverage:[0.0, 82.82174491882324]
ws:[8.70441722869873, 13.340140104293823]
memory len:9386
memory used:2656.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34*		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.30854209534825383, the reward is 241.25 with loss [62.546403884887695, 59.138041973114014] in episode 798
Report: 
rewardSum:241.25
loss:[62.546403884887695, 59.138041973114014]
policies:[0, 6, 4]
qAverage:[0.0, 82.30796305338542]
ws:[3.405053496360779, 6.41015350818634]
memory len:9406
memory used:2656.0
now epsilon is 0.3083878435844607, the reward is -1.0 with loss [7.297154903411865, 7.598034858703613] in episode 799
Report: 
rewardSum:-1.0
loss:[7.297154903411865, 7.598034858703613]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:9410
memory used:2656.0
now epsilon is 0.30777160730626313, the reward is 243.25 with loss [49.53304719924927, 40.10716366767883] in episode 800
Report: 
rewardSum:243.25
loss:[49.53304719924927, 40.10716366767883]
policies:[0, 4, 4]
qAverage:[0.0, 85.36648254394531]
ws:[5.613506412506103, 9.434996604919434]
memory len:9426
memory used:2657.0
now epsilon is 0.30746395109407504, the reward is 247.25 with loss [20.692482709884644, 12.621216416358948] in episode 801
Report: 
rewardSum:247.25
loss:[20.692482709884644, 12.621216416358948]
policies:[0, 2, 2]
qAverage:[0.0, 81.37970733642578]
ws:[4.6025417645772295, 10.040430704752604]
memory len:9434
memory used:2656.0
now epsilon is 0.30715660242274734, the reward is 247.25 with loss [32.46842575073242, 18.792666912078857] in episode 802
Report: 
rewardSum:247.25
loss:[32.46842575073242, 18.792666912078857]
policies:[1, 2, 1]
qAverage:[0.0, 44.73703384399414]
ws:[0.7195505499839783, 2.2535979747772217]
memory len:9442
memory used:2656.0
now epsilon is 0.3068495609848545, the reward is 247.25 with loss [24.93861961364746, 22.869118690490723] in episode 803
Report: 
rewardSum:247.25
loss:[24.93861961364746, 22.869118690490723]
policies:[0, 4, 0]
qAverage:[0.0, 87.18253326416016]
ws:[5.1093145370483395, 9.884266185760499]
memory len:9450
memory used:2656.0
now epsilon is 0.30638957421896823, the reward is 245.25 with loss [27.942548990249634, 38.19619131088257] in episode 804
Report: 
rewardSum:245.25
loss:[27.942548990249634, 38.19619131088257]
policies:[1, 2, 3]
qAverage:[0.0, 67.6548843383789]
ws:[3.1187925338745117, 5.0284318923950195]
memory len:9462
memory used:2656.0
now epsilon is 0.30577733098427806, the reward is 243.25 with loss [56.81561267375946, 26.656973719596863] in episode 805
Report: 
rewardSum:243.25
loss:[56.81561267375946, 26.656973719596863]
policies:[1, 3, 4]
qAverage:[0.0, 72.26141611735027]
ws:[6.136719385782878, 10.36762022972107]
memory len:9478
memory used:2658.0
now epsilon is 0.305471668300683, the reward is 247.25 with loss [23.98449993133545, 20.975220918655396] in episode 806
Report: 
rewardSum:247.25
loss:[23.98449993133545, 20.975220918655396]
policies:[1, 2, 1]
qAverage:[0.0, 71.3305180867513]
ws:[5.941759705543518, 10.268969774246216]
memory len:9486
memory used:2658.0
now epsilon is 0.3051663111651672, the reward is 59.6875 with loss [29.48338747024536, 23.4151132106781] in episode 807
Report: 
rewardSum:59.6875
loss:[29.48338747024536, 23.4151132106781]
policies:[0, 3, 1]
qAverage:[0.0, 62.98146057128906]
ws:[1.6428693135579426, 3.007270574569702]
memory len:9494
memory used:2658.0
now epsilon is 0.304785043957479, the reward is 246.25 with loss [31.502410173416138, 26.190808534622192] in episode 808
Report: 
rewardSum:246.25
loss:[31.502410173416138, 26.190808534622192]
policies:[0, 4, 1]
qAverage:[0.0, 84.90621185302734]
ws:[9.53365957736969, 14.358308792114258]
memory len:9504
memory used:2658.0
now epsilon is 0.30448037318886517, the reward is 247.25 with loss [23.470694303512573, 21.242258667945862] in episode 809
Report: 
rewardSum:247.25
loss:[23.470694303512573, 21.242258667945862]
policies:[0, 3, 1]
qAverage:[0.0, 75.77836608886719]
ws:[1.8523303642868996, 3.355804890394211]
memory len:9512
memory used:2658.0
now epsilon is 0.3041760069767874, the reward is 59.6875 with loss [23.703311920166016, 19.98132014274597] in episode 810
Report: 
rewardSum:59.6875
loss:[23.703311920166016, 19.98132014274597]
policies:[1, 1, 2]
qAverage:[0.0, 51.001651763916016]
ws:[1.1508742570877075, 1.8890300989151]
memory len:9520
memory used:2658.0
now epsilon is 0.3038719450168034, the reward is 247.25 with loss [17.666051387786865, 13.845945954322815] in episode 811
Report: 
rewardSum:247.25
loss:[17.666051387786865, 13.845945954322815]
policies:[1, 3, 0]
qAverage:[0.0, 84.28290748596191]
ws:[5.448491036891937, 9.009129643440247]
memory len:9528
memory used:2658.0
now epsilon is 0.3035681870047752, the reward is 247.25 with loss [14.930573254823685, 15.820280075073242] in episode 812
Report: 
rewardSum:247.25
loss:[14.930573254823685, 15.820280075073242]
policies:[1, 3, 0]
qAverage:[0.0, 75.69379806518555]
ws:[4.945254061371088, 7.456113636493683]
memory len:9536
memory used:2658.0
now epsilon is 0.3028858412141514, the reward is 54.6875 with loss [43.06887610256672, 54.4655704498291] in episode 813
Report: 
rewardSum:54.6875
loss:[43.06887610256672, 54.4655704498291]
policies:[0, 3, 6]
qAverage:[0.0, 56.98855209350586]
ws:[2.362642288208008, 3.6691577434539795]
memory len:9554
memory used:2658.0
now epsilon is 0.30137500338801887, the reward is 542.6875 with loss [110.86502104997635, 92.01394951343536] in episode 814
Report: 
rewardSum:542.6875
loss:[110.86502104997635, 92.01394951343536]
policies:[1, 6, 13]
qAverage:[0.0, 83.4015375773112]
ws:[2.4157356272141137, 5.756277362505595]
memory len:9594
memory used:2658.0
now epsilon is 0.30107374138142234, the reward is 247.25 with loss [16.832953810691833, 27.78747034072876] in episode 815
Report: 
rewardSum:247.25
loss:[16.832953810691833, 27.78747034072876]
policies:[0, 2, 2]
qAverage:[0.0, 76.23596445719402]
ws:[8.487959543863932, 14.920149803161621]
memory len:9602
memory used:2658.0
now epsilon is 0.30062241293191494, the reward is 245.25 with loss [33.086814761161804, 33.84747302532196] in episode 816
Report: 
rewardSum:245.25
loss:[33.086814761161804, 33.84747302532196]
policies:[2, 3, 1]
qAverage:[0.0, 76.62778218587239]
ws:[10.907508532206217, 18.559842427571613]
memory len:9614
memory used:2658.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17*		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.3003219032336002, the reward is 247.25 with loss [19.02181077003479, 27.841928601264954] in episode 817
Report: 
rewardSum:247.25
loss:[19.02181077003479, 27.841928601264954]
policies:[0, 3, 1]
qAverage:[0.0, 85.50900459289551]
ws:[6.372180104255676, 12.420182943344116]
memory len:9622
memory used:2658.0
now epsilon is 0.2998717018367012, the reward is 245.25 with loss [34.41157829761505, 41.70723581314087] in episode 818
Report: 
rewardSum:245.25
loss:[34.41157829761505, 41.70723581314087]
policies:[0, 3, 3]
qAverage:[0.0, 83.25937461853027]
ws:[6.58029305934906, 12.564690470695496]
memory len:9634
memory used:2658.0
now epsilon is 0.2995719425680119, the reward is 247.25 with loss [39.546427965164185, 27.785115003585815] in episode 819
Report: 
rewardSum:247.25
loss:[39.546427965164185, 27.785115003585815]
policies:[0, 3, 1]
qAverage:[0.0, 68.40738677978516]
ws:[1.0819879571596782, 3.1039387385050454]
memory len:9642
memory used:2659.0
now epsilon is 0.2992724829462003, the reward is 247.25 with loss [37.295337200164795, 15.500938415527344] in episode 820
Report: 
rewardSum:247.25
loss:[37.295337200164795, 15.500938415527344]
policies:[1, 3, 0]
qAverage:[0.0, 75.15990956624348]
ws:[7.772777557373047, 14.529741923014322]
memory len:9650
memory used:2657.0
now epsilon is 0.2988985793410639, the reward is 246.25 with loss [28.897144079208374, 49.805867195129395] in episode 821
Report: 
rewardSum:246.25
loss:[28.897144079208374, 49.805867195129395]
policies:[1, 3, 1]
qAverage:[0.0, 68.5510737101237]
ws:[0.9701085488001505, 3.2701405684153237]
memory len:9660
memory used:2662.0
now epsilon is 0.29859979283001015, the reward is 247.25 with loss [14.491783142089844, 29.6189866065979] in episode 822
Report: 
rewardSum:247.25
loss:[14.491783142089844, 29.6189866065979]
policies:[0, 4, 0]
qAverage:[0.0, 86.76519927978515]
ws:[4.402844280004501, 8.637057256698608]
memory len:9668
memory used:2662.0
now epsilon is 0.29785413260022275, the reward is 241.25 with loss [45.356518507003784, 40.193344831466675] in episode 823
Report: 
rewardSum:241.25
loss:[45.356518507003784, 40.193344831466675]
policies:[2, 4, 4]
qAverage:[0.0, 71.75491905212402]
ws:[2.5049892961978912, 3.569836139678955]
memory len:9688
memory used:2663.0
now epsilon is 0.29755639014430757, the reward is 247.25 with loss [40.0947527885437, 26.011815547943115] in episode 824
Report: 
rewardSum:247.25
loss:[40.0947527885437, 26.011815547943115]
policies:[0, 4, 0]
qAverage:[0.0, 85.113037109375]
ws:[7.738803672790527, 11.345339488983154]
memory len:9696
memory used:2663.0
now epsilon is 0.2969617978274213, the reward is 243.25 with loss [38.45387065410614, 48.03871488571167] in episode 825
Report: 
rewardSum:243.25
loss:[38.45387065410614, 48.03871488571167]
policies:[2, 3, 3]
qAverage:[0.0, 60.961856842041016]
ws:[7.646716594696045, 14.372438430786133]
memory len:9712
memory used:2663.0
now epsilon is 0.29651663343958257, the reward is 245.25 with loss [34.91827178001404, 39.121237099170685] in episode 826
Report: 
rewardSum:245.25
loss:[34.91827178001404, 39.121237099170685]
policies:[2, 2, 2]
qAverage:[0.0, 57.16468811035156]
ws:[1.8330227136611938, 3.9335665702819824]
memory len:9724
memory used:2663.0
now epsilon is 0.2959241188174409, the reward is 243.25 with loss [47.588876724243164, 51.31762444972992] in episode 827
Report: 
rewardSum:243.25
loss:[47.588876724243164, 51.31762444972992]
policies:[1, 3, 4]
qAverage:[0.0, 85.73445320129395]
ws:[8.84206759929657, 16.140784978866577]
memory len:9740
memory used:2663.0
now epsilon is 0.295628305651674, the reward is 247.25 with loss [20.374162912368774, 15.032267570495605] in episode 828
Report: 
rewardSum:247.25
loss:[20.374162912368774, 15.032267570495605]
policies:[0, 4, 0]
qAverage:[0.0, 80.88174629211426]
ws:[4.372888967394829, 10.155178427696228]
memory len:9748
memory used:2663.0
now epsilon is 0.29533278818816133, the reward is 247.25 with loss [18.608776450157166, 32.62616324424744] in episode 829
Report: 
rewardSum:247.25
loss:[18.608776450157166, 32.62616324424744]
policies:[1, 3, 0]
qAverage:[0.0, 83.24591064453125]
ws:[4.775775622576475, 11.111414134502411]
memory len:9756
memory used:2663.0
now epsilon is 0.2949638067397788, the reward is 246.25 with loss [31.57291579246521, 28.076014280319214] in episode 830
Report: 
rewardSum:246.25
loss:[31.57291579246521, 28.076014280319214]
policies:[0, 4, 1]
qAverage:[0.0, 83.05720138549805]
ws:[4.680131584405899, 10.575733542442322]
memory len:9766
memory used:2663.0
now epsilon is 0.2946689535260325, the reward is 247.25 with loss [29.21921157836914, 26.18824017047882] in episode 831
Report: 
rewardSum:247.25
loss:[29.21921157836914, 26.18824017047882]
policies:[0, 3, 1]
qAverage:[0.0, 76.86414337158203]
ws:[8.86779816945394, 16.322266896565754]
memory len:9774
memory used:2664.0
now epsilon is 0.29422722625582065, the reward is 245.25 with loss [35.05174541473389, 30.853948712348938] in episode 832
Report: 
rewardSum:245.25
loss:[35.05174541473389, 30.853948712348938]
policies:[0, 5, 1]
qAverage:[0.0, 88.86441548665364]
ws:[2.0554674913485846, 6.377912878990173]
memory len:9786
memory used:2663.0
now epsilon is 0.29393310934638667, the reward is 247.25 with loss [27.788716316223145, 14.086106777191162] in episode 833
Report: 
rewardSum:247.25
loss:[27.788716316223145, 14.086106777191162]
policies:[1, 3, 0]
qAverage:[0.0, 82.93425178527832]
ws:[3.091018795967102, 8.29636337980628]
memory len:9794
memory used:2663.0
now epsilon is 0.2933457572535242, the reward is 243.25 with loss [55.54167461395264, 63.51095962524414] in episode 834
Report: 
rewardSum:243.25
loss:[55.54167461395264, 63.51095962524414]
policies:[2, 4, 2]
qAverage:[0.0, 87.7095458984375]
ws:[4.294583824276924, 9.687503957748413]
memory len:9810
memory used:2663.0
now epsilon is 0.29319910270900723, the reward is -1.0 with loss [12.674010753631592, 15.349822998046875] in episode 835
Report: 
rewardSum:-1.0
loss:[12.674010753631592, 15.349822998046875]
policies:[1, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:9814
memory used:2663.0
now epsilon is 0.29290601353763795, the reward is 247.25 with loss [13.177471995353699, 25.780853271484375] in episode 836
Report: 
rewardSum:247.25
loss:[13.177471995353699, 25.780853271484375]
policies:[0, 4, 0]
qAverage:[0.0, 76.38916969299316]
ws:[3.548851251602173, 5.376755714416504]
memory len:9822
memory used:2663.0
now epsilon is 0.2926132173455499, the reward is 247.25 with loss [19.08165693283081, 14.983609408140182] in episode 837
Report: 
rewardSum:247.25
loss:[19.08165693283081, 14.983609408140182]
policies:[0, 4, 0]
qAverage:[0.0, 87.34115753173828]
ws:[7.342210125923157, 12.672300243377686]
memory len:9830
memory used:2663.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18*		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.2921745717529984, the reward is 57.6875 with loss [28.25958752632141, 37.180646896362305] in episode 838
Report: 
rewardSum:57.6875
loss:[28.25958752632141, 37.180646896362305]
policies:[1, 3, 2]
qAverage:[0.0, 64.39955139160156]
ws:[1.3979997634887695, 2.2043807109196982]
memory len:9842
memory used:2663.0
now epsilon is 0.2917365837177425, the reward is 245.25 with loss [34.97633409500122, 53.0150990486145] in episode 839
Report: 
rewardSum:245.25
loss:[34.97633409500122, 53.0150990486145]
policies:[1, 4, 1]
qAverage:[0.0, 68.36220804850261]
ws:[7.467967669169108, 10.166917641957602]
memory len:9854
memory used:2663.0
now epsilon is 0.29144495651701136, the reward is 247.25 with loss [23.023810148239136, 25.2312273979187] in episode 840
Report: 
rewardSum:247.25
loss:[23.023810148239136, 25.2312273979187]
policies:[0, 4, 0]
qAverage:[0.0, 86.08960723876953]
ws:[9.32384967803955, 14.923752069473267]
memory len:9862
memory used:2664.0
now epsilon is 0.29115362083413887, the reward is 247.25 with loss [26.005648136138916, 15.412720084190369] in episode 841
Report: 
rewardSum:247.25
loss:[26.005648136138916, 15.412720084190369]
policies:[1, 3, 0]
qAverage:[17.920042419433592, 67.31742095947266]
ws:[4.0836424946784975, 7.07973604798317]
memory len:9870
memory used:2663.0
now epsilon is 0.2906444839776217, the reward is 244.25 with loss [35.59252452850342, 44.52070665359497] in episode 842
Report: 
rewardSum:244.25
loss:[35.59252452850342, 44.52070665359497]
policies:[1, 4, 2]
qAverage:[0.0, 86.72083740234375]
ws:[7.646780300140381, 11.504415583610534]
memory len:9884
memory used:2664.0
now epsilon is 0.2903539484671615, the reward is 59.6875 with loss [12.374917268753052, 42.661070585250854] in episode 843
Report: 
rewardSum:59.6875
loss:[12.374917268753052, 42.661070585250854]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:9892
memory used:2664.0
now epsilon is 0.2902087896400497, the reward is -1.0 with loss [5.871708393096924, 11.360353469848633] in episode 844
Report: 
rewardSum:-1.0
loss:[5.871708393096924, 11.360353469848633]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:9896
memory used:2670.0
now epsilon is 0.28991868966056894, the reward is 247.25 with loss [25.561455249786377, 22.642138957977295] in episode 845
Report: 
rewardSum:247.25
loss:[25.561455249786377, 22.642138957977295]
policies:[1, 3, 0]
qAverage:[47.711421966552734, 0.0]
ws:[2.4843671321868896, 2.424136161804199]
memory len:9904
memory used:2670.0
now epsilon is 0.2894840833342671, the reward is 245.25 with loss [29.185526609420776, 28.930255889892578] in episode 846
Report: 
rewardSum:245.25
loss:[29.185526609420776, 28.930255889892578]
policies:[1, 3, 2]
qAverage:[0.0, 70.83705139160156]
ws:[8.746776262919107, 10.608237266540527]
memory len:9916
memory used:2670.0
now epsilon is 0.2891947077893725, the reward is 247.25 with loss [20.32722306251526, 33.53774404525757] in episode 847
Report: 
rewardSum:247.25
loss:[20.32722306251526, 33.53774404525757]
policies:[1, 2, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:9924
memory used:2670.0
now epsilon is 0.288905621511525, the reward is 59.6875 with loss [28.30053299665451, 19.750028133392334] in episode 848
Report: 
rewardSum:59.6875
loss:[28.30053299665451, 19.750028133392334]
policies:[1, 2, 1]
qAverage:[29.655466715494793, 34.023101806640625]
ws:[1.3055068651835124, 1.114412920549512]
memory len:9932
memory used:2670.0
now epsilon is 0.28861682421156615, the reward is 247.25 with loss [39.034809589385986, 24.33200764656067] in episode 849
Report: 
rewardSum:247.25
loss:[39.034809589385986, 24.33200764656067]
policies:[0, 4, 0]
qAverage:[0.0, 87.28035430908203]
ws:[5.539804691076279, 9.616215193271637]
memory len:9940
memory used:2670.0
now epsilon is 0.28832831560062633, the reward is 247.25 with loss [27.958075046539307, 19.749808430671692] in episode 850
Report: 
rewardSum:247.25
loss:[27.958075046539307, 19.749808430671692]
policies:[0, 2, 2]
qAverage:[0.0, 69.74287414550781]
ws:[4.670279184977214, 7.834163506825765]
memory len:9948
memory used:2670.0
now epsilon is 0.28789609334493554, the reward is 245.25 with loss [32.20352864265442, 36.876789808273315] in episode 851
Report: 
rewardSum:245.25
loss:[32.20352864265442, 36.876789808273315]
policies:[0, 2, 4]
qAverage:[0.0, 72.65225728352864]
ws:[5.966047286987305, 8.939061164855957]
memory len:9960
memory used:2670.0
now epsilon is 0.2873926528878006, the reward is 244.25 with loss [35.46430015563965, 37.31126594543457] in episode 852
Report: 
rewardSum:244.25
loss:[35.46430015563965, 37.31126594543457]
policies:[0, 5, 2]
qAverage:[0.0, 90.5481808980306]
ws:[6.767078598340352, 11.140035629272461]
memory len:9974
memory used:2670.0
now epsilon is 0.2871053679891968, the reward is 247.25 with loss [13.170040130615234, 25.41191554069519] in episode 853
Report: 
rewardSum:247.25
loss:[13.170040130615234, 25.41191554069519]
policies:[0, 4, 0]
qAverage:[0.0, 84.82992095947266]
ws:[5.086896228790283, 9.01469259262085]
memory len:9982
memory used:2670.0
now epsilon is 0.2867466656752107, the reward is 58.6875 with loss [22.871921062469482, 19.06777060031891] in episode 854
Report: 
rewardSum:58.6875
loss:[22.871921062469482, 19.06777060031891]
policies:[1, 2, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:9992
memory used:2670.0
now epsilon is 0.2863168144121055, the reward is 245.25 with loss [33.12078046798706, 25.177140712738037] in episode 855
Report: 
rewardSum:245.25
loss:[33.12078046798706, 25.177140712738037]
policies:[0, 5, 1]
qAverage:[0.0, 87.4472671508789]
ws:[5.6224706172943115, 9.037935161590577]
memory len:10000
memory used:2670.0
now epsilon is 0.28603060494860516, the reward is 247.25 with loss [20.976495265960693, 11.621418952941895] in episode 856
Report: 
rewardSum:247.25
loss:[20.976495265960693, 11.621418952941895]
policies:[2, 1, 1]
qAverage:[0.0, 51.94987106323242]
ws:[7.272693634033203, 9.984942436218262]
memory len:10000
memory used:2670.0
now epsilon is 0.2856732454168609, the reward is 246.25 with loss [25.297856330871582, 24.103499174118042] in episode 857
Report: 
rewardSum:246.25
loss:[25.297856330871582, 24.103499174118042]
policies:[1, 3, 1]
qAverage:[0.0, 80.79299926757812]
ws:[7.031298875808716, 11.268567085266113]
memory len:10000
memory used:2670.0
now epsilon is 0.2853876792810576, the reward is 247.25 with loss [18.50175404548645, 21.8645281791687] in episode 858
Report: 
rewardSum:247.25
loss:[18.50175404548645, 21.8645281791687]
policies:[0, 4, 0]
qAverage:[0.0, 79.73998641967773]
ws:[4.567684769630432, 7.653164923191071]
memory len:10000
memory used:2670.0
now epsilon is 0.2850311230046696, the reward is 246.25 with loss [40.642154693603516, 31.477134466171265] in episode 859
Report: 
rewardSum:246.25
loss:[40.642154693603516, 31.477134466171265]
policies:[0, 3, 2]
qAverage:[0.0, 69.99986521402995]
ws:[1.799352725346883, 2.8532000382741294]
memory len:10000
memory used:2670.0
now epsilon is 0.28460384344778494, the reward is 245.25 with loss [34.07699203491211, 30.212425231933594] in episode 860
Report: 
rewardSum:245.25
loss:[34.07699203491211, 30.212425231933594]
policies:[0, 4, 2]
qAverage:[0.0, 80.02377128601074]
ws:[4.522538006305695, 7.2301167249679565]
memory len:10000
memory used:2670.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19*		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.28417720440979444, the reward is 245.25 with loss [27.58687114715576, 25.656340658664703] in episode 861
Report: 
rewardSum:245.25
loss:[27.58687114715576, 25.656340658664703]
policies:[0, 5, 1]
qAverage:[0.0, 88.7646853129069]
ws:[4.003814945618312, 7.22323211034139]
memory len:10000
memory used:2670.0
now epsilon is 0.28403513356866483, the reward is -1.0 with loss [14.926711082458496, 10.601456642150879] in episode 862
Report: 
rewardSum:-1.0
loss:[14.926711082458496, 10.601456642150879]
policies:[0, 1, 1]
qAverage:[0.0, 47.817138671875]
ws:[1.2616688013076782, 1.812239408493042]
memory len:10000
memory used:2670.0
now epsilon is 0.2837512049305202, the reward is 247.25 with loss [22.640388011932373, 27.772008419036865] in episode 863
Report: 
rewardSum:247.25
loss:[22.640388011932373, 27.772008419036865]
policies:[1, 3, 0]
qAverage:[0.0, 84.80721473693848]
ws:[8.525509238243103, 13.355538845062256]
memory len:10000
memory used:2670.0
now epsilon is 0.2834675601145582, the reward is 247.25 with loss [21.87675142288208, 28.972223043441772] in episode 864
Report: 
rewardSum:247.25
loss:[21.87675142288208, 28.972223043441772]
policies:[0, 4, 0]
qAverage:[0.0, 87.66439666748047]
ws:[4.773591423034668, 7.959830474853516]
memory len:10000
memory used:2670.0
now epsilon is 0.283042624436657, the reward is 245.25 with loss [30.496379852294922, 25.687485218048096] in episode 865
Report: 
rewardSum:245.25
loss:[30.496379852294922, 25.687485218048096]
policies:[0, 4, 2]
qAverage:[0.0, 80.15247090657552]
ws:[5.584966500600179, 9.17486015955607]
memory len:10000
memory used:2670.0
now epsilon is 0.2827596879355155, the reward is 247.25 with loss [22.300774097442627, 39.05188846588135] in episode 866
Report: 
rewardSum:247.25
loss:[22.300774097442627, 39.05188846588135]
policies:[1, 3, 0]
qAverage:[17.350390625, 67.38315124511719]
ws:[3.729452431201935, 6.032213592529297]
memory len:10000
memory used:2671.0
now epsilon is 0.2823358134024739, the reward is 245.25 with loss [46.88377141952515, 24.155773878097534] in episode 867
Report: 
rewardSum:245.25
loss:[46.88377141952515, 24.155773878097534]
policies:[0, 5, 1]
qAverage:[0.0, 77.50198745727539]
ws:[3.8290523290634155, 5.774723589420319]
memory len:10000
memory used:2670.0
now epsilon is 0.28205358344735654, the reward is 247.25 with loss [15.15335988998413, 17.56081449985504] in episode 868
Report: 
rewardSum:247.25
loss:[15.15335988998413, 17.56081449985504]
policies:[0, 3, 1]
qAverage:[0.0, 75.70733261108398]
ws:[3.8635256132110953, 5.679759752005339]
memory len:10000
memory used:2670.0
now epsilon is 0.28177163561637575, the reward is 247.25 with loss [18.72858691215515, 30.04836368560791] in episode 869
Report: 
rewardSum:247.25
loss:[18.72858691215515, 30.04836368560791]
policies:[1, 2, 1]
qAverage:[0.0, 67.2646484375]
ws:[1.4054642915725708, 2.669521768887838]
memory len:10000
memory used:2670.0
now epsilon is 0.28148996962751316, the reward is 247.25 with loss [15.759993076324463, 19.078844785690308] in episode 870
Report: 
rewardSum:247.25
loss:[15.759993076324463, 19.078844785690308]
policies:[1, 3, 0]
qAverage:[0.0, 68.17930348714192]
ws:[1.4035049676895142, 2.704785188039144]
memory len:10000
memory used:2670.0
now epsilon is 0.2812085851990323, the reward is 247.25 with loss [29.629523754119873, 22.268539428710938] in episode 871
Report: 
rewardSum:247.25
loss:[29.629523754119873, 22.268539428710938]
policies:[0, 4, 0]
qAverage:[0.0, 85.47401428222656]
ws:[5.25959587097168, 10.14231972694397]
memory len:10000
memory used:2670.0
now epsilon is 0.28106799848196934, the reward is -1.0 with loss [5.661257266998291, 11.41374921798706] in episode 872
Report: 
rewardSum:-1.0
loss:[5.661257266998291, 11.41374921798706]
policies:[0, 1, 1]
qAverage:[0.0, 46.299678802490234]
ws:[0.2586590349674225, 1.5494930744171143]
memory len:10000
memory used:2670.0
now epsilon is 0.2807870358664212, the reward is 247.25 with loss [17.23988163471222, 21.49006700515747] in episode 873
Report: 
rewardSum:247.25
loss:[17.23988163471222, 21.49006700515747]
policies:[1, 2, 1]
qAverage:[0.0, 62.07316207885742]
ws:[4.616580963134766, 11.569019317626953]
memory len:10000
memory used:2670.0
now epsilon is 0.2805063541081451, the reward is 247.25 with loss [18.50042414665222, 28.41033172607422] in episode 874
Report: 
rewardSum:247.25
loss:[18.50042414665222, 28.41033172607422]
policies:[0, 4, 0]
qAverage:[0.0, 86.30289611816406]
ws:[5.285406494140625, 11.109735298156739]
memory len:10000
memory used:2676.0
now epsilon is 0.2802259529263893, the reward is 247.25 with loss [28.12570595741272, 10.502333164215088] in episode 875
Report: 
rewardSum:247.25
loss:[28.12570595741272, 10.502333164215088]
policies:[0, 4, 0]
qAverage:[0.0, 85.48523864746093]
ws:[4.641091442108154, 11.08580379486084]
memory len:10000
memory used:2676.0
now epsilon is 0.2796659911708331, the reward is 243.25 with loss [38.99477446079254, 28.75675940513611] in episode 876
Report: 
rewardSum:243.25
loss:[38.99477446079254, 28.75675940513611]
policies:[1, 5, 2]
qAverage:[0.0, 87.01080131530762]
ws:[7.878371238708496, 13.058895587921143]
memory len:10000
memory used:2676.0
now epsilon is 0.2793864300369309, the reward is 247.25 with loss [21.86528778076172, 22.021692037582397] in episode 877
Report: 
rewardSum:247.25
loss:[21.86528778076172, 22.021692037582397]
policies:[0, 3, 1]
qAverage:[0.0, 75.46164894104004]
ws:[6.089788198471069, 9.183882236480713]
memory len:10000
memory used:2676.0
now epsilon is 0.2790373715722549, the reward is 246.25 with loss [47.145831823349, 11.753195762634277] in episode 878
Report: 
rewardSum:246.25
loss:[47.145831823349, 11.753195762634277]
policies:[0, 4, 1]
qAverage:[0.0, 74.60755729675293]
ws:[5.462573513388634, 9.041379749774933]
memory len:10000
memory used:2676.0
now epsilon is 0.27889787032630453, the reward is -1.0 with loss [13.765308380126953, 20.46460723876953] in episode 879
Report: 
rewardSum:-1.0
loss:[13.765308380126953, 20.46460723876953]
policies:[0, 1, 1]
qAverage:[0.0, 44.38661193847656]
ws:[-0.5896211266517639, 0.4410287141799927]
memory len:10000
memory used:2676.0
now epsilon is 0.2786190770252496, the reward is 247.25 with loss [30.109353065490723, 20.46691060066223] in episode 880
Report: 
rewardSum:247.25
loss:[30.109353065490723, 20.46691060066223]
policies:[0, 3, 1]
qAverage:[0.0, 83.42612266540527]
ws:[5.935459852218628, 10.968939185142517]
memory len:10000
memory used:2676.0
now epsilon is 0.2783405624129656, the reward is 247.25 with loss [7.14694344997406, 16.623055696487427] in episode 881
Report: 
rewardSum:247.25
loss:[7.14694344997406, 16.623055696487427]
policies:[0, 4, 0]
qAverage:[0.0, 82.19167518615723]
ws:[5.072702929377556, 10.59683382511139]
memory len:10000
memory used:2676.0
now epsilon is 0.2780623262108684, the reward is 247.25 with loss [22.702499389648438, 17.162070274353027] in episode 882
Report: 
rewardSum:247.25
loss:[22.702499389648438, 17.162070274353027]
policies:[0, 3, 1]
qAverage:[0.0, 74.62306467692058]
ws:[3.5342466632525125, 8.301457087198893]
memory len:10000
memory used:2676.0
now epsilon is 0.27778436814065205, the reward is 247.25 with loss [18.972045421600342, 15.841425895690918] in episode 883
Report: 
rewardSum:247.25
loss:[18.972045421600342, 15.841425895690918]
policies:[0, 3, 1]
qAverage:[0.0, 83.0520076751709]
ws:[8.070024251937866, 13.554978132247925]
memory len:10000
memory used:2677.0
now epsilon is 0.2775066879242891, the reward is 59.6875 with loss [21.097095847129822, 18.84876012802124] in episode 884
Report: 
rewardSum:59.6875
loss:[21.097095847129822, 18.84876012802124]
policies:[0, 1, 3]
qAverage:[0.0, 44.75925827026367]
ws:[1.311202883720398, 2.4657986164093018]
memory len:10000
memory used:2677.0
now epsilon is 0.27715997796270875, the reward is 58.6875 with loss [26.00469136238098, 34.65641164779663] in episode 885
Report: 
rewardSum:58.6875
loss:[26.00469136238098, 34.65641164779663]
policies:[0, 2, 3]
qAverage:[0.0, 66.37832132975261]
ws:[3.037449916203817, 5.440943241119385]
memory len:10000
memory used:2677.0
now epsilon is 0.2767444977466478, the reward is 994.0 with loss [41.8675742149353, 34.79204964637756] in episode 886
Report: 
rewardSum:994.0
loss:[41.8675742149353, 34.79204964637756]
policies:[1, 3, 2]
qAverage:[17.69848175048828, 57.39741668701172]
ws:[-1.6634060859680175, 0.5561415195465088]
memory len:10000
memory used:2677.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21*		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.276329640361528, the reward is 245.25 with loss [17.990736722946167, 36.30707907676697] in episode 887
Report: 
rewardSum:245.25
loss:[17.990736722946167, 36.30707907676697]
policies:[1, 3, 2]
qAverage:[0.0, 76.55322265625]
ws:[4.462303280830383, 7.5992531180381775]
memory len:10000
memory used:2676.0
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.2756395929198463, the reward is 241.25 with loss [58.49027410894632, 43.630167722702026] in episode 888
Report: 
rewardSum:241.25
loss:[58.49027410894632, 43.630167722702026]
policies:[0, 7, 3]
qAverage:[0.0, 91.01151943206787]
ws:[6.524873971939087, 11.733342856168747]
memory len:10000
memory used:2677.0
now epsilon is 0.27536405667454744, the reward is 247.25 with loss [25.122882843017578, 14.970298528671265] in episode 889
Report: 
rewardSum:247.25
loss:[25.122882843017578, 14.970298528671265]
policies:[1, 1, 2]
qAverage:[0.0, 49.20273208618164]
ws:[6.504386901855469, 9.697388648986816]
memory len:10000
memory used:2677.0
now epsilon is 0.275088795862185, the reward is 247.25 with loss [15.629332304000854, 15.882076352834702] in episode 890
Report: 
rewardSum:247.25
loss:[15.629332304000854, 15.882076352834702]
policies:[0, 3, 1]
qAverage:[0.0, 69.81960550944011]
ws:[4.235101103782654, 8.454453627268473]
memory len:10000
memory used:2677.0
now epsilon is 0.2748138102074293, the reward is 247.25 with loss [17.862786769866943, 39.58206653594971] in episode 891
Report: 
rewardSum:247.25
loss:[17.862786769866943, 39.58206653594971]
policies:[0, 4, 0]
qAverage:[0.0, 83.36047058105468]
ws:[6.51985342502594, 11.467902183532715]
memory len:10000
memory used:2677.0
now epsilon is 0.274401847044202, the reward is 245.25 with loss [33.74106752872467, 39.53517413139343] in episode 892
Report: 
rewardSum:245.25
loss:[33.74106752872467, 39.53517413139343]
policies:[1, 2, 3]
qAverage:[0.0, 72.43701934814453]
ws:[3.2979251543680825, 6.261802991231282]
memory len:10000
memory used:2677.0
now epsilon is 0.2739905014396329, the reward is 245.25 with loss [24.665472574532032, 25.795430183410645] in episode 893
Report: 
rewardSum:245.25
loss:[24.665472574532032, 25.795430183410645]
policies:[1, 3, 2]
qAverage:[0.0, 78.90943717956543]
ws:[3.598887652158737, 7.739817440509796]
memory len:10000
memory used:2677.0
now epsilon is 0.27371661366750805, the reward is 247.25 with loss [41.4402871131897, 19.644846439361572] in episode 894
Report: 
rewardSum:247.25
loss:[41.4402871131897, 19.644846439361572]
policies:[0, 3, 1]
qAverage:[0.0, 68.95791625976562]
ws:[1.5919907093048096, 3.687659422556559]
memory len:10000
memory used:2677.0
now epsilon is 0.27330629527081174, the reward is 57.6875 with loss [46.87014591693878, 34.01423239707947] in episode 895
Report: 
rewardSum:57.6875
loss:[46.87014591693878, 34.01423239707947]
policies:[0, 3, 3]
qAverage:[0.0, 73.46328926086426]
ws:[4.649101108312607, 7.5856382846832275]
memory len:10000
memory used:2677.0
now epsilon is 0.27289659196716515, the reward is 245.25 with loss [31.215174555778503, 39.188928723335266] in episode 896
Report: 
rewardSum:245.25
loss:[31.215174555778503, 39.188928723335266]
policies:[0, 5, 1]
qAverage:[0.0, 78.4250274658203]
ws:[3.5364742279052734, 7.798965120315552]
memory len:10000
memory used:2677.0
now epsilon is 0.272215117497455, the reward is 241.25 with loss [52.537476539611816, 33.444778472185135] in episode 897
Report: 
rewardSum:241.25
loss:[52.537476539611816, 33.444778472185135]
policies:[2, 4, 4]
qAverage:[0.0, 70.69732284545898]
ws:[4.177579194307327, 7.251193642616272]
memory len:10000
memory used:2676.0
now epsilon is 0.2719430044436142, the reward is 247.25 with loss [17.082128763198853, 13.362350940704346] in episode 898
Report: 
rewardSum:247.25
loss:[17.082128763198853, 13.362350940704346]
policies:[0, 4, 0]
qAverage:[0.0, 70.3417256673177]
ws:[5.5224839846293134, 11.187355677286783]
memory len:10000
memory used:2676.0
now epsilon is 0.2716711634008019, the reward is 247.25 with loss [27.91380524635315, 17.0531165599823] in episode 899
Report: 
rewardSum:247.25
loss:[27.91380524635315, 17.0531165599823]
policies:[1, 3, 0]
qAverage:[0.0, 68.22153218587239]
ws:[2.406296650568644, 5.572100321451823]
memory len:10000
memory used:2676.0
now epsilon is 0.27133174419858475, the reward is 246.25 with loss [24.493854999542236, 29.45815086364746] in episode 900
Report: 
rewardSum:246.25
loss:[24.493854999542236, 29.45815086364746]
policies:[1, 3, 1]
qAverage:[0.0, 68.23009999593098]
ws:[5.808905919392903, 9.971393585205078]
memory len:10000
memory used:2676.0
now epsilon is 0.27112829626089835, the reward is -2.0 with loss [20.11996603012085, 9.219408988952637] in episode 901
Report: 
rewardSum:-2.0
loss:[20.11996603012085, 9.219408988952637]
policies:[0, 1, 2]
qAverage:[0.0, 43.03081130981445]
ws:[-0.6798977851867676, 0.548162579536438]
memory len:10000
memory used:2676.0
now epsilon is 0.2708572696208041, the reward is 247.25 with loss [26.014808416366577, 16.855038166046143] in episode 902
Report: 
rewardSum:247.25
loss:[26.014808416366577, 16.855038166046143]
policies:[0, 3, 1]
qAverage:[0.0, 77.25059700012207]
ws:[2.361989364027977, 7.326546907424927]
memory len:10000
memory used:2676.0
now epsilon is 0.2704512375604362, the reward is 245.25 with loss [30.839772701263428, 29.100507497787476] in episode 903
Report: 
rewardSum:245.25
loss:[30.839772701263428, 29.100507497787476]
policies:[0, 4, 2]
qAverage:[0.0, 69.70722389221191]
ws:[-0.3474303334951401, 2.1863967180252075]
memory len:10000
memory used:2676.0
now epsilon is 0.2700458141676306, the reward is 245.25 with loss [25.51556658744812, 32.441450357437134] in episode 904
Report: 
rewardSum:245.25
loss:[25.51556658744812, 32.441450357437134]
policies:[1, 2, 3]
qAverage:[0.0, 54.15590286254883]
ws:[1.243215560913086, 3.829451560974121]
memory len:10000
memory used:2676.0
now epsilon is 0.2697084256363656, the reward is 246.25 with loss [22.88124394416809, 28.356091499328613] in episode 905
Report: 
rewardSum:246.25
loss:[22.88124394416809, 28.356091499328613]
policies:[0, 3, 2]
qAverage:[0.0, 80.00836563110352]
ws:[2.939885511994362, 8.633859992027283]
memory len:10000
memory used:2676.0
now epsilon is 0.2694388183345332, the reward is 247.25 with loss [21.998734712600708, 22.83002257347107] in episode 906
Report: 
rewardSum:247.25
loss:[21.998734712600708, 22.83002257347107]
policies:[2, 2, 0]
qAverage:[0.0, 64.01475524902344]
ws:[2.232779622077942, 4.995143890380859]
memory len:10000
memory used:2676.0
now epsilon is 0.2691694805389167, the reward is 59.6875 with loss [25.912147998809814, 10.835789680480957] in episode 907
Report: 
rewardSum:59.6875
loss:[25.912147998809814, 10.835789680480957]
policies:[1, 1, 2]
qAverage:[0.0, 53.30836868286133]
ws:[2.9028196334838867, 5.091726303100586]
memory len:10000
memory used:2677.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36*		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.268564454485883, the reward is 991.0 with loss [61.468720465898514, 60.99302411079407] in episode 908
Report: 
rewardSum:991.0
loss:[61.468720465898514, 60.99302411079407]
policies:[0, 2, 7]
qAverage:[0.0, 65.12241872151692]
ws:[-7.74890152613322, -6.1508049964904785]
memory len:10000
memory used:2677.0
now epsilon is 0.2682959907262834, the reward is 59.6875 with loss [14.042894124984741, 18.24456238746643] in episode 909
Report: 
rewardSum:59.6875
loss:[14.042894124984741, 18.24456238746643]
policies:[1, 1, 2]
qAverage:[0.0, 51.609703063964844]
ws:[0.9084882140159607, 3.562814235687256]
memory len:10000
memory used:2677.0
now epsilon is 0.267759868028129, the reward is 55.6875 with loss [42.154887676239014, 52.17689800262451] in episode 910
Report: 
rewardSum:55.6875
loss:[42.154887676239014, 52.17689800262451]
policies:[1, 4, 3]
qAverage:[0.0, 74.56568450927735]
ws:[0.6594918489456176, 3.790680456161499]
memory len:10000
memory used:2677.0
now epsilon is 0.26749220855331746, the reward is 59.6875 with loss [16.9866144657135, 16.414548754692078] in episode 911
Report: 
rewardSum:59.6875
loss:[16.9866144657135, 16.414548754692078]
policies:[0, 3, 1]
qAverage:[0.0, 64.09436798095703]
ws:[1.4549731413523357, 3.607977787653605]
memory len:10000
memory used:2677.0
now epsilon is 0.2672248166376252, the reward is 247.25 with loss [22.335378170013428, 16.81857095658779] in episode 912
Report: 
rewardSum:247.25
loss:[22.335378170013428, 16.81857095658779]
policies:[0, 3, 1]
qAverage:[0.0, 59.1973876953125]
ws:[4.9188980261484785, 8.160815080006918]
memory len:10000
memory used:2677.0
now epsilon is 0.2669576920135934, the reward is 247.25 with loss [27.03744339942932, 30.280500411987305] in episode 913
Report: 
rewardSum:247.25
loss:[27.03744339942932, 30.280500411987305]
policies:[1, 2, 1]
qAverage:[0.0, 43.634918212890625]
ws:[0.32340189814567566, 1.4839966297149658]
memory len:10000
memory used:2677.0
now epsilon is 0.2666908344140305, the reward is 247.25 with loss [8.90392291545868, 29.321290016174316] in episode 914
Report: 
rewardSum:247.25
loss:[8.90392291545868, 29.321290016174316]
policies:[0, 3, 1]
qAverage:[0.0, 72.4393539428711]
ws:[4.304308891296387, 9.688401063283285]
memory len:10000
memory used:2677.0
now epsilon is 0.2664242435720123, the reward is 247.25 with loss [18.60468316078186, 23.82437551021576] in episode 915
Report: 
rewardSum:247.25
loss:[18.60468316078186, 23.82437551021576]
policies:[0, 4, 0]
qAverage:[0.0, 79.29677886962891]
ws:[5.340479445457459, 10.62787446975708]
memory len:10000
memory used:2677.0
now epsilon is 0.2660248568961407, the reward is 245.25 with loss [27.706663727760315, 33.57407283782959] in episode 916
Report: 
rewardSum:245.25
loss:[27.706663727760315, 33.57407283782959]
policies:[0, 4, 2]
qAverage:[0.0, 75.24459648132324]
ws:[2.4623080641031265, 7.020381212234497]
memory len:10000
memory used:2677.0
now epsilon is 0.2657589317819404, the reward is 247.25 with loss [17.31941866874695, 33.580119609832764] in episode 917
Report: 
rewardSum:247.25
loss:[17.31941866874695, 33.580119609832764]
policies:[0, 4, 0]
qAverage:[0.0, 77.38965225219727]
ws:[2.208038777112961, 6.969282448291779]
memory len:10000
memory used:2677.0
now epsilon is 0.26549327249314897, the reward is 247.25 with loss [11.651270151138306, 15.18552041053772] in episode 918
Report: 
rewardSum:247.25
loss:[11.651270151138306, 15.18552041053772]
policies:[1, 3, 0]
qAverage:[0.0, 70.92795054117839]
ws:[6.283758322397868, 13.126057942708334]
memory len:10000
memory used:2677.0
now epsilon is 0.2652278787640407, the reward is 59.6875 with loss [21.94540548324585, 30.347264528274536] in episode 919
Report: 
rewardSum:59.6875
loss:[21.94540548324585, 30.347264528274536]
policies:[0, 2, 2]
qAverage:[0.0, 60.072977701822914]
ws:[1.1903771758079529, 3.069228251775106]
memory len:10000
memory used:2677.0
now epsilon is 0.2649627503291555, the reward is 247.25 with loss [23.221959590911865, 18.94688844680786] in episode 920
Report: 
rewardSum:247.25
loss:[23.221959590911865, 18.94688844680786]
policies:[0, 4, 0]
qAverage:[0.0, 72.36559104919434]
ws:[2.0597008764743805, 4.481731534004211]
memory len:10000
memory used:2677.0
now epsilon is 0.26463171245156786, the reward is 246.25 with loss [30.24071168899536, 31.370360136032104] in episode 921
Report: 
rewardSum:246.25
loss:[30.24071168899536, 31.370360136032104]
policies:[0, 3, 2]
qAverage:[0.0, 72.94419860839844]
ws:[1.9055768847465515, 4.339386343955994]
memory len:10000
memory used:2678.0
now epsilon is 0.26436717995947, the reward is 247.25 with loss [22.67915177345276, 17.790032744407654] in episode 922
Report: 
rewardSum:247.25
loss:[22.67915177345276, 17.790032744407654]
policies:[0, 4, 0]
qAverage:[0.0, 78.28128051757812]
ws:[2.912628948688507, 7.834494233131409]
memory len:10000
memory used:2683.0
now epsilon is 0.2639048842319251, the reward is 244.25 with loss [67.03637552261353, 42.25234365463257] in episode 923
Report: 
rewardSum:244.25
loss:[67.03637552261353, 42.25234365463257]
policies:[0, 4, 3]
qAverage:[0.0, 77.33975219726562]
ws:[2.6826614290475845, 8.214425504207611]
memory len:10000
memory used:2683.0
now epsilon is 0.2636410782955318, the reward is 247.25 with loss [26.89077138900757, 15.748123168945312] in episode 924
Report: 
rewardSum:247.25
loss:[26.89077138900757, 15.748123168945312]
policies:[0, 3, 1]
qAverage:[0.0, 66.82234191894531]
ws:[5.01967978477478, 9.683334668477377]
memory len:10000
memory used:2683.0
now epsilon is 0.2633775360661641, the reward is 247.25 with loss [23.13920760154724, 22.486858367919922] in episode 925
Report: 
rewardSum:247.25
loss:[23.13920760154724, 22.486858367919922]
policies:[0, 3, 1]
qAverage:[0.0, 77.72039031982422]
ws:[3.2331856545060873, 9.702701926231384]
memory len:10000
memory used:2683.0
now epsilon is 0.2631142572802139, the reward is 247.25 with loss [38.27378845214844, 16.823291063308716] in episode 926
Report: 
rewardSum:247.25
loss:[38.27378845214844, 16.823291063308716]
policies:[1, 3, 0]
qAverage:[0.0, 60.72333272298177]
ws:[0.9506828784942627, 3.46562123298645]
memory len:10000
memory used:2683.0
now epsilon is 0.26285124167433666, the reward is 247.25 with loss [28.79118299484253, 18.33406352996826] in episode 927
Report: 
rewardSum:247.25
loss:[28.79118299484253, 18.33406352996826]
policies:[2, 2, 0]
qAverage:[0.0, 47.472843170166016]
ws:[1.035142421722412, 3.2749531269073486]
memory len:10000
memory used:2683.0
now epsilon is 0.26252284186320446, the reward is 58.6875 with loss [26.90404224395752, 45.12639331817627] in episode 928
Report: 
rewardSum:58.6875
loss:[26.90404224395752, 45.12639331817627]
policies:[1, 1, 3]
qAverage:[0.0, 47.96562576293945]
ws:[0.7013330459594727, 2.44763445854187]
memory len:10000
memory used:2684.0
now epsilon is 0.26226041745100037, the reward is 247.25 with loss [23.617299795150757, 15.288553476333618] in episode 929
Report: 
rewardSum:247.25
loss:[23.617299795150757, 15.288553476333618]
policies:[0, 3, 1]
qAverage:[0.0, 43.4188232421875]
ws:[-0.5658839344978333, 0.6683171391487122]
memory len:10000
memory used:2684.0
now epsilon is 0.2619982553648157, the reward is 247.25 with loss [14.502197742462158, 8.449806690216064] in episode 930
Report: 
rewardSum:247.25
loss:[14.502197742462158, 8.449806690216064]
policies:[0, 4, 0]
qAverage:[0.0, 79.64526557922363]
ws:[5.139358937740326, 12.074260950088501]
memory len:10000
memory used:2684.0
now epsilon is 0.26160550352327383, the reward is 245.25 with loss [31.972214937210083, 28.465946912765503] in episode 931
Report: 
rewardSum:245.25
loss:[31.972214937210083, 28.465946912765503]
policies:[0, 4, 2]
qAverage:[0.0, 72.68204498291016]
ws:[2.3469132483005524, 7.161022126674652]
memory len:10000
memory used:2684.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20*		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.2613439961054651, the reward is 247.25 with loss [27.869507312774658, 32.83205270767212] in episode 932
Report: 
rewardSum:247.25
loss:[27.869507312774658, 32.83205270767212]
policies:[2, 1, 1]
qAverage:[0.0, 58.00493240356445]
ws:[3.1832406520843506, 11.389935493469238]
memory len:10000
memory used:2684.0
now epsilon is 0.2609522250396486, the reward is 245.25 with loss [25.91848659515381, 20.042637825012207] in episode 933
Report: 
rewardSum:245.25
loss:[25.91848659515381, 20.042637825012207]
policies:[1, 4, 1]
qAverage:[0.0, 76.59592819213867]
ws:[2.727110981941223, 9.265888035297394]
memory len:10000
memory used:2684.0
now epsilon is 0.26069137065538484, the reward is 247.25 with loss [21.078994035720825, 14.16301417350769] in episode 934
Report: 
rewardSum:247.25
loss:[21.078994035720825, 14.16301417350769]
policies:[1, 2, 1]
qAverage:[0.0, 68.05265045166016]
ws:[4.26988951365153, 9.693218072255453]
memory len:10000
memory used:2684.0
now epsilon is 0.2603656693334444, the reward is 58.6875 with loss [22.21452784538269, 36.70517826080322] in episode 935
Report: 
rewardSum:58.6875
loss:[22.21452784538269, 36.70517826080322]
policies:[1, 2, 2]
qAverage:[0.0, 62.5876210530599]
ws:[0.48343437413374585, 3.7826383908589682]
memory len:10000
memory used:2684.0
now epsilon is 0.2601054012849652, the reward is 247.25 with loss [24.566879272460938, 24.059693098068237] in episode 936
Report: 
rewardSum:247.25
loss:[24.566879272460938, 24.059693098068237]
policies:[2, 1, 1]
qAverage:[0.0, 48.84682083129883]
ws:[6.152363300323486, 11.176233291625977]
memory len:10000
memory used:2684.0
now epsilon is 0.2598453934069501, the reward is 59.6875 with loss [33.073251247406006, 27.382051944732666] in episode 937
Report: 
rewardSum:59.6875
loss:[33.073251247406006, 27.382051944732666]
policies:[0, 2, 2]
qAverage:[0.0, 66.37242126464844]
ws:[2.1857744057973227, 5.592561721801758]
memory len:10000
memory used:2684.0
now epsilon is 0.25958564543932644, the reward is 247.25 with loss [14.61883020401001, 29.96360492706299] in episode 938
Report: 
rewardSum:247.25
loss:[14.61883020401001, 29.96360492706299]
policies:[0, 3, 1]
qAverage:[0.0, 72.44243621826172]
ws:[4.38456866145134, 9.602249503135681]
memory len:10000
memory used:2684.0
now epsilon is 0.25932615712228113, the reward is 247.25 with loss [29.12456703186035, 20.519867092370987] in episode 939
Report: 
rewardSum:247.25
loss:[29.12456703186035, 20.519867092370987]
policies:[1, 3, 0]
qAverage:[0.0, 79.64700508117676]
ws:[2.1096023321151733, 10.010735750198364]
memory len:10000
memory used:2684.0
now epsilon is 0.2590669281962609, the reward is 247.25 with loss [15.867847442626953, 23.842724800109863] in episode 940
Report: 
rewardSum:247.25
loss:[15.867847442626953, 23.842724800109863]
policies:[1, 2, 1]
qAverage:[0.0, 67.53958384195964]
ws:[4.656526486078898, 10.59781551361084]
memory len:10000
memory used:2684.0
now epsilon is 0.25880795840197207, the reward is 247.25 with loss [27.561174869537354, 29.726470947265625] in episode 941
Report: 
rewardSum:247.25
loss:[27.561174869537354, 29.726470947265625]
policies:[0, 2, 2]
qAverage:[0.0, 74.82413736979167]
ws:[2.275856892267863, 10.113217830657959]
memory len:10000
memory used:2684.0
now epsilon is 0.25854924748038005, the reward is 247.25 with loss [18.289923667907715, 9.840490102767944] in episode 942
Report: 
rewardSum:247.25
loss:[18.289923667907715, 9.840490102767944]
policies:[0, 2, 2]
qAverage:[0.0, 70.89635721842448]
ws:[5.797000885009766, 15.154690424601236]
memory len:10000
memory used:2684.0
now epsilon is 0.25816166591829753, the reward is 245.25 with loss [36.84926927089691, 27.882214069366455] in episode 943
Report: 
rewardSum:245.25
loss:[36.84926927089691, 27.882214069366455]
policies:[0, 5, 1]
qAverage:[0.0, 83.73373031616211]
ws:[4.210079511006673, 10.690390129884085]
memory len:10000
memory used:2684.0
now epsilon is 0.2578391251466082, the reward is 246.25 with loss [28.774824142456055, 44.09110641479492] in episode 944
Report: 
rewardSum:246.25
loss:[28.774824142456055, 44.09110641479492]
policies:[0, 3, 2]
qAverage:[0.0, 75.38476371765137]
ws:[4.855368992313743, 13.265513062477112]
memory len:10000
memory used:2684.0
now epsilon is 0.2575169873493458, the reward is 246.25 with loss [20.30275249481201, 35.83265662193298] in episode 945
Report: 
rewardSum:246.25
loss:[20.30275249481201, 35.83265662193298]
policies:[0, 4, 1]
qAverage:[0.0, 81.37903900146485]
ws:[1.7820994876325131, 8.836722087860107]
memory len:10000
memory used:2684.0
now epsilon is 0.257259566914773, the reward is 247.25 with loss [29.853233337402344, 18.17647284269333] in episode 946
Report: 
rewardSum:247.25
loss:[29.853233337402344, 18.17647284269333]
policies:[0, 4, 0]
qAverage:[0.0, 80.72503204345703]
ws:[5.090582346916198, 13.177486371994018]
memory len:10000
memory used:2684.0
now epsilon is 0.2568739186648663, the reward is 245.25 with loss [44.48357057571411, 34.24205815792084] in episode 947
Report: 
rewardSum:245.25
loss:[44.48357057571411, 34.24205815792084]
policies:[1, 4, 1]
qAverage:[0.0, 80.6124481201172]
ws:[2.832101011276245, 9.917336082458496]
memory len:10000
memory used:2684.0
now epsilon is 0.2567454977601538, the reward is -1.0 with loss [4.083489656448364, 19.134206771850586] in episode 948
Report: 
rewardSum:-1.0
loss:[4.083489656448364, 19.134206771850586]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:2684.0
now epsilon is 0.2564888485259097, the reward is 247.25 with loss [32.96497297286987, 12.384387135505676] in episode 949
Report: 
rewardSum:247.25
loss:[32.96497297286987, 12.384387135505676]
policies:[1, 2, 1]
qAverage:[0.0, 63.74634043375651]
ws:[0.9022796750068665, 4.599780400594075]
memory len:10000
memory used:2684.0
now epsilon is 0.25623245584467247, the reward is 247.25 with loss [17.69095468521118, 22.61047101020813] in episode 950
Report: 
rewardSum:247.25
loss:[17.69095468521118, 22.61047101020813]
policies:[0, 3, 1]
qAverage:[0.0, 79.2065486907959]
ws:[5.5997593998909, 15.914500713348389]
memory len:10000
memory used:2684.0
now epsilon is 0.2559763194599852, the reward is 247.25 with loss [19.83966088294983, 13.482431411743164] in episode 951
Report: 
rewardSum:247.25
loss:[19.83966088294983, 13.482431411743164]
policies:[0, 4, 0]
qAverage:[0.0, 81.45098114013672]
ws:[1.5632911264896392, 8.303346538543702]
memory len:10000
memory used:2684.0
now epsilon is 0.2557204391156475, the reward is 247.25 with loss [26.89275050163269, 17.722419023513794] in episode 952
Report: 
rewardSum:247.25
loss:[26.89275050163269, 17.722419023513794]
policies:[1, 3, 0]
qAverage:[0.0, 78.7004508972168]
ws:[4.364378571510315, 12.662633061408997]
memory len:10000
memory used:2683.0
now epsilon is 0.25495433176656734, the reward is 239.25 with loss [70.63668131828308, 71.98815417289734] in episode 953
Report: 
rewardSum:239.25
loss:[70.63668131828308, 71.98815417289734]
policies:[2, 3, 7]
qAverage:[0.0, 63.86785634358724]
ws:[9.040065129597982, 15.385209401448568]
memory len:10000
memory used:2683.0
now epsilon is 0.25469947302674156, the reward is 247.25 with loss [21.250927209854126, 13.339774131774902] in episode 954
Report: 
rewardSum:247.25
loss:[21.250927209854126, 13.339774131774902]
policies:[0, 4, 0]
qAverage:[0.0, 81.18557281494141]
ws:[2.6435087084770204, 7.755229377746582]
memory len:10000
memory used:2695.0
now epsilon is 0.2544448690500995, the reward is 247.25 with loss [18.555587589740753, 35.8817138671875] in episode 955
Report: 
rewardSum:247.25
loss:[18.555587589740753, 35.8817138671875]
policies:[2, 2, 0]
qAverage:[0.0, 68.52604166666667]
ws:[2.6073570251464844, 9.624082406361898]
memory len:10000
memory used:2696.0
now epsilon is 0.25419051958197353, the reward is 247.25 with loss [19.478703498840332, 23.159530878067017] in episode 956
Report: 
rewardSum:247.25
loss:[19.478703498840332, 23.159530878067017]
policies:[0, 4, 0]
qAverage:[0.0, 74.23686790466309]
ws:[5.387059152126312, 12.329872786998749]
memory len:10000
memory used:2696.0
############# STATE ###############
0-		8*		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.2536825831538717, the reward is 992.0 with loss [37.824991941452026, 45.3680260181427] in episode 957
Report: 
rewardSum:992.0
loss:[37.824991941452026, 45.3680260181427]
policies:[4, 1, 3]
qAverage:[62.58040428161621, 0.0]
ws:[-6.059561789035797, -6.875455468893051]
memory len:10000
memory used:2684.0
now epsilon is 0.2533022970273017, the reward is 245.25 with loss [43.80398750305176, 57.193652629852295] in episode 958
Report: 
rewardSum:245.25
loss:[43.80398750305176, 57.193652629852295]
policies:[1, 3, 2]
qAverage:[22.21268653869629, 54.88859939575195]
ws:[4.743314206600189, 10.294745743274689]
memory len:10000
memory used:2684.0
now epsilon is 0.2529858274303798, the reward is 246.25 with loss [40.00679540634155, 33.90260553359985] in episode 959
Report: 
rewardSum:246.25
loss:[40.00679540634155, 33.90260553359985]
policies:[0, 4, 1]
qAverage:[0.0, 81.33223114013671]
ws:[2.4371546506881714, 6.635036897659302]
memory len:10000
memory used:2684.0
now epsilon is 0.25273293645682415, the reward is 247.25 with loss [21.696820735931396, 17.17210865020752] in episode 960
Report: 
rewardSum:247.25
loss:[21.696820735931396, 17.17210865020752]
policies:[1, 3, 0]
qAverage:[16.03209686279297, 62.63601531982422]
ws:[0.9711089134216309, 5.1915154457092285]
memory len:10000
memory used:2684.0
now epsilon is 0.25248029827942375, the reward is 247.25 with loss [28.21152114868164, 32.27886748313904] in episode 961
Report: 
rewardSum:247.25
loss:[28.21152114868164, 32.27886748313904]
policies:[0, 4, 0]
qAverage:[0.0, 78.98373565673828]
ws:[2.3230778902769087, 8.730578637123108]
memory len:10000
memory used:2684.0
now epsilon is 0.2522279126454772, the reward is 247.25 with loss [37.95312547683716, 30.074306964874268] in episode 962
Report: 
rewardSum:247.25
loss:[37.95312547683716, 30.074306964874268]
policies:[0, 3, 1]
qAverage:[0.0, 70.6750431060791]
ws:[-0.7476461306214333, 1.698949832469225]
memory len:10000
memory used:2684.0
now epsilon is 0.2519757793025358, the reward is 247.25 with loss [28.10212540626526, 32.165771484375] in episode 963
Report: 
rewardSum:247.25
loss:[28.10212540626526, 32.165771484375]
policies:[0, 3, 1]
qAverage:[0.0, 46.23427200317383]
ws:[-0.016238901764154434, 1.7124871015548706]
memory len:10000
memory used:2684.0
now epsilon is 0.25159805178214745, the reward is 245.25 with loss [27.208049058914185, 34.903425216674805] in episode 964
Report: 
rewardSum:245.25
loss:[27.208049058914185, 34.903425216674805]
policies:[1, 4, 1]
qAverage:[0.0, 69.78396987915039]
ws:[3.8982221335172653, 6.1895807683467865]
memory len:10000
memory used:2684.0
now epsilon is 0.2513465480639108, the reward is 59.6875 with loss [17.00365424156189, 35.385282933712006] in episode 965
Report: 
rewardSum:59.6875
loss:[17.00365424156189, 35.385282933712006]
policies:[2, 0, 2]
qAverage:[62.482696533203125, 0.0]
ws:[4.947835763295491, 4.81912883122762]
memory len:10000
memory used:2684.0
now epsilon is 0.25096976380067276, the reward is 245.25 with loss [25.160826206207275, 24.78276300430298] in episode 966
Report: 
rewardSum:245.25
loss:[25.160826206207275, 24.78276300430298]
policies:[1, 3, 2]
qAverage:[0.0, 76.35394859313965]
ws:[5.475204735994339, 11.129443883895874]
memory len:10000
memory used:2684.0
now epsilon is 0.25071888813484894, the reward is 247.25 with loss [25.57105302810669, 29.852458000183105] in episode 967
Report: 
rewardSum:247.25
loss:[25.57105302810669, 29.852458000183105]
policies:[1, 3, 0]
qAverage:[0.0, 77.81390571594238]
ws:[0.811992347240448, 5.678321897983551]
memory len:10000
memory used:2684.0
now epsilon is 0.25034304477326935, the reward is 245.25 with loss [41.16471481323242, 23.206501483917236] in episode 968
Report: 
rewardSum:245.25
loss:[41.16471481323242, 23.206501483917236]
policies:[0, 4, 2]
qAverage:[0.0, 75.07796630859374]
ws:[2.718173325061798, 5.5942018032073975]
memory len:10000
memory used:2684.0
now epsilon is 0.25009279559149244, the reward is 247.25 with loss [20.454253315925598, 29.31411838531494] in episode 969
Report: 
rewardSum:247.25
loss:[20.454253315925598, 29.31411838531494]
policies:[0, 3, 1]
qAverage:[0.0, 66.31300862630208]
ws:[4.564517021179199, 8.412057558695475]
memory len:10000
memory used:2684.0
now epsilon is 0.2498427965650695, the reward is 247.25 with loss [21.34650731086731, 15.779336929321289] in episode 970
Report: 
rewardSum:247.25
loss:[21.34650731086731, 15.779336929321289]
policies:[0, 3, 1]
qAverage:[0.0, 64.52016957600911]
ws:[1.9748708407084148, 5.155758380889893]
memory len:10000
memory used:2684.0
now epsilon is 0.24959304744393895, the reward is 247.25 with loss [11.865219056606293, 24.448660373687744] in episode 971
Report: 
rewardSum:247.25
loss:[11.865219056606293, 24.448660373687744]
policies:[0, 4, 0]
qAverage:[0.0, 70.11399841308594]
ws:[4.27726936340332, 12.889153162638346]
memory len:10000
memory used:2684.0
now epsilon is 0.24934354797828925, the reward is 247.25 with loss [23.340052604675293, 32.029237270355225] in episode 972
Report: 
rewardSum:247.25
loss:[23.340052604675293, 32.029237270355225]
policies:[0, 4, 0]
qAverage:[0.0, 80.2520523071289]
ws:[1.9414822578430175, 8.006650257110596]
memory len:10000
memory used:2684.0
now epsilon is 0.2489697663379929, the reward is 245.25 with loss [33.51772391796112, 29.057287216186523] in episode 973
Report: 
rewardSum:245.25
loss:[33.51772391796112, 29.057287216186523]
policies:[0, 4, 2]
qAverage:[0.0, 78.72113609313965]
ws:[5.979077160358429, 15.060423374176025]
memory len:10000
memory used:2684.0
now epsilon is 0.2485965450198534, the reward is 245.25 with loss [32.01300668716431, 42.06960105895996] in episode 974
Report: 
rewardSum:245.25
loss:[32.01300668716431, 42.06960105895996]
policies:[1, 4, 1]
qAverage:[0.0, 73.10645294189453]
ws:[2.3003949150443077, 9.130154550075531]
memory len:10000
memory used:2684.0
now epsilon is 0.24828595467258088, the reward is 246.25 with loss [21.091338872909546, 30.449207544326782] in episode 975
Report: 
rewardSum:246.25
loss:[21.091338872909546, 30.449207544326782]
policies:[1, 3, 1]
qAverage:[0.0, 76.48435592651367]
ws:[0.6916199177503586, 6.860235303640366]
memory len:10000
memory used:2684.0
now epsilon is 0.24778981704647413, the reward is 992.0 with loss [48.953528881073, 49.60439145565033] in episode 976
Report: 
rewardSum:992.0
loss:[48.953528881073, 49.60439145565033]
policies:[1, 4, 3]
qAverage:[13.89334233601888, 64.40683492024739]
ws:[-5.42977390686671, -3.8208083311716714]
memory len:10000
memory used:2689.0
now epsilon is 0.24754212013512317, the reward is 247.25 with loss [28.11898446083069, 15.368810892105103] in episode 977
Report: 
rewardSum:247.25
loss:[28.11898446083069, 15.368810892105103]
policies:[0, 4, 0]
qAverage:[0.0, 80.59097137451172]
ws:[3.0593728542327883, 7.829762744903564]
memory len:10000
memory used:2689.0
now epsilon is 0.2472946708278127, the reward is 247.25 with loss [16.356366276741028, 23.751132011413574] in episode 978
Report: 
rewardSum:247.25
loss:[16.356366276741028, 23.751132011413574]
policies:[0, 4, 0]
qAverage:[0.0, 59.711490631103516]
ws:[2.831890821456909, 10.645657539367676]
memory len:10000
memory used:2689.0
############# STATE ###############
0-		8-		16-		24*		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.24692396058305982, the reward is 245.25 with loss [30.725281715393066, 40.906025648117065] in episode 979
Report: 
rewardSum:245.25
loss:[30.725281715393066, 40.906025648117065]
policies:[1, 4, 1]
qAverage:[0.0, 80.58720092773437]
ws:[3.7972920894622804, 8.531639766693115]
memory len:10000
memory used:2689.0
now epsilon is 0.246492167604735, the reward is 56.6875 with loss [36.70636320114136, 35.19232535362244] in episode 980
Report: 
rewardSum:56.6875
loss:[36.70636320114136, 35.19232535362244]
policies:[1, 3, 3]
qAverage:[0.0, 70.52234077453613]
ws:[3.1697323322296143, 4.773206293582916]
memory len:10000
memory used:2689.0
now epsilon is 0.24618420641432426, the reward is 58.6875 with loss [21.089260458946228, 30.440582752227783] in episode 981
Report: 
rewardSum:58.6875
loss:[21.089260458946228, 30.440582752227783]
policies:[0, 3, 2]
qAverage:[0.0, 70.91893196105957]
ws:[4.3288795948028564, 5.791180610656738]
memory len:10000
memory used:2689.0
now epsilon is 0.2458766299829739, the reward is 246.25 with loss [22.76486110687256, 18.66401082277298] in episode 982
Report: 
rewardSum:246.25
loss:[22.76486110687256, 18.66401082277298]
policies:[0, 4, 1]
qAverage:[0.0, 73.26919301350911]
ws:[4.994887669881185, 11.604791641235352]
memory len:10000
memory used:2690.0
now epsilon is 0.24563084554136086, the reward is 247.25 with loss [31.354664981365204, 18.51296615600586] in episode 983
Report: 
rewardSum:247.25
loss:[31.354664981365204, 18.51296615600586]
policies:[0, 4, 0]
qAverage:[0.0, 80.1370361328125]
ws:[3.7377740859985353, 8.326511764526368]
memory len:10000
memory used:2690.0
now epsilon is 0.24538530679203563, the reward is 247.25 with loss [15.232271909713745, 26.679452896118164] in episode 984
Report: 
rewardSum:247.25
loss:[15.232271909713745, 26.679452896118164]
policies:[0, 3, 1]
qAverage:[0.0, 72.84850056966145]
ws:[3.8513689041137695, 10.785402297973633]
memory len:10000
memory used:2690.0
now epsilon is 0.24507872848602572, the reward is 58.6875 with loss [35.39743423461914, 32.63137769699097] in episode 985
Report: 
rewardSum:58.6875
loss:[35.39743423461914, 32.63137769699097]
policies:[2, 1, 2]
qAverage:[0.0, 46.920738220214844]
ws:[5.351590156555176, 9.294730186462402]
memory len:10000
memory used:2690.0
now epsilon is 0.2447113400780319, the reward is 245.25 with loss [43.54308319091797, 22.55650559067726] in episode 986
Report: 
rewardSum:245.25
loss:[43.54308319091797, 22.55650559067726]
policies:[1, 3, 2]
qAverage:[0.0, 74.25031852722168]
ws:[3.3321426063776016, 9.30939769744873]
memory len:10000
memory used:2690.0
now epsilon is 0.24434450240833827, the reward is 245.25 with loss [37.93908333778381, 24.35433840751648] in episode 987
Report: 
rewardSum:245.25
loss:[37.93908333778381, 24.35433840751648]
policies:[0, 2, 4]
qAverage:[0.0, 59.29215749104818]
ws:[5.00466513633728, 8.644745349884033]
memory len:10000
memory used:2690.0
now epsilon is 0.2441002495198478, the reward is 247.25 with loss [27.181804418563843, 16.83661437034607] in episode 988
Report: 
rewardSum:247.25
loss:[27.181804418563843, 16.83661437034607]
policies:[0, 3, 1]
qAverage:[0.0, 78.48762702941895]
ws:[2.823869526386261, 8.443982720375061]
memory len:10000
memory used:2690.0
now epsilon is 0.24373432791328498, the reward is 245.25 with loss [54.086172580718994, 34.5389084815979] in episode 989
Report: 
rewardSum:245.25
loss:[54.086172580718994, 34.5389084815979]
policies:[0, 3, 3]
qAverage:[0.0, 75.91230583190918]
ws:[1.3072766065597534, 5.913658432662487]
memory len:10000
memory used:2690.0
now epsilon is 0.2433689548461948, the reward is 57.6875 with loss [25.271233201026917, 21.776978492736816] in episode 990
Report: 
rewardSum:57.6875
loss:[25.271233201026917, 21.776978492736816]
policies:[0, 3, 3]
qAverage:[0.0, 66.24582417805989]
ws:[3.8950860500335693, 6.975900014241536]
memory len:10000
memory used:2690.0
now epsilon is 0.24312567713949712, the reward is 247.25 with loss [27.163286209106445, 24.883455753326416] in episode 991
Report: 
rewardSum:247.25
loss:[27.163286209106445, 24.883455753326416]
policies:[1, 2, 1]
qAverage:[0.0, 72.00343068440755]
ws:[6.34358024597168, 13.605400085449219]
memory len:10000
memory used:2690.0
now epsilon is 0.24294337846390812, the reward is -2.0 with loss [18.347865104675293, 19.1610746383667] in episode 992
Report: 
rewardSum:-2.0
loss:[18.347865104675293, 19.1610746383667]
policies:[1, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:2690.0
now epsilon is 0.24263985104248464, the reward is 246.25 with loss [25.096888303756714, 30.11035966873169] in episode 993
Report: 
rewardSum:246.25
loss:[25.096888303756714, 30.11035966873169]
policies:[0, 4, 1]
qAverage:[0.0, 82.20063781738281]
ws:[2.7874209880828857, 7.973508739471436]
memory len:10000
memory used:2690.0
now epsilon is 0.2423973021662223, the reward is 247.25 with loss [32.19069266319275, 25.212132930755615] in episode 994
Report: 
rewardSum:247.25
loss:[32.19069266319275, 25.212132930755615]
policies:[1, 2, 1]
qAverage:[0.0, 67.22750345865886]
ws:[5.689916133880615, 10.248998959859213]
memory len:10000
memory used:2690.0
now epsilon is 0.2420339333847088, the reward is 57.6875 with loss [31.04041811823845, 47.73372936248779] in episode 995
Report: 
rewardSum:57.6875
loss:[31.04041811823845, 47.73372936248779]
policies:[0, 2, 4]
qAverage:[0.0, 68.2034912109375]
ws:[6.012988408406575, 11.182468096415201]
memory len:10000
memory used:2690.0
now epsilon is 0.24179199019892297, the reward is 247.25 with loss [29.87566900253296, 45.99028539657593] in episode 996
Report: 
rewardSum:247.25
loss:[29.87566900253296, 45.99028539657593]
policies:[1, 3, 0]
qAverage:[0.0, 75.54146194458008]
ws:[1.4287393987178802, 8.139641404151917]
memory len:10000
memory used:2690.0
now epsilon is 0.24155028886560934, the reward is 247.25 with loss [29.51988124847412, 16.361722946166992] in episode 997
Report: 
rewardSum:247.25
loss:[29.51988124847412, 16.361722946166992]
policies:[1, 3, 0]
qAverage:[0.0, 76.73768107096355]
ws:[2.0404136578241983, 9.368151187896729]
memory len:10000
memory used:2689.0
now epsilon is 0.24130882914300614, the reward is 247.25 with loss [18.630196809768677, 20.437803745269775] in episode 998
Report: 
rewardSum:247.25
loss:[18.630196809768677, 20.437803745269775]
policies:[0, 3, 1]
qAverage:[0.0, 71.61472829182942]
ws:[2.0693745613098145, 9.001164436340332]
memory len:10000
memory used:2689.0
now epsilon is 0.2410676107895932, the reward is 247.25 with loss [27.28859043121338, 26.020588159561157] in episode 999
Report: 
rewardSum:247.25
loss:[27.28859043121338, 26.020588159561157]
policies:[1, 3, 0]
qAverage:[0.0, 67.69504038492839]
ws:[4.755521774291992, 9.446492513020834]
memory len:10000
memory used:2689.0
now epsilon is 0.2408266335640919, the reward is 247.25 with loss [22.205646514892578, 21.784462451934814] in episode 1000
Report: 
rewardSum:247.25
loss:[22.205646514892578, 21.784462451934814]
policies:[0, 2, 2]
qAverage:[0.0, 68.22613016764323]
ws:[5.3995334307352705, 10.325519879659018]
memory len:10000
memory used:2690.0
now epsilon is 0.24046561931347057, the reward is 245.25 with loss [40.26536679267883, 23.290385007858276] in episode 1001
Report: 
rewardSum:245.25
loss:[40.26536679267883, 23.290385007858276]
policies:[1, 4, 1]
qAverage:[0.0, 80.95286407470704]
ws:[4.602130913734436, 11.450676250457764]
memory len:10000
memory used:2690.0
############# STATE ###############
0-		8-		16*		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.24022524385373623, the reward is 247.25 with loss [16.064096689224243, 28.79319953918457] in episode 1002
Report: 
rewardSum:247.25
loss:[16.064096689224243, 28.79319953918457]
policies:[0, 4, 0]
qAverage:[0.0, 82.38520622253418]
ws:[2.9214123487472534, 8.68989634513855]
memory len:10000
memory used:2690.0
now epsilon is 0.2399851086793358, the reward is 247.25 with loss [19.93328356742859, 21.553920030593872] in episode 1003
Report: 
rewardSum:247.25
loss:[19.93328356742859, 21.553920030593872]
policies:[0, 4, 0]
qAverage:[0.0, 82.33146667480469]
ws:[4.5568838834762575, 10.337755060195922]
memory len:10000
memory used:2690.0
now epsilon is 0.23974521355007414, the reward is 247.25 with loss [24.968163013458252, 26.680368900299072] in episode 1004
Report: 
rewardSum:247.25
loss:[24.968163013458252, 26.680368900299072]
policies:[0, 3, 1]
qAverage:[0.0, 76.23320388793945]
ws:[4.831210196018219, 11.075141072273254]
memory len:10000
memory used:2690.0
now epsilon is 0.239505558225996, the reward is 59.6875 with loss [27.760682582855225, 22.20103883743286] in episode 1005
Report: 
rewardSum:59.6875
loss:[27.760682582855225, 22.20103883743286]
policies:[0, 3, 1]
qAverage:[0.0, 73.60843849182129]
ws:[4.4431984424591064, 8.107459545135498]
memory len:10000
memory used:2690.0
now epsilon is 0.23914652435028644, the reward is 245.25 with loss [22.484988689422607, 48.09983253479004] in episode 1006
Report: 
rewardSum:245.25
loss:[22.484988689422607, 48.09983253479004]
policies:[1, 3, 2]
qAverage:[0.0, 69.62986501057942]
ws:[2.633792241414388, 5.0285162925720215]
memory len:10000
memory used:2690.0
now epsilon is 0.23890746749093705, the reward is 247.25 with loss [21.455291032791138, 22.413525104522705] in episode 1007
Report: 
rewardSum:247.25
loss:[21.455291032791138, 22.413525104522705]
policies:[0, 3, 1]
qAverage:[0.0, 81.08906364440918]
ws:[2.849322736263275, 7.553557634353638]
memory len:10000
memory used:2690.0
now epsilon is 0.23866864959881567, the reward is 247.25 with loss [14.78120493888855, 19.73504114151001] in episode 1008
Report: 
rewardSum:247.25
loss:[14.78120493888855, 19.73504114151001]
policies:[1, 3, 0]
qAverage:[0.0, 73.92941093444824]
ws:[2.0149101316928864, 4.270742893218994]
memory len:10000
memory used:2690.0
now epsilon is 0.23843007043504463, the reward is 247.25 with loss [32.93162250518799, 24.373437881469727] in episode 1009
Report: 
rewardSum:247.25
loss:[32.93162250518799, 24.373437881469727]
policies:[0, 4, 0]
qAverage:[0.0, 75.84635353088379]
ws:[4.888136088848114, 10.430230975151062]
memory len:10000
memory used:2690.0
now epsilon is 0.23813218182854484, the reward is 246.25 with loss [29.326568841934204, 25.973631381988525] in episode 1010
Report: 
rewardSum:246.25
loss:[29.326568841934204, 25.973631381988525]
policies:[0, 3, 2]
qAverage:[0.0, 48.27529525756836]
ws:[7.244074821472168, 11.029881477355957]
memory len:10000
memory used:2690.0
now epsilon is 0.23777520673032015, the reward is 245.25 with loss [35.718618392944336, 48.56275177001953] in episode 1011
Report: 
rewardSum:245.25
loss:[35.718618392944336, 48.56275177001953]
policies:[1, 4, 1]
qAverage:[0.0, 65.8939921061198]
ws:[8.76177183787028, 13.19214948018392]
memory len:10000
memory used:2690.0
now epsilon is 0.23753752067443235, the reward is 247.25 with loss [32.40928506851196, 23.497291564941406] in episode 1012
Report: 
rewardSum:247.25
loss:[32.40928506851196, 23.497291564941406]
policies:[1, 3, 0]
qAverage:[0.0, 79.04218864440918]
ws:[2.737528920173645, 7.220158219337463]
memory len:10000
memory used:2690.0
now epsilon is 0.23712214165137713, the reward is 244.25 with loss [27.03450846672058, 52.69940519332886] in episode 1013
Report: 
rewardSum:244.25
loss:[27.03450846672058, 52.69940519332886]
policies:[0, 5, 2]
qAverage:[0.0, 79.78362579345703]
ws:[3.9428537487983704, 8.639767360687255]
memory len:10000
memory used:2690.0
now epsilon is 0.2367666806668211, the reward is 245.25 with loss [48.2894344329834, 47.63996934890747] in episode 1014
Report: 
rewardSum:245.25
loss:[48.2894344329834, 47.63996934890747]
policies:[0, 4, 2]
qAverage:[0.0, 79.85711364746093]
ws:[2.698580431938171, 6.101239490509033]
memory len:10000
memory used:2689.0
now epsilon is 0.2365300027588626, the reward is 247.25 with loss [33.4861216545105, 10.78572678565979] in episode 1015
Report: 
rewardSum:247.25
loss:[33.4861216545105, 10.78572678565979]
policies:[0, 4, 0]
qAverage:[0.0, 81.16482086181641]
ws:[2.9321271181106567, 6.367336082458496]
memory len:10000
memory used:2689.0
now epsilon is 0.23617542942770017, the reward is 245.25 with loss [66.9529914855957, 45.73064994812012] in episode 1016
Report: 
rewardSum:245.25
loss:[66.9529914855957, 45.73064994812012]
policies:[4, 1, 1]
qAverage:[0.0, 53.341529846191406]
ws:[1.856447696685791, 3.521392822265625]
memory len:10000
memory used:2690.0
now epsilon is 0.2359393425492985, the reward is 247.25 with loss [15.652766466140747, 32.30386781692505] in episode 1017
Report: 
rewardSum:247.25
loss:[15.652766466140747, 32.30386781692505]
policies:[1, 2, 1]
qAverage:[0.0, 71.79844156901042]
ws:[1.7344090938568115, 6.651243209838867]
memory len:10000
memory used:2690.0
now epsilon is 0.2357034916692574, the reward is 247.25 with loss [16.1272075176239, 25.413021564483643] in episode 1018
Report: 
rewardSum:247.25
loss:[16.1272075176239, 25.413021564483643]
policies:[0, 4, 0]
qAverage:[0.0, 77.77005195617676]
ws:[0.5216308683156967, 4.968166679143906]
memory len:10000
memory used:2689.0
now epsilon is 0.23535015733013345, the reward is 245.25 with loss [28.008414268493652, 31.302236557006836] in episode 1019
Report: 
rewardSum:245.25
loss:[28.008414268493652, 31.302236557006836]
policies:[0, 5, 1]
qAverage:[0.0, 82.03909912109376]
ws:[1.9002888202667236, 7.1115914836525915]
memory len:10000
memory used:2689.0
now epsilon is 0.2350561166905503, the reward is 246.25 with loss [27.075971603393555, 28.904207229614258] in episode 1020
Report: 
rewardSum:246.25
loss:[27.075971603393555, 28.904207229614258]
policies:[1, 2, 2]
qAverage:[0.0, 71.23532104492188]
ws:[8.242081642150879, 16.72315279642741]
memory len:10000
memory used:2689.0
now epsilon is 0.23482114870521348, the reward is 247.25 with loss [18.6700758934021, 14.945125460624695] in episode 1021
Report: 
rewardSum:247.25
loss:[18.6700758934021, 14.945125460624695]
policies:[0, 4, 0]
qAverage:[0.0, 62.348523457845054]
ws:[3.7756905555725098, 5.785412311553955]
memory len:10000
memory used:2689.0
now epsilon is 0.23458641559976362, the reward is 247.25 with loss [29.21069097518921, 27.828270196914673] in episode 1022
Report: 
rewardSum:247.25
loss:[29.21069097518921, 27.828270196914673]
policies:[0, 4, 0]
qAverage:[0.0, 70.34869384765625]
ws:[2.8841499487559, 9.46461280186971]
memory len:10000
memory used:2689.0
now epsilon is 0.234351917139409, the reward is 247.25 with loss [27.64380121231079, 29.12621307373047] in episode 1023
Report: 
rewardSum:247.25
loss:[27.64380121231079, 29.12621307373047]
policies:[0, 4, 0]
qAverage:[0.0, 83.74019317626953]
ws:[1.4829042315483094, 6.704171895980835]
memory len:10000
memory used:2690.0
now epsilon is 0.23411765308959245, the reward is 247.25 with loss [28.10614448785782, 19.757541179656982] in episode 1024
Report: 
rewardSum:247.25
loss:[28.10614448785782, 19.757541179656982]
policies:[0, 3, 1]
qAverage:[0.0, 73.9367167154948]
ws:[2.821494181950887, 9.210663795471191]
memory len:10000
memory used:2690.0
now epsilon is 0.23382515231018738, the reward is 246.25 with loss [22.19614267349243, 36.69336795806885] in episode 1025
Report: 
rewardSum:246.25
loss:[22.19614267349243, 36.69336795806885]
policies:[1, 3, 1]
qAverage:[0.0, 73.41241073608398]
ws:[4.377068042755127, 7.699037492275238]
memory len:10000
memory used:2690.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2*		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.2335914148276962, the reward is 59.6875 with loss [13.260891914367676, 30.592666149139404] in episode 1026
Report: 
rewardSum:59.6875
loss:[13.260891914367676, 30.592666149139404]
policies:[0, 2, 2]
qAverage:[0.0, 65.4504903157552]
ws:[1.5229313373565674, 2.9540518124898276]
memory len:10000
memory used:2690.0
now epsilon is 0.23306635941854353, the reward is 242.25 with loss [52.7972891330719, 60.695772647857666] in episode 1027
Report: 
rewardSum:242.25
loss:[52.7972891330719, 60.695772647857666]
policies:[0, 5, 4]
qAverage:[0.0, 76.99214363098145]
ws:[2.788680136203766, 4.042599081993103]
memory len:10000
memory used:2690.0
now epsilon is 0.23283338044444404, the reward is 247.25 with loss [26.20992422103882, 30.06957197189331] in episode 1028
Report: 
rewardSum:247.25
loss:[26.20992422103882, 30.06957197189331]
policies:[0, 4, 0]
qAverage:[0.0, 78.67067337036133]
ws:[1.6642729938030243, 5.590838432312012]
memory len:10000
memory used:2690.0
now epsilon is 0.2326006343619661, the reward is 247.25 with loss [18.121013641357422, 26.161386966705322] in episode 1029
Report: 
rewardSum:247.25
loss:[18.121013641357422, 26.161386966705322]
policies:[1, 3, 0]
qAverage:[0.0, 77.88728904724121]
ws:[1.5523785650730133, 5.9144288301467896]
memory len:10000
memory used:2690.0
now epsilon is 0.23236812093830542, the reward is 247.25 with loss [38.61824655532837, 22.41100299358368] in episode 1030
Report: 
rewardSum:247.25
loss:[38.61824655532837, 22.41100299358368]
policies:[0, 2, 2]
qAverage:[0.0, 49.07182312011719]
ws:[5.522480487823486, 9.1495361328125]
memory len:10000
memory used:2690.0
now epsilon is 0.23213583994089038, the reward is 247.25 with loss [18.311771631240845, 26.18692922592163] in episode 1031
Report: 
rewardSum:247.25
loss:[18.311771631240845, 26.18692922592163]
policies:[1, 2, 1]
qAverage:[0.0, 66.25693003336589]
ws:[1.5333250363667805, 3.9003685315450034]
memory len:10000
memory used:2690.0
now epsilon is 0.23190379113738188, the reward is 247.25 with loss [25.684929847717285, 26.960301399230957] in episode 1032
Report: 
rewardSum:247.25
loss:[25.684929847717285, 26.960301399230957]
policies:[0, 3, 1]
qAverage:[0.0, 79.9034652709961]
ws:[1.8013050258159637, 6.410827815532684]
memory len:10000
memory used:2690.0
now epsilon is 0.23167197429567313, the reward is 247.25 with loss [18.283994674682617, 19.901443123817444] in episode 1033
Report: 
rewardSum:247.25
loss:[18.283994674682617, 19.901443123817444]
policies:[0, 3, 1]
qAverage:[0.0, 80.60194396972656]
ws:[2.452911674976349, 6.930094122886658]
memory len:10000
memory used:2690.0
now epsilon is 0.2313246834543216, the reward is 245.25 with loss [41.44401168823242, 30.72095775604248] in episode 1034
Report: 
rewardSum:245.25
loss:[41.44401168823242, 30.72095775604248]
policies:[1, 4, 1]
qAverage:[0.0, 79.85820388793945]
ws:[6.1268534660339355, 11.97591781616211]
memory len:10000
memory used:2690.0
now epsilon is 0.23109344550316668, the reward is 247.25 with loss [24.471724033355713, 23.646245002746582] in episode 1035
Report: 
rewardSum:247.25
loss:[24.471724033355713, 23.646245002746582]
policies:[1, 3, 0]
qAverage:[0.0, 70.1151606241862]
ws:[2.7370174725850425, 7.842596610387166]
memory len:10000
memory used:2691.0
now epsilon is 0.2308624387032632, the reward is 247.25 with loss [32.7801775932312, 28.389501571655273] in episode 1036
Report: 
rewardSum:247.25
loss:[32.7801775932312, 28.389501571655273]
policies:[0, 3, 1]
qAverage:[0.0, 81.62621307373047]
ws:[2.999397397041321, 8.237611889839172]
memory len:10000
memory used:2690.0
now epsilon is 0.230458732316262, the reward is 244.25 with loss [51.98921823501587, 45.928221225738525] in episode 1037
Report: 
rewardSum:244.25
loss:[51.98921823501587, 45.928221225738525]
policies:[1, 2, 4]
qAverage:[0.0, 66.04748280843098]
ws:[1.7419131994247437, 3.7800474961598716]
memory len:10000
memory used:2690.0
now epsilon is 0.2302283599915676, the reward is 247.25 with loss [22.599788188934326, 21.254033088684082] in episode 1038
Report: 
rewardSum:247.25
loss:[22.599788188934326, 21.254033088684082]
policies:[1, 3, 0]
qAverage:[0.0, 75.14887237548828]
ws:[0.9749029674567282, 2.9003288447856903]
memory len:10000
memory used:2690.0
now epsilon is 0.2299982179528227, the reward is 247.25 with loss [18.18623685836792, 24.387829899787903] in episode 1039
Report: 
rewardSum:247.25
loss:[18.18623685836792, 24.387829899787903]
policies:[0, 3, 1]
qAverage:[0.0, 75.58087158203125]
ws:[2.694870710372925, 8.893421649932861]
memory len:10000
memory used:2690.0
now epsilon is 0.22959602281831754, the reward is 244.25 with loss [38.93958520889282, 49.36472272872925] in episode 1040
Report: 
rewardSum:244.25
loss:[38.93958520889282, 49.36472272872925]
policies:[2, 2, 3]
qAverage:[0.0, 72.21832784016927]
ws:[6.889980951944987, 14.228475570678711]
memory len:10000
memory used:2690.0
now epsilon is 0.2292518439586262, the reward is 245.25 with loss [37.44556796550751, 34.462584018707275] in episode 1041
Report: 
rewardSum:245.25
loss:[37.44556796550751, 34.462584018707275]
policies:[0, 5, 1]
qAverage:[0.0, 83.88089752197266]
ws:[3.741213619709015, 7.60294234752655]
memory len:10000
memory used:2692.0
now epsilon is 0.22902267806978174, the reward is 247.25 with loss [33.493475914001465, 19.54999303817749] in episode 1042
Report: 
rewardSum:247.25
loss:[33.493475914001465, 19.54999303817749]
policies:[0, 3, 1]
qAverage:[0.0, 74.30313491821289]
ws:[2.7635417580604553, 4.188603639602661]
memory len:10000
memory used:2692.0
now epsilon is 0.22867935868988165, the reward is 57.6875 with loss [34.217002391815186, 34.3699517250061] in episode 1043
Report: 
rewardSum:57.6875
loss:[34.217002391815186, 34.3699517250061]
policies:[0, 4, 2]
qAverage:[0.0, 79.89901123046874]
ws:[1.8861692070960998, 4.516099643707276]
memory len:10000
memory used:2692.0
now epsilon is 0.22845076507165973, the reward is 247.25 with loss [24.11648201942444, 16.60765790939331] in episode 1044
Report: 
rewardSum:247.25
loss:[24.11648201942444, 16.60765790939331]
policies:[0, 4, 0]
qAverage:[0.0, 73.14523887634277]
ws:[1.379193008877337, 4.155488610267639]
memory len:10000
memory used:2692.0
now epsilon is 0.22822239996134774, the reward is 247.25 with loss [29.39334487915039, 21.483630895614624] in episode 1045
Report: 
rewardSum:247.25
loss:[29.39334487915039, 21.483630895614624]
policies:[0, 3, 1]
qAverage:[0.0, 72.90396690368652]
ws:[4.448241680860519, 8.21456891298294]
memory len:10000
memory used:2692.0
now epsilon is 0.2278802802485996, the reward is 245.25 with loss [30.0021390914917, 37.72696924209595] in episode 1046
Report: 
rewardSum:245.25
loss:[30.0021390914917, 37.72696924209595]
policies:[0, 5, 1]
qAverage:[0.0, 82.57771148681641]
ws:[2.638494944572449, 7.822771739959717]
memory len:10000
memory used:2692.0
now epsilon is 0.22765248540921448, the reward is 247.25 with loss [22.533504486083984, 18.551448702812195] in episode 1047
Report: 
rewardSum:247.25
loss:[22.533504486083984, 18.551448702812195]
policies:[1, 2, 1]
qAverage:[0.0, 62.497266133626304]
ws:[4.244476238886516, 7.856481234232585]
memory len:10000
memory used:2692.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20*		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.2273112200341777, the reward is 245.25 with loss [31.081788420677185, 25.395547747612] in episode 1048
Report: 
rewardSum:245.25
loss:[31.081788420677185, 25.395547747612]
policies:[0, 5, 1]
qAverage:[0.0, 85.3509890238444]
ws:[3.010580817858378, 8.521164655685425]
memory len:10000
memory used:2691.0
now epsilon is 0.2269704662373738, the reward is 245.25 with loss [41.30281662940979, 50.0681676864624] in episode 1049
Report: 
rewardSum:245.25
loss:[41.30281662940979, 50.0681676864624]
policies:[0, 5, 1]
qAverage:[0.0, 82.75434722900391]
ws:[1.0951851844787597, 5.92740273475647]
memory len:10000
memory used:2691.0
now epsilon is 0.22685699518990926, the reward is -1.0 with loss [10.976199626922607, 12.551047325134277] in episode 1050
Report: 
rewardSum:-1.0
loss:[10.976199626922607, 12.551047325134277]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:2691.0
now epsilon is 0.2265169223046779, the reward is 245.25 with loss [29.073925495147705, 47.130027770996094] in episode 1051
Report: 
rewardSum:245.25
loss:[29.073925495147705, 47.130027770996094]
policies:[0, 2, 4]
qAverage:[0.0, 68.84088134765625]
ws:[2.0911617974440255, 8.013322512308756]
memory len:10000
memory used:2691.0
now epsilon is 0.2262904903120627, the reward is 247.25 with loss [20.22090196609497, 22.766682624816895] in episode 1052
Report: 
rewardSum:247.25
loss:[20.22090196609497, 22.766682624816895]
policies:[0, 3, 1]
qAverage:[0.0, 77.84057235717773]
ws:[3.007818251848221, 7.6346206068992615]
memory len:10000
memory used:2691.0
now epsilon is 0.22606428466654221, the reward is 247.25 with loss [21.548331260681152, 27.14845848083496] in episode 1053
Report: 
rewardSum:247.25
loss:[21.548331260681152, 27.14845848083496]
policies:[0, 3, 1]
qAverage:[0.0, 63.85197194417318]
ws:[1.8574878374735515, 3.490506966908773]
memory len:10000
memory used:2691.0
now epsilon is 0.2258383051418543, the reward is 247.25 with loss [28.518193244934082, 19.062018036842346] in episode 1054
Report: 
rewardSum:247.25
loss:[28.518193244934082, 19.062018036842346]
policies:[0, 4, 0]
qAverage:[0.0, 82.66623077392578]
ws:[2.246316361427307, 6.198355674743652]
memory len:10000
memory used:2690.0
now epsilon is 0.2254997593369914, the reward is 245.25 with loss [37.01198172569275, 30.77716636657715] in episode 1055
Report: 
rewardSum:245.25
loss:[37.01198172569275, 30.77716636657715]
policies:[1, 4, 1]
qAverage:[0.0, 75.70451545715332]
ws:[1.7519813105463982, 5.592446208000183]
memory len:10000
memory used:2691.0
now epsilon is 0.22527434412597133, the reward is 247.25 with loss [16.39124619960785, 22.096789836883545] in episode 1056
Report: 
rewardSum:247.25
loss:[16.39124619960785, 22.096789836883545]
policies:[0, 4, 0]
qAverage:[0.0, 70.48059336344402]
ws:[2.4836488564809165, 7.6406175295511884]
memory len:10000
memory used:2691.0
now epsilon is 0.22476798342340054, the reward is 242.25 with loss [68.25708293914795, 60.007206201553345] in episode 1057
Report: 
rewardSum:242.25
loss:[68.25708293914795, 60.007206201553345]
policies:[0, 4, 5]
qAverage:[0.0, 80.7294692993164]
ws:[2.9817835330963134, 8.047161436080932]
memory len:10000
memory used:2692.0
now epsilon is 0.22454329971392384, the reward is 59.6875 with loss [22.77819061279297, 33.72265577316284] in episode 1058
Report: 
rewardSum:59.6875
loss:[22.77819061279297, 33.72265577316284]
policies:[0, 3, 1]
qAverage:[0.0, 74.37057495117188]
ws:[2.1721772253513336, 4.682681143283844]
memory len:10000
memory used:2692.0
now epsilon is 0.22431884060391424, the reward is 247.25 with loss [34.694721698760986, 22.784366369247437] in episode 1059
Report: 
rewardSum:247.25
loss:[34.694721698760986, 22.784366369247437]
policies:[0, 4, 0]
qAverage:[0.0, 81.0475845336914]
ws:[4.432084310054779, 10.868893814086913]
memory len:10000
memory used:2692.0
now epsilon is 0.223982572571835, the reward is 245.25 with loss [44.10712957382202, 27.649914979934692] in episode 1060
Report: 
rewardSum:245.25
loss:[44.10712957382202, 27.649914979934692]
policies:[0, 5, 1]
qAverage:[0.0, 81.59092407226562]
ws:[2.737983226776123, 8.192227554321288]
memory len:10000
memory used:2692.0
now epsilon is 0.22375867397872984, the reward is 59.6875 with loss [20.94791316986084, 26.976511478424072] in episode 1061
Report: 
rewardSum:59.6875
loss:[20.94791316986084, 26.976511478424072]
policies:[1, 2, 1]
qAverage:[0.0, 62.032440185546875]
ws:[3.7862468709548316, 7.270720998446147]
memory len:10000
memory used:2692.0
now epsilon is 0.22331154801272424, the reward is 243.25 with loss [35.058789134025574, 33.780417680740356] in episode 1062
Report: 
rewardSum:243.25
loss:[35.058789134025574, 33.780417680740356]
policies:[1, 5, 2]
qAverage:[0.0, 67.37982432047527]
ws:[1.9257595787445705, 8.286913633346558]
memory len:10000
memory used:2691.0
now epsilon is 0.22308832019258593, the reward is 247.25 with loss [20.777134895324707, 23.206071376800537] in episode 1063
Report: 
rewardSum:247.25
loss:[20.777134895324707, 23.206071376800537]
policies:[2, 2, 0]
qAverage:[0.0, 69.19566853841145]
ws:[1.137092153231303, 7.137738585472107]
memory len:10000
memory used:2691.0
now epsilon is 0.22286531551657127, the reward is 247.25 with loss [25.259060859680176, 30.908875703811646] in episode 1064
Report: 
rewardSum:247.25
loss:[25.259060859680176, 30.908875703811646]
policies:[2, 1, 1]
qAverage:[0.0, 44.357845306396484]
ws:[-0.40057459473609924, 1.2762707471847534]
memory len:10000
memory used:2697.0
now epsilon is 0.22253122640989742, the reward is 245.25 with loss [34.23963975906372, 32.06606721878052] in episode 1065
Report: 
rewardSum:245.25
loss:[34.23963975906372, 32.06606721878052]
policies:[0, 5, 1]
qAverage:[0.0, 76.11936950683594]
ws:[0.7832493185997009, 6.225053906440735]
memory len:10000
memory used:2697.0
now epsilon is 0.22230877861879011, the reward is 247.25 with loss [19.10919713973999, 14.938468217849731] in episode 1066
Report: 
rewardSum:247.25
loss:[19.10919713973999, 14.938468217849731]
policies:[1, 2, 1]
qAverage:[0.0, 62.10754140218099]
ws:[3.2706182400385537, 7.056833108266194]
memory len:10000
memory used:2697.0
now epsilon is 0.22208655319206994, the reward is 247.25 with loss [17.322784423828125, 30.42957639694214] in episode 1067
Report: 
rewardSum:247.25
loss:[17.322784423828125, 30.42957639694214]
policies:[0, 2, 2]
qAverage:[0.0, 44.218597412109375]
ws:[0.5445401668548584, 1.8565629720687866]
memory len:10000
memory used:2697.0
now epsilon is 0.2219755237958835, the reward is -1.0 with loss [7.651050329208374, 20.616402626037598] in episode 1068
Report: 
rewardSum:-1.0
loss:[7.651050329208374, 20.616402626037598]
policies:[0, 1, 1]
qAverage:[0.0, 44.72027587890625]
ws:[1.2397416830062866, 1.9207504987716675]
memory len:10000
memory used:2697.0
now epsilon is 0.22169819309116168, the reward is 246.25 with loss [23.288995504379272, 26.83984351158142] in episode 1069
Report: 
rewardSum:246.25
loss:[23.288995504379272, 26.83984351158142]
policies:[1, 2, 2]
qAverage:[0.0, 52.45659637451172]
ws:[2.1647539138793945, 4.193084716796875]
memory len:10000
memory used:2697.0
now epsilon is 0.22147657802103768, the reward is 59.6875 with loss [26.953657627105713, 16.160677194595337] in episode 1070
Report: 
rewardSum:59.6875
loss:[26.953657627105713, 16.160677194595337]
policies:[0, 2, 2]
qAverage:[0.0, 66.43287404378255]
ws:[1.3879101673762004, 2.8691730896631875]
memory len:10000
memory used:2697.0
now epsilon is 0.22125518448289203, the reward is 247.25 with loss [38.004762172698975, 22.615552186965942] in episode 1071
Report: 
rewardSum:247.25
loss:[38.004762172698975, 22.615552186965942]
policies:[1, 3, 0]
qAverage:[0.0, 67.64435577392578]
ws:[2.6594698429107666, 4.62812344233195]
memory len:10000
memory used:2697.0
############# STATE ###############
0-		8-		16*		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.2209235090637739, the reward is 245.25 with loss [33.721564054489136, 37.885289430618286] in episode 1072
Report: 
rewardSum:245.25
loss:[33.721564054489136, 37.885289430618286]
policies:[1, 4, 1]
qAverage:[0.0, 81.54104614257812]
ws:[3.8973005533218386, 8.55961651802063]
memory len:10000
memory used:2697.0
now epsilon is 0.22070266838721916, the reward is 247.25 with loss [24.288727283477783, 27.88703680038452] in episode 1073
Report: 
rewardSum:247.25
loss:[24.288727283477783, 27.88703680038452]
policies:[0, 4, 0]
qAverage:[0.0, 83.04413146972657]
ws:[4.058582735061646, 8.716395950317382]
memory len:10000
memory used:2697.0
now epsilon is 0.22048204846853958, the reward is 247.25 with loss [25.466956615447998, 32.34842324256897] in episode 1074
Report: 
rewardSum:247.25
loss:[25.466956615447998, 32.34842324256897]
policies:[0, 4, 0]
qAverage:[0.0, 80.48216094970704]
ws:[4.542402696609497, 8.915268039703369]
memory len:10000
memory used:2697.0
now epsilon is 0.22026164908706, the reward is 59.6875 with loss [21.25808596611023, 28.721553325653076] in episode 1075
Report: 
rewardSum:59.6875
loss:[21.25808596611023, 28.721553325653076]
policies:[0, 3, 1]
qAverage:[0.0, 62.771593729654946]
ws:[2.077040354410807, 3.055750052134196]
memory len:10000
memory used:2697.0
now epsilon is 0.2200414700223259, the reward is 247.25 with loss [18.211681604385376, 19.1567120552063] in episode 1076
Report: 
rewardSum:247.25
loss:[18.211681604385376, 19.1567120552063]
policies:[1, 3, 0]
qAverage:[0.0, 78.56355476379395]
ws:[5.77517956495285, 12.879894495010376]
memory len:10000
memory used:2697.0
now epsilon is 0.21971161403742054, the reward is 245.25 with loss [38.460906982421875, 36.628583908081055] in episode 1077
Report: 
rewardSum:245.25
loss:[38.460906982421875, 36.628583908081055]
policies:[1, 4, 1]
qAverage:[0.0, 76.96674346923828]
ws:[2.5770286917686462, 5.572222709655762]
memory len:10000
memory used:2697.0
now epsilon is 0.21949198480150728, the reward is 247.25 with loss [20.485112190246582, 16.6701899766922] in episode 1078
Report: 
rewardSum:247.25
loss:[20.485112190246582, 16.6701899766922]
policies:[1, 1, 2]
qAverage:[0.0, 57.581966400146484]
ws:[3.938210964202881, 11.769421577453613]
memory len:10000
memory used:2697.0
now epsilon is 0.2192725751124827, the reward is 247.25 with loss [33.82288360595703, 21.74657702445984] in episode 1079
Report: 
rewardSum:247.25
loss:[33.82288360595703, 21.74657702445984]
policies:[1, 1, 2]
qAverage:[0.0, 52.586517333984375]
ws:[1.8166706562042236, 3.978754758834839]
memory len:10000
memory used:2697.0
now epsilon is 0.2191629525294624, the reward is -1.0 with loss [14.861623048782349, 11.864801406860352] in episode 1080
Report: 
rewardSum:-1.0
loss:[14.861623048782349, 11.864801406860352]
policies:[1, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:2697.0
now epsilon is 0.2188344134974607, the reward is 245.25 with loss [36.33361506462097, 35.60525870323181] in episode 1081
Report: 
rewardSum:245.25
loss:[36.33361506462097, 35.60525870323181]
policies:[2, 2, 2]
qAverage:[0.0, 67.83553822835286]
ws:[3.545848568280538, 10.634915510813395]
memory len:10000
memory used:2697.0
now epsilon is 0.218615661133192, the reward is 247.25 with loss [27.611496448516846, 30.108290910720825] in episode 1082
Report: 
rewardSum:247.25
loss:[27.611496448516846, 30.108290910720825]
policies:[0, 4, 0]
qAverage:[0.0, 80.89200286865234]
ws:[5.16750054359436, 12.354351568222047]
memory len:10000
memory used:2698.0
now epsilon is 0.21839712743926915, the reward is 247.25 with loss [21.321659564971924, 19.59777808189392] in episode 1083
Report: 
rewardSum:247.25
loss:[21.321659564971924, 19.59777808189392]
policies:[2, 1, 1]
qAverage:[0.0, 49.170982360839844]
ws:[6.202793598175049, 10.767128944396973]
memory len:10000
memory used:2698.0
now epsilon is 0.21817881219710375, the reward is 59.6875 with loss [16.683780193328857, 23.64549446105957] in episode 1084
Report: 
rewardSum:59.6875
loss:[16.683780193328857, 23.64549446105957]
policies:[0, 2, 2]
qAverage:[0.0, 61.94692738850912]
ws:[0.8798922200997671, 2.347622871398926]
memory len:10000
memory used:2698.0
now epsilon is 0.21796071518832594, the reward is 247.25 with loss [25.786426067352295, 22.60196304321289] in episode 1085
Report: 
rewardSum:247.25
loss:[25.786426067352295, 22.60196304321289]
policies:[0, 3, 1]
qAverage:[0.0, 70.95582326253255]
ws:[6.051680246988933, 13.704868952433268]
memory len:10000
memory used:2698.0
now epsilon is 0.21774283619478413, the reward is 247.25 with loss [21.747032165527344, 17.26695227622986] in episode 1086
Report: 
rewardSum:247.25
loss:[21.747032165527344, 17.26695227622986]
policies:[0, 4, 0]
qAverage:[0.0, 81.52715911865235]
ws:[1.6508651494979858, 6.387898457050324]
memory len:10000
memory used:2698.0
now epsilon is 0.217416426006369, the reward is 245.25 with loss [40.224183559417725, 47.59601926803589] in episode 1087
Report: 
rewardSum:245.25
loss:[40.224183559417725, 47.59601926803589]
policies:[0, 3, 3]
qAverage:[0.0, 73.20541000366211]
ws:[3.357225403189659, 6.7826007306575775]
memory len:10000
memory used:2698.0
now epsilon is 0.21709050512732897, the reward is 245.25 with loss [35.13440179824829, 30.36014634370804] in episode 1088
Report: 
rewardSum:245.25
loss:[35.13440179824829, 30.36014634370804]
policies:[1, 3, 2]
qAverage:[0.0, 69.74184608459473]
ws:[3.5551579892635345, 5.741830766201019]
memory len:10000
memory used:2698.0
now epsilon is 0.21660253965960463, the reward is 242.25 with loss [67.97866451740265, 67.72934055328369] in episode 1089
Report: 
rewardSum:242.25
loss:[67.97866451740265, 67.72934055328369]
policies:[0, 4, 5]
qAverage:[0.0, 79.16414260864258]
ws:[2.7145479321479797, 7.214655518531799]
memory len:10000
memory used:2698.0
now epsilon is 0.21638601833236062, the reward is 59.6875 with loss [35.10327434539795, 26.900490760803223] in episode 1090
Report: 
rewardSum:59.6875
loss:[35.10327434539795, 26.900490760803223]
policies:[3, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:2698.0
now epsilon is 0.2161697134452619, the reward is 247.25 with loss [47.36508560180664, 34.25636339187622] in episode 1091
Report: 
rewardSum:247.25
loss:[47.36508560180664, 34.25636339187622]
policies:[0, 4, 0]
qAverage:[0.0, 80.0786636352539]
ws:[0.46839935779571534, 3.6372628211975098]
memory len:10000
memory used:2698.0
now epsilon is 0.21595362478194943, the reward is 247.25 with loss [14.063982963562012, 18.87772560119629] in episode 1092
Report: 
rewardSum:247.25
loss:[14.063982963562012, 18.87772560119629]
policies:[0, 3, 1]
qAverage:[0.0, 67.29439544677734]
ws:[0.379682977994283, 4.747524817784627]
memory len:10000
memory used:2698.0
now epsilon is 0.215683817688249, the reward is 246.25 with loss [23.790207147598267, 19.290775537490845] in episode 1093
Report: 
rewardSum:246.25
loss:[23.790207147598267, 19.290775537490845]
policies:[0, 3, 2]
qAverage:[0.0, 57.371421813964844]
ws:[2.223818302154541, 8.003138542175293]
memory len:10000
memory used:2698.0
now epsilon is 0.21546821473851302, the reward is 247.25 with loss [42.726210594177246, 15.596903085708618] in episode 1094
Report: 
rewardSum:247.25
loss:[42.726210594177246, 15.596903085708618]
policies:[0, 3, 1]
qAverage:[0.0, 75.96549415588379]
ws:[1.6276860013604164, 5.803378880023956]
memory len:10000
memory used:2698.0
now epsilon is 0.21525282731088913, the reward is 247.25 with loss [21.39749526977539, 19.782500743865967] in episode 1095
Report: 
rewardSum:247.25
loss:[21.39749526977539, 19.782500743865967]
policies:[1, 2, 1]
qAverage:[0.0, 68.89909108479817]
ws:[2.240885098775228, 6.8297756512959795]
memory len:10000
memory used:2698.0
now epsilon is 0.21503765518993606, the reward is 247.25 with loss [36.985536098480225, 20.017784118652344] in episode 1096
Report: 
rewardSum:247.25
loss:[36.985536098480225, 20.017784118652344]
policies:[0, 3, 1]
qAverage:[0.0, 72.52723693847656]
ws:[3.682864263653755, 6.8363431096076965]
memory len:10000
memory used:2697.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10*		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.21482269816042784, the reward is 247.25 with loss [42.01395893096924, 20.916848182678223] in episode 1097
Report: 
rewardSum:247.25
loss:[42.01395893096924, 20.916848182678223]
policies:[2, 1, 1]
qAverage:[0.0, 46.9786376953125]
ws:[5.885291576385498, 9.268954277038574]
memory len:10000
memory used:2697.0
now epsilon is 0.21460795600735366, the reward is 247.25 with loss [16.86676263809204, 25.770654916763306] in episode 1098
Report: 
rewardSum:247.25
loss:[16.86676263809204, 25.770654916763306]
policies:[1, 3, 0]
qAverage:[0.0, 69.9800853729248]
ws:[4.059279948472977, 7.0121360421180725]
memory len:10000
memory used:2697.0
now epsilon is 0.21439342851591767, the reward is 247.25 with loss [32.91594886779785, 21.586566925048828] in episode 1099
Report: 
rewardSum:247.25
loss:[32.91594886779785, 21.586566925048828]
policies:[1, 3, 0]
qAverage:[0.0, 68.72495079040527]
ws:[3.4712981581687927, 6.45434895157814]
memory len:10000
memory used:2697.0
now epsilon is 0.2141791154715387, the reward is 247.25 with loss [21.656224727630615, 17.423895359039307] in episode 1100
Report: 
rewardSum:247.25
loss:[21.656224727630615, 17.423895359039307]
policies:[0, 3, 1]
qAverage:[0.0, 74.68046760559082]
ws:[3.9777227640151978, 9.506203413009644]
memory len:10000
memory used:2698.0
now epsilon is 0.21385804752433377, the reward is 245.25 with loss [38.66585850715637, 33.98000907897949] in episode 1101
Report: 
rewardSum:245.25
loss:[38.66585850715637, 33.98000907897949]
policies:[1, 3, 2]
qAverage:[0.0, 69.71689097086589]
ws:[2.5420730908711753, 8.43210252126058]
memory len:10000
memory used:2698.0
now epsilon is 0.21375113186669958, the reward is -1.0 with loss [13.065185546875, 4.626669645309448] in episode 1102
Report: 
rewardSum:-1.0
loss:[13.065185546875, 4.626669645309448]
policies:[0, 1, 1]
qAverage:[0.0, 42.185611724853516]
ws:[-0.5446972250938416, 0.34469518065452576]
memory len:10000
memory used:2698.0
now epsilon is 0.21353746087814873, the reward is 247.25 with loss [21.09572124481201, 24.437804222106934] in episode 1103
Report: 
rewardSum:247.25
loss:[21.09572124481201, 24.437804222106934]
policies:[0, 2, 2]
qAverage:[0.0, 71.11310577392578]
ws:[1.1910416682561238, 6.843177159627278]
memory len:10000
memory used:2698.0
now epsilon is 0.2131640504727803, the reward is 56.6875 with loss [52.083165884017944, 60.87329864501953] in episode 1104
Report: 
rewardSum:56.6875
loss:[52.083165884017944, 60.87329864501953]
policies:[0, 3, 4]
qAverage:[0.0, 70.72208976745605]
ws:[1.0463044792413712, 2.9959340691566467]
memory len:10000
memory used:2698.0
now epsilon is 0.21295096634550456, the reward is 247.25 with loss [19.381235122680664, 36.02100372314453] in episode 1105
Report: 
rewardSum:247.25
loss:[19.381235122680664, 36.02100372314453]
policies:[0, 3, 1]
qAverage:[0.0, 69.32735697428386]
ws:[3.268941084543864, 8.015186468760172]
memory len:10000
memory used:2698.0
now epsilon is 0.2126317394709826, the reward is 245.25 with loss [36.91983962059021, 51.901132106781006] in episode 1106
Report: 
rewardSum:245.25
loss:[36.91983962059021, 51.901132106781006]
policies:[0, 3, 3]
qAverage:[0.0, 73.28308296203613]
ws:[1.789674460887909, 5.239311754703522]
memory len:10000
memory used:2697.0
now epsilon is 0.2124191874551253, the reward is 247.25 with loss [20.810590505599976, 26.690503120422363] in episode 1107
Report: 
rewardSum:247.25
loss:[20.810590505599976, 26.690503120422363]
policies:[0, 4, 0]
qAverage:[0.0, 78.00729064941406]
ws:[0.540300065279007, 3.8038447499275208]
memory len:10000
memory used:2697.0
now epsilon is 0.21210075775056236, the reward is 245.25 with loss [40.35218000411987, 38.685105323791504] in episode 1108
Report: 
rewardSum:245.25
loss:[40.35218000411987, 38.685105323791504]
policies:[2, 3, 1]
qAverage:[0.0, 72.56804466247559]
ws:[1.930526152253151, 6.176757872104645]
memory len:10000
memory used:2697.0
now epsilon is 0.2118887365173405, the reward is 247.25 with loss [33.55619835853577, 17.4151132106781] in episode 1109
Report: 
rewardSum:247.25
loss:[33.55619835853577, 17.4151132106781]
policies:[0, 3, 1]
qAverage:[0.0, 56.96550750732422]
ws:[1.4028522968292236, 3.0285096168518066]
memory len:10000
memory used:2697.0
now epsilon is 0.2115711019920522, the reward is 245.25 with loss [30.440300941467285, 27.267523288726807] in episode 1110
Report: 
rewardSum:245.25
loss:[30.440300941467285, 27.267523288726807]
policies:[0, 3, 3]
qAverage:[0.0, 62.68047587076823]
ws:[-0.1694318652153015, 4.746938387552897]
memory len:10000
memory used:2697.0
now epsilon is 0.21135961021600108, the reward is 247.25 with loss [25.305537700653076, 22.150423049926758] in episode 1111
Report: 
rewardSum:247.25
loss:[25.305537700653076, 22.150423049926758]
policies:[0, 4, 0]
qAverage:[0.0, 75.36246185302734]
ws:[-2.0502078771591186, 2.470431888103485]
memory len:10000
memory used:2697.0
now epsilon is 0.21114832985242976, the reward is 247.25 with loss [40.317808628082275, 27.23561954498291] in episode 1112
Report: 
rewardSum:247.25
loss:[40.317808628082275, 27.23561954498291]
policies:[1, 3, 0]
qAverage:[0.0, 70.78244400024414]
ws:[-1.8492076396942139, 2.4948406368494034]
memory len:10000
memory used:2697.0
now epsilon is 0.2109372606900051, the reward is 247.25 with loss [19.3024001121521, 19.70052146911621] in episode 1113
Report: 
rewardSum:247.25
loss:[19.3024001121521, 19.70052146911621]
policies:[1, 3, 0]
qAverage:[0.0, 72.38501167297363]
ws:[-1.5697251111268997, 3.408112306147814]
memory len:10000
memory used:2697.0
now epsilon is 0.2107264025176051, the reward is 247.25 with loss [24.12482213973999, 16.677862405776978] in episode 1114
Report: 
rewardSum:247.25
loss:[24.12482213973999, 16.677862405776978]
policies:[1, 2, 1]
qAverage:[0.0, 66.44944508870442]
ws:[-0.8113479316234589, 5.464322725931804]
memory len:10000
memory used:2697.0
now epsilon is 0.2105157551243189, the reward is 247.25 with loss [35.27849578857422, 20.353797793388367] in episode 1115
Report: 
rewardSum:247.25
loss:[35.27849578857422, 20.353797793388367]
policies:[0, 2, 2]
qAverage:[0.0, 69.07706705729167]
ws:[0.18044654528299967, 6.951479911804199]
memory len:10000
memory used:2697.0
now epsilon is 0.21025274196987148, the reward is 246.25 with loss [40.72141170501709, 29.757583141326904] in episode 1116
Report: 
rewardSum:246.25
loss:[40.72141170501709, 29.757583141326904]
policies:[0, 3, 2]
qAverage:[0.0, 73.14501190185547]
ws:[1.5847558379173279, 9.817893505096436]
memory len:10000
memory used:2697.0
now epsilon is 0.2100425680595399, the reward is 247.25 with loss [24.094179391860962, 17.838530778884888] in episode 1117
Report: 
rewardSum:247.25
loss:[24.094179391860962, 17.838530778884888]
policies:[0, 4, 0]
qAverage:[0.0, 76.54707183837891]
ws:[-0.7580682277679444, 6.491015243530273]
memory len:10000
memory used:2697.0
now epsilon is 0.20983260424431657, the reward is 247.25 with loss [26.025036811828613, 31.538210153579712] in episode 1118
Report: 
rewardSum:247.25
loss:[26.025036811828613, 31.538210153579712]
policies:[0, 4, 0]
qAverage:[0.0, 67.5031967163086]
ws:[0.003475626309712728, 7.533134301503499]
memory len:10000
memory used:2697.0
now epsilon is 0.20957044460160662, the reward is 246.25 with loss [24.126200437545776, 47.50284481048584] in episode 1119
Report: 
rewardSum:246.25
loss:[24.126200437545776, 47.50284481048584]
policies:[2, 2, 1]
qAverage:[0.0, 62.768269856770836]
ws:[1.4340976476669312, 4.457525253295898]
memory len:10000
memory used:2697.0
now epsilon is 0.20915167027736467, the reward is 243.25 with loss [47.331132888793945, 58.38987350463867] in episode 1120
Report: 
rewardSum:243.25
loss:[47.331132888793945, 58.38987350463867]
policies:[0, 6, 2]
qAverage:[0.0, 76.18373761858258]
ws:[1.4296600307737077, 5.112269537789481]
memory len:10000
memory used:2697.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26*		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.20883813878629187, the reward is 245.25 with loss [34.060631275177, 45.663355350494385] in episode 1121
Report: 
rewardSum:245.25
loss:[34.060631275177, 45.663355350494385]
policies:[1, 3, 2]
qAverage:[0.0, 57.26302591959635]
ws:[2.559338092803955, 5.517990579207738]
memory len:10000
memory used:2697.0
now epsilon is 0.20862937894875608, the reward is 247.25 with loss [25.191615104675293, 26.95466947555542] in episode 1122
Report: 
rewardSum:247.25
loss:[25.191615104675293, 26.95466947555542]
policies:[1, 3, 0]
qAverage:[0.0, 69.32548141479492]
ws:[0.7562450394034386, 4.807030983269215]
memory len:10000
memory used:2697.0
now epsilon is 0.20842082779278592, the reward is 247.25 with loss [37.83157801628113, 20.181608200073242] in episode 1123
Report: 
rewardSum:247.25
loss:[37.83157801628113, 20.181608200073242]
policies:[0, 4, 0]
qAverage:[0.0, 70.51886138916015]
ws:[3.092661240696907, 8.427017557621003]
memory len:10000
memory used:2697.0
now epsilon is 0.2082124851097781, the reward is 247.25 with loss [27.982401609420776, 25.289522409439087] in episode 1124
Report: 
rewardSum:247.25
loss:[27.982401609420776, 25.289522409439087]
policies:[1, 3, 0]
qAverage:[0.0, 63.32229868570963]
ws:[1.9772690137227376, 7.71986738840739]
memory len:10000
memory used:2697.0
now epsilon is 0.20800435069133782, the reward is 247.25 with loss [28.945160627365112, 29.06039047241211] in episode 1125
Report: 
rewardSum:247.25
loss:[28.945160627365112, 29.06039047241211]
policies:[0, 4, 0]
qAverage:[0.0, 65.92671394348145]
ws:[1.4864384233951569, 6.3859657645225525]
memory len:10000
memory used:2697.0
now epsilon is 0.20769253910439045, the reward is 245.25 with loss [43.53041648864746, 34.502474784851074] in episode 1126
Report: 
rewardSum:245.25
loss:[43.53041648864746, 34.502474784851074]
policies:[0, 5, 1]
qAverage:[0.0, 74.52454376220703]
ws:[-0.5553379505872726, 5.634121457735698]
memory len:10000
memory used:2697.0
now epsilon is 0.20748492443700828, the reward is 247.25 with loss [35.60303211212158, 26.144481658935547] in episode 1127
Report: 
rewardSum:247.25
loss:[35.60303211212158, 26.144481658935547]
policies:[0, 3, 1]
qAverage:[0.0, 64.0851338704427]
ws:[-3.6130310694376626, 4.940647840499878]
memory len:10000
memory used:2697.0
now epsilon is 0.20727751730645094, the reward is 247.25 with loss [21.34771990776062, 18.71145248413086] in episode 1128
Report: 
rewardSum:247.25
loss:[21.34771990776062, 18.71145248413086]
policies:[0, 4, 0]
qAverage:[0.0, 63.68178176879883]
ws:[-1.4559168219566345, 4.7278952449560165]
memory len:10000
memory used:2697.0
now epsilon is 0.2069667952884017, the reward is 245.25 with loss [33.95908522605896, 36.27662754058838] in episode 1129
Report: 
rewardSum:245.25
loss:[33.95908522605896, 36.27662754058838]
policies:[0, 5, 1]
qAverage:[0.0, 73.46778869628906]
ws:[-1.8054058750470479, 4.117435077826182]
memory len:10000
memory used:2697.0
now epsilon is 0.20675990609272693, the reward is 247.25 with loss [26.155619382858276, 20.823999166488647] in episode 1130
Report: 
rewardSum:247.25
loss:[26.155619382858276, 20.823999166488647]
policies:[0, 4, 0]
qAverage:[0.0, 68.09583892822266]
ws:[0.5390270113945007, 6.720981931686401]
memory len:10000
memory used:2697.0
now epsilon is 0.2065532237086773, the reward is 59.6875 with loss [30.885266304016113, 26.319734811782837] in episode 1131
Report: 
rewardSum:59.6875
loss:[30.885266304016113, 26.319734811782837]
policies:[0, 3, 1]
qAverage:[0.0, 61.34584999084473]
ws:[0.4647420644760132, 5.268542647361755]
memory len:10000
memory used:2697.0
now epsilon is 0.20634674792951876, the reward is 247.25 with loss [22.862711429595947, 29.52282476425171] in episode 1132
Report: 
rewardSum:247.25
loss:[22.862711429595947, 29.52282476425171]
policies:[1, 2, 1]
qAverage:[0.0, 59.52167510986328]
ws:[-3.6082370281219482, 1.6402537027994792]
memory len:10000
memory used:2697.0
now epsilon is 0.2059859118379311, the reward is 244.25 with loss [48.04288148880005, 63.20052719116211] in episode 1133
Report: 
rewardSum:244.25
loss:[48.04288148880005, 63.20052719116211]
policies:[0, 5, 2]
qAverage:[0.0, 72.08067448933919]
ws:[0.8969091375668844, 6.105856736501058]
memory len:10000
memory used:2698.0
now epsilon is 0.2057800031579368, the reward is 247.25 with loss [18.745307207107544, 22.106015920639038] in episode 1134
Report: 
rewardSum:247.25
loss:[18.745307207107544, 22.106015920639038]
policies:[1, 2, 1]
qAverage:[0.0, 41.195899963378906]
ws:[1.589310884475708, 3.562479257583618]
memory len:10000
memory used:2698.0
now epsilon is 0.20542015812615674, the reward is 244.25 with loss [41.662102699279785, 41.06395387649536] in episode 1135
Report: 
rewardSum:244.25
loss:[41.662102699279785, 41.06395387649536]
policies:[0, 5, 2]
qAverage:[0.0, 71.84772618611653]
ws:[2.2739203770955405, 5.690393686294556]
memory len:10000
memory used:2698.0
now epsilon is 0.20521481498775196, the reward is 247.25 with loss [26.853548049926758, 33.07298803329468] in episode 1136
Report: 
rewardSum:247.25
loss:[26.853548049926758, 33.07298803329468]
policies:[0, 3, 1]
qAverage:[0.0, 66.89035224914551]
ws:[1.5091994404792786, 6.930075168609619]
memory len:10000
memory used:2698.0
now epsilon is 0.20500967711549473, the reward is 59.6875 with loss [24.029155015945435, 15.511044025421143] in episode 1137
Report: 
rewardSum:59.6875
loss:[24.029155015945435, 15.511044025421143]
policies:[1, 2, 1]
qAverage:[0.0, 44.927940368652344]
ws:[1.2336499691009521, 3.126281499862671]
memory len:10000
memory used:2698.0
now epsilon is 0.2047023547323403, the reward is 245.25 with loss [53.65010213851929, 43.60796856880188] in episode 1138
Report: 
rewardSum:245.25
loss:[53.65010213851929, 43.60796856880188]
policies:[0, 5, 1]
qAverage:[0.0, 71.09973907470703]
ws:[-1.2748233179251354, 3.7430902322133384]
memory len:10000
memory used:2698.0
now epsilon is 0.2043954930447419, the reward is 245.25 with loss [34.93800735473633, 36.36908006668091] in episode 1139
Report: 
rewardSum:245.25
loss:[34.93800735473633, 36.36908006668091]
policies:[0, 4, 2]
qAverage:[0.0, 58.227420806884766]
ws:[-1.103078916668892, 2.4564438462257385]
memory len:10000
memory used:2698.0
now epsilon is 0.20419117418723312, the reward is 247.25 with loss [29.178933143615723, 23.289409637451172] in episode 1140
Report: 
rewardSum:247.25
loss:[29.178933143615723, 23.289409637451172]
policies:[0, 4, 0]
qAverage:[0.0, 55.700462341308594]
ws:[1.7757254044214885, 3.9608426888783774]
memory len:10000
memory used:2698.0
now epsilon is 0.2039360628070821, the reward is 246.25 with loss [25.69643211364746, 40.45894718170166] in episode 1141
Report: 
rewardSum:246.25
loss:[25.69643211364746, 40.45894718170166]
policies:[1, 3, 1]
qAverage:[0.0, 50.6489003499349]
ws:[5.089713096618652, 7.490569114685059]
memory len:10000
memory used:2698.0
now epsilon is 0.20373220320755336, the reward is 247.25 with loss [24.84343433380127, 26.020230770111084] in episode 1142
Report: 
rewardSum:247.25
loss:[24.84343433380127, 26.020230770111084]
policies:[0, 3, 1]
qAverage:[0.0, 62.69013214111328]
ws:[-0.3911720812320709, 4.589446425437927]
memory len:10000
memory used:2699.0
now epsilon is 0.20352854739118958, the reward is 247.25 with loss [17.24048376083374, 35.47170162200928] in episode 1143
Report: 
rewardSum:247.25
loss:[17.24048376083374, 35.47170162200928]
policies:[0, 3, 1]
qAverage:[0.0, 63.01837730407715]
ws:[-1.08059945050627, 4.425747275352478]
memory len:10000
memory used:2699.0
now epsilon is 0.20332509515428399, the reward is 247.25 with loss [25.031453132629395, 28.77467393875122] in episode 1144
Report: 
rewardSum:247.25
loss:[25.031453132629395, 28.77467393875122]
policies:[0, 2, 2]
qAverage:[0.0, 56.77425638834635]
ws:[-2.0018779834111533, 4.145450592041016]
memory len:10000
memory used:2699.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17*		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.20312184629333338, the reward is 59.6875 with loss [25.207340240478516, 36.90134263038635] in episode 1145
Report: 
rewardSum:59.6875
loss:[25.207340240478516, 36.90134263038635]
policies:[0, 3, 1]
qAverage:[0.0, 55.33074951171875]
ws:[-0.2668311297893524, 1.9244765043258667]
memory len:10000
memory used:2698.0
now epsilon is 0.20291880060503809, the reward is 247.25 with loss [24.863046169281006, 33.55774545669556] in episode 1146
Report: 
rewardSum:247.25
loss:[24.863046169281006, 33.55774545669556]
policies:[0, 3, 1]
qAverage:[0.0, 66.3768367767334]
ws:[-1.7640537954866886, 4.026559233665466]
memory len:10000
memory used:2699.0
now epsilon is 0.20271595788630167, the reward is 247.25 with loss [17.4872989654541, 12.32220733165741] in episode 1147
Report: 
rewardSum:247.25
loss:[17.4872989654541, 12.32220733165741]
policies:[0, 4, 0]
qAverage:[0.0, 68.60645751953125]
ws:[-1.5378465205430984, 3.561679172515869]
memory len:10000
memory used:2698.0
now epsilon is 0.20251331793423066, the reward is 247.25 with loss [23.177337884902954, 19.79673743247986] in episode 1148
Report: 
rewardSum:247.25
loss:[23.177337884902954, 19.79673743247986]
policies:[1, 3, 0]
qAverage:[0.0, 62.579206466674805]
ws:[0.44194692373275757, 6.678169667720795]
memory len:10000
memory used:2698.0
now epsilon is 0.20220973775029136, the reward is 245.25 with loss [35.96824526786804, 31.1033034324646] in episode 1149
Report: 
rewardSum:245.25
loss:[35.96824526786804, 31.1033034324646]
policies:[1, 3, 2]
qAverage:[0.0, 62.74764442443848]
ws:[-1.3777354005724192, 3.5892199873924255]
memory len:10000
memory used:2698.0
now epsilon is 0.2020076038285554, the reward is 59.6875 with loss [21.932504653930664, 23.20347785949707] in episode 1150
Report: 
rewardSum:59.6875
loss:[21.932504653930664, 23.20347785949707]
policies:[0, 3, 1]
qAverage:[0.0, 60.799259185791016]
ws:[2.327152229845524, 4.922945728525519]
memory len:10000
memory used:2698.0
now epsilon is 0.20180567196495364, the reward is 247.25 with loss [22.00300168991089, 21.641422033309937] in episode 1151
Report: 
rewardSum:247.25
loss:[22.00300168991089, 21.641422033309937]
policies:[0, 4, 0]
qAverage:[0.0, 66.2102222442627]
ws:[-0.4768226891756058, 4.119866997003555]
memory len:10000
memory used:2700.0
now epsilon is 0.20140241360442493, the reward is 243.25 with loss [38.76006817817688, 53.39994764328003] in episode 1152
Report: 
rewardSum:243.25
loss:[38.76006817817688, 53.39994764328003]
policies:[2, 2, 4]
qAverage:[0.0, 55.50882212320963]
ws:[0.4212820927302043, 1.895682116349538]
memory len:10000
memory used:2700.0
now epsilon is 0.2010502236111707, the reward is 244.25 with loss [48.27222228050232, 36.66639733314514] in episode 1153
Report: 
rewardSum:244.25
loss:[48.27222228050232, 36.66639733314514]
policies:[1, 3, 3]
qAverage:[0.0, 58.710652669270836]
ws:[-1.7605843146642048, 4.60391092300415]
memory len:10000
memory used:2701.0
now epsilon is 0.20074883669752216, the reward is 57.6875 with loss [41.67414569854736, 34.64854907989502] in episode 1154
Report: 
rewardSum:57.6875
loss:[41.67414569854736, 34.64854907989502]
policies:[0, 4, 2]
qAverage:[0.0, 55.863929748535156]
ws:[2.4417773882548013, 5.231940905253093]
memory len:10000
memory used:2701.0
now epsilon is 0.2005481631290924, the reward is 247.25 with loss [28.96259570121765, 30.13423442840576] in episode 1155
Report: 
rewardSum:247.25
loss:[28.96259570121765, 30.13423442840576]
policies:[2, 2, 0]
qAverage:[0.0, 50.52186838785807]
ws:[1.7250793774922688, 4.062567075093587]
memory len:10000
memory used:2701.0
now epsilon is 0.20034769015899104, the reward is 247.25 with loss [23.545747756958008, 9.837509512901306] in episode 1156
Report: 
rewardSum:247.25
loss:[23.545747756958008, 9.837509512901306]
policies:[0, 3, 1]
qAverage:[0.0, 55.67530314127604]
ws:[4.636015971501668, 10.004809697469076]
memory len:10000
memory used:2701.0
now epsilon is 0.1999473452118814, the reward is 243.25 with loss [50.96567749977112, 31.950323343276978] in episode 1157
Report: 
rewardSum:243.25
loss:[50.96567749977112, 31.950323343276978]
policies:[1, 4, 3]
qAverage:[0.0, 68.09929504394532]
ws:[-0.9263369798660278, 4.7672612309455875]
memory len:10000
memory used:2701.0
now epsilon is 0.19959769967933236, the reward is 244.25 with loss [42.559457302093506, 44.83136224746704] in episode 1158
Report: 
rewardSum:244.25
loss:[42.559457302093506, 44.83136224746704]
policies:[0, 5, 2]
qAverage:[0.0, 70.27181879679362]
ws:[-0.27415744463602704, 6.725757956504822]
memory len:10000
memory used:2701.0
now epsilon is 0.19939817681631633, the reward is 247.25 with loss [18.957247018814087, 28.09086847305298] in episode 1159
Report: 
rewardSum:247.25
loss:[18.957247018814087, 28.09086847305298]
policies:[0, 4, 0]
qAverage:[0.0, 60.699623107910156]
ws:[3.979130506515503, 8.600894093513489]
memory len:10000
memory used:2701.0
now epsilon is 0.19899972923507425, the reward is 243.25 with loss [46.6065034866333, 49.876036643981934] in episode 1160
Report: 
rewardSum:243.25
loss:[46.6065034866333, 49.876036643981934]
policies:[0, 5, 3]
qAverage:[0.0, 69.40080261230469]
ws:[1.733752449353536, 7.810256878534953]
memory len:10000
memory used:2701.0
now epsilon is 0.19880080411830095, the reward is 247.25 with loss [31.822643280029297, 20.734339952468872] in episode 1161
Report: 
rewardSum:247.25
loss:[31.822643280029297, 20.734339952468872]
policies:[1, 2, 1]
qAverage:[0.0, 49.902453104654946]
ws:[1.872149109840393, 6.612089643875758]
memory len:10000
memory used:2701.0
now epsilon is 0.19860207785205994, the reward is 247.25 with loss [28.212294101715088, 22.453129291534424] in episode 1162
Report: 
rewardSum:247.25
loss:[28.212294101715088, 22.453129291534424]
policies:[1, 2, 1]
qAverage:[0.0, 54.48868052164713]
ws:[-0.42593924204508465, 6.6752223173777265]
memory len:10000
memory used:2700.0
now epsilon is 0.19835394935001585, the reward is 246.25 with loss [29.38500213623047, 30.764179706573486] in episode 1163
Report: 
rewardSum:246.25
loss:[29.38500213623047, 30.764179706573486]
policies:[0, 4, 1]
qAverage:[0.0, 66.1482940673828]
ws:[-2.4438181638717653, 7.343457221984863]
memory len:10000
memory used:2700.0
now epsilon is 0.19810613085355777, the reward is 246.25 with loss [26.958929538726807, 36.44400119781494] in episode 1164
Report: 
rewardSum:246.25
loss:[26.958929538726807, 36.44400119781494]
policies:[0, 4, 1]
qAverage:[0.0, 66.55191040039062]
ws:[0.6253615856170655, 8.497357702255249]
memory len:10000
memory used:2700.0
now epsilon is 0.19800709016976417, the reward is -1.0 with loss [13.961048126220703, 9.961860179901123] in episode 1165
Report: 
rewardSum:-1.0
loss:[13.961048126220703, 9.961860179901123]
policies:[0, 1, 1]
qAverage:[0.0, 35.30220413208008]
ws:[1.4339503049850464, 1.6886272430419922]
memory len:10000
memory used:2700.0
now epsilon is 0.19780915731987858, the reward is 247.25 with loss [27.552263736724854, 29.29381513595581] in episode 1166
Report: 
rewardSum:247.25
loss:[27.552263736724854, 29.29381513595581]
policies:[0, 3, 1]
qAverage:[0.0, 56.12125905354818]
ws:[-1.0891494750976562, 6.480314413706462]
memory len:10000
memory used:2700.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19*		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.1976114223286304, the reward is 247.25 with loss [17.416661024093628, 22.499704599380493] in episode 1167
Report: 
rewardSum:247.25
loss:[17.416661024093628, 22.499704599380493]
policies:[1, 2, 1]
qAverage:[0.0, 58.652244567871094]
ws:[0.25456078847249347, 10.187309583028158]
memory len:10000
memory used:2700.0
now epsilon is 0.1974138849982352, the reward is 247.25 with loss [31.66463327407837, 22.127391576766968] in episode 1168
Report: 
rewardSum:247.25
loss:[31.66463327407837, 22.127391576766968]
policies:[0, 3, 1]
qAverage:[0.0, 65.36006546020508]
ws:[-1.9230197370052338, 5.569065034389496]
memory len:10000
memory used:2700.0
now epsilon is 0.19721654513110626, the reward is 247.25 with loss [30.018123626708984, 27.90687847137451] in episode 1169
Report: 
rewardSum:247.25
loss:[30.018123626708984, 27.90687847137451]
policies:[0, 4, 0]
qAverage:[0.0, 67.20157318115234]
ws:[-0.6213176012039184, 6.724968528747558]
memory len:10000
memory used:2700.0
now epsilon is 0.19701940252985434, the reward is 59.6875 with loss [26.135445594787598, 29.806379795074463] in episode 1170
Report: 
rewardSum:59.6875
loss:[26.135445594787598, 29.806379795074463]
policies:[1, 2, 1]
qAverage:[0.0, 52.24650573730469]
ws:[-0.7620583872000376, 2.2942891120910645]
memory len:10000
memory used:2700.0
now epsilon is 0.19682245699728754, the reward is 59.6875 with loss [22.824647426605225, 38.604801654815674] in episode 1171
Report: 
rewardSum:59.6875
loss:[22.824647426605225, 38.604801654815674]
policies:[0, 3, 1]
qAverage:[0.0, 55.303820292154946]
ws:[3.005058924357096, 7.023160934448242]
memory len:10000
memory used:2700.0
now epsilon is 0.19662570833641102, the reward is 247.25 with loss [25.25866675376892, 19.495453357696533] in episode 1172
Report: 
rewardSum:247.25
loss:[25.25866675376892, 19.495453357696533]
policies:[1, 3, 0]
qAverage:[0.0, 56.91720072428385]
ws:[-1.9987533688545227, 4.79386838277181]
memory len:10000
memory used:2700.0
now epsilon is 0.19642915635042693, the reward is 247.25 with loss [19.837713718414307, 19.279446601867676] in episode 1173
Report: 
rewardSum:247.25
loss:[19.837713718414307, 19.279446601867676]
policies:[0, 3, 1]
qAverage:[0.0, 66.51526832580566]
ws:[-0.5360550284385681, 6.520532131195068]
memory len:10000
memory used:2700.0
now epsilon is 0.19623280084273412, the reward is 247.25 with loss [19.472437858581543, 25.881969451904297] in episode 1174
Report: 
rewardSum:247.25
loss:[19.472437858581543, 25.881969451904297]
policies:[0, 4, 0]
qAverage:[0.0, 67.06053314208984]
ws:[1.4823131799697875, 8.82051911354065]
memory len:10000
memory used:2700.0
now epsilon is 0.1959876324565237, the reward is 246.25 with loss [33.717408180236816, 34.61468172073364] in episode 1175
Report: 
rewardSum:246.25
loss:[33.717408180236816, 34.61468172073364]
policies:[0, 4, 1]
qAverage:[0.0, 66.83148384094238]
ws:[-1.6595422104001045, 6.0751707553863525]
memory len:10000
memory used:2700.0
now epsilon is 0.19579171830718092, the reward is 247.25 with loss [25.338412284851074, 26.115621089935303] in episode 1176
Report: 
rewardSum:247.25
loss:[25.338412284851074, 26.115621089935303]
policies:[0, 3, 1]
qAverage:[0.0, 59.18440246582031]
ws:[-0.8162124454975128, 1.8684274777770042]
memory len:10000
memory used:2700.0
now epsilon is 0.1955959999985319, the reward is -3.0 with loss [19.115717887878418, 23.5351824760437] in episode 1177
Report: 
rewardSum:-3.0
loss:[19.115717887878418, 23.5351824760437]
policies:[0, 1, 3]
qAverage:[0.0, 35.692649841308594]
ws:[-0.7593163251876831, 0.21259023249149323]
memory len:10000
memory used:2700.0
now epsilon is 0.19540047733480936, the reward is 247.25 with loss [26.65255331993103, 26.975494384765625] in episode 1178
Report: 
rewardSum:247.25
loss:[26.65255331993103, 26.975494384765625]
policies:[0, 1, 3]
qAverage:[0.0, 50.02064895629883]
ws:[-4.169489860534668, 7.711081504821777]
memory len:10000
memory used:2700.0
now epsilon is 0.19520515012044182, the reward is 247.25 with loss [19.913912296295166, 22.668488264083862] in episode 1179
Report: 
rewardSum:247.25
loss:[19.913912296295166, 22.668488264083862]
policies:[0, 4, 0]
qAverage:[0.0, 67.40654602050782]
ws:[-2.2083045840263367, 4.938016796112061]
memory len:10000
memory used:2700.0
now epsilon is 0.19501001816005312, the reward is 247.25 with loss [31.112682342529297, 28.610229969024658] in episode 1180
Report: 
rewardSum:247.25
loss:[31.112682342529297, 28.610229969024658]
policies:[0, 4, 0]
qAverage:[0.0, 67.30420227050782]
ws:[-2.00937157869339, 4.768209314346313]
memory len:10000
memory used:2700.0
now epsilon is 0.1948150812584625, the reward is 247.25 with loss [30.826609134674072, 19.769561529159546] in episode 1181
Report: 
rewardSum:247.25
loss:[30.826609134674072, 19.769561529159546]
policies:[0, 3, 1]
qAverage:[0.0, 56.1416269938151]
ws:[0.38346510877211887, 4.094366232554118]
memory len:10000
memory used:2700.0
now epsilon is 0.19462033922068436, the reward is 247.25 with loss [32.69515371322632, 28.330021858215332] in episode 1182
Report: 
rewardSum:247.25
loss:[32.69515371322632, 28.330021858215332]
policies:[1, 3, 0]
qAverage:[0.0, 56.60641733805338]
ws:[-3.069220860799154, 6.454540729522705]
memory len:10000
memory used:2700.0
now epsilon is 0.1944257918519279, the reward is 247.25 with loss [21.385869026184082, 30.77955722808838] in episode 1183
Report: 
rewardSum:247.25
loss:[21.385869026184082, 30.77955722808838]
policies:[1, 3, 0]
qAverage:[0.0, 67.2350959777832]
ws:[-0.8398937582969666, 7.991737246513367]
memory len:10000
memory used:2700.0
now epsilon is 0.1942314389575971, the reward is 247.25 with loss [33.56577968597412, 36.453909397125244] in episode 1184
Report: 
rewardSum:247.25
loss:[33.56577968597412, 36.453909397125244]
policies:[0, 4, 0]
qAverage:[0.0, 62.05419731140137]
ws:[1.5165294408798218, 9.29287052154541]
memory len:10000
memory used:2700.0
now epsilon is 0.19394027383044882, the reward is 245.25 with loss [30.98007345199585, 52.52461910247803] in episode 1185
Report: 
rewardSum:245.25
loss:[30.98007345199585, 52.52461910247803]
policies:[0, 4, 2]
qAverage:[0.0, 69.5997085571289]
ws:[0.23610985279083252, 6.236926651000976]
memory len:10000
memory used:2700.0
now epsilon is 0.19374640627210057, the reward is 247.25 with loss [22.063995361328125, 19.004972219467163] in episode 1186
Report: 
rewardSum:247.25
loss:[22.063995361328125, 19.004972219467163]
policies:[0, 4, 0]
qAverage:[0.0, 68.68527526855469]
ws:[-1.256382018327713, 4.775079131126404]
memory len:10000
memory used:2700.0
now epsilon is 0.19355273250862245, the reward is 247.25 with loss [20.243924856185913, 36.39314103126526] in episode 1187
Report: 
rewardSum:247.25
loss:[20.243924856185913, 36.39314103126526]
policies:[0, 3, 1]
qAverage:[0.0, 65.35292434692383]
ws:[-1.2682478725910187, 5.497154921293259]
memory len:10000
memory used:2701.0
now epsilon is 0.19326258480507236, the reward is 245.25 with loss [48.57022571563721, 39.92269694805145] in episode 1188
Report: 
rewardSum:245.25
loss:[48.57022571563721, 39.92269694805145]
policies:[0, 5, 1]
qAverage:[0.0, 68.95311737060547]
ws:[0.8926510711510977, 7.5746991237004595]
memory len:10000
memory used:2701.0
now epsilon is 0.19306939468165846, the reward is 247.25 with loss [14.394516348838806, 13.41510283946991] in episode 1189
Report: 
rewardSum:247.25
loss:[14.394516348838806, 13.41510283946991]
policies:[0, 3, 1]
qAverage:[0.0, 69.32187843322754]
ws:[-0.48099538683891296, 7.432189106941223]
memory len:10000
memory used:2701.0
now epsilon is 0.19287639767593373, the reward is 247.25 with loss [20.669734954833984, 38.656800270080566] in episode 1190
Report: 
rewardSum:247.25
loss:[20.669734954833984, 38.656800270080566]
policies:[1, 3, 0]
qAverage:[0.0, 60.13583119710287]
ws:[1.0496764183044434, 11.969183286031088]
memory len:10000
memory used:2701.0
now epsilon is 0.1926835935948529, the reward is 59.6875 with loss [26.50885319709778, 30.94087028503418] in episode 1191
Report: 
rewardSum:59.6875
loss:[26.50885319709778, 30.94087028503418]
policies:[0, 3, 1]
qAverage:[0.0, 60.867828369140625]
ws:[2.231978714466095, 4.3575000166893005]
memory len:10000
memory used:2700.0
now epsilon is 0.1924909822455637, the reward is 247.25 with loss [16.41304838657379, 28.674796104431152] in episode 1192
Report: 
rewardSum:247.25
loss:[16.41304838657379, 28.674796104431152]
policies:[0, 3, 1]
qAverage:[0.0, 62.689109802246094]
ws:[0.11204582452774048, 6.011372745037079]
memory len:10000
memory used:2700.0
now epsilon is 0.1922985634354066, the reward is 59.6875 with loss [24.482837677001953, 29.478733777999878] in episode 1193
Report: 
rewardSum:59.6875
loss:[24.482837677001953, 29.478733777999878]
policies:[0, 3, 1]
qAverage:[0.0, 57.48428853352865]
ws:[5.125944455464681, 9.350098292032877]
memory len:10000
memory used:2700.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20*		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.1921063369719146, the reward is 247.25 with loss [25.499862670898438, 36.334391355514526] in episode 1194
Report: 
rewardSum:247.25
loss:[25.499862670898438, 36.334391355514526]
policies:[2, 2, 0]
qAverage:[30.745489501953124, 36.94356842041016]
ws:[-0.4093942403793335, 2.6809434086084365]
memory len:10000
memory used:2700.0
now epsilon is 0.19191430266281317, the reward is 247.25 with loss [37.24198579788208, 26.461466789245605] in episode 1195
Report: 
rewardSum:247.25
loss:[37.24198579788208, 26.461466789245605]
policies:[1, 3, 0]
qAverage:[17.003948211669922, 50.45007133483887]
ws:[-1.2763349413871765, 3.887670576572418]
memory len:10000
memory used:2700.0
now epsilon is 0.1915308097396447, the reward is 243.25 with loss [49.929898500442505, 40.55287313461304] in episode 1196
Report: 
rewardSum:243.25
loss:[49.929898500442505, 40.55287313461304]
policies:[0, 6, 2]
qAverage:[0.0, 68.70187759399414]
ws:[-0.20693218211332956, 6.136048986266057]
memory len:10000
memory used:2700.0
now epsilon is 0.1913393507419888, the reward is 247.25 with loss [21.322630405426025, 15.267127960920334] in episode 1197
Report: 
rewardSum:247.25
loss:[21.322630405426025, 15.267127960920334]
policies:[0, 3, 1]
qAverage:[0.0, 61.79763412475586]
ws:[0.572707049548626, 4.987090110778809]
memory len:10000
memory used:2700.0
now epsilon is 0.19105252103673487, the reward is 245.25 with loss [39.15223455429077, 39.71896934509277] in episode 1198
Report: 
rewardSum:245.25
loss:[39.15223455429077, 39.71896934509277]
policies:[0, 4, 2]
qAverage:[0.0, 70.51476745605468]
ws:[-1.1676640629768371, 6.01959810256958]
memory len:10000
memory used:2700.0
now epsilon is 0.1908615401484535, the reward is 247.25 with loss [19.52181100845337, 36.791831970214844] in episode 1199
Report: 
rewardSum:247.25
loss:[19.52181100845337, 36.791831970214844]
policies:[1, 3, 0]
qAverage:[0.0, 61.550106048583984]
ws:[1.1225800663232803, 4.021631419658661]
memory len:10000
memory used:2700.0
now epsilon is 0.1906707501694545, the reward is 247.25 with loss [17.70026969909668, 22.86241340637207] in episode 1200
Report: 
rewardSum:247.25
loss:[17.70026969909668, 22.86241340637207]
policies:[0, 3, 1]
qAverage:[0.0, 59.198656717936196]
ws:[4.8976796468098955, 9.575230280558268]
memory len:10000
memory used:2700.0
now epsilon is 0.19038492273845523, the reward is 245.25 with loss [41.64531588554382, 52.42577123641968] in episode 1201
Report: 
rewardSum:245.25
loss:[41.64531588554382, 52.42577123641968]
policies:[0, 5, 1]
qAverage:[0.0, 66.02836227416992]
ws:[0.004845142364501953, 6.44854611158371]
memory len:10000
memory used:2700.0
now epsilon is 0.19009952378072847, the reward is 245.25 with loss [43.63347673416138, 26.4483699798584] in episode 1202
Report: 
rewardSum:245.25
loss:[43.63347673416138, 26.4483699798584]
policies:[1, 4, 1]
qAverage:[13.901237487792969, 51.88580322265625]
ws:[3.6967279434204103, 7.4326270580291744]
memory len:10000
memory used:2700.0
now epsilon is 0.18990949553238873, the reward is 247.25 with loss [28.12868309020996, 20.023004055023193] in episode 1203
Report: 
rewardSum:247.25
loss:[28.12868309020996, 20.023004055023193]
policies:[1, 3, 0]
qAverage:[14.089714050292969, 55.868106079101565]
ws:[1.3661505937576295, 5.552928996086121]
memory len:10000
memory used:2700.0
now epsilon is 0.18971965724104858, the reward is 247.25 with loss [20.921960830688477, 22.54139471054077] in episode 1204
Report: 
rewardSum:247.25
loss:[20.921960830688477, 22.54139471054077]
policies:[2, 2, 0]
qAverage:[17.475244522094727, 47.494991302490234]
ws:[0.9747125133872032, 6.094112545251846]
memory len:10000
memory used:2700.0
now epsilon is 0.1895300087168223, the reward is 247.25 with loss [23.823967218399048, 17.60157036781311] in episode 1205
Report: 
rewardSum:247.25
loss:[23.823967218399048, 17.60157036781311]
policies:[1, 3, 0]
qAverage:[13.866712951660157, 55.1755126953125]
ws:[2.0530040740966795, 6.179815769195557]
memory len:10000
memory used:2700.0
now epsilon is 0.18934054977001388, the reward is 247.25 with loss [23.724544763565063, 15.537600755691528] in episode 1206
Report: 
rewardSum:247.25
loss:[23.724544763565063, 15.537600755691528]
policies:[1, 3, 0]
qAverage:[0.0, 66.23419952392578]
ws:[1.669196645418803, 7.352279186248779]
memory len:10000
memory used:2700.0
now epsilon is 0.189151280211117, the reward is 247.25 with loss [21.131386518478394, 22.79262399673462] in episode 1207
Report: 
rewardSum:247.25
loss:[21.131386518478394, 22.79262399673462]
policies:[1, 3, 0]
qAverage:[13.83828125, 55.640873718261716]
ws:[3.1778070092201234, 8.153545761108399]
memory len:10000
memory used:2700.0
now epsilon is 0.18896219985081475, the reward is 247.25 with loss [21.822190761566162, 19.25908136367798] in episode 1208
Report: 
rewardSum:247.25
loss:[21.822190761566162, 19.25908136367798]
policies:[2, 2, 0]
qAverage:[17.92892074584961, 43.711679458618164]
ws:[4.120230615139008, 6.388261616230011]
memory len:10000
memory used:2700.0
now epsilon is 0.1886789336440613, the reward is 245.25 with loss [34.52275991439819, 34.18629741668701] in episode 1209
Report: 
rewardSum:245.25
loss:[34.52275991439819, 34.18629741668701]
policies:[1, 4, 1]
qAverage:[14.098829650878907, 55.65894470214844]
ws:[2.672744798660278, 6.0877975702285765]
memory len:10000
memory used:2700.0
now epsilon is 0.18853745981818024, the reward is -2.0 with loss [21.485872268676758, 11.126641273498535] in episode 1210
Report: 
rewardSum:-2.0
loss:[21.485872268676758, 11.126641273498535]
policies:[1, 0, 2]
qAverage:[35.13447570800781, 0.0]
ws:[1.2986425161361694, 0.4765222668647766]
memory len:10000
memory used:2700.0
now epsilon is 0.1884432028718624, the reward is -1.0 with loss [9.06059741973877, 8.594661951065063] in episode 1211
Report: 
rewardSum:-1.0
loss:[9.06059741973877, 8.594661951065063]
policies:[0, 1, 1]
qAverage:[0.0, 37.092185974121094]
ws:[0.5456283092498779, 0.6030369997024536]
memory len:10000
memory used:2700.0
now epsilon is 0.18811367449551133, the reward is 244.25 with loss [35.52276062965393, 44.02431523799896] in episode 1212
Report: 
rewardSum:244.25
loss:[35.52276062965393, 44.02431523799896]
policies:[1, 3, 3]
qAverage:[14.381637573242188, 56.46002197265625]
ws:[2.3185296773910524, 7.362391793727875]
memory len:10000
memory used:2700.0
now epsilon is 0.18792563135188742, the reward is 247.25 with loss [28.255462169647217, 24.42022132873535] in episode 1213
Report: 
rewardSum:247.25
loss:[28.255462169647217, 24.42022132873535]
policies:[1, 3, 0]
qAverage:[14.307015991210937, 55.303976440429686]
ws:[3.3057233929634093, 8.680349826812744]
memory len:10000
memory used:2700.0
now epsilon is 0.18773777618090268, the reward is 247.25 with loss [32.41770362854004, 22.561898708343506] in episode 1214
Report: 
rewardSum:247.25
loss:[32.41770362854004, 22.561898708343506]
policies:[0, 1, 3]
qAverage:[0.0, 36.67170715332031]
ws:[1.4894269704818726, 1.5963287353515625]
memory len:10000
memory used:2700.0
now epsilon is 0.18745634546213946, the reward is 245.25 with loss [40.54850387573242, 26.311974048614502] in episode 1215
Report: 
rewardSum:245.25
loss:[40.54850387573242, 26.311974048614502]
policies:[0, 3, 3]
qAverage:[0.0, 62.119157791137695]
ws:[2.0327426195144653, 4.333941578865051]
memory len:10000
memory used:2700.0
now epsilon is 0.18722214216124133, the reward is 246.25 with loss [43.46049118041992, 32.26089322566986] in episode 1216
Report: 
rewardSum:246.25
loss:[43.46049118041992, 32.26089322566986]
policies:[0, 3, 2]
qAverage:[0.0, 65.5255839029948]
ws:[0.9543341596921285, 8.520210901896158]
memory len:10000
memory used:2700.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21*		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.18684802535189945, the reward is 243.25 with loss [38.30492281913757, 41.14142847061157] in episode 1217
Report: 
rewardSum:243.25
loss:[38.30492281913757, 41.14142847061157]
policies:[1, 4, 3]
qAverage:[0.0, 65.28091621398926]
ws:[3.7230527997016907, 7.540418982505798]
memory len:10000
memory used:2700.0
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.1866145820710341, the reward is 246.25 with loss [32.443889141082764, 37.763365745544434] in episode 1218
Report: 
rewardSum:246.25
loss:[32.443889141082764, 37.763365745544434]
policies:[1, 3, 1]
qAverage:[0.0, 70.0026683807373]
ws:[3.285437822341919, 11.597562551498413]
memory len:10000
memory used:2700.0
now epsilon is 0.18642803745776867, the reward is 247.25 with loss [25.085354804992676, 16.983933091163635] in episode 1219
Report: 
rewardSum:247.25
loss:[25.085354804992676, 16.983933091163635]
policies:[0, 4, 0]
qAverage:[0.0, 69.69624481201171]
ws:[1.9216988235712051, 6.6492901802062985]
memory len:10000
memory used:2700.0
now epsilon is 0.18633483509079216, the reward is -1.0 with loss [10.52586555480957, 7.658716917037964] in episode 1220
Report: 
rewardSum:-1.0
loss:[10.52586555480957, 7.658716917037964]
policies:[0, 1, 1]
qAverage:[0.0, 36.881290435791016]
ws:[1.0029624700546265, 1.3321311473846436]
memory len:10000
memory used:2700.0
now epsilon is 0.18614857011961936, the reward is 247.25 with loss [26.78077793121338, 20.19819188117981] in episode 1221
Report: 
rewardSum:247.25
loss:[26.78077793121338, 20.19819188117981]
policies:[0, 4, 0]
qAverage:[0.0, 62.398101806640625]
ws:[3.147051602602005, 7.527149975299835]
memory len:10000
memory used:2700.0
now epsilon is 0.18596249134357998, the reward is 247.25 with loss [27.73403286933899, 24.03773832321167] in episode 1222
Report: 
rewardSum:247.25
loss:[27.73403286933899, 24.03773832321167]
policies:[0, 3, 1]
qAverage:[0.0, 65.26041221618652]
ws:[-1.1487417165189981, 5.100533217191696]
memory len:10000
memory used:2700.0
now epsilon is 0.18573015442690463, the reward is 246.25 with loss [27.2067232131958, 27.307956218719482] in episode 1223
Report: 
rewardSum:246.25
loss:[27.2067232131958, 27.307956218719482]
policies:[1, 2, 2]
qAverage:[0.0, 41.32624435424805]
ws:[3.073728561401367, 8.371248245239258]
memory len:10000
memory used:2700.0
now epsilon is 0.18554449390967823, the reward is 247.25 with loss [29.325188159942627, 29.490520477294922] in episode 1224
Report: 
rewardSum:247.25
loss:[29.325188159942627, 29.490520477294922]
policies:[0, 4, 0]
qAverage:[0.0, 70.5004165649414]
ws:[-1.1963789075613023, 4.978053593635559]
memory len:10000
memory used:2700.0
now epsilon is 0.18535901898335796, the reward is 247.25 with loss [25.71004343032837, 26.78252911567688] in episode 1225
Report: 
rewardSum:247.25
loss:[25.71004343032837, 26.78252911567688]
policies:[1, 3, 0]
qAverage:[0.0, 47.2249641418457]
ws:[0.2619323432445526, 3.2027459144592285]
memory len:10000
memory used:2700.0
now epsilon is 0.18517372946242255, the reward is 247.25 with loss [37.006964683532715, 20.428484439849854] in episode 1226
Report: 
rewardSum:247.25
loss:[37.006964683532715, 20.428484439849854]
policies:[1, 3, 0]
qAverage:[0.0, 69.05576705932617]
ws:[-2.4046881198883057, 5.131863713264465]
memory len:10000
memory used:2700.0
now epsilon is 0.18498862516153608, the reward is 247.25 with loss [31.182010650634766, 34.19542741775513] in episode 1227
Report: 
rewardSum:247.25
loss:[31.182010650634766, 34.19542741775513]
policies:[1, 3, 0]
qAverage:[0.0, 62.99711608886719]
ws:[-1.3155764043331146, 2.3660647571086884]
memory len:10000
memory used:2700.0
now epsilon is 0.18480370589554795, the reward is 247.25 with loss [21.47212862968445, 28.88591432571411] in episode 1228
Report: 
rewardSum:247.25
loss:[21.47212862968445, 28.88591432571411]
policies:[0, 4, 0]
qAverage:[0.0, 70.6599349975586]
ws:[-2.952636384963989, 4.038898539543152]
memory len:10000
memory used:2700.0
now epsilon is 0.1846189714794926, the reward is 247.25 with loss [24.145951509475708, 17.646599292755127] in episode 1229
Report: 
rewardSum:247.25
loss:[24.145951509475708, 17.646599292755127]
policies:[0, 4, 0]
qAverage:[0.0, 69.50839424133301]
ws:[-0.6236904263496399, 7.590394139289856]
memory len:10000
memory used:2700.0
now epsilon is 0.18443442172858948, the reward is 247.25 with loss [19.402255475521088, 19.81270682811737] in episode 1230
Report: 
rewardSum:247.25
loss:[19.402255475521088, 19.81270682811737]
policies:[1, 3, 0]
qAverage:[0.0, 62.53513844807943]
ws:[-2.1131374835968018, 5.684805711110433]
memory len:10000
memory used:2700.0
now epsilon is 0.1842500564582426, the reward is 247.25 with loss [20.445137977600098, 28.38549280166626] in episode 1231
Report: 
rewardSum:247.25
loss:[20.445137977600098, 28.38549280166626]
policies:[0, 4, 0]
qAverage:[0.0, 70.25310363769532]
ws:[0.6134696960449219, 6.3750901222229]
memory len:10000
memory used:2700.0
now epsilon is 0.18406587548404066, the reward is 59.6875 with loss [30.74292230606079, 30.34724187850952] in episode 1232
Report: 
rewardSum:59.6875
loss:[30.74292230606079, 30.34724187850952]
policies:[0, 3, 1]
qAverage:[0.0, 62.34047508239746]
ws:[5.1915119886398315, 7.883338212966919]
memory len:10000
memory used:2701.0
now epsilon is 0.18397385405041586, the reward is -1.0 with loss [9.296151638031006, 17.898340702056885] in episode 1233
Report: 
rewardSum:-1.0
loss:[9.296151638031006, 17.898340702056885]
policies:[0, 1, 1]
qAverage:[0.0, 36.10544967651367]
ws:[2.4330344200134277, 2.6856677532196045]
memory len:10000
memory used:2701.0
now epsilon is 0.18374400168776933, the reward is 246.25 with loss [32.2768874168396, 46.47533893585205] in episode 1234
Report: 
rewardSum:246.25
loss:[32.2768874168396, 46.47533893585205]
policies:[2, 1, 2]
qAverage:[0.0, 40.99614715576172]
ws:[5.726779460906982, 9.039814949035645]
memory len:10000
memory used:2701.0
now epsilon is 0.18356032657859894, the reward is 247.25 with loss [27.575137615203857, 29.06728172302246] in episode 1235
Report: 
rewardSum:247.25
loss:[27.575137615203857, 29.06728172302246]
policies:[1, 3, 0]
qAverage:[17.79166030883789, 44.009782791137695]
ws:[3.0837196707725525, 4.275352895259857]
memory len:10000
memory used:2701.0
now epsilon is 0.18337683507567104, the reward is 247.25 with loss [20.510006189346313, 23.112570643424988] in episode 1236
Report: 
rewardSum:247.25
loss:[20.510006189346313, 23.112570643424988]
policies:[1, 1, 2]
qAverage:[24.148666381835938, 26.974222819010418]
ws:[4.489265521367391, 7.176115353902181]
memory len:10000
memory used:2701.0
now epsilon is 0.18328515811918542, the reward is -1.0 with loss [12.091985821723938, 12.85488510131836] in episode 1237
Report: 
rewardSum:-1.0
loss:[12.091985821723938, 12.85488510131836]
policies:[0, 1, 1]
qAverage:[0.0, 37.05812072753906]
ws:[0.43348005414009094, 0.5509406328201294]
memory len:10000
memory used:2701.0
now epsilon is 0.1829646495540379, the reward is 244.25 with loss [42.23888802528381, 30.70750641822815] in episode 1238
Report: 
rewardSum:244.25
loss:[42.23888802528381, 30.70750641822815]
policies:[1, 4, 2]
qAverage:[0.0, 65.48144340515137]
ws:[0.35221900045871735, 8.182406544685364]
memory len:10000
memory used:2701.0
now epsilon is 0.1827817535047929, the reward is 247.25 with loss [30.136445999145508, 23.568776607513428] in episode 1239
Report: 
rewardSum:247.25
loss:[30.136445999145508, 23.568776607513428]
policies:[0, 3, 1]
qAverage:[0.0, 66.72123718261719]
ws:[-1.8181513547897339, 6.20450480779012]
memory len:10000
memory used:2701.0
now epsilon is 0.18255339052295183, the reward is 246.25 with loss [31.300713539123535, 26.733397960662842] in episode 1240
Report: 
rewardSum:246.25
loss:[31.300713539123535, 26.733397960662842]
policies:[0, 4, 1]
qAverage:[0.0, 62.01171366373698]
ws:[0.398678461710612, 9.557413101196289]
memory len:10000
memory used:2701.0
now epsilon is 0.18232531285214684, the reward is 246.25 with loss [23.55098867416382, 23.438231587409973] in episode 1241
Report: 
rewardSum:246.25
loss:[23.55098867416382, 23.438231587409973]
policies:[0, 3, 2]
qAverage:[0.0, 64.40464528401692]
ws:[-0.7181609272956848, 6.119718869527181]
memory len:10000
memory used:2701.0
now epsilon is 0.18205199575588346, the reward is 245.25 with loss [32.530593395233154, 27.693020343780518] in episode 1242
Report: 
rewardSum:245.25
loss:[32.530593395233154, 27.693020343780518]
policies:[0, 5, 1]
qAverage:[0.0, 70.2836898803711]
ws:[1.67738037109375, 7.70504674911499]
memory len:10000
memory used:2701.0
now epsilon is 0.18187001201824848, the reward is 247.25 with loss [18.066932201385498, 18.765276432037354] in episode 1243
Report: 
rewardSum:247.25
loss:[18.066932201385498, 18.765276432037354]
policies:[0, 3, 1]
qAverage:[0.0, 59.306396484375]
ws:[-0.7024596532185873, 5.066978255907695]
memory len:10000
memory used:2701.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20*		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.18159737744653368, the reward is 245.25 with loss [48.36638045310974, 39.157761096954346] in episode 1244
Report: 
rewardSum:245.25
loss:[48.36638045310974, 39.157761096954346]
policies:[0, 4, 2]
qAverage:[0.0, 65.3803726196289]
ws:[1.5938873052597047, 4.23311824798584]
memory len:10000
memory used:2702.0
now epsilon is 0.1814158481567546, the reward is 247.25 with loss [27.220643997192383, 26.072386264801025] in episode 1245
Report: 
rewardSum:247.25
loss:[27.220643997192383, 26.072386264801025]
policies:[1, 3, 0]
qAverage:[0.0, 71.91365051269531]
ws:[0.5337373614311218, 7.438870668411255]
memory len:10000
memory used:2702.0
now epsilon is 0.18123450032820315, the reward is 247.25 with loss [28.18845272064209, 26.43670094013214] in episode 1246
Report: 
rewardSum:247.25
loss:[28.18845272064209, 26.43670094013214]
policies:[0, 4, 0]
qAverage:[0.0, 70.7835922241211]
ws:[1.4855441093444823, 7.3971914768219]
memory len:10000
memory used:2702.0
now epsilon is 0.18105333377948615, the reward is 247.25 with loss [36.35836839675903, 16.57803988456726] in episode 1247
Report: 
rewardSum:247.25
loss:[36.35836839675903, 16.57803988456726]
policies:[0, 4, 0]
qAverage:[0.0, 71.83623352050782]
ws:[1.9610849380493165, 7.142010879516602]
memory len:10000
memory used:2701.0
now epsilon is 0.18087234832939172, the reward is 247.25 with loss [31.377922534942627, 26.86722183227539] in episode 1248
Report: 
rewardSum:247.25
loss:[31.377922534942627, 26.86722183227539]
policies:[0, 3, 1]
qAverage:[0.0, 70.14268493652344]
ws:[1.0387119352817535, 7.046530246734619]
memory len:10000
memory used:2701.0
now epsilon is 0.18069154379688915, the reward is 247.25 with loss [29.494994163513184, 30.54381513595581] in episode 1249
Report: 
rewardSum:247.25
loss:[29.494994163513184, 30.54381513595581]
policies:[0, 3, 1]
qAverage:[0.0, 65.60934448242188]
ws:[0.3391164541244507, 5.28724405169487]
memory len:10000
memory used:2701.0
now epsilon is 0.18051092000112867, the reward is 247.25 with loss [23.518008708953857, 28.214653968811035] in episode 1250
Report: 
rewardSum:247.25
loss:[23.518008708953857, 28.214653968811035]
policies:[0, 4, 0]
qAverage:[0.0, 67.8420295715332]
ws:[0.4587191045284271, 5.570226714015007]
memory len:10000
memory used:2701.0
now epsilon is 0.18024032279371538, the reward is 245.25 with loss [43.88165760040283, 30.96579599380493] in episode 1251
Report: 
rewardSum:245.25
loss:[43.88165760040283, 30.96579599380493]
policies:[0, 4, 2]
qAverage:[0.0, 63.9891471862793]
ws:[1.6500695794820786, 2.678599715232849]
memory len:10000
memory used:2701.0
now epsilon is 0.18006015004977843, the reward is 247.25 with loss [27.00184917449951, 30.282691478729248] in episode 1252
Report: 
rewardSum:247.25
loss:[27.00184917449951, 30.282691478729248]
policies:[0, 4, 0]
qAverage:[0.0, 55.029354095458984]
ws:[-1.0967953205108643, 7.116379737854004]
memory len:10000
memory used:2702.0
now epsilon is 0.1798801574110319, the reward is 247.25 with loss [18.42733144760132, 29.080416679382324] in episode 1253
Report: 
rewardSum:247.25
loss:[18.42733144760132, 29.080416679382324]
policies:[1, 3, 0]
qAverage:[0.0, 53.58019510904948]
ws:[2.611370106538137, 6.018585125605266]
memory len:10000
memory used:2702.0
now epsilon is 0.1797003446974381, the reward is 247.25 with loss [36.265902042388916, 32.93173599243164] in episode 1254
Report: 
rewardSum:247.25
loss:[36.265902042388916, 32.93173599243164]
policies:[0, 3, 1]
qAverage:[0.0, 64.11849594116211]
ws:[1.442321076989174, 4.261178195476532]
memory len:10000
memory used:2702.0
now epsilon is 0.17952071172913936, the reward is 247.25 with loss [24.687501907348633, 27.27736759185791] in episode 1255
Report: 
rewardSum:247.25
loss:[24.687501907348633, 27.27736759185791]
policies:[0, 3, 1]
qAverage:[0.0, 75.1955738067627]
ws:[0.8984351754188538, 8.217609763145447]
memory len:10000
memory used:2701.0
now epsilon is 0.17934125832645778, the reward is 247.25 with loss [22.961551666259766, 32.78387641906738] in episode 1256
Report: 
rewardSum:247.25
loss:[22.961551666259766, 32.78387641906738]
policies:[0, 4, 0]
qAverage:[0.0, 60.56182607014974]
ws:[1.7930243015289307, 5.675506114959717]
memory len:10000
memory used:2701.0
now epsilon is 0.1791619843098951, the reward is 247.25 with loss [20.612629413604736, 31.778223037719727] in episode 1257
Report: 
rewardSum:247.25
loss:[20.612629413604736, 31.778223037719727]
policies:[0, 4, 0]
qAverage:[0.0, 74.55630035400391]
ws:[2.237181544303894, 10.118597030639648]
memory len:10000
memory used:2701.0
now epsilon is 0.1789828895001324, the reward is 247.25 with loss [18.912119150161743, 15.08289384841919] in episode 1258
Report: 
rewardSum:247.25
loss:[18.912119150161743, 15.08289384841919]
policies:[0, 4, 0]
qAverage:[0.0, 75.00267181396484]
ws:[0.8003596067428589, 7.258510732650757]
memory len:10000
memory used:2701.0
now epsilon is 0.1788039737180301, the reward is 247.25 with loss [23.31581473350525, 21.79822587966919] in episode 1259
Report: 
rewardSum:247.25
loss:[23.31581473350525, 21.79822587966919]
policies:[0, 4, 0]
qAverage:[0.0, 73.87741394042969]
ws:[2.605350947380066, 10.555834579467774]
memory len:10000
memory used:2701.0
now epsilon is 0.1786252367846277, the reward is 247.25 with loss [32.808765172958374, 22.53080451488495] in episode 1260
Report: 
rewardSum:247.25
loss:[32.808765172958374, 22.53080451488495]
policies:[0, 4, 0]
qAverage:[0.0, 75.02844696044922]
ws:[1.8979181051254272, 9.281281185150146]
memory len:10000
memory used:2701.0
now epsilon is 0.1784466785211435, the reward is 247.25 with loss [32.08557915687561, 38.14326858520508] in episode 1261
Report: 
rewardSum:247.25
loss:[32.08557915687561, 38.14326858520508]
policies:[0, 2, 2]
qAverage:[0.0, 63.180686950683594]
ws:[0.8385270436604818, 9.116742451985678]
memory len:10000
memory used:2701.0
now epsilon is 0.1782682987489746, the reward is 247.25 with loss [18.308278560638428, 22.36366844177246] in episode 1262
Report: 
rewardSum:247.25
loss:[18.308278560638428, 22.36366844177246]
policies:[0, 4, 0]
qAverage:[0.0, 74.52256317138672]
ws:[1.6418916761875153, 8.66747064590454]
memory len:10000
memory used:2701.0
now epsilon is 0.17809009728969658, the reward is 247.25 with loss [32.5480523109436, 24.62266492843628] in episode 1263
Report: 
rewardSum:247.25
loss:[32.5480523109436, 24.62266492843628]
policies:[0, 4, 0]
qAverage:[0.0, 76.11782684326172]
ws:[3.053970658779144, 11.227631664276123]
memory len:10000
memory used:2700.0
now epsilon is 0.17791207396506348, the reward is 59.6875 with loss [31.497347831726074, 22.575568675994873] in episode 1264
Report: 
rewardSum:59.6875
loss:[31.497347831726074, 22.575568675994873]
policies:[0, 2, 2]
qAverage:[0.0, 62.422533671061196]
ws:[1.7592209577560425, 5.439767996470134]
memory len:10000
memory used:2700.0
now epsilon is 0.17764537259109817, the reward is 245.25 with loss [37.73923873901367, 39.892313957214355] in episode 1265
Report: 
rewardSum:245.25
loss:[37.73923873901367, 39.892313957214355]
policies:[0, 4, 2]
qAverage:[0.0, 76.80975189208985]
ws:[3.356493902206421, 11.04015588760376]
memory len:10000
memory used:2700.0
now epsilon is 0.17746779382441966, the reward is 247.25 with loss [40.18201494216919, 27.477707862854004] in episode 1266
Report: 
rewardSum:247.25
loss:[40.18201494216919, 27.477707862854004]
policies:[0, 4, 0]
qAverage:[0.0, 78.67261352539063]
ws:[3.8932984113693236, 12.69414587020874]
memory len:10000
memory used:2700.0
now epsilon is 0.17729039256992687, the reward is 247.25 with loss [20.903057098388672, 26.59868049621582] in episode 1267
Report: 
rewardSum:247.25
loss:[20.903057098388672, 26.59868049621582]
policies:[0, 3, 1]
qAverage:[0.0, 78.32052040100098]
ws:[4.110273957252502, 14.592721700668335]
memory len:10000
memory used:2700.0
now epsilon is 0.17702462313542225, the reward is 245.25 with loss [44.34553813934326, 29.67585062980652] in episode 1268
Report: 
rewardSum:245.25
loss:[44.34553813934326, 29.67585062980652]
policies:[0, 5, 1]
qAverage:[0.0, 78.24361928304036]
ws:[1.373192568620046, 9.391268014907837]
memory len:10000
memory used:2700.0
now epsilon is 0.17684766488545717, the reward is 247.25 with loss [35.289435386657715, 21.382964372634888] in episode 1269
Report: 
rewardSum:247.25
loss:[35.289435386657715, 21.382964372634888]
policies:[0, 3, 1]
qAverage:[0.0, 78.70375061035156]
ws:[1.8545235395431519, 11.117469787597656]
memory len:10000
memory used:2700.0
now epsilon is 0.17667088352739374, the reward is 247.25 with loss [24.510157823562622, 23.671684622764587] in episode 1270
Report: 
rewardSum:247.25
loss:[24.510157823562622, 23.671684622764587]
policies:[0, 2, 2]
qAverage:[0.0, 51.74954605102539]
ws:[1.4475421905517578, 4.705668926239014]
memory len:10000
memory used:2700.0
############# STATE ###############
0-		8-		16-		24*		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.17631785077984646, the reward is 243.25 with loss [38.5434730052948, 54.9911413192749] in episode 1271
Report: 
rewardSum:243.25
loss:[38.5434730052948, 54.9911413192749]
policies:[1, 4, 3]
qAverage:[0.0, 69.6997299194336]
ws:[1.1603115275502205, 8.33575189113617]
memory len:10000
memory used:2700.0
now epsilon is 0.1760535392465728, the reward is 245.25 with loss [42.923511028289795, 38.39710092544556] in episode 1272
Report: 
rewardSum:245.25
loss:[42.923511028289795, 38.39710092544556]
policies:[0, 5, 1]
qAverage:[0.0, 80.30924987792969]
ws:[3.299296188354492, 11.445116138458252]
memory len:10000
memory used:2700.0
now epsilon is 0.1758775517164008, the reward is 247.25 with loss [23.529018878936768, 27.757984161376953] in episode 1273
Report: 
rewardSum:247.25
loss:[23.529018878936768, 27.757984161376953]
policies:[0, 3, 1]
qAverage:[0.0, 65.94191233317058]
ws:[3.729424794514974, 8.59167448679606]
memory len:10000
memory used:2701.0
now epsilon is 0.17565781467274774, the reward is 246.25 with loss [29.605539798736572, 36.286025285720825] in episode 1274
Report: 
rewardSum:246.25
loss:[29.605539798736572, 36.286025285720825]
policies:[1, 3, 1]
qAverage:[0.0, 72.50452423095703]
ws:[1.2149754166603088, 3.3162758350372314]
memory len:10000
memory used:2701.0
now epsilon is 0.1754822227187776, the reward is 247.25 with loss [30.647452116012573, 27.45828604698181] in episode 1275
Report: 
rewardSum:247.25
loss:[30.647452116012573, 27.45828604698181]
policies:[0, 3, 1]
qAverage:[0.0, 81.8609561920166]
ws:[3.495907336473465, 10.67203962802887]
memory len:10000
memory used:2700.0
now epsilon is 0.17521916384445532, the reward is 245.25 with loss [44.82597732543945, 48.023699164390564] in episode 1276
Report: 
rewardSum:245.25
loss:[44.82597732543945, 48.023699164390564]
policies:[1, 4, 1]
qAverage:[0.0, 68.74786376953125]
ws:[1.8743069767951965, 4.178786873817444]
memory len:10000
memory used:2701.0
now epsilon is 0.1750440103768468, the reward is 247.25 with loss [43.76871109008789, 30.474761724472046] in episode 1277
Report: 
rewardSum:247.25
loss:[43.76871109008789, 30.474761724472046]
policies:[0, 4, 0]
qAverage:[0.0, 83.50909271240235]
ws:[2.027449995279312, 7.2539952278137205]
memory len:10000
memory used:2701.0
now epsilon is 0.1748690319970343, the reward is 59.6875 with loss [20.15861463546753, 23.264606952667236] in episode 1278
Report: 
rewardSum:59.6875
loss:[20.15861463546753, 23.264606952667236]
policies:[0, 2, 2]
qAverage:[0.0, 58.839497884114586]
ws:[1.309048096338908, 2.654751777648926]
memory len:10000
memory used:2701.0
now epsilon is 0.17469422852999564, the reward is 247.25 with loss [19.57712984085083, 28.680439949035645] in episode 1279
Report: 
rewardSum:247.25
loss:[19.57712984085083, 28.680439949035645]
policies:[1, 2, 1]
qAverage:[0.0, 77.40465037027995]
ws:[1.4160868550340335, 8.329522450764975]
memory len:10000
memory used:2701.0
now epsilon is 0.17443235090845818, the reward is 245.25 with loss [30.943849563598633, 34.13035774230957] in episode 1280
Report: 
rewardSum:245.25
loss:[30.943849563598633, 34.13035774230957]
policies:[1, 4, 1]
qAverage:[0.0, 78.03931579589843]
ws:[2.8553789615631104, 7.385212564468384]
memory len:10000
memory used:2701.0
now epsilon is 0.17425798395878, the reward is 247.25 with loss [28.07249164581299, 24.202054500579834] in episode 1281
Report: 
rewardSum:247.25
loss:[28.07249164581299, 24.202054500579834]
policies:[0, 4, 0]
qAverage:[0.0, 83.43943786621094]
ws:[3.8972585201263428, 8.520105266571045]
memory len:10000
memory used:2701.0
now epsilon is 0.17408379131067478, the reward is 247.25 with loss [24.060906887054443, 19.827858209609985] in episode 1282
Report: 
rewardSum:247.25
loss:[24.060906887054443, 19.827858209609985]
policies:[0, 4, 0]
qAverage:[0.0, 84.05514831542969]
ws:[2.5670612335205076, 7.110396051406861]
memory len:10000
memory used:2701.0
now epsilon is 0.17386629534670883, the reward is 246.25 with loss [27.549546718597412, 23.743480920791626] in episode 1283
Report: 
rewardSum:246.25
loss:[27.549546718597412, 23.743480920791626]
policies:[0, 4, 1]
qAverage:[0.0, 83.01942749023438]
ws:[3.2454823732376097, 9.872115659713746]
memory len:10000
memory used:2701.0
now epsilon is 0.17360565884901766, the reward is 245.25 with loss [41.00074100494385, 30.812814116477966] in episode 1284
Report: 
rewardSum:245.25
loss:[41.00074100494385, 30.812814116477966]
policies:[0, 5, 1]
qAverage:[0.0, 84.98041025797527]
ws:[2.3208626409371695, 8.699855963389078]
memory len:10000
memory used:2701.0
now epsilon is 0.17343211828144106, the reward is 59.6875 with loss [27.64973735809326, 25.26559591293335] in episode 1285
Report: 
rewardSum:59.6875
loss:[27.64973735809326, 25.26559591293335]
policies:[1, 1, 2]
qAverage:[0.0, 50.36223602294922]
ws:[5.597161293029785, 11.562837600708008]
memory len:10000
memory used:2701.0
now epsilon is 0.17325875118936515, the reward is 247.25 with loss [20.821826219558716, 21.224714040756226] in episode 1286
Report: 
rewardSum:247.25
loss:[20.821826219558716, 21.224714040756226]
policies:[0, 4, 0]
qAverage:[0.0, 82.36489868164062]
ws:[4.284192085266113, 12.532585382461548]
memory len:10000
memory used:2701.0
now epsilon is 0.1730855573993795, the reward is 247.25 with loss [21.294175148010254, 23.2825984954834] in episode 1287
Report: 
rewardSum:247.25
loss:[21.294175148010254, 23.2825984954834]
policies:[0, 4, 0]
qAverage:[0.0, 87.57796173095703]
ws:[4.9123118877410885, 12.595992851257325]
memory len:10000
memory used:2701.0
now epsilon is 0.172912536738247, the reward is 247.25 with loss [32.71414756774902, 25.415573120117188] in episode 1288
Report: 
rewardSum:247.25
loss:[32.71414756774902, 25.415573120117188]
policies:[0, 2, 2]
qAverage:[0.0, 67.17366536458333]
ws:[3.211464802424113, 7.393326123555501]
memory len:10000
memory used:2701.0
now epsilon is 0.17269650411064547, the reward is 246.25 with loss [28.945815801620483, 38.98946714401245] in episode 1289
Report: 
rewardSum:246.25
loss:[28.945815801620483, 38.98946714401245]
policies:[0, 4, 1]
qAverage:[0.0, 87.79993896484375]
ws:[5.332314300537109, 12.771664047241211]
memory len:10000
memory used:2701.0
now epsilon is 0.172523872356931, the reward is 247.25 with loss [25.31462574005127, 22.797233819961548] in episode 1290
Report: 
rewardSum:247.25
loss:[25.31462574005127, 22.797233819961548]
policies:[0, 4, 0]
qAverage:[0.0, 70.54134877522786]
ws:[2.1954992612202964, 5.979307492574056]
memory len:10000
memory used:2701.0
now epsilon is 0.17235141317024416, the reward is 247.25 with loss [27.866890907287598, 26.858474016189575] in episode 1291
Report: 
rewardSum:247.25
loss:[27.866890907287598, 26.858474016189575]
policies:[1, 2, 1]
qAverage:[0.0, 43.9462890625]
ws:[-0.22564679384231567, 1.5989350080490112]
memory len:10000
memory used:2701.0
now epsilon is 0.1719640100551644, the reward is 242.25 with loss [56.11478900909424, 67.13128554821014] in episode 1292
Report: 
rewardSum:242.25
loss:[56.11478900909424, 67.13128554821014]
policies:[0, 5, 4]
qAverage:[0.0, 90.21366500854492]
ws:[3.9980697010954223, 8.321223258972168]
memory len:10000
memory used:2701.0
now epsilon is 0.17179211052086593, the reward is 247.25 with loss [20.96176314353943, 41.52384805679321] in episode 1293
Report: 
rewardSum:247.25
loss:[20.96176314353943, 41.52384805679321]
policies:[0, 4, 0]
qAverage:[0.0, 87.38784790039062]
ws:[3.579927623271942, 7.238370895385742]
memory len:10000
memory used:2701.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18*		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.1716203828216502, the reward is 59.6875 with loss [23.9709734916687, 25.05357789993286] in episode 1294
Report: 
rewardSum:59.6875
loss:[23.9709734916687, 25.05357789993286]
policies:[0, 3, 1]
qAverage:[0.0, 60.76321156819662]
ws:[1.2363403836886089, 1.8687001466751099]
memory len:10000
memory used:2701.0
now epsilon is 0.17136311308790536, the reward is 245.25 with loss [44.73088550567627, 38.96172857284546] in episode 1295
Report: 
rewardSum:245.25
loss:[44.73088550567627, 38.96172857284546]
policies:[1, 3, 2]
qAverage:[0.0, 94.54746437072754]
ws:[2.781538976356387, 5.441162042319775]
memory len:10000
memory used:2701.0
now epsilon is 0.17119181422527538, the reward is 247.25 with loss [26.634195804595947, 28.50670051574707] in episode 1296
Report: 
rewardSum:247.25
loss:[26.634195804595947, 28.50670051574707]
policies:[1, 3, 0]
qAverage:[0.0, 79.93046379089355]
ws:[1.3945174664258957, 3.927576929330826]
memory len:10000
memory used:2701.0
now epsilon is 0.17102068659728165, the reward is 247.25 with loss [35.14297676086426, 34.0236029624939] in episode 1297
Report: 
rewardSum:247.25
loss:[35.14297676086426, 34.0236029624939]
policies:[0, 4, 0]
qAverage:[0.0, 91.8047866821289]
ws:[5.209246587753296, 11.533566284179688]
memory len:10000
memory used:2701.0
now epsilon is 0.17084973003275372, the reward is 247.25 with loss [29.439929485321045, 26.00675368309021] in episode 1298
Report: 
rewardSum:247.25
loss:[29.439929485321045, 26.00675368309021]
policies:[1, 2, 1]
qAverage:[0.0, 64.85031127929688]
ws:[6.236964066823323, 12.476778666178385]
memory len:10000
memory used:2702.0
now epsilon is 0.1706789443606923, the reward is 247.25 with loss [32.40442609786987, 36.69360589981079] in episode 1299
Report: 
rewardSum:247.25
loss:[32.40442609786987, 36.69360589981079]
policies:[0, 3, 1]
qAverage:[0.0, 91.97938919067383]
ws:[9.47034215927124, 17.596960067749023]
memory len:10000
memory used:2702.0
now epsilon is 0.17046570232791647, the reward is 246.25 with loss [27.30327534675598, 31.400334119796753] in episode 1300
Report: 
rewardSum:246.25
loss:[27.30327534675598, 31.400334119796753]
policies:[0, 3, 2]
qAverage:[0.0, 91.00779342651367]
ws:[11.20670771598816, 19.857797145843506]
memory len:10000
memory used:2702.0
now epsilon is 0.17029530053957348, the reward is 247.25 with loss [23.74449396133423, 29.787164211273193] in episode 1301
Report: 
rewardSum:247.25
loss:[23.74449396133423, 29.787164211273193]
policies:[2, 2, 0]
qAverage:[0.0, 45.020721435546875]
ws:[2.149534225463867, 3.9300482273101807]
memory len:10000
memory used:2702.0
now epsilon is 0.1700400171874011, the reward is 245.25 with loss [52.41900634765625, 34.9127631187439] in episode 1302
Report: 
rewardSum:245.25
loss:[52.41900634765625, 34.9127631187439]
policies:[0, 5, 1]
qAverage:[0.0, 93.27129618326823]
ws:[7.309554139773051, 12.12510347366333]
memory len:10000
memory used:2702.0
now epsilon is 0.16982757341436222, the reward is 246.25 with loss [31.93004560470581, 35.825461745262146] in episode 1303
Report: 
rewardSum:246.25
loss:[31.93004560470581, 35.825461745262146]
policies:[0, 4, 1]
qAverage:[0.0, 83.10979461669922]
ws:[8.811481833457947, 13.147410333156586]
memory len:10000
memory used:2702.0
now epsilon is 0.16965780951567436, the reward is 247.25 with loss [30.78447151184082, 23.587907791137695] in episode 1304
Report: 
rewardSum:247.25
loss:[30.78447151184082, 23.587907791137695]
policies:[0, 3, 1]
qAverage:[0.0, 91.0821647644043]
ws:[8.26782476902008, 13.167653322219849]
memory len:10000
memory used:2702.0
now epsilon is 0.1694882153172343, the reward is 247.25 with loss [26.963927745819092, 17.6853768825531] in episode 1305
Report: 
rewardSum:247.25
loss:[26.963927745819092, 17.6853768825531]
policies:[0, 4, 0]
qAverage:[0.0, 92.42132263183593]
ws:[6.907302886247635, 12.509315410256386]
memory len:10000
memory used:2702.0
now epsilon is 0.16931879064940544, the reward is 247.25 with loss [35.93799448013306, 26.586887538433075] in episode 1306
Report: 
rewardSum:247.25
loss:[35.93799448013306, 26.586887538433075]
policies:[0, 4, 0]
qAverage:[0.0, 90.60259704589843]
ws:[6.555649590492249, 11.878578186035156]
memory len:10000
memory used:2702.0
now epsilon is 0.1690649711468954, the reward is 245.25 with loss [35.402989864349365, 39.57353615760803] in episode 1307
Report: 
rewardSum:245.25
loss:[35.402989864349365, 39.57353615760803]
policies:[1, 3, 2]
qAverage:[0.0, 73.25752512613933]
ws:[4.412386576334636, 8.049877484639486]
memory len:10000
memory used:2702.0
now epsilon is 0.1688959695645468, the reward is 247.25 with loss [28.376022815704346, 26.656713008880615] in episode 1308
Report: 
rewardSum:247.25
loss:[28.376022815704346, 26.656713008880615]
policies:[0, 3, 1]
qAverage:[0.0, 72.58729553222656]
ws:[10.85986042022705, 18.060352325439453]
memory len:10000
memory used:2702.0
now epsilon is 0.16872713692041552, the reward is 247.25 with loss [25.95454740524292, 18.95219922065735] in episode 1309
Report: 
rewardSum:247.25
loss:[25.95454740524292, 18.95219922065735]
policies:[1, 3, 0]
qAverage:[0.0, 76.31282806396484]
ws:[8.046489715576172, 14.279086112976074]
memory len:10000
memory used:2702.0
now epsilon is 0.16855847304562668, the reward is 247.25 with loss [23.63240146636963, 24.83999514579773] in episode 1310
Report: 
rewardSum:247.25
loss:[23.63240146636963, 24.83999514579773]
policies:[0, 2, 2]
qAverage:[0.0, 66.99949900309245]
ws:[2.38007386525472, 5.387295087178548]
memory len:10000
memory used:2702.0
now epsilon is 0.16838997777147424, the reward is 247.25 with loss [32.43839383125305, 23.732648849487305] in episode 1311
Report: 
rewardSum:247.25
loss:[32.43839383125305, 23.732648849487305]
policies:[1, 2, 1]
qAverage:[0.0, 76.25580596923828]
ws:[1.8519412676493328, 5.426671028137207]
memory len:10000
memory used:2702.0
now epsilon is 0.16822165092942074, the reward is 247.25 with loss [18.16809868812561, 19.158021211624146] in episode 1312
Report: 
rewardSum:247.25
loss:[18.16809868812561, 19.158021211624146]
policies:[1, 2, 1]
qAverage:[0.0, 78.21892801920573]
ws:[5.211387713750203, 11.299309849739075]
memory len:10000
memory used:2702.0
now epsilon is 0.16801147897800947, the reward is 246.25 with loss [59.91899538040161, 41.37581396102905] in episode 1313
Report: 
rewardSum:246.25
loss:[59.91899538040161, 41.37581396102905]
policies:[0, 4, 1]
qAverage:[0.0, 93.72178344726562]
ws:[6.662837368249893, 14.53150749206543]
memory len:10000
memory used:2702.0
now epsilon is 0.16784353049283607, the reward is 247.25 with loss [18.818230152130127, 24.77838134765625] in episode 1314
Report: 
rewardSum:247.25
loss:[18.818230152130127, 24.77838134765625]
policies:[1, 3, 0]
qAverage:[0.0, 92.94214248657227]
ws:[6.037884831428528, 11.447003483772278]
memory len:10000
memory used:2701.0
now epsilon is 0.16767574989317763, the reward is 247.25 with loss [28.376837015151978, 23.980474710464478] in episode 1315
Report: 
rewardSum:247.25
loss:[28.376837015151978, 23.980474710464478]
policies:[0, 4, 0]
qAverage:[0.0, 93.717578125]
ws:[5.668231630325318, 11.592657881975175]
memory len:10000
memory used:2701.0
now epsilon is 0.16742439341196458, the reward is 245.25 with loss [36.61060094833374, 40.6924729347229] in episode 1316
Report: 
rewardSum:245.25
loss:[36.61060094833374, 40.6924729347229]
policies:[0, 4, 2]
qAverage:[0.0, 81.52376747131348]
ws:[1.496455505490303, 3.2415485829114914]
memory len:10000
memory used:2701.0
now epsilon is 0.16725703179223678, the reward is 247.25 with loss [24.276444911956787, 34.178457260131836] in episode 1317
Report: 
rewardSum:247.25
loss:[24.276444911956787, 34.178457260131836]
policies:[0, 4, 0]
qAverage:[0.0, 92.62976379394532]
ws:[6.842365956306457, 12.935387563705444]
memory len:10000
memory used:2701.0
now epsilon is 0.1670898374713786, the reward is 247.25 with loss [24.844492435455322, 19.550803661346436] in episode 1318
Report: 
rewardSum:247.25
loss:[24.844492435455322, 19.550803661346436]
policies:[0, 3, 1]
qAverage:[0.0, 91.40562438964844]
ws:[6.763626893361409, 12.814446926116943]
memory len:10000
memory used:2701.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21*		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.16692281028215383, the reward is 247.25 with loss [23.79713010787964, 27.288016319274902] in episode 1319
Report: 
rewardSum:247.25
loss:[23.79713010787964, 27.288016319274902]
policies:[0, 4, 0]
qAverage:[0.0, 92.03064346313477]
ws:[5.792418867349625, 10.817219913005829]
memory len:10000
memory used:2701.0
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.16675595005749352, the reward is 247.25 with loss [25.37757968902588, 45.294434547424316] in episode 1320
Report: 
rewardSum:247.25
loss:[25.37757968902588, 45.294434547424316]
policies:[0, 3, 1]
qAverage:[0.0, 94.37844657897949]
ws:[8.091976165771484, 15.690777778625488]
memory len:10000
memory used:2702.0
now epsilon is 0.16658925663049573, the reward is 247.25 with loss [26.44368886947632, 20.896198511123657] in episode 1321
Report: 
rewardSum:247.25
loss:[26.44368886947632, 20.896198511123657]
policies:[0, 4, 0]
qAverage:[0.0, 95.70478973388671]
ws:[8.124398326873779, 16.26555805206299]
memory len:10000
memory used:2702.0
now epsilon is 0.1664227298344253, the reward is 247.25 with loss [9.595017075538635, 27.12387180328369] in episode 1322
Report: 
rewardSum:247.25
loss:[9.595017075538635, 27.12387180328369]
policies:[0, 3, 1]
qAverage:[0.0, 81.30631637573242]
ws:[4.327600181102753, 8.923210859298706]
memory len:10000
memory used:2702.0
now epsilon is 0.16625636950271383, the reward is 247.25 with loss [23.77720856666565, 30.863389015197754] in episode 1323
Report: 
rewardSum:247.25
loss:[23.77720856666565, 30.863389015197754]
policies:[0, 2, 2]
qAverage:[0.0, 74.7469991048177]
ws:[1.895734469095866, 5.434944788614909]
memory len:10000
memory used:2702.0
now epsilon is 0.1660071407618608, the reward is 245.25 with loss [35.743250608444214, 38.76542091369629] in episode 1324
Report: 
rewardSum:245.25
loss:[35.743250608444214, 38.76542091369629]
policies:[0, 5, 1]
qAverage:[0.0, 97.13882954915364]
ws:[5.973955457874884, 12.181963483492533]
memory len:10000
memory used:2702.0
now epsilon is 0.16584119586340193, the reward is 247.25 with loss [27.42448377609253, 16.74410390853882] in episode 1325
Report: 
rewardSum:247.25
loss:[27.42448377609253, 16.74410390853882]
policies:[0, 4, 0]
qAverage:[0.0, 95.87207794189453]
ws:[5.66086767911911, 11.43619613647461]
memory len:10000
memory used:2702.0
now epsilon is 0.16567541684762258, the reward is 247.25 with loss [28.210886478424072, 35.1463565826416] in episode 1326
Report: 
rewardSum:247.25
loss:[28.210886478424072, 35.1463565826416]
policies:[0, 3, 1]
qAverage:[0.0, 95.10071563720703]
ws:[8.111871242523193, 15.54753065109253]
memory len:10000
memory used:2702.0
now epsilon is 0.1654270589912906, the reward is 245.25 with loss [51.70136070251465, 19.88661539554596] in episode 1327
Report: 
rewardSum:245.25
loss:[51.70136070251465, 19.88661539554596]
policies:[1, 4, 1]
qAverage:[0.0, 87.24125099182129]
ws:[5.780538365244865, 13.829844117164612]
memory len:10000
memory used:2702.0
now epsilon is 0.16526169395710788, the reward is 247.25 with loss [36.885730266571045, 27.632408618927002] in episode 1328
Report: 
rewardSum:247.25
loss:[36.885730266571045, 27.632408618927002]
policies:[0, 4, 0]
qAverage:[0.0, 96.37244567871093]
ws:[5.743502396345138, 14.529509735107421]
memory len:10000
memory used:2702.0
now epsilon is 0.16505522010240134, the reward is 246.25 with loss [26.843092918395996, 35.359697341918945] in episode 1329
Report: 
rewardSum:246.25
loss:[26.843092918395996, 35.359697341918945]
policies:[0, 3, 2]
qAverage:[0.0, 90.41113471984863]
ws:[6.489161521196365, 15.27904224395752]
memory len:10000
memory used:2702.0
now epsilon is 0.16489022676769122, the reward is 247.25 with loss [19.188733100891113, 25.151917457580566] in episode 1330
Report: 
rewardSum:247.25
loss:[19.188733100891113, 25.151917457580566]
policies:[0, 4, 0]
qAverage:[0.0, 98.28116149902344]
ws:[7.692978763580323, 17.90196681022644]
memory len:10000
memory used:2702.0
now epsilon is 0.16464304596060875, the reward is 245.25 with loss [31.023667335510254, 43.86922025680542] in episode 1331
Report: 
rewardSum:245.25
loss:[31.023667335510254, 43.86922025680542]
policies:[1, 4, 1]
qAverage:[0.0, 103.00672149658203]
ws:[8.004157066345215, 17.101006698608398]
memory len:10000
memory used:2702.0
now epsilon is 0.16447846464550084, the reward is 247.25 with loss [15.528943061828613, 32.863016843795776] in episode 1332
Report: 
rewardSum:247.25
loss:[15.528943061828613, 32.863016843795776]
policies:[0, 4, 0]
qAverage:[0.0, 89.1303939819336]
ws:[7.7831909991800785, 16.167202711105347]
memory len:10000
memory used:2702.0
now epsilon is 0.1643140478500003, the reward is 247.25 with loss [35.2727313041687, 34.003268241882324] in episode 1333
Report: 
rewardSum:247.25
loss:[35.2727313041687, 34.003268241882324]
policies:[0, 3, 1]
qAverage:[0.0, 92.89664713541667]
ws:[10.066581805547079, 20.51535479227702]
memory len:10000
memory used:2702.0
now epsilon is 0.16414979540964927, the reward is 59.6875 with loss [16.84346294403076, 26.162519931793213] in episode 1334
Report: 
rewardSum:59.6875
loss:[16.84346294403076, 26.162519931793213]
policies:[0, 3, 1]
qAverage:[0.0, 84.01633071899414]
ws:[1.350883573293686, 6.170514702796936]
memory len:10000
memory used:2702.0
now epsilon is 0.16398570716015418, the reward is 247.25 with loss [24.256994009017944, 14.982648730278015] in episode 1335
Report: 
rewardSum:247.25
loss:[24.256994009017944, 14.982648730278015]
policies:[0, 4, 0]
qAverage:[0.0, 96.61039581298829]
ws:[6.50886857509613, 15.168493556976319]
memory len:10000
memory used:2702.0
now epsilon is 0.16373988228477848, the reward is 57.6875 with loss [47.37256383895874, 41.3457670211792] in episode 1336
Report: 
rewardSum:57.6875
loss:[47.37256383895874, 41.3457670211792]
policies:[0, 4, 2]
qAverage:[0.0, 87.04607543945312]
ws:[1.2609796285629273, 5.434743118286133]
memory len:10000
memory used:2702.0
now epsilon is 0.1635762037947165, the reward is 247.25 with loss [18.94766867160797, 34.52507162094116] in episode 1337
Report: 
rewardSum:247.25
loss:[18.94766867160797, 34.52507162094116]
policies:[0, 3, 1]
qAverage:[0.0, 94.35797627766927]
ws:[9.893120288848877, 16.091969172159832]
memory len:10000
memory used:2702.0
now epsilon is 0.16324933750239926, the reward is 55.6875 with loss [54.669793128967285, 56.18136405944824] in episode 1338
Report: 
rewardSum:55.6875
loss:[54.669793128967285, 56.18136405944824]
policies:[0, 4, 4]
qAverage:[0.0, 86.61473999023437]
ws:[2.3672167122364045, 3.4710723161697388]
memory len:10000
memory used:2702.0
now epsilon is 0.163086149373196, the reward is 247.25 with loss [23.31975269317627, 29.76015567779541] in episode 1339
Report: 
rewardSum:247.25
loss:[23.31975269317627, 29.76015567779541]
policies:[1, 3, 0]
qAverage:[0.0, 98.55484008789062]
ws:[7.904845595359802, 12.977917671203613]
memory len:10000
memory used:2702.0
now epsilon is 0.1629231243709366, the reward is 247.25 with loss [32.89854192733765, 27.550976276397705] in episode 1340
Report: 
rewardSum:247.25
loss:[32.89854192733765, 27.550976276397705]
policies:[0, 4, 0]
qAverage:[0.0, 100.76395263671876]
ws:[6.3019781216979025, 11.446428561210633]
memory len:10000
memory used:2702.0
now epsilon is 0.16276026233255525, the reward is 247.25 with loss [30.629642486572266, 29.112829208374023] in episode 1341
Report: 
rewardSum:247.25
loss:[30.629642486572266, 29.112829208374023]
policies:[0, 4, 0]
qAverage:[0.0, 99.66541595458985]
ws:[8.331201142072677, 16.332035779953003]
memory len:10000
memory used:2702.0
now epsilon is 0.16255691370437542, the reward is 246.25 with loss [47.50326108932495, 32.36268472671509] in episode 1342
Report: 
rewardSum:246.25
loss:[47.50326108932495, 32.36268472671509]
policies:[0, 4, 1]
qAverage:[0.0, 89.53518422444661]
ws:[9.696336189905802, 20.14226468404134]
memory len:10000
memory used:2701.0
now epsilon is 0.1623538191349197, the reward is 246.25 with loss [40.43277072906494, 20.408783197402954] in episode 1343
Report: 
rewardSum:246.25
loss:[40.43277072906494, 20.408783197402954]
policies:[0, 4, 1]
qAverage:[0.0, 101.22128295898438]
ws:[9.425357055664062, 20.403776931762696]
memory len:10000
memory used:2702.0
############# STATE ###############
0-		8-		16*		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.16219152618832047, the reward is 247.25 with loss [36.456249952316284, 26.666959762573242] in episode 1344
Report: 
rewardSum:247.25
loss:[36.456249952316284, 26.666959762573242]
policies:[0, 4, 0]
qAverage:[0.0, 100.34027862548828]
ws:[9.194422006607056, 20.177914142608643]
memory len:10000
memory used:2702.0
now epsilon is 0.16202939547381814, the reward is 247.25 with loss [24.174806118011475, 27.238219261169434] in episode 1345
Report: 
rewardSum:247.25
loss:[24.174806118011475, 27.238219261169434]
policies:[0, 3, 1]
qAverage:[0.0, 92.72809410095215]
ws:[10.515416979789734, 21.3539217710495]
memory len:10000
memory used:2702.0
now epsilon is 0.16186742682924143, the reward is 247.25 with loss [20.14585041999817, 24.12700653076172] in episode 1346
Report: 
rewardSum:247.25
loss:[20.14585041999817, 24.12700653076172]
policies:[0, 4, 0]
qAverage:[0.0, 96.983154296875]
ws:[7.112804144620895, 16.03899049758911]
memory len:10000
memory used:2702.0
now epsilon is 0.16170562009258121, the reward is 247.25 with loss [23.414276361465454, 41.15590000152588] in episode 1347
Report: 
rewardSum:247.25
loss:[23.414276361465454, 41.15590000152588]
policies:[1, 3, 0]
qAverage:[0.0, 95.50953102111816]
ws:[8.222387790679932, 17.326946020126343]
memory len:10000
memory used:2702.0
now epsilon is 0.16154397510199023, the reward is 247.25 with loss [26.392761826515198, 18.86830770969391] in episode 1348
Report: 
rewardSum:247.25
loss:[26.392761826515198, 18.86830770969391]
policies:[0, 3, 1]
qAverage:[0.0, 89.0915470123291]
ws:[2.8269982263445854, 8.991493225097656]
memory len:10000
memory used:2702.0
now epsilon is 0.16130181053634093, the reward is 245.25 with loss [43.38760018348694, 40.58635973930359] in episode 1349
Report: 
rewardSum:245.25
loss:[43.38760018348694, 40.58635973930359]
policies:[2, 3, 1]
qAverage:[0.0, 92.32114219665527]
ws:[8.344437643885612, 14.408406972885132]
memory len:10000
memory used:2701.0
now epsilon is 0.16114056920390285, the reward is 247.25 with loss [24.694839477539062, 19.800697326660156] in episode 1350
Report: 
rewardSum:247.25
loss:[24.694839477539062, 19.800697326660156]
policies:[0, 4, 0]
qAverage:[0.0, 99.28340911865234]
ws:[8.93637411594391, 13.773970746994019]
memory len:10000
memory used:2702.0
now epsilon is 0.16097948905234177, the reward is 247.25 with loss [28.39299511909485, 21.58588433265686] in episode 1351
Report: 
rewardSum:247.25
loss:[28.39299511909485, 21.58588433265686]
policies:[0, 3, 1]
qAverage:[0.0, 78.7122090657552]
ws:[2.515661140282949, 4.6009758313496905]
memory len:10000
memory used:2702.0
now epsilon is 0.1607381706867376, the reward is 245.25 with loss [49.27670478820801, 35.9664511680603] in episode 1352
Report: 
rewardSum:245.25
loss:[49.27670478820801, 35.9664511680603]
policies:[0, 5, 1]
qAverage:[0.0, 100.74888763427734]
ws:[8.166391563415527, 11.433358931541443]
memory len:10000
memory used:2702.0
now epsilon is 0.16057749278281938, the reward is 247.25 with loss [21.94795846939087, 31.12581491470337] in episode 1353
Report: 
rewardSum:247.25
loss:[21.94795846939087, 31.12581491470337]
policies:[0, 3, 1]
qAverage:[0.0, 94.4345588684082]
ws:[7.855797678232193, 11.75987035036087]
memory len:10000
memory used:2702.0
now epsilon is 0.16041697549656092, the reward is 247.25 with loss [33.93569564819336, 28.302238941192627] in episode 1354
Report: 
rewardSum:247.25
loss:[33.93569564819336, 28.302238941192627]
policies:[0, 4, 0]
qAverage:[0.0, 98.48905754089355]
ws:[11.818190217018127, 16.78730297088623]
memory len:10000
memory used:2702.0
now epsilon is 0.16025661866740476, the reward is 247.25 with loss [33.15591883659363, 22.58641815185547] in episode 1355
Report: 
rewardSum:247.25
loss:[33.15591883659363, 22.58641815185547]
policies:[0, 4, 0]
qAverage:[0.0, 101.79316558837891]
ws:[7.042723774909973, 10.09055244922638]
memory len:10000
memory used:2702.0
now epsilon is 0.16001638392991288, the reward is 245.25 with loss [50.91647911071777, 38.547396659851074] in episode 1356
Report: 
rewardSum:245.25
loss:[50.91647911071777, 38.547396659851074]
policies:[0, 5, 1]
qAverage:[0.0, 99.31226806640625]
ws:[11.563880717754364, 16.24093804359436]
memory len:10000
memory used:2702.0
now epsilon is 0.15981646343524103, the reward is 58.6875 with loss [26.80231237411499, 32.76824140548706] in episode 1357
Report: 
rewardSum:58.6875
loss:[26.80231237411499, 32.76824140548706]
policies:[1, 2, 2]
qAverage:[0.0, 77.91162618001302]
ws:[2.7844814459482827, 4.296459515889485]
memory len:10000
memory used:2702.0
now epsilon is 0.1596567068929917, the reward is 247.25 with loss [25.702619075775146, 23.739428520202637] in episode 1358
Report: 
rewardSum:247.25
loss:[25.702619075775146, 23.739428520202637]
policies:[0, 4, 0]
qAverage:[0.0, 101.34620971679688]
ws:[9.851567095518112, 12.865874171257019]
memory len:10000
memory used:2702.0
now epsilon is 0.15949711004738587, the reward is 247.25 with loss [28.1514310836792, 37.43304443359375] in episode 1359
Report: 
rewardSum:247.25
loss:[28.1514310836792, 37.43304443359375]
policies:[1, 2, 1]
qAverage:[0.0, 89.64461008707683]
ws:[10.488895098368326, 13.202537536621094]
memory len:10000
memory used:2702.0
now epsilon is 0.15933767273878682, the reward is 247.25 with loss [22.36765432357788, 20.02114564180374] in episode 1360
Report: 
rewardSum:247.25
loss:[22.36765432357788, 20.02114564180374]
policies:[1, 3, 0]
qAverage:[0.0, 98.6772518157959]
ws:[9.763983011245728, 13.352314949035645]
memory len:10000
memory used:2702.0
now epsilon is 0.15909881555896319, the reward is 245.25 with loss [42.32752561569214, 73.7290735244751] in episode 1361
Report: 
rewardSum:245.25
loss:[42.32752561569214, 73.7290735244751]
policies:[0, 4, 2]
qAverage:[0.0, 97.95456123352051]
ws:[10.994044601917267, 15.019833445549011]
memory len:10000
memory used:2702.0
now epsilon is 0.158939776395517, the reward is 247.25 with loss [31.0169620513916, 31.010602474212646] in episode 1362
Report: 
rewardSum:247.25
loss:[31.0169620513916, 31.010602474212646]
policies:[1, 3, 0]
qAverage:[0.0, 91.57826232910156]
ws:[2.6942958533763885, 4.980261117219925]
memory len:10000
memory used:2714.0
now epsilon is 0.15878089621160454, the reward is 247.25 with loss [22.74104642868042, 27.169148445129395] in episode 1363
Report: 
rewardSum:247.25
loss:[22.74104642868042, 27.169148445129395]
policies:[0, 4, 0]
qAverage:[0.0, 103.95663757324219]
ws:[9.330327429249882, 13.25004026889801]
memory len:10000
memory used:2714.0
now epsilon is 0.15854287367476763, the reward is 245.25 with loss [35.948909282684326, 42.084659576416016] in episode 1364
Report: 
rewardSum:245.25
loss:[35.948909282684326, 42.084659576416016]
policies:[0, 5, 1]
qAverage:[0.0, 106.75840632120769]
ws:[5.746126679082711, 9.29977571964264]
memory len:10000
memory used:2714.0
now epsilon is 0.1583843902447622, the reward is 247.25 with loss [16.418363571166992, 42.304065227508545] in episode 1365
Report: 
rewardSum:247.25
loss:[16.418363571166992, 42.304065227508545]
policies:[1, 3, 0]
qAverage:[0.0, 98.9770450592041]
ws:[9.987574964761734, 14.683576703071594]
memory len:10000
memory used:2701.0
now epsilon is 0.15822606523876537, the reward is 247.25 with loss [29.92121720314026, 23.939701557159424] in episode 1366
Report: 
rewardSum:247.25
loss:[29.92121720314026, 23.939701557159424]
policies:[1, 3, 0]
qAverage:[0.0, 85.95304870605469]
ws:[11.659044663111368, 16.12736749649048]
memory len:10000
memory used:2701.0
now epsilon is 0.15806789849841257, the reward is 247.25 with loss [18.706202626228333, 19.199106693267822] in episode 1367
Report: 
rewardSum:247.25
loss:[18.706202626228333, 19.199106693267822]
policies:[0, 3, 1]
qAverage:[0.0, 81.58969116210938]
ws:[2.1309279203414917, 4.789867242177327]
memory len:10000
memory used:2701.0
now epsilon is 0.15790988986549748, the reward is 247.25 with loss [22.813755989074707, 29.491557598114014] in episode 1368
Report: 
rewardSum:247.25
loss:[22.813755989074707, 29.491557598114014]
policies:[0, 4, 0]
qAverage:[0.0, 81.68192036946614]
ws:[0.3642388582229614, 3.072902282079061]
memory len:10000
memory used:2702.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10*		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.15767317302188338, the reward is 245.25 with loss [37.50504684448242, 35.675360441207886] in episode 1369
Report: 
rewardSum:245.25
loss:[37.50504684448242, 35.675360441207886]
policies:[1, 4, 1]
qAverage:[0.0, 94.17199516296387]
ws:[8.619333326816559, 13.98586168885231]
memory len:10000
memory used:2702.0
now epsilon is 0.15751555896644745, the reward is 247.25 with loss [29.049822568893433, 26.633200645446777] in episode 1370
Report: 
rewardSum:247.25
loss:[29.049822568893433, 26.633200645446777]
policies:[0, 4, 0]
qAverage:[0.0, 103.89359436035156]
ws:[10.263381958007812, 16.57378759384155]
memory len:10000
memory used:2702.0
now epsilon is 0.1573581024659715, the reward is 247.25 with loss [28.896136045455933, 21.55635666847229] in episode 1371
Report: 
rewardSum:247.25
loss:[28.896136045455933, 21.55635666847229]
policies:[1, 2, 1]
qAverage:[0.0, 85.45628611246745]
ws:[3.644636631011963, 8.491977055867514]
memory len:10000
memory used:2702.0
now epsilon is 0.15720080336295972, the reward is 247.25 with loss [27.29482340812683, 19.338823795318604] in episode 1372
Report: 
rewardSum:247.25
loss:[27.29482340812683, 19.338823795318604]
policies:[0, 4, 0]
qAverage:[0.0, 105.75340881347657]
ws:[5.727541899681091, 11.682010650634766]
memory len:10000
memory used:2702.0
now epsilon is 0.15696514948455242, the reward is 245.25 with loss [53.78572607040405, 36.55813121795654] in episode 1373
Report: 
rewardSum:245.25
loss:[53.78572607040405, 36.55813121795654]
policies:[0, 5, 1]
qAverage:[0.0, 104.75244140625]
ws:[10.564489889144898, 17.08171057701111]
memory len:10000
memory used:2702.0
now epsilon is 0.15680824318718925, the reward is 247.25 with loss [25.435827255249023, 34.69543647766113] in episode 1374
Report: 
rewardSum:247.25
loss:[25.435827255249023, 34.69543647766113]
policies:[0, 4, 0]
qAverage:[0.0, 106.67524719238281]
ws:[11.81745891571045, 16.241002368927003]
memory len:10000
memory used:2702.0
now epsilon is 0.15665149373729337, the reward is 247.25 with loss [33.83609914779663, 24.0971941947937] in episode 1375
Report: 
rewardSum:247.25
loss:[33.83609914779663, 24.0971941947937]
policies:[1, 3, 0]
qAverage:[0.0, 94.44249725341797]
ws:[8.473540663719177, 12.973512172698975]
memory len:10000
memory used:2702.0
now epsilon is 0.15649490097807614, the reward is 247.25 with loss [22.746488094329834, 19.747779607772827] in episode 1376
Report: 
rewardSum:247.25
loss:[22.746488094329834, 19.747779607772827]
policies:[1, 2, 1]
qAverage:[0.0, 85.11458079020183]
ws:[2.368162473042806, 4.482823371887207]
memory len:10000
memory used:2702.0
now epsilon is 0.15633846475290566, the reward is 247.25 with loss [24.304988980293274, 30.75359606742859] in episode 1377
Report: 
rewardSum:247.25
loss:[24.304988980293274, 30.75359606742859]
policies:[0, 4, 0]
qAverage:[0.0, 100.80168342590332]
ws:[9.054715156555176, 12.6159128844738]
memory len:10000
memory used:2702.0
now epsilon is 0.1561821849053065, the reward is 247.25 with loss [26.665255546569824, 21.973042488098145] in episode 1378
Report: 
rewardSum:247.25
loss:[26.665255546569824, 21.973042488098145]
policies:[0, 3, 1]
qAverage:[0.0, 93.60892740885417]
ws:[11.962258656819662, 15.729821840922037]
memory len:10000
memory used:2702.0
now epsilon is 0.15602606127895977, the reward is 247.25 with loss [28.455356121063232, 25.91875720024109] in episode 1379
Report: 
rewardSum:247.25
loss:[28.455356121063232, 25.91875720024109]
policies:[1, 3, 0]
qAverage:[0.0, 94.47432899475098]
ws:[1.9406994692981243, 4.120906889438629]
memory len:10000
memory used:2702.0
now epsilon is 0.1558700937177028, the reward is 247.25 with loss [21.0802960395813, 25.830955505371094] in episode 1380
Report: 
rewardSum:247.25
loss:[21.0802960395813, 25.830955505371094]
policies:[0, 3, 1]
qAverage:[0.0, 105.70403289794922]
ws:[11.209945321083069, 15.775643467903137]
memory len:10000
memory used:2702.0
now epsilon is 0.155714282065529, the reward is 247.25 with loss [39.239110469818115, 40.786638259887695] in episode 1381
Report: 
rewardSum:247.25
loss:[39.239110469818115, 40.786638259887695]
policies:[0, 4, 0]
qAverage:[0.0, 105.68314514160156]
ws:[10.117676615715027, 15.063007307052612]
memory len:10000
memory used:2702.0
now epsilon is 0.15540312586518415, the reward is 55.6875 with loss [56.317434310913086, 58.81246280670166] in episode 1382
Report: 
rewardSum:55.6875
loss:[56.317434310913086, 58.81246280670166]
policies:[0, 4, 4]
qAverage:[0.0, 104.24227752685547]
ws:[4.802472686767578, 7.283770561218262]
memory len:10000
memory used:2737.0
now epsilon is 0.1552477810057791, the reward is 247.25 with loss [26.469685077667236, 21.64645028114319] in episode 1383
Report: 
rewardSum:247.25
loss:[26.469685077667236, 21.64645028114319]
policies:[1, 3, 0]
qAverage:[0.0, 79.64896138509114]
ws:[0.6305577357610067, 1.7738259335358937]
memory len:10000
memory used:2704.0
now epsilon is 0.15501505483055933, the reward is 245.25 with loss [32.71254062652588, 45.259803771972656] in episode 1384
Report: 
rewardSum:245.25
loss:[32.71254062652588, 45.259803771972656]
policies:[0, 5, 1]
qAverage:[0.0, 112.51182174682617]
ws:[6.937934041023254, 11.202276448408762]
memory len:10000
memory used:2703.0
now epsilon is 0.1548600978966865, the reward is 247.25 with loss [21.81516146659851, 25.83353042602539] in episode 1385
Report: 
rewardSum:247.25
loss:[21.81516146659851, 25.83353042602539]
policies:[1, 2, 1]
qAverage:[0.0, 96.72855122884114]
ws:[11.781056563059488, 16.624026616414387]
memory len:10000
memory used:2703.0
now epsilon is 0.1547052958616484, the reward is 247.25 with loss [31.08774185180664, 24.595853328704834] in episode 1386
Report: 
rewardSum:247.25
loss:[31.08774185180664, 24.595853328704834]
policies:[0, 4, 0]
qAverage:[0.0, 106.97036895751953]
ws:[10.105658388137817, 16.08933973312378]
memory len:10000
memory used:2704.0
now epsilon is 0.1544733829057345, the reward is 245.25 with loss [55.290109634399414, 39.36133813858032] in episode 1387
Report: 
rewardSum:245.25
loss:[55.290109634399414, 39.36133813858032]
policies:[0, 5, 1]
qAverage:[0.0, 99.25512313842773]
ws:[2.9078306704759598, 7.278880000114441]
memory len:10000
memory used:2703.0
now epsilon is 0.15431896744069337, the reward is 247.25 with loss [20.998801946640015, 34.58651542663574] in episode 1388
Report: 
rewardSum:247.25
loss:[20.998801946640015, 34.58651542663574]
policies:[1, 3, 0]
qAverage:[0.0, 93.74087778727214]
ws:[13.170491218566895, 18.353266716003418]
memory len:10000
memory used:2702.0
now epsilon is 0.15416470633322113, the reward is 247.25 with loss [24.534629583358765, 25.254592895507812] in episode 1389
Report: 
rewardSum:247.25
loss:[24.534629583358765, 25.254592895507812]
policies:[0, 4, 0]
qAverage:[0.0, 108.95462493896484]
ws:[8.199144804477692, 13.279799842834473]
memory len:10000
memory used:2703.0
now epsilon is 0.1540105994290181, the reward is 247.25 with loss [20.782973289489746, 35.59898042678833] in episode 1390
Report: 
rewardSum:247.25
loss:[20.782973289489746, 35.59898042678833]
policies:[0, 4, 0]
qAverage:[0.0, 106.95848846435547]
ws:[7.884689918160438, 12.1780846118927]
memory len:10000
memory used:2703.0
now epsilon is 0.15385664657393883, the reward is 247.25 with loss [28.765800952911377, 22.41533374786377] in episode 1391
Report: 
rewardSum:247.25
loss:[28.765800952911377, 22.41533374786377]
policies:[1, 3, 0]
qAverage:[0.0, 107.37993240356445]
ws:[10.181108936667442, 16.476463437080383]
memory len:10000
memory used:2702.0
now epsilon is 0.15370284761399194, the reward is 247.25 with loss [26.174593925476074, 36.78306531906128] in episode 1392
Report: 
rewardSum:247.25
loss:[26.174593925476074, 36.78306531906128]
policies:[1, 3, 0]
qAverage:[0.0, 93.1250171661377]
ws:[1.765480786561966, 4.833859503269196]
memory len:10000
memory used:2702.0
now epsilon is 0.15354920239533998, the reward is 247.25 with loss [32.18438649177551, 25.410130500793457] in episode 1393
Report: 
rewardSum:247.25
loss:[32.18438649177551, 25.410130500793457]
policies:[0, 4, 0]
qAverage:[0.0, 108.59751739501954]
ws:[9.716298389434815, 13.441027784347535]
memory len:10000
memory used:2702.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20*		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.15339571076429934, the reward is 247.25 with loss [26.441242218017578, 31.48656702041626] in episode 1394
Report: 
rewardSum:247.25
loss:[26.441242218017578, 31.48656702041626]
policies:[0, 4, 0]
qAverage:[0.0, 104.93722534179688]
ws:[10.933694314956664, 14.671418380737304]
memory len:10000
memory used:2702.0
now epsilon is 0.15324237256733994, the reward is 247.25 with loss [20.717122316360474, 42.019174575805664] in episode 1395
Report: 
rewardSum:247.25
loss:[20.717122316360474, 42.019174575805664]
policies:[0, 4, 0]
qAverage:[0.0, 108.01710815429688]
ws:[9.396357202529908, 12.864671611785889]
memory len:10000
memory used:2702.0
now epsilon is 0.1529361558623121, the reward is 243.25 with loss [38.83547830581665, 57.93338060379028] in episode 1396
Report: 
rewardSum:243.25
loss:[38.83547830581665, 57.93338060379028]
policies:[2, 4, 2]
qAverage:[0.0, 95.6644458770752]
ws:[1.7981625385582447, 4.760116338729858]
memory len:10000
memory used:2702.0
now epsilon is 0.1527832770479503, the reward is 247.25 with loss [29.395899295806885, 17.635464668273926] in episode 1397
Report: 
rewardSum:247.25
loss:[29.395899295806885, 17.635464668273926]
policies:[0, 4, 0]
qAverage:[0.0, 106.63358459472656]
ws:[7.302326250076294, 9.825748658180236]
memory len:10000
memory used:2702.0
now epsilon is 0.1526305510550829, the reward is 247.25 with loss [19.31761860847473, 39.19791316986084] in episode 1398
Report: 
rewardSum:247.25
loss:[19.31761860847473, 39.19791316986084]
policies:[0, 4, 0]
qAverage:[0.0, 99.49931716918945]
ws:[7.923103213310242, 10.583280965685844]
memory len:10000
memory used:2702.0
now epsilon is 0.1524779777309457, the reward is 247.25 with loss [17.0118191242218, 25.978943347930908] in episode 1399
Report: 
rewardSum:247.25
loss:[17.0118191242218, 25.978943347930908]
policies:[0, 4, 0]
qAverage:[0.0, 105.91816711425781]
ws:[10.872868299484253, 15.044552564620972]
memory len:10000
memory used:2702.0
now epsilon is 0.15240174827195385, the reward is -1.0 with loss [8.339083194732666, 19.07693910598755] in episode 1400
Report: 
rewardSum:-1.0
loss:[8.339083194732666, 19.07693910598755]
policies:[1, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:2702.0
now epsilon is 0.15217328847856834, the reward is 245.25 with loss [27.18581748008728, 36.954890727996826] in episode 1401
Report: 
rewardSum:245.25
loss:[27.18581748008728, 36.954890727996826]
policies:[0, 5, 1]
qAverage:[0.0, 111.14458338419597]
ws:[8.60899051030477, 13.297029972076416]
memory len:10000
memory used:2702.0
now epsilon is 0.15202117224556275, the reward is 247.25 with loss [35.67867660522461, 25.78472900390625] in episode 1402
Report: 
rewardSum:247.25
loss:[35.67867660522461, 25.78472900390625]
policies:[0, 4, 0]
qAverage:[0.0, 104.20131988525391]
ws:[10.095759010314941, 16.02646908760071]
memory len:10000
memory used:2703.0
now epsilon is 0.15186920807175605, the reward is 247.25 with loss [20.416380167007446, 23.784420013427734] in episode 1403
Report: 
rewardSum:247.25
loss:[20.416380167007446, 23.784420013427734]
policies:[0, 4, 0]
qAverage:[0.0, 108.14641723632812]
ws:[7.8972497344017025, 13.3434663772583]
memory len:10000
memory used:2702.0
now epsilon is 0.1516415465895808, the reward is 245.25 with loss [28.147308826446533, 41.00674819946289] in episode 1404
Report: 
rewardSum:245.25
loss:[28.147308826446533, 41.00674819946289]
policies:[1, 4, 1]
qAverage:[0.0, 96.6861629486084]
ws:[2.8526391685009003, 8.222447514533997]
memory len:10000
memory used:2702.0
now epsilon is 0.1514899618990942, the reward is 247.25 with loss [18.887214183807373, 27.269095182418823] in episode 1405
Report: 
rewardSum:247.25
loss:[18.887214183807373, 27.269095182418823]
policies:[1, 3, 0]
qAverage:[0.0, 95.40365028381348]
ws:[2.122245818376541, 6.659293055534363]
memory len:10000
memory used:2702.0
now epsilon is 0.1513385287364633, the reward is 247.25 with loss [30.677238941192627, 34.415929317474365] in episode 1406
Report: 
rewardSum:247.25
loss:[30.677238941192627, 34.415929317474365]
policies:[0, 4, 0]
qAverage:[0.0, 98.7856216430664]
ws:[9.831017255783081, 15.689789086580276]
memory len:10000
memory used:2702.0
now epsilon is 0.15118724695021704, the reward is 247.25 with loss [26.112112998962402, 25.990588903427124] in episode 1407
Report: 
rewardSum:247.25
loss:[26.112112998962402, 25.990588903427124]
policies:[0, 4, 0]
qAverage:[0.0, 100.35735130310059]
ws:[6.558312892913818, 11.654660522937775]
memory len:10000
memory used:2702.0
now epsilon is 0.15103611638903583, the reward is 59.6875 with loss [40.41664123535156, 31.57707118988037] in episode 1408
Report: 
rewardSum:59.6875
loss:[40.41664123535156, 31.57707118988037]
policies:[0, 3, 1]
qAverage:[0.0, 96.3681411743164]
ws:[-0.6584891155362129, 2.129069223999977]
memory len:10000
memory used:2702.0
now epsilon is 0.15088513690175132, the reward is 247.25 with loss [24.287288069725037, 20.236260175704956] in episode 1409
Report: 
rewardSum:247.25
loss:[24.287288069725037, 20.236260175704956]
policies:[1, 3, 0]
qAverage:[0.0, 87.58794911702473]
ws:[11.836688995361328, 15.404777606328329]
memory len:10000
memory used:2701.0
now epsilon is 0.1507343083373462, the reward is 247.25 with loss [24.915746927261353, 36.140395164489746] in episode 1410
Report: 
rewardSum:247.25
loss:[24.915746927261353, 36.140395164489746]
policies:[0, 4, 0]
qAverage:[0.0, 103.63868560791016]
ws:[10.930033111572266, 14.436752891540527]
memory len:10000
memory used:2701.0
now epsilon is 0.1505836305449542, the reward is 247.25 with loss [24.82759928703308, 30.664462566375732] in episode 1411
Report: 
rewardSum:247.25
loss:[24.82759928703308, 30.664462566375732]
policies:[1, 2, 1]
qAverage:[0.0, 90.93713633219402]
ws:[13.683504581451416, 17.3686949412028]
memory len:10000
memory used:2701.0
now epsilon is 0.15035789622424187, the reward is 245.25 with loss [42.89006161689758, 42.303425788879395] in episode 1412
Report: 
rewardSum:245.25
loss:[42.89006161689758, 42.303425788879395]
policies:[1, 4, 1]
qAverage:[0.0, 105.15628662109376]
ws:[9.467191526293755, 14.57815866470337]
memory len:10000
memory used:2701.0
now epsilon is 0.15020759470283193, the reward is 247.25 with loss [24.68312120437622, 24.664870738983154] in episode 1413
Report: 
rewardSum:247.25
loss:[24.68312120437622, 24.664870738983154]
policies:[1, 3, 0]
qAverage:[0.0, 94.65752983093262]
ws:[0.804924339056015, 4.018401458859444]
memory len:10000
memory used:2703.0
now epsilon is 0.15005744342658975, the reward is 247.25 with loss [29.17637014389038, 26.75685405731201] in episode 1414
Report: 
rewardSum:247.25
loss:[29.17637014389038, 26.75685405731201]
policies:[0, 4, 0]
qAverage:[0.0, 106.04046936035157]
ws:[7.353090163320303, 10.260676097869872]
memory len:10000
memory used:2703.0
now epsilon is 0.14990744224532646, the reward is 247.25 with loss [28.38555097579956, 19.36139941215515] in episode 1415
Report: 
rewardSum:247.25
loss:[28.38555097579956, 19.36139941215515]
policies:[0, 4, 0]
qAverage:[0.0, 103.24039306640626]
ws:[5.601370406150818, 8.174516129493714]
memory len:10000
memory used:2703.0
now epsilon is 0.1496827215733483, the reward is 57.6875 with loss [41.05989480018616, 38.70108461380005] in episode 1416
Report: 
rewardSum:57.6875
loss:[41.05989480018616, 38.70108461380005]
policies:[0, 4, 2]
qAverage:[0.0, 94.78871536254883]
ws:[0.5875581502914429, 2.7278436794877052]
memory len:10000
memory used:2702.0
now epsilon is 0.14953309497344097, the reward is 247.25 with loss [28.475202083587646, 19.74662697315216] in episode 1417
Report: 
rewardSum:247.25
loss:[28.475202083587646, 19.74662697315216]
policies:[0, 4, 0]
qAverage:[0.0, 105.55126342773437]
ws:[6.596251584589481, 9.446364855766296]
memory len:10000
memory used:2702.0
now epsilon is 0.14930893547153704, the reward is 245.25 with loss [54.00364542007446, 44.31487560272217] in episode 1418
Report: 
rewardSum:245.25
loss:[54.00364542007446, 44.31487560272217]
policies:[0, 5, 1]
qAverage:[0.0, 105.61112213134766]
ws:[7.129896064599355, 9.44952787955602]
memory len:10000
memory used:2703.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21*		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.1491596825175851, the reward is 247.25 with loss [49.062777519226074, 30.792115211486816] in episode 1419
Report: 
rewardSum:247.25
loss:[49.062777519226074, 30.792115211486816]
policies:[1, 3, 0]
qAverage:[26.34010009765625, 80.31919860839844]
ws:[5.037796878814698, 6.498512062430382]
memory len:10000
memory used:2703.0
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.1490105787606266, the reward is 247.25 with loss [40.42447280883789, 29.673233032226562] in episode 1420
Report: 
rewardSum:247.25
loss:[40.42447280883789, 29.673233032226562]
policies:[0, 4, 0]
qAverage:[0.0, 100.75308837890626]
ws:[4.1117461088113485, 6.674189805984497]
memory len:10000
memory used:2703.0
now epsilon is 0.14886162405152042, the reward is 247.25 with loss [32.80270195007324, 29.673073291778564] in episode 1421
Report: 
rewardSum:247.25
loss:[32.80270195007324, 29.673073291778564]
policies:[0, 4, 0]
qAverage:[0.0, 102.35934295654297]
ws:[5.536632776260376, 8.789626717939973]
memory len:10000
memory used:2703.0
now epsilon is 0.1487128182412747, the reward is 247.25 with loss [25.765462934970856, 29.554316520690918] in episode 1422
Report: 
rewardSum:247.25
loss:[25.765462934970856, 29.554316520690918]
policies:[0, 4, 0]
qAverage:[0.0, 102.52065887451172]
ws:[7.0476365208625795, 10.335706281661988]
memory len:10000
memory used:2703.0
now epsilon is 0.1485641611810463, the reward is 247.25 with loss [32.264309883117676, 23.74200963973999] in episode 1423
Report: 
rewardSum:247.25
loss:[32.264309883117676, 23.74200963973999]
policies:[0, 4, 0]
qAverage:[0.0, 100.49566192626953]
ws:[8.401782512664795, 12.18415699005127]
memory len:10000
memory used:2703.0
now epsilon is 0.14841565272214102, the reward is 247.25 with loss [36.73603582382202, 16.46811079978943] in episode 1424
Report: 
rewardSum:247.25
loss:[36.73603582382202, 16.46811079978943]
policies:[1, 3, 0]
qAverage:[0.0, 96.52812004089355]
ws:[9.231572151184082, 12.107624650001526]
memory len:10000
memory used:2703.0
now epsilon is 0.14826729271601327, the reward is 59.6875 with loss [17.81659460067749, 28.307417154312134] in episode 1425
Report: 
rewardSum:59.6875
loss:[17.81659460067749, 28.307417154312134]
policies:[0, 3, 1]
qAverage:[0.0, 91.64945030212402]
ws:[1.8527577072381973, 4.452548444271088]
memory len:10000
memory used:2703.0
now epsilon is 0.1481190810142659, the reward is 247.25 with loss [19.90599799156189, 25.915339946746826] in episode 1426
Report: 
rewardSum:247.25
loss:[19.90599799156189, 25.915339946746826]
policies:[0, 4, 0]
qAverage:[0.0, 100.95355987548828]
ws:[5.96820941567421, 8.868846702575684]
memory len:10000
memory used:2703.0
now epsilon is 0.1479710174686502, the reward is 247.25 with loss [21.211910486221313, 26.75825262069702] in episode 1427
Report: 
rewardSum:247.25
loss:[21.211910486221313, 26.75825262069702]
policies:[1, 2, 1]
qAverage:[0.0, 62.63756561279297]
ws:[0.5117521286010742, 2.0928986072540283]
memory len:10000
memory used:2703.0
now epsilon is 0.14767533425355933, the reward is 243.25 with loss [59.25595712661743, 48.04837417602539] in episode 1428
Report: 
rewardSum:243.25
loss:[59.25595712661743, 48.04837417602539]
policies:[0, 2, 6]
qAverage:[0.0, 75.64214579264323]
ws:[-1.4054763317108154, -0.47447295983632404]
memory len:10000
memory used:2703.0
now epsilon is 0.147527714288327, the reward is 247.25 with loss [18.638776063919067, 29.957533359527588] in episode 1429
Report: 
rewardSum:247.25
loss:[18.638776063919067, 29.957533359527588]
policies:[0, 3, 1]
qAverage:[0.0, 89.36448860168457]
ws:[9.356603026390076, 12.909478664398193]
memory len:10000
memory used:2703.0
now epsilon is 0.14738024188771165, the reward is 247.25 with loss [26.04918670654297, 22.741621732711792] in episode 1430
Report: 
rewardSum:247.25
loss:[26.04918670654297, 22.741621732711792]
policies:[0, 4, 0]
qAverage:[0.0, 86.70651054382324]
ws:[0.353640696965158, 3.441926896572113]
memory len:10000
memory used:2703.0
now epsilon is 0.14723291690420398, the reward is 247.25 with loss [30.78813362121582, 25.311423301696777] in episode 1431
Report: 
rewardSum:247.25
loss:[30.78813362121582, 25.311423301696777]
policies:[0, 4, 0]
qAverage:[0.0, 95.79533233642579]
ws:[2.6838812828063965, 6.456792187690735]
memory len:10000
memory used:2703.0
now epsilon is 0.14708573919044213, the reward is 247.25 with loss [35.39099979400635, 17.66074800491333] in episode 1432
Report: 
rewardSum:247.25
loss:[35.39099979400635, 17.66074800491333]
policies:[0, 3, 1]
qAverage:[0.0, 88.71942138671875]
ws:[4.623232737183571, 8.300497859716415]
memory len:10000
memory used:2703.0
now epsilon is 0.1469387085992116, the reward is 247.25 with loss [20.06748139858246, 36.04609727859497] in episode 1433
Report: 
rewardSum:247.25
loss:[20.06748139858246, 36.04609727859497]
policies:[0, 3, 1]
qAverage:[0.0, 89.02621650695801]
ws:[6.382869452238083, 10.33643925189972]
memory len:10000
memory used:2703.0
now epsilon is 0.14679182498344504, the reward is 59.6875 with loss [28.76077651977539, 32.74427080154419] in episode 1434
Report: 
rewardSum:59.6875
loss:[28.76077651977539, 32.74427080154419]
policies:[0, 2, 2]
qAverage:[0.0, 79.57807159423828]
ws:[4.996492942174275, 10.907208124796549]
memory len:10000
memory used:2703.0
now epsilon is 0.14664508819622205, the reward is 247.25 with loss [27.154944896697998, 29.374935626983643] in episode 1435
Report: 
rewardSum:247.25
loss:[27.154944896697998, 29.374935626983643]
policies:[0, 3, 1]
qAverage:[0.0, 88.14233207702637]
ws:[7.681965842843056, 12.554933905601501]
memory len:10000
memory used:2703.0
now epsilon is 0.14649849809076917, the reward is 247.25 with loss [27.85719394683838, 16.35104250907898] in episode 1436
Report: 
rewardSum:247.25
loss:[27.85719394683838, 16.35104250907898]
policies:[0, 4, 0]
qAverage:[0.0, 80.90694427490234]
ws:[0.7946792046229044, 4.966554482777913]
memory len:10000
memory used:2702.0
now epsilon is 0.1463520545204596, the reward is 247.25 with loss [43.66933250427246, 23.34203815460205] in episode 1437
Report: 
rewardSum:247.25
loss:[43.66933250427246, 23.34203815460205]
policies:[1, 3, 0]
qAverage:[0.0, 89.404541015625]
ws:[6.043189274147153, 11.088824391365051]
memory len:10000
memory used:2702.0
now epsilon is 0.1462057573388132, the reward is 247.25 with loss [26.758921146392822, 21.453800439834595] in episode 1438
Report: 
rewardSum:247.25
loss:[26.758921146392822, 21.453800439834595]
policies:[0, 3, 1]
qAverage:[0.0, 92.21863555908203]
ws:[8.743100672960281, 15.42521584033966]
memory len:10000
memory used:2703.0
now epsilon is 0.1460596063994961, the reward is 247.25 with loss [23.836527824401855, 39.9974627494812] in episode 1439
Report: 
rewardSum:247.25
loss:[23.836527824401855, 39.9974627494812]
policies:[0, 4, 0]
qAverage:[0.0, 94.78681030273438]
ws:[5.158345663547516, 9.912875413894653]
memory len:10000
memory used:2703.0
now epsilon is 0.1458406538751428, the reward is 245.25 with loss [49.85575246810913, 37.62250375747681] in episode 1440
Report: 
rewardSum:245.25
loss:[49.85575246810913, 37.62250375747681]
policies:[1, 4, 1]
qAverage:[0.0, 93.15964202880859]
ws:[2.4380900859832764, 7.223305797576904]
memory len:10000
memory used:2702.0
now epsilon is 0.14569486790239838, the reward is 247.25 with loss [25.913162231445312, 23.57205319404602] in episode 1441
Report: 
rewardSum:247.25
loss:[25.913162231445312, 23.57205319404602]
policies:[0, 3, 1]
qAverage:[0.0, 82.0653190612793]
ws:[-0.05453919619321823, 2.5886755883693695]
memory len:10000
memory used:2702.0
now epsilon is 0.14540373300516926, the reward is 243.25 with loss [58.03717851638794, 55.01909255981445] in episode 1442
Report: 
rewardSum:243.25
loss:[58.03717851638794, 55.01909255981445]
policies:[1, 5, 2]
qAverage:[0.0, 88.427197265625]
ws:[3.0944113612174986, 7.273445105552673]
memory len:10000
memory used:2702.0
now epsilon is 0.14511317986850317, the reward is 243.25 with loss [53.257426738739014, 74.16005849838257] in episode 1443
Report: 
rewardSum:243.25
loss:[53.257426738739014, 74.16005849838257]
policies:[1, 5, 2]
qAverage:[0.0, 89.78741963704427]
ws:[2.4202552239100137, 5.945134739081065]
memory len:10000
memory used:2702.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18*		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.14496812109700813, the reward is 247.25 with loss [20.671282291412354, 20.465706825256348] in episode 1444
Report: 
rewardSum:247.25
loss:[20.671282291412354, 20.465706825256348]
policies:[0, 4, 0]
qAverage:[0.0, 81.12874221801758]
ws:[1.710348218679428, 4.762542366981506]
memory len:10000
memory used:2702.0
now epsilon is 0.1448232073298966, the reward is 247.25 with loss [30.734654426574707, 25.125104904174805] in episode 1445
Report: 
rewardSum:247.25
loss:[30.734654426574707, 25.125104904174805]
policies:[0, 4, 0]
qAverage:[0.0, 90.06106262207031]
ws:[3.942816984653473, 9.757493877410889]
memory len:10000
memory used:2702.0
now epsilon is 0.14467843842221856, the reward is 247.25 with loss [35.5978524684906, 26.911863327026367] in episode 1446
Report: 
rewardSum:247.25
loss:[35.5978524684906, 26.911863327026367]
policies:[0, 4, 0]
qAverage:[0.0, 91.25509033203124]
ws:[3.6772033736109733, 9.425427770614624]
memory len:10000
memory used:2702.0
now epsilon is 0.14453381422916892, the reward is 247.25 with loss [31.937049388885498, 37.2111177444458] in episode 1447
Report: 
rewardSum:247.25
loss:[31.937049388885498, 37.2111177444458]
policies:[0, 4, 0]
qAverage:[0.0, 89.744775390625]
ws:[5.504408133029938, 12.587016820907593]
memory len:10000
memory used:2702.0
now epsilon is 0.1443893346060873, the reward is 247.25 with loss [22.108439922332764, 22.00062084197998] in episode 1448
Report: 
rewardSum:247.25
loss:[22.108439922332764, 22.00062084197998]
policies:[0, 4, 0]
qAverage:[0.0, 91.22502136230469]
ws:[3.091708791255951, 7.480022811889649]
memory len:10000
memory used:2703.0
now epsilon is 0.14424499940845792, the reward is 247.25 with loss [31.8544340133667, 18.4915292263031] in episode 1449
Report: 
rewardSum:247.25
loss:[31.8544340133667, 18.4915292263031]
policies:[0, 4, 0]
qAverage:[0.0, 89.68290710449219]
ws:[2.5452023088932036, 6.854543364048004]
memory len:10000
memory used:2703.0
now epsilon is 0.1441008084919095, the reward is 247.25 with loss [25.486233234405518, 26.876256942749023] in episode 1450
Report: 
rewardSum:247.25
loss:[25.486233234405518, 26.876256942749023]
policies:[1, 3, 0]
qAverage:[0.0, 77.85853576660156]
ws:[5.601990362008412, 10.146697044372559]
memory len:10000
memory used:2703.0
now epsilon is 0.14395676171221505, the reward is 247.25 with loss [32.70257616043091, 21.600825309753418] in episode 1451
Report: 
rewardSum:247.25
loss:[32.70257616043091, 21.600825309753418]
policies:[0, 3, 1]
qAverage:[0.0, 87.04904556274414]
ws:[4.898104935884476, 8.453435182571411]
memory len:10000
memory used:2703.0
now epsilon is 0.14381285892529175, the reward is 247.25 with loss [22.264666080474854, 30.984525203704834] in episode 1452
Report: 
rewardSum:247.25
loss:[22.264666080474854, 30.984525203704834]
policies:[1, 3, 0]
qAverage:[0.0, 80.8336067199707]
ws:[7.40263956785202, 12.215142130851746]
memory len:10000
memory used:2703.0
now epsilon is 0.14352548475414742, the reward is 243.25 with loss [60.924137592315674, 44.821585178375244] in episode 1453
Report: 
rewardSum:243.25
loss:[60.924137592315674, 44.821585178375244]
policies:[1, 5, 2]
qAverage:[18.03165054321289, 70.33116404215495]
ws:[3.481700678666433, 5.222120136022568]
memory len:10000
memory used:2703.0
now epsilon is 0.14338201308248028, the reward is 247.25 with loss [27.62561321258545, 21.892467975616455] in episode 1454
Report: 
rewardSum:247.25
loss:[27.62561321258545, 21.892467975616455]
policies:[2, 2, 0]
qAverage:[27.009307861328125, 54.28877258300781]
ws:[0.6838984712958336, 0.8856149315834045]
memory len:10000
memory used:2704.0
now epsilon is 0.14316707443869536, the reward is 245.25 with loss [54.140671491622925, 43.30507707595825] in episode 1455
Report: 
rewardSum:245.25
loss:[54.140671491622925, 43.30507707595825]
policies:[0, 5, 1]
qAverage:[0.0, 86.3369369506836]
ws:[1.9900224566459657, 4.916788768768311]
memory len:10000
memory used:2704.0
now epsilon is 0.14302396104296222, the reward is 247.25 with loss [28.246397495269775, 19.430089473724365] in episode 1456
Report: 
rewardSum:247.25
loss:[28.246397495269775, 19.430089473724365]
policies:[0, 3, 1]
qAverage:[0.0, 81.66144943237305]
ws:[4.156660497188568, 8.55730739235878]
memory len:10000
memory used:2704.0
now epsilon is 0.14288099070696622, the reward is 247.25 with loss [31.94298553466797, 31.103458404541016] in episode 1457
Report: 
rewardSum:247.25
loss:[31.94298553466797, 31.103458404541016]
policies:[0, 3, 1]
qAverage:[0.0, 83.08711814880371]
ws:[4.993970990180969, 10.216569066047668]
memory len:10000
memory used:2703.0
now epsilon is 0.14273816328770128, the reward is 247.25 with loss [16.223668336868286, 30.44090700149536] in episode 1458
Report: 
rewardSum:247.25
loss:[16.223668336868286, 30.44090700149536]
policies:[0, 3, 1]
qAverage:[0.0, 85.18588066101074]
ws:[5.306425034999847, 9.384641289710999]
memory len:10000
memory used:2703.0
now epsilon is 0.14259547864230426, the reward is 247.25 with loss [25.755820274353027, 26.24059820175171] in episode 1459
Report: 
rewardSum:247.25
loss:[25.755820274353027, 26.24059820175171]
policies:[1, 3, 0]
qAverage:[21.794699096679686, 67.22344970703125]
ws:[1.7584501087665558, 3.00713164806366]
memory len:10000
memory used:2703.0
now epsilon is 0.1424529366280548, the reward is 247.25 with loss [26.041680574417114, 22.05689311027527] in episode 1460
Report: 
rewardSum:247.25
loss:[26.041680574417114, 22.05689311027527]
policies:[2, 2, 0]
qAverage:[0.0, 76.88177744547527]
ws:[2.0732888678709664, 5.640224456787109]
memory len:10000
memory used:2703.0
now epsilon is 0.14231053710237523, the reward is 247.25 with loss [31.267446041107178, 30.5309796333313] in episode 1461
Report: 
rewardSum:247.25
loss:[31.267446041107178, 30.5309796333313]
policies:[0, 3, 1]
qAverage:[0.0, 78.29265022277832]
ws:[2.9652275443077087, 4.034713089466095]
memory len:10000
memory used:2703.0
now epsilon is 0.14216827992283046, the reward is 247.25 with loss [32.35380220413208, 20.01819944381714] in episode 1462
Report: 
rewardSum:247.25
loss:[32.35380220413208, 20.01819944381714]
policies:[2, 2, 0]
qAverage:[27.844924926757812, 54.67776870727539]
ws:[6.552262544631958, 9.433768272399902]
memory len:10000
memory used:2703.0
now epsilon is 0.14177780549016056, the reward is 240.25 with loss [60.39176273345947, 72.34184312820435] in episode 1463
Report: 
rewardSum:240.25
loss:[60.39176273345947, 72.34184312820435]
policies:[1, 6, 4]
qAverage:[15.005038670131139, 71.03742980957031]
ws:[6.003473894936698, 8.907648018428258]
memory len:10000
memory used:2704.0
now epsilon is 0.14156527165432073, the reward is 245.25 with loss [46.16008949279785, 34.30677843093872] in episode 1464
Report: 
rewardSum:245.25
loss:[46.16008949279785, 34.30677843093872]
policies:[0, 5, 1]
qAverage:[0.0, 87.43878173828125]
ws:[3.234761486450831, 7.166897336641948]
memory len:10000
memory used:2704.0
now epsilon is 0.14142375946079602, the reward is 247.25 with loss [24.300880908966064, 27.094192504882812] in episode 1465
Report: 
rewardSum:247.25
loss:[24.300880908966064, 27.094192504882812]
policies:[0, 4, 0]
qAverage:[0.0, 78.31758117675781]
ws:[3.9558840692043304, 8.006781369447708]
memory len:10000
memory used:2704.0
now epsilon is 0.14128238872640658, the reward is 247.25 with loss [20.874688625335693, 22.40846824645996] in episode 1466
Report: 
rewardSum:247.25
loss:[20.874688625335693, 22.40846824645996]
policies:[2, 2, 0]
qAverage:[0.0, 66.07030232747395]
ws:[-0.1196436882019043, 2.8553216457366943]
memory len:10000
memory used:2704.0
now epsilon is 0.14107059755141396, the reward is 245.25 with loss [42.34212398529053, 49.12660217285156] in episode 1467
Report: 
rewardSum:245.25
loss:[42.34212398529053, 49.12660217285156]
policies:[1, 3, 2]
qAverage:[0.0, 76.17327499389648]
ws:[2.198929399251938, 6.902010440826416]
memory len:10000
memory used:2703.0
############# STATE ###############
0-		8-		16*		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.14092957984652027, the reward is 247.25 with loss [28.533104419708252, 19.333598613739014] in episode 1468
Report: 
rewardSum:247.25
loss:[28.533104419708252, 19.333598613739014]
policies:[0, 4, 0]
qAverage:[0.0, 82.68113708496094]
ws:[3.509378445148468, 6.848639035224915]
memory len:10000
memory used:2703.0
now epsilon is 0.14078870310645866, the reward is 247.25 with loss [26.361655712127686, 26.24778699874878] in episode 1469
Report: 
rewardSum:247.25
loss:[26.361655712127686, 26.24778699874878]
policies:[1, 3, 0]
qAverage:[0.0, 76.66025733947754]
ws:[2.3218965269625187, 5.952671527862549]
memory len:10000
memory used:2703.0
now epsilon is 0.14064796719031716, the reward is 247.25 with loss [31.732340335845947, 41.37843322753906] in episode 1470
Report: 
rewardSum:247.25
loss:[31.732340335845947, 41.37843322753906]
policies:[0, 3, 1]
qAverage:[0.0, 80.0724868774414]
ws:[5.592236280441284, 10.177974820137024]
memory len:10000
memory used:2703.0
now epsilon is 0.14050737195732463, the reward is 247.25 with loss [26.23025608062744, 21.33197021484375] in episode 1471
Report: 
rewardSum:247.25
loss:[26.23025608062744, 21.33197021484375]
policies:[0, 4, 0]
qAverage:[0.0, 79.13651466369629]
ws:[3.4060765765607357, 6.55893149971962]
memory len:10000
memory used:2703.0
now epsilon is 0.14036691726685066, the reward is 247.25 with loss [24.471198081970215, 31.423248291015625] in episode 1472
Report: 
rewardSum:247.25
loss:[24.471198081970215, 31.423248291015625]
policies:[0, 4, 0]
qAverage:[0.0, 79.85917663574219]
ws:[2.3628735840320587, 5.95365846157074]
memory len:10000
memory used:2703.0
now epsilon is 0.14022660297840542, the reward is 247.25 with loss [26.739264011383057, 34.89431619644165] in episode 1473
Report: 
rewardSum:247.25
loss:[26.739264011383057, 34.89431619644165]
policies:[0, 3, 1]
qAverage:[0.0, 70.32230567932129]
ws:[2.9145691990852356, 6.06550145149231]
memory len:10000
memory used:2703.0
now epsilon is 0.14008642895163956, the reward is 247.25 with loss [25.40307855606079, 27.305033206939697] in episode 1474
Report: 
rewardSum:247.25
loss:[25.40307855606079, 27.305033206939697]
policies:[1, 3, 0]
qAverage:[0.0, 76.81739616394043]
ws:[1.995513677597046, 5.866025984287262]
memory len:10000
memory used:2703.0
now epsilon is 0.13994639504634393, the reward is 247.25 with loss [30.227309703826904, 27.47328758239746] in episode 1475
Report: 
rewardSum:247.25
loss:[30.227309703826904, 27.47328758239746]
policies:[0, 4, 0]
qAverage:[0.0, 77.45338745117188]
ws:[2.1643882274627684, 6.541761839389801]
memory len:10000
memory used:2709.0
now epsilon is 0.13980650112244963, the reward is 247.25 with loss [30.769664764404297, 32.67131733894348] in episode 1476
Report: 
rewardSum:247.25
loss:[30.769664764404297, 32.67131733894348]
policies:[0, 4, 0]
qAverage:[0.0, 78.80106658935547]
ws:[1.8526358604431152, 6.175925102829933]
memory len:10000
memory used:2709.0
now epsilon is 0.13959692239567942, the reward is 245.25 with loss [37.08430624008179, 38.62092041969299] in episode 1477
Report: 
rewardSum:245.25
loss:[37.08430624008179, 38.62092041969299]
policies:[0, 4, 2]
qAverage:[0.0, 74.77706146240234]
ws:[-0.06642939448356629, 2.234713816642761]
memory len:10000
memory used:2709.0
now epsilon is 0.1394573778134054, the reward is 247.25 with loss [28.356529235839844, 22.754109382629395] in episode 1478
Report: 
rewardSum:247.25
loss:[28.356529235839844, 22.754109382629395]
policies:[0, 4, 0]
qAverage:[0.0, 79.4150894165039]
ws:[3.915307193994522, 8.097392535209655]
memory len:10000
memory used:2709.0
now epsilon is 0.1393179727233931, the reward is 247.25 with loss [17.692342281341553, 31.556716918945312] in episode 1479
Report: 
rewardSum:247.25
loss:[17.692342281341553, 31.556716918945312]
policies:[0, 4, 0]
qAverage:[0.0, 77.56957702636718]
ws:[2.88874294757843, 6.047314739227295]
memory len:10000
memory used:2709.0
now epsilon is 0.13910912633137876, the reward is 245.25 with loss [41.72788143157959, 41.132686614990234] in episode 1480
Report: 
rewardSum:245.25
loss:[41.72788143157959, 41.132686614990234]
policies:[0, 4, 2]
qAverage:[0.0, 77.80344085693359]
ws:[1.9433653339743615, 5.966940450668335]
memory len:10000
memory used:2709.0
now epsilon is 0.138970069362276, the reward is 247.25 with loss [22.61354637145996, 31.445420742034912] in episode 1481
Report: 
rewardSum:247.25
loss:[22.61354637145996, 31.445420742034912]
policies:[0, 4, 0]
qAverage:[0.0, 78.62467041015626]
ws:[3.01036696434021, 7.812739074230194]
memory len:10000
memory used:2709.0
now epsilon is 0.13883115139800464, the reward is 247.25 with loss [21.955405473709106, 32.46619415283203] in episode 1482
Report: 
rewardSum:247.25
loss:[21.955405473709106, 32.46619415283203]
policies:[0, 4, 0]
qAverage:[0.0, 78.35597534179688]
ws:[1.7662531912326813, 5.794030332565308]
memory len:10000
memory used:2709.0
now epsilon is 0.138692372299612, the reward is 247.25 with loss [26.05885887145996, 37.949524879455566] in episode 1483
Report: 
rewardSum:247.25
loss:[26.05885887145996, 37.949524879455566]
policies:[0, 4, 0]
qAverage:[0.0, 78.1303726196289]
ws:[2.647349601984024, 7.806862378120423]
memory len:10000
memory used:2709.0
now epsilon is 0.1385537319282843, the reward is 247.25 with loss [33.73421573638916, 23.373750925064087] in episode 1484
Report: 
rewardSum:247.25
loss:[33.73421573638916, 23.373750925064087]
policies:[1, 3, 0]
qAverage:[0.0, 69.09745979309082]
ws:[2.30459688231349, 7.013314664363861]
memory len:10000
memory used:2709.0
now epsilon is 0.1383806263378101, the reward is 246.25 with loss [30.903140544891357, 26.726073503494263] in episode 1485
Report: 
rewardSum:246.25
loss:[30.903140544891357, 26.726073503494263]
policies:[0, 1, 4]
qAverage:[0.0, 48.813018798828125]
ws:[-0.061447564512491226, 1.7130862474441528]
memory len:10000
memory used:2709.0
now epsilon is 0.13824229759555892, the reward is 247.25 with loss [26.35597276687622, 19.13190448284149] in episode 1486
Report: 
rewardSum:247.25
loss:[26.35597276687622, 19.13190448284149]
policies:[0, 3, 1]
qAverage:[0.0, 68.86864280700684]
ws:[2.3011152148246765, 7.565646912902594]
memory len:10000
memory used:2709.0
now epsilon is 0.1381041071301854, the reward is 247.25 with loss [19.63840425014496, 36.04564571380615] in episode 1487
Report: 
rewardSum:247.25
loss:[19.63840425014496, 36.04564571380615]
policies:[1, 3, 0]
qAverage:[0.0, 59.71283213297526]
ws:[-0.7536661823590597, 0.9040467341740926]
memory len:10000
memory used:2709.0
now epsilon is 0.1378970803989411, the reward is 245.25 with loss [42.45167922973633, 51.16974496841431] in episode 1488
Report: 
rewardSum:245.25
loss:[42.45167922973633, 51.16974496841431]
policies:[1, 3, 2]
qAverage:[0.0, 69.00057983398438]
ws:[2.2688970612362027, 7.074290990829468]
memory len:10000
memory used:2721.0
now epsilon is 0.13775923502132933, the reward is 59.6875 with loss [25.796113967895508, 28.114102840423584] in episode 1489
Report: 
rewardSum:59.6875
loss:[25.796113967895508, 28.114102840423584]
policies:[1, 2, 1]
qAverage:[0.0, 62.141194661458336]
ws:[-0.10915926843881607, 2.176845153172811]
memory len:10000
memory used:2721.0
now epsilon is 0.1376215274374117, the reward is 247.25 with loss [24.640278339385986, 35.30918788909912] in episode 1490
Report: 
rewardSum:247.25
loss:[24.640278339385986, 35.30918788909912]
policies:[1, 3, 0]
qAverage:[0.0, 70.1440601348877]
ws:[0.9611649960279465, 4.582910537719727]
memory len:10000
memory used:2721.0
now epsilon is 0.1374152241234389, the reward is 994.0 with loss [46.6120879650116, 44.28933310508728] in episode 1491
Report: 
rewardSum:994.0
loss:[46.6120879650116, 44.28933310508728]
policies:[1, 3, 2]
qAverage:[24.91510772705078, 45.53194046020508]
ws:[-9.00272199511528, -9.010098785161972]
memory len:10000
memory used:2721.0
now epsilon is 0.1372778604214366, the reward is 247.25 with loss [40.574472427368164, 22.407198429107666] in episode 1492
Report: 
rewardSum:247.25
loss:[40.574472427368164, 22.407198429107666]
policies:[0, 4, 0]
qAverage:[0.0, 74.40150146484375]
ws:[0.5895229995250701, 4.070132422447204]
memory len:10000
memory used:2721.0
now epsilon is 0.1371406340316335, the reward is 247.25 with loss [30.918330192565918, 29.686701774597168] in episode 1493
Report: 
rewardSum:247.25
loss:[30.918330192565918, 29.686701774597168]
policies:[0, 4, 0]
qAverage:[0.0, 73.88269805908203]
ws:[1.0680764079093934, 5.016128158569336]
memory len:10000
memory used:2721.0
############# STATE ###############
0-		8-		16*		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.13700354481676894, the reward is 247.25 with loss [34.14540767669678, 34.34353160858154] in episode 1494
Report: 
rewardSum:247.25
loss:[34.14540767669678, 34.34353160858154]
policies:[1, 2, 1]
qAverage:[0.0, 58.76232401529948]
ws:[1.1847888628641765, 2.991664171218872]
memory len:10000
memory used:2722.0
now epsilon is 0.13686659263971931, the reward is 247.25 with loss [25.762949466705322, 26.023056983947754] in episode 1495
Report: 
rewardSum:247.25
loss:[25.762949466705322, 26.023056983947754]
policies:[0, 4, 0]
qAverage:[0.0, 70.1855972290039]
ws:[2.034562087059021, 6.051662588119507]
memory len:10000
memory used:2722.0
now epsilon is 0.13669559491915736, the reward is 246.25 with loss [38.0991096496582, 50.940683364868164] in episode 1496
Report: 
rewardSum:246.25
loss:[38.0991096496582, 50.940683364868164]
policies:[1, 3, 1]
qAverage:[0.0, 64.18910026550293]
ws:[1.7624287754297256, 7.53455525636673]
memory len:10000
memory used:2722.0
now epsilon is 0.13655895057654335, the reward is 247.25 with loss [21.32641911506653, 21.461825132369995] in episode 1497
Report: 
rewardSum:247.25
loss:[21.32641911506653, 21.461825132369995]
policies:[0, 3, 1]
qAverage:[0.0, 67.77684020996094]
ws:[-2.0122234523296356, 5.336730360984802]
memory len:10000
memory used:2722.0
now epsilon is 0.1364224428270389, the reward is 59.6875 with loss [20.651081800460815, 20.895276069641113] in episode 1498
Report: 
rewardSum:59.6875
loss:[20.651081800460815, 20.895276069641113]
policies:[0, 3, 1]
qAverage:[0.0, 62.46194839477539]
ws:[-2.413972795009613, 0.9328776448965073]
memory len:10000
memory used:2722.0
now epsilon is 0.13635424013202804, the reward is -1.0 with loss [14.239513397216797, 27.973567962646484] in episode 1499
Report: 
rewardSum:-1.0
loss:[14.239513397216797, 27.973567962646484]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:2722.0
now epsilon is 0.13614983656132745, the reward is 245.25 with loss [40.34207344055176, 41.282320976257324] in episode 1500
Report: 
rewardSum:245.25
loss:[40.34207344055176, 41.282320976257324]
policies:[1, 4, 1]
qAverage:[0.0, 70.32462615966797]
ws:[0.7906603813171387, 6.428239536285401]
memory len:10000
memory used:2722.0
now epsilon is 0.136013737772446, the reward is 247.25 with loss [24.61160659790039, 28.35880422592163] in episode 1501
Report: 
rewardSum:247.25
loss:[24.61160659790039, 28.35880422592163]
policies:[0, 4, 0]
qAverage:[0.0, 70.81270599365234]
ws:[1.5160934925079346, 7.306962966918945]
memory len:10000
memory used:2722.0
now epsilon is 0.1358777750313249, the reward is 247.25 with loss [30.767355918884277, 38.475675106048584] in episode 1502
Report: 
rewardSum:247.25
loss:[30.767355918884277, 38.475675106048584]
policies:[0, 4, 0]
qAverage:[0.0, 70.23869018554687]
ws:[1.740675687789917, 6.90571870803833]
memory len:10000
memory used:2722.0
now epsilon is 0.13574194820196742, the reward is 59.6875 with loss [26.80190420150757, 28.58513355255127] in episode 1503
Report: 
rewardSum:59.6875
loss:[26.80190420150757, 28.58513355255127]
policies:[0, 3, 1]
qAverage:[0.0, 64.20061111450195]
ws:[1.344143569469452, 4.640238344669342]
memory len:10000
memory used:2722.0
now epsilon is 0.1356062571485127, the reward is 247.25 with loss [27.031585216522217, 33.32671356201172] in episode 1504
Report: 
rewardSum:247.25
loss:[27.031585216522217, 33.32671356201172]
policies:[0, 4, 0]
qAverage:[0.0, 70.03685150146484]
ws:[-0.17577257752418518, 3.9616059064865112]
memory len:10000
memory used:2722.0
now epsilon is 0.13547070173523576, the reward is 247.25 with loss [21.756227016448975, 23.726318359375] in episode 1505
Report: 
rewardSum:247.25
loss:[21.756227016448975, 23.726318359375]
policies:[0, 4, 0]
qAverage:[0.0, 70.47610321044922]
ws:[2.3240244150161744, 7.235905456542969]
memory len:10000
memory used:2722.0
now epsilon is 0.13533528182654728, the reward is 247.25 with loss [29.423110961914062, 24.633902549743652] in episode 1506
Report: 
rewardSum:247.25
loss:[29.423110961914062, 24.633902549743652]
policies:[0, 3, 1]
qAverage:[0.0, 67.79813003540039]
ws:[1.8074601590633392, 8.447369933128357]
memory len:10000
memory used:2722.0
now epsilon is 0.1351999972869935, the reward is 247.25 with loss [29.998496532440186, 30.331554889678955] in episode 1507
Report: 
rewardSum:247.25
loss:[29.998496532440186, 30.331554889678955]
policies:[0, 4, 0]
qAverage:[0.0, 68.47751159667969]
ws:[-0.501707011461258, 3.484361720085144]
memory len:10000
memory used:2722.0
now epsilon is 0.135064847981256, the reward is 247.25 with loss [24.140756607055664, 26.431116580963135] in episode 1508
Report: 
rewardSum:247.25
loss:[24.140756607055664, 26.431116580963135]
policies:[0, 4, 0]
qAverage:[0.0, 61.249267578125]
ws:[-0.7773317843675613, 2.913599044084549]
memory len:10000
memory used:2722.0
now epsilon is 0.1347949545306327, the reward is 243.25 with loss [57.14287984371185, 49.49372410774231] in episode 1509
Report: 
rewardSum:243.25
loss:[57.14287984371185, 49.49372410774231]
policies:[0, 5, 3]
qAverage:[0.0, 67.84537048339844]
ws:[0.5251088477671146, 4.130301904678345]
memory len:10000
memory used:2722.0
now epsilon is 0.13466021011578586, the reward is 247.25 with loss [28.285642623901367, 17.885720252990723] in episode 1510
Report: 
rewardSum:247.25
loss:[28.285642623901367, 17.885720252990723]
policies:[0, 4, 0]
qAverage:[0.0, 66.77968902587891]
ws:[1.6796585083007813, 4.437716007232666]
memory len:10000
memory used:2722.0
now epsilon is 0.13452560039483313, the reward is 247.25 with loss [25.97770643234253, 29.00471591949463] in episode 1511
Report: 
rewardSum:247.25
loss:[25.97770643234253, 29.00471591949463]
policies:[1, 3, 0]
qAverage:[0.0, 64.41408348083496]
ws:[-0.6518975049257278, 2.4466704428195953]
memory len:10000
memory used:2722.0
now epsilon is 0.13439112523313113, the reward is 247.25 with loss [23.072969436645508, 18.5948224067688] in episode 1512
Report: 
rewardSum:247.25
loss:[23.072969436645508, 18.5948224067688]
policies:[1, 3, 0]
qAverage:[0.0, 57.05082448323568]
ws:[-0.08442942301432292, 5.252729415893555]
memory len:10000
memory used:2722.0
now epsilon is 0.13425678449617104, the reward is 247.25 with loss [23.520265579223633, 33.277708530426025] in episode 1513
Report: 
rewardSum:247.25
loss:[23.520265579223633, 33.277708530426025]
policies:[0, 4, 0]
qAverage:[0.0, 67.81502532958984]
ws:[-0.9143180005252362, 1.93301842212677]
memory len:10000
memory used:2722.0
now epsilon is 0.13412257804957856, the reward is 59.6875 with loss [32.951032638549805, 34.08057737350464] in episode 1514
Report: 
rewardSum:59.6875
loss:[32.951032638549805, 34.08057737350464]
policies:[0, 3, 1]
qAverage:[0.0, 59.73582649230957]
ws:[1.0504355430603027, 2.232757568359375]
memory len:10000
memory used:2722.0
now epsilon is 0.1339885057591136, the reward is 247.25 with loss [19.503787517547607, 35.18062210083008] in episode 1515
Report: 
rewardSum:247.25
loss:[19.503787517547607, 35.18062210083008]
policies:[0, 4, 0]
qAverage:[0.0, 67.24860992431641]
ws:[2.017879343032837, 5.8638654232025145]
memory len:10000
memory used:2722.0
now epsilon is 0.13385456749067043, the reward is 247.25 with loss [19.736509799957275, 23.899424076080322] in episode 1516
Report: 
rewardSum:247.25
loss:[19.736509799957275, 23.899424076080322]
policies:[1, 3, 0]
qAverage:[17.236428833007814, 51.569007873535156]
ws:[0.746868371963501, 3.687929701805115]
memory len:10000
memory used:2722.0
now epsilon is 0.13372076311027717, the reward is 247.25 with loss [34.392820835113525, 22.259740352630615] in episode 1517
Report: 
rewardSum:247.25
loss:[34.392820835113525, 22.259740352630615]
policies:[0, 3, 1]
qAverage:[0.0, 65.04459381103516]
ws:[-1.2076469361782074, 2.9681812822818756]
memory len:10000
memory used:2722.0
now epsilon is 0.13358709248409606, the reward is 247.25 with loss [28.753525257110596, 15.941853046417236] in episode 1518
Report: 
rewardSum:247.25
loss:[28.753525257110596, 15.941853046417236]
policies:[2, 2, 0]
qAverage:[0.0, 55.223856608072914]
ws:[0.27939526240030926, 4.79092808564504]
memory len:10000
memory used:2722.0
now epsilon is 0.13338683704153098, the reward is 245.25 with loss [43.58330774307251, 44.56837511062622] in episode 1519
Report: 
rewardSum:245.25
loss:[43.58330774307251, 44.56837511062622]
policies:[0, 5, 1]
qAverage:[0.0, 65.4485232035319]
ws:[-0.0020103255907694497, 5.1919051806132]
memory len:10000
memory used:2722.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20*		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.13325350021621718, the reward is 247.25 with loss [28.092968463897705, 20.03948998451233] in episode 1520
Report: 
rewardSum:247.25
loss:[28.092968463897705, 20.03948998451233]
policies:[1, 3, 0]
qAverage:[0.0, 59.20565223693848]
ws:[0.33757174015045166, 4.006662964820862]
memory len:10000
memory used:2722.0
now epsilon is 0.13312029667773573, the reward is 59.6875 with loss [30.925294876098633, 25.535236835479736] in episode 1521
Report: 
rewardSum:59.6875
loss:[30.925294876098633, 25.535236835479736]
policies:[0, 3, 1]
qAverage:[0.0, 50.64392344156901]
ws:[3.7405196030934653, 6.911824067433675]
memory len:10000
memory used:2722.0
now epsilon is 0.1329872262928498, the reward is 59.6875 with loss [27.8563175201416, 22.773999214172363] in episode 1522
Report: 
rewardSum:59.6875
loss:[27.8563175201416, 22.773999214172363]
policies:[0, 3, 1]
qAverage:[0.0, 52.656700134277344]
ws:[2.261846601963043, 5.975674152374268]
memory len:10000
memory used:2722.0
now epsilon is 0.13285428892845563, the reward is 59.6875 with loss [34.998597621917725, 27.48069190979004] in episode 1523
Report: 
rewardSum:59.6875
loss:[34.998597621917725, 27.48069190979004]
policies:[0, 3, 1]
qAverage:[0.0, 58.92346954345703]
ws:[3.1606585681438446, 7.074628233909607]
memory len:10000
memory used:2722.0
now epsilon is 0.1327214844515827, the reward is 247.25 with loss [26.65974235534668, 39.722907066345215] in episode 1524
Report: 
rewardSum:247.25
loss:[26.65974235534668, 39.722907066345215]
policies:[0, 4, 0]
qAverage:[0.0, 63.51846008300781]
ws:[2.6759403705596925, 7.273976898193359]
memory len:10000
memory used:2721.0
now epsilon is 0.13258881272939324, the reward is 247.25 with loss [26.670958995819092, 23.239440441131592] in episode 1525
Report: 
rewardSum:247.25
loss:[26.670958995819092, 23.239440441131592]
policies:[0, 4, 0]
qAverage:[0.0, 65.1448760986328]
ws:[3.4950371026992797, 8.130096340179444]
memory len:10000
memory used:2722.0
now epsilon is 0.13245627362918236, the reward is 247.25 with loss [28.996277809143066, 21.05996584892273] in episode 1526
Report: 
rewardSum:247.25
loss:[28.996277809143066, 21.05996584892273]
policies:[1, 3, 0]
qAverage:[0.0, 60.979291915893555]
ws:[1.6375178098678589, 6.660970568656921]
memory len:10000
memory used:2722.0
now epsilon is 0.1323238670183778, the reward is 247.25 with loss [28.905078411102295, 25.240513801574707] in episode 1527
Report: 
rewardSum:247.25
loss:[28.905078411102295, 25.240513801574707]
policies:[1, 2, 1]
qAverage:[0.0, 53.864298502604164]
ws:[-0.841453472773234, 1.7872334321339924]
memory len:10000
memory used:2722.0
now epsilon is 0.13219159276453987, the reward is 247.25 with loss [33.186238527297974, 30.56836700439453] in episode 1528
Report: 
rewardSum:247.25
loss:[33.186238527297974, 30.56836700439453]
policies:[1, 3, 0]
qAverage:[0.0, 61.24730682373047]
ws:[-1.0750046111643314, 3.5803415179252625]
memory len:10000
memory used:2722.0
now epsilon is 0.13202643587267732, the reward is 246.25 with loss [44.25603246688843, 34.149250507354736] in episode 1529
Report: 
rewardSum:246.25
loss:[44.25603246688843, 34.149250507354736]
policies:[0, 4, 1]
qAverage:[0.0, 58.723161697387695]
ws:[0.2473660111427307, 4.22529137134552]
memory len:10000
memory used:2722.0
now epsilon is 0.13189445893846696, the reward is 59.6875 with loss [22.43446111679077, 20.804184436798096] in episode 1530
Report: 
rewardSum:59.6875
loss:[22.43446111679077, 20.804184436798096]
policies:[1, 1, 2]
qAverage:[0.0, 40.14283752441406]
ws:[-0.6350798606872559, 1.9022117853164673]
memory len:10000
memory used:2722.0
now epsilon is 0.1317626139317077, the reward is 247.25 with loss [34.385530948638916, 22.544007778167725] in episode 1531
Report: 
rewardSum:247.25
loss:[34.385530948638916, 22.544007778167725]
policies:[0, 4, 0]
qAverage:[0.0, 58.86765594482422]
ws:[-0.3006235957145691, 4.184502029418946]
memory len:10000
memory used:2722.0
now epsilon is 0.1316309007205216, the reward is 247.25 with loss [21.917391777038574, 29.56903600692749] in episode 1532
Report: 
rewardSum:247.25
loss:[21.917391777038574, 29.56903600692749]
policies:[0, 4, 0]
qAverage:[0.0, 61.89626007080078]
ws:[-0.2691291719675064, 3.530534029006958]
memory len:10000
memory used:2722.0
now epsilon is 0.1314335777322833, the reward is 245.25 with loss [40.732911586761475, 40.291181802749634] in episode 1533
Report: 
rewardSum:245.25
loss:[40.732911586761475, 40.291181802749634]
policies:[0, 5, 1]
qAverage:[0.0, 61.535377502441406]
ws:[2.5766598701477053, 8.715406131744384]
memory len:10000
memory used:2722.0
now epsilon is 0.1313021934339286, the reward is 247.25 with loss [31.64754009246826, 22.69209623336792] in episode 1534
Report: 
rewardSum:247.25
loss:[31.64754009246826, 22.69209623336792]
policies:[1, 3, 0]
qAverage:[0.0, 59.241933822631836]
ws:[1.2052242755889893, 6.511735796928406]
memory len:10000
memory used:2722.0
now epsilon is 0.13117094047061134, the reward is 247.25 with loss [30.713537216186523, 20.960389375686646] in episode 1535
Report: 
rewardSum:247.25
loss:[30.713537216186523, 20.960389375686646]
policies:[0, 4, 0]
qAverage:[0.0, 60.602491760253905]
ws:[-0.9236491918563843, 2.3559197425842284]
memory len:10000
memory used:2722.0
now epsilon is 0.13103981871104578, the reward is 247.25 with loss [26.904676914215088, 32.178088665008545] in episode 1536
Report: 
rewardSum:247.25
loss:[26.904676914215088, 32.178088665008545]
policies:[0, 4, 0]
qAverage:[0.0, 61.778343200683594]
ws:[-1.5066717386245727, 3.1537772536277773]
memory len:10000
memory used:2722.0
now epsilon is 0.1309088280240773, the reward is 247.25 with loss [35.69581413269043, 26.162373065948486] in episode 1537
Report: 
rewardSum:247.25
loss:[35.69581413269043, 26.162373065948486]
policies:[1, 3, 0]
qAverage:[0.0, 58.01766395568848]
ws:[-0.6670105755329132, 5.14897620677948]
memory len:10000
memory used:2724.0
now epsilon is 0.13077796827868246, the reward is 247.25 with loss [29.21911859512329, 24.119019269943237] in episode 1538
Report: 
rewardSum:247.25
loss:[29.21911859512329, 24.119019269943237]
policies:[1, 3, 0]
qAverage:[0.0, 55.905826568603516]
ws:[-0.18011802062392235, 2.892083764076233]
memory len:10000
memory used:2724.0
now epsilon is 0.1306472393439688, the reward is 247.25 with loss [25.84329891204834, 35.62222337722778] in episode 1539
Report: 
rewardSum:247.25
loss:[25.84329891204834, 35.62222337722778]
policies:[0, 4, 0]
qAverage:[0.0, 60.54025421142578]
ws:[0.01411701962351799, 5.058634519577026]
memory len:10000
memory used:2723.0
now epsilon is 0.13058192388974926, the reward is -1.0 with loss [12.306972980499268, 11.936112880706787] in episode 1540
Report: 
rewardSum:-1.0
loss:[12.306972980499268, 11.936112880706787]
policies:[0, 1, 1]
qAverage:[0.0, 34.88401794433594]
ws:[-0.9544762372970581, 0.23100191354751587]
memory len:10000
memory used:2723.0
now epsilon is 0.13045139092592015, the reward is 247.25 with loss [26.902000904083252, 20.42205262184143] in episode 1541
Report: 
rewardSum:247.25
loss:[26.902000904083252, 20.42205262184143]
policies:[1, 2, 1]
qAverage:[0.0, 53.62893931070963]
ws:[-3.2389413515726724, 1.3036012649536133]
memory len:10000
memory used:2723.0
now epsilon is 0.13032098844611312, the reward is 247.25 with loss [26.555885314941406, 31.989371299743652] in episode 1542
Report: 
rewardSum:247.25
loss:[26.555885314941406, 31.989371299743652]
policies:[2, 2, 0]
qAverage:[0.0, 51.321581522623696]
ws:[-3.408124804496765, 0.8317225575447083]
memory len:10000
memory used:2723.0
now epsilon is 0.13019071631989312, the reward is 247.25 with loss [33.885366916656494, 19.167789697647095] in episode 1543
Report: 
rewardSum:247.25
loss:[33.885366916656494, 19.167789697647095]
policies:[0, 4, 0]
qAverage:[0.0, 59.19584350585937]
ws:[-0.7426474243402481, 5.195181488990784]
memory len:10000
memory used:2723.0
now epsilon is 0.13012562909865294, the reward is -1.0 with loss [15.613007545471191, 18.39186954498291] in episode 1544
Report: 
rewardSum:-1.0
loss:[15.613007545471191, 18.39186954498291]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:2723.0
now epsilon is 0.12999555225853288, the reward is 247.25 with loss [19.342321395874023, 26.944927215576172] in episode 1545
Report: 
rewardSum:247.25
loss:[19.342321395874023, 26.944927215576172]
policies:[0, 4, 0]
qAverage:[0.0, 57.150014877319336]
ws:[-1.0484988242387772, 4.085251092910767]
memory len:10000
memory used:2723.0
now epsilon is 0.12986560544648224, the reward is 247.25 with loss [30.815826892852783, 24.628612518310547] in episode 1546
Report: 
rewardSum:247.25
loss:[30.815826892852783, 24.628612518310547]
policies:[0, 4, 0]
qAverage:[0.0, 58.70242919921875]
ws:[-1.8815263628959655, 2.6891894817352293]
memory len:10000
memory used:2723.0
now epsilon is 0.1297357885325217, the reward is 247.25 with loss [29.959819555282593, 36.320106506347656] in episode 1547
Report: 
rewardSum:247.25
loss:[29.959819555282593, 36.320106506347656]
policies:[1, 3, 0]
qAverage:[0.0, 56.6158504486084]
ws:[-2.449375480413437, 2.646769404411316]
memory len:10000
memory used:2723.0
now epsilon is 0.12954130643648987, the reward is 245.25 with loss [50.03011465072632, 41.412338733673096] in episode 1548
Report: 
rewardSum:245.25
loss:[50.03011465072632, 41.412338733673096]
policies:[0, 5, 1]
qAverage:[0.0, 58.43565673828125]
ws:[-1.7616745471954345, 3.692002034187317]
memory len:10000
memory used:2723.0
############# STATE ###############
0-		8*		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.12941181369994748, the reward is 247.25 with loss [29.868172883987427, 26.6411075592041] in episode 1549
Report: 
rewardSum:247.25
loss:[29.868172883987427, 26.6411075592041]
policies:[1, 3, 0]
qAverage:[0.0, 56.26716613769531]
ws:[-5.727242887020111, -2.288047134876251]
memory len:10000
memory used:2723.0
now epsilon is 0.12928245040758993, the reward is 247.25 with loss [17.553758144378662, 17.79057765007019] in episode 1550
Report: 
rewardSum:247.25
loss:[17.553758144378662, 17.79057765007019]
policies:[1, 3, 0]
qAverage:[0.0, 47.14587148030599]
ws:[-2.545450528462728, -0.4732842445373535]
memory len:10000
memory used:2723.0
now epsilon is 0.12915321643002164, the reward is 247.25 with loss [39.55061674118042, 22.09121799468994] in episode 1551
Report: 
rewardSum:247.25
loss:[39.55061674118042, 22.09121799468994]
policies:[0, 4, 0]
qAverage:[0.0, 59.437443542480466]
ws:[0.13304426670074462, 4.2075543880462645]
memory len:10000
memory used:2723.0
now epsilon is 0.1289596076461642, the reward is 245.25 with loss [47.15032434463501, 37.6644492149353] in episode 1552
Report: 
rewardSum:245.25
loss:[47.15032434463501, 37.6644492149353]
policies:[0, 4, 2]
qAverage:[0.0, 58.93033905029297]
ws:[-1.8366716250777244, 1.96054265499115]
memory len:10000
memory used:2723.0
now epsilon is 0.12876628909403484, the reward is 245.25 with loss [38.06600475311279, 43.50790357589722] in episode 1553
Report: 
rewardSum:245.25
loss:[38.06600475311279, 43.50790357589722]
policies:[1, 4, 1]
qAverage:[0.0, 56.84088287353516]
ws:[0.03582044243812561, 3.7517009019851684]
memory len:10000
memory used:2723.0
now epsilon is 0.12863757108425183, the reward is 247.25 with loss [29.88439702987671, 29.900320529937744] in episode 1554
Report: 
rewardSum:247.25
loss:[29.88439702987671, 29.900320529937744]
policies:[0, 4, 0]
qAverage:[0.0, 55.86219596862793]
ws:[-4.905022084712982, -1.2070224098861217]
memory len:10000
memory used:2723.0
now epsilon is 0.1285089817442174, the reward is 247.25 with loss [23.663628101348877, 25.064553260803223] in episode 1555
Report: 
rewardSum:247.25
loss:[23.663628101348877, 25.064553260803223]
policies:[0, 2, 2]
qAverage:[0.0, 47.03750864664713]
ws:[-5.608920176823934, -1.5549927353858948]
memory len:10000
memory used:2723.0
now epsilon is 0.12838052094531002, the reward is 247.25 with loss [34.08381986618042, 22.443821668624878] in episode 1556
Report: 
rewardSum:247.25
loss:[34.08381986618042, 22.443821668624878]
policies:[0, 4, 0]
qAverage:[0.0, 56.264875411987305]
ws:[-5.2526408433914185, -1.3780984431505203]
memory len:10000
memory used:2723.0
now epsilon is 0.12825218855903678, the reward is 247.25 with loss [24.514138221740723, 31.367879390716553] in episode 1557
Report: 
rewardSum:247.25
loss:[24.514138221740723, 31.367879390716553]
policies:[0, 4, 0]
qAverage:[0.0, 56.54281921386719]
ws:[-1.9436371743679046, 1.4461357593536377]
memory len:10000
memory used:2723.0
now epsilon is 0.1281239844570332, the reward is 247.25 with loss [32.75366497039795, 19.277425527572632] in episode 1558
Report: 
rewardSum:247.25
loss:[32.75366497039795, 19.277425527572632]
policies:[0, 4, 0]
qAverage:[0.0, 57.36976013183594]
ws:[-1.6397085428237914, 1.1960093021392821]
memory len:10000
memory used:2723.0
now epsilon is 0.12799590851106313, the reward is 247.25 with loss [31.275301456451416, 22.862714290618896] in episode 1559
Report: 
rewardSum:247.25
loss:[31.275301456451416, 22.862714290618896]
policies:[3, 1, 0]
qAverage:[0.0, 32.124874114990234]
ws:[-0.07883880287408829, 0.8346191048622131]
memory len:10000
memory used:2723.0
now epsilon is 0.1278679605930185, the reward is 247.25 with loss [24.649043083190918, 28.669881343841553] in episode 1560
Report: 
rewardSum:247.25
loss:[24.649043083190918, 28.669881343841553]
policies:[0, 4, 0]
qAverage:[0.0, 57.388404846191406]
ws:[-4.977137660980224, -1.6007925969548524]
memory len:10000
memory used:2723.0
now epsilon is 0.12774014057491945, the reward is 247.25 with loss [30.421969413757324, 37.33825349807739] in episode 1561
Report: 
rewardSum:247.25
loss:[30.421969413757324, 37.33825349807739]
policies:[0, 4, 0]
qAverage:[0.0, 57.079071044921875]
ws:[-1.7498501181602477, 3.818553590774536]
memory len:10000
memory used:2723.0
now epsilon is 0.127612448328914, the reward is 247.25 with loss [32.436081409454346, 23.092731475830078] in episode 1562
Report: 
rewardSum:247.25
loss:[32.436081409454346, 23.092731475830078]
policies:[0, 4, 0]
qAverage:[0.0, 57.37361145019531]
ws:[-1.6645175695419312, 3.5888339042663575]
memory len:10000
memory used:2723.0
now epsilon is 0.12748488372727795, the reward is 247.25 with loss [23.144572257995605, 28.97920298576355] in episode 1563
Report: 
rewardSum:247.25
loss:[23.144572257995605, 28.97920298576355]
policies:[0, 4, 0]
qAverage:[0.0, 56.88241119384766]
ws:[-4.016407382488251, 0.007542848587036133]
memory len:10000
memory used:2723.0
now epsilon is 0.12735744664241475, the reward is 247.25 with loss [23.281848192214966, 22.608351945877075] in episode 1564
Report: 
rewardSum:247.25
loss:[23.281848192214966, 22.608351945877075]
policies:[0, 4, 0]
qAverage:[0.0, 54.524539947509766]
ws:[-4.264052301645279, -0.29487451910972595]
memory len:10000
memory used:2724.0
now epsilon is 0.12723013694685548, the reward is 247.25 with loss [24.211052417755127, 25.440430164337158] in episode 1565
Report: 
rewardSum:247.25
loss:[24.211052417755127, 25.440430164337158]
policies:[0, 4, 0]
qAverage:[0.0, 51.24685478210449]
ws:[2.321373164653778, 5.935627460479736]
memory len:10000
memory used:2725.0
now epsilon is 0.1271029545132586, the reward is 247.25 with loss [22.924556255340576, 31.40086555480957] in episode 1566
Report: 
rewardSum:247.25
loss:[22.924556255340576, 31.40086555480957]
policies:[0, 4, 0]
qAverage:[0.0, 57.175201416015625]
ws:[-3.3184169471263885, 1.0274720191955566]
memory len:10000
memory used:2724.0
now epsilon is 0.12691241920079635, the reward is 245.25 with loss [37.31880283355713, 56.96591329574585] in episode 1567
Report: 
rewardSum:245.25
loss:[37.31880283355713, 56.96591329574585]
policies:[0, 5, 1]
qAverage:[0.0, 56.7202262878418]
ws:[-2.5402399798234305, 0.9281044602394104]
memory len:10000
memory used:2723.0
now epsilon is 0.12678555436582123, the reward is 247.25 with loss [24.135931491851807, 22.426255226135254] in episode 1568
Report: 
rewardSum:247.25
loss:[24.135931491851807, 22.426255226135254]
policies:[0, 4, 0]
qAverage:[0.0, 56.064361572265625]
ws:[-2.6883747298270464, 1.91808180809021]
memory len:10000
memory used:2723.0
now epsilon is 0.1266588163481147, the reward is 247.25 with loss [21.19551992416382, 34.62605619430542] in episode 1569
Report: 
rewardSum:247.25
loss:[21.19551992416382, 34.62605619430542]
policies:[1, 3, 0]
qAverage:[0.0, 51.20278263092041]
ws:[-3.2372092604637146, 2.476153463125229]
memory len:10000
memory used:2723.0
now epsilon is 0.12653220502090706, the reward is 247.25 with loss [31.331369161605835, 31.512203216552734] in episode 1570
Report: 
rewardSum:247.25
loss:[31.331369161605835, 31.512203216552734]
policies:[0, 4, 0]
qAverage:[0.0, 49.470380783081055]
ws:[0.9250949621200562, 4.044301271438599]
memory len:10000
memory used:2723.0
now epsilon is 0.12640572025755528, the reward is 247.25 with loss [27.43936824798584, 21.752707481384277] in episode 1571
Report: 
rewardSum:247.25
loss:[27.43936824798584, 21.752707481384277]
policies:[0, 4, 0]
qAverage:[0.0, 54.93882827758789]
ws:[0.42504191398620605, 5.946603167057037]
memory len:10000
memory used:2723.0
now epsilon is 0.12621623014303734, the reward is 245.25 with loss [36.47751474380493, 36.414135456085205] in episode 1572
Report: 
rewardSum:245.25
loss:[36.47751474380493, 36.414135456085205]
policies:[0, 5, 1]
qAverage:[0.0, 56.75407346089681]
ws:[-0.9932556394487619, 2.6225684036811194]
memory len:10000
memory used:2723.0
now epsilon is 0.12609006123609262, the reward is 247.25 with loss [23.313600540161133, 29.902132034301758] in episode 1573
Report: 
rewardSum:247.25
loss:[23.313600540161133, 29.902132034301758]
policies:[0, 4, 0]
qAverage:[0.0, 50.700408935546875]
ws:[-1.2913778722286224, 3.1308177709579468]
memory len:10000
memory used:2723.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20*		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.12590104431427518, the reward is 245.25 with loss [51.70657253265381, 36.80570077896118] in episode 1574
Report: 
rewardSum:245.25
loss:[51.70657253265381, 36.80570077896118]
policies:[0, 5, 1]
qAverage:[0.0, 56.77145957946777]
ws:[-1.3641788462797801, 3.6394369900226593]
memory len:10000
memory used:2723.0
now epsilon is 0.12577519047498423, the reward is 247.25 with loss [24.01798963546753, 24.445011138916016] in episode 1575
Report: 
rewardSum:247.25
loss:[24.01798963546753, 24.445011138916016]
policies:[0, 3, 1]
qAverage:[0.0, 51.2170991897583]
ws:[-4.099024325609207, 0.3888723850250244]
memory len:10000
memory used:2723.0
now epsilon is 0.12564946244234523, the reward is 247.25 with loss [18.345011949539185, 24.987115144729614] in episode 1576
Report: 
rewardSum:247.25
loss:[18.345011949539185, 24.987115144729614]
policies:[0, 4, 0]
qAverage:[0.0, 55.081312561035155]
ws:[-2.5440555572509767, 2.8079187870025635]
memory len:10000
memory used:2723.0
now epsilon is 0.12539838329411096, the reward is 243.25 with loss [61.56061124801636, 53.55703178048134] in episode 1577
Report: 
rewardSum:243.25
loss:[61.56061124801636, 53.55703178048134]
policies:[1, 5, 2]
qAverage:[0.0, 54.53028806050619]
ws:[-3.0982262591520944, 1.2725932200749714]
memory len:10000
memory used:2723.0
now epsilon is 0.12524171366939188, the reward is 246.25 with loss [37.929903984069824, 37.40805244445801] in episode 1578
Report: 
rewardSum:246.25
loss:[37.929903984069824, 37.40805244445801]
policies:[0, 4, 1]
qAverage:[0.0, 48.23412227630615]
ws:[-0.6798062399029732, 1.877404659986496]
memory len:10000
memory used:2723.0
now epsilon is 0.125116518913538, the reward is 247.25 with loss [27.230454206466675, 41.98145580291748] in episode 1579
Report: 
rewardSum:247.25
loss:[27.230454206466675, 41.98145580291748]
policies:[0, 4, 0]
qAverage:[0.0, 48.56571388244629]
ws:[-0.5665506720542908, 2.994011476635933]
memory len:10000
memory used:2724.0
now epsilon is 0.12499144930549977, the reward is 247.25 with loss [24.062872886657715, 22.868788242340088] in episode 1580
Report: 
rewardSum:247.25
loss:[24.062872886657715, 22.868788242340088]
policies:[0, 4, 0]
qAverage:[0.0, 53.5228271484375]
ws:[-4.1683214664459225, 1.4435465335845947]
memory len:10000
memory used:2724.0
now epsilon is 0.12486650472017628, the reward is 247.25 with loss [25.899084091186523, 35.82748794555664] in episode 1581
Report: 
rewardSum:247.25
loss:[25.899084091186523, 35.82748794555664]
policies:[0, 3, 1]
qAverage:[0.0, 52.56127166748047]
ws:[-2.4001441299915314, 4.004027843475342]
memory len:10000
memory used:2723.0
now epsilon is 0.12474168503259173, the reward is 247.25 with loss [24.998652935028076, 26.930043935775757] in episode 1582
Report: 
rewardSum:247.25
loss:[24.998652935028076, 26.930043935775757]
policies:[0, 3, 1]
qAverage:[0.0, 48.21588897705078]
ws:[2.208064556121826, 5.225733995437622]
memory len:10000
memory used:2723.0
now epsilon is 0.12455468941139812, the reward is 245.25 with loss [42.7884681224823, 33.32858490943909] in episode 1583
Report: 
rewardSum:245.25
loss:[42.7884681224823, 33.32858490943909]
policies:[0, 5, 1]
qAverage:[0.0, 56.00072224934896]
ws:[-3.3239468137423196, 1.498706231514613]
memory len:10000
memory used:2723.0
now epsilon is 0.12443018142221109, the reward is 247.25 with loss [27.809507608413696, 24.61926531791687] in episode 1584
Report: 
rewardSum:247.25
loss:[27.809507608413696, 24.61926531791687]
policies:[0, 4, 0]
qAverage:[0.0, 44.95276641845703]
ws:[1.4784563382466633, 5.514987945556641]
memory len:10000
memory used:2723.0
now epsilon is 0.12424365276449574, the reward is 245.25 with loss [37.258947134017944, 35.02729034423828] in episode 1585
Report: 
rewardSum:245.25
loss:[37.258947134017944, 35.02729034423828]
policies:[0, 5, 1]
qAverage:[0.0, 55.287176513671874]
ws:[-2.3575434654951097, 1.6450839996337892]
memory len:10000
memory used:2723.0
now epsilon is 0.1241194556953363, the reward is 247.25 with loss [26.13535737991333, 35.266714572906494] in episode 1586
Report: 
rewardSum:247.25
loss:[26.13535737991333, 35.266714572906494]
policies:[2, 2, 0]
qAverage:[16.986806869506836, 34.84942436218262]
ws:[-3.4502175161615014, 0.744864784181118]
memory len:10000
memory used:2723.0
now epsilon is 0.12399538277667987, the reward is 59.6875 with loss [21.836747765541077, 36.847768783569336] in episode 1587
Report: 
rewardSum:59.6875
loss:[21.836747765541077, 36.847768783569336]
policies:[1, 2, 1]
qAverage:[17.044023513793945, 33.85639953613281]
ws:[0.08921980857849121, 1.752955511212349]
memory len:10000
memory used:2723.0
now epsilon is 0.12387143388442251, the reward is 247.25 with loss [23.41270112991333, 24.39738702774048] in episode 1588
Report: 
rewardSum:247.25
loss:[23.41270112991333, 24.39738702774048]
policies:[1, 3, 0]
qAverage:[13.52777557373047, 40.24342575073242]
ws:[-4.54185026884079, -2.100663757324219]
memory len:10000
memory used:2723.0
now epsilon is 0.12380950590944492, the reward is -1.0 with loss [12.48993968963623, 10.68064260482788] in episode 1589
Report: 
rewardSum:-1.0
loss:[12.48993968963623, 10.68064260482788]
policies:[1, 0, 1]
qAverage:[31.94359588623047, 0.0]
ws:[-0.3131520450115204, -1.3636400699615479]
memory len:10000
memory used:2723.0
now epsilon is 0.1236857428243626, the reward is 247.25 with loss [19.753600120544434, 22.611483573913574] in episode 1590
Report: 
rewardSum:247.25
loss:[19.753600120544434, 22.611483573913574]
policies:[2, 2, 0]
qAverage:[21.70746103922526, 24.93555450439453]
ws:[-5.258132656415303, -3.056055804093679]
memory len:10000
memory used:2723.0
now epsilon is 0.12353121293009794, the reward is 246.25 with loss [36.85893106460571, 45.21419668197632] in episode 1591
Report: 
rewardSum:246.25
loss:[36.85893106460571, 45.21419668197632]
policies:[2, 2, 1]
qAverage:[16.4583683013916, 35.144691467285156]
ws:[-4.667877480387688, -3.2584588527679443]
memory len:10000
memory used:2723.0
now epsilon is 0.12340772803365249, the reward is 247.25 with loss [23.09476137161255, 29.151793003082275] in episode 1592
Report: 
rewardSum:247.25
loss:[23.09476137161255, 29.151793003082275]
policies:[1, 2, 1]
qAverage:[16.516569137573242, 32.58541774749756]
ws:[1.8402170091867447, 3.982463300228119]
memory len:10000
memory used:2723.0
now epsilon is 0.12328436657580435, the reward is 247.25 with loss [27.00253129005432, 29.282557010650635] in episode 1593
Report: 
rewardSum:247.25
loss:[27.00253129005432, 29.282557010650635]
policies:[2, 2, 0]
qAverage:[32.900169372558594, 0.0]
ws:[1.1726996898651123, 1.0936588048934937]
memory len:10000
memory used:2723.0
now epsilon is 0.12303801348245418, the reward is 243.25 with loss [51.791736125946045, 66.34033703804016] in episode 1594
Report: 
rewardSum:243.25
loss:[51.791736125946045, 66.34033703804016]
policies:[1, 5, 2]
qAverage:[0.0, 52.4588425954183]
ws:[1.0037893230716388, 3.9884913563728333]
memory len:10000
memory used:2723.0
now epsilon is 0.12288429284513726, the reward is 246.25 with loss [29.287949085235596, 33.912413597106934] in episode 1595
Report: 
rewardSum:246.25
loss:[29.287949085235596, 33.912413597106934]
policies:[0, 4, 1]
qAverage:[0.0, 52.26661682128906]
ws:[-2.7956997394561767, 4.55043773651123]
memory len:10000
memory used:2723.0
now epsilon is 0.12276145462622218, the reward is 247.25 with loss [31.206971645355225, 30.51166343688965] in episode 1596
Report: 
rewardSum:247.25
loss:[31.206971645355225, 30.51166343688965]
policies:[0, 4, 0]
qAverage:[0.0, 52.76427154541015]
ws:[-1.6839565753936767, 5.4277338027954105]
memory len:10000
memory used:2723.0
now epsilon is 0.12263873919946934, the reward is 247.25 with loss [21.727368116378784, 26.07606601715088] in episode 1597
Report: 
rewardSum:247.25
loss:[21.727368116378784, 26.07606601715088]
policies:[0, 4, 0]
qAverage:[0.0, 52.48426895141601]
ws:[-0.5182846069335938, 6.8382487297058105]
memory len:10000
memory used:2723.0
now epsilon is 0.12251614644213266, the reward is 247.25 with loss [24.82636070251465, 43.03141784667969] in episode 1598
Report: 
rewardSum:247.25
loss:[24.82636070251465, 43.03141784667969]
policies:[0, 4, 0]
qAverage:[0.0, 52.48714370727539]
ws:[-2.546507877111435, 3.81933708190918]
memory len:10000
memory used:2723.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19*		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.12239367623158869, the reward is 247.25 with loss [21.282238721847534, 29.647566080093384] in episode 1599
Report: 
rewardSum:247.25
loss:[21.282238721847534, 29.647566080093384]
policies:[1, 3, 0]
qAverage:[0.0, 51.64678478240967]
ws:[-3.251952037215233, 4.451471954584122]
memory len:10000
memory used:2723.0
now epsilon is 0.12227132844533657, the reward is 247.25 with loss [28.80734896659851, 21.683263301849365] in episode 1600
Report: 
rewardSum:247.25
loss:[28.80734896659851, 21.683263301849365]
policies:[1, 3, 0]
qAverage:[0.0, 46.003111839294434]
ws:[3.22919475287199, 7.470547735691071]
memory len:10000
memory used:2723.0
now epsilon is 0.12214910296099792, the reward is 247.25 with loss [40.583998680114746, 33.308302879333496] in episode 1601
Report: 
rewardSum:247.25
loss:[40.583998680114746, 33.308302879333496]
policies:[0, 4, 0]
qAverage:[0.0, 49.72928047180176]
ws:[0.632530689239502, 4.741798400878906]
memory len:10000
memory used:2724.0
now epsilon is 0.12199649290640263, the reward is 246.25 with loss [26.846323251724243, 34.72503995895386] in episode 1602
Report: 
rewardSum:246.25
loss:[26.846323251724243, 34.72503995895386]
policies:[2, 2, 1]
qAverage:[16.324718475341797, 32.256381034851074]
ws:[2.375786632299423, 4.439081311225891]
memory len:10000
memory used:2724.0
now epsilon is 0.12187454215455677, the reward is 247.25 with loss [36.21112060546875, 18.335445642471313] in episode 1603
Report: 
rewardSum:247.25
loss:[36.21112060546875, 18.335445642471313]
policies:[1, 3, 0]
qAverage:[0.0, 50.91947841644287]
ws:[-1.6899929121136665, 2.929789662361145]
memory len:10000
memory used:2723.0
now epsilon is 0.12169184456062956, the reward is 245.25 with loss [36.55145263671875, 31.45113229751587] in episode 1604
Report: 
rewardSum:245.25
loss:[36.55145263671875, 31.45113229751587]
policies:[0, 5, 1]
qAverage:[0.0, 53.27999941507975]
ws:[0.7394922971725464, 5.561707258224487]
memory len:10000
memory used:2723.0
now epsilon is 0.12157019834290539, the reward is 247.25 with loss [25.891491889953613, 26.45946455001831] in episode 1605
Report: 
rewardSum:247.25
loss:[25.891491889953613, 26.45946455001831]
policies:[0, 4, 0]
qAverage:[0.0, 49.66329479217529]
ws:[-3.703080400824547, 1.707215741276741]
memory len:10000
memory used:2723.0
now epsilon is 0.1214486737257892, the reward is 247.25 with loss [23.984536170959473, 31.268736362457275] in episode 1606
Report: 
rewardSum:247.25
loss:[23.984536170959473, 31.268736362457275]
policies:[0, 4, 0]
qAverage:[0.0, 51.357320404052736]
ws:[-2.7513785399496555, 2.146664619445801]
memory len:10000
memory used:2723.0
now epsilon is 0.12132727058772601, the reward is 247.25 with loss [26.015384674072266, 36.310096740722656] in episode 1607
Report: 
rewardSum:247.25
loss:[26.015384674072266, 36.310096740722656]
policies:[0, 4, 0]
qAverage:[0.0, 52.05805282592773]
ws:[-0.6732800751924515, 4.770016407966613]
memory len:10000
memory used:2724.0
now epsilon is 0.12114539338825295, the reward is 245.25 with loss [31.748202562332153, 37.74123668670654] in episode 1608
Report: 
rewardSum:245.25
loss:[31.748202562332153, 37.74123668670654]
policies:[0, 4, 2]
qAverage:[0.0, 46.58658790588379]
ws:[1.9067394435405731, 5.687309920787811]
memory len:10000
memory used:2723.0
now epsilon is 0.12102429341681611, the reward is 247.25 with loss [31.84814739227295, 30.890814304351807] in episode 1609
Report: 
rewardSum:247.25
loss:[31.84814739227295, 30.890814304351807]
policies:[0, 4, 0]
qAverage:[0.0, 51.241309356689456]
ws:[0.27046566009521483, 4.359384489059448]
memory len:10000
memory used:2723.0
now epsilon is 0.1207824565166328, the reward is 243.25 with loss [58.440348625183105, 57.747172832489014] in episode 1610
Report: 
rewardSum:243.25
loss:[58.440348625183105, 57.747172832489014]
policies:[0, 5, 3]
qAverage:[0.0, 51.866543451944985]
ws:[-2.8661361734072366, 1.5133349696795146]
memory len:10000
memory used:2723.0
now epsilon is 0.12066171934598895, the reward is 247.25 with loss [24.642987728118896, 31.55800437927246] in episode 1611
Report: 
rewardSum:247.25
loss:[24.642987728118896, 31.55800437927246]
policies:[1, 3, 0]
qAverage:[12.64124755859375, 39.3447151184082]
ws:[-4.648515367507935, -1.4817316591739655]
memory len:10000
memory used:2723.0
now epsilon is 0.12054110286724684, the reward is 247.25 with loss [29.767332553863525, 16.786184310913086] in episode 1612
Report: 
rewardSum:247.25
loss:[29.767332553863525, 16.786184310913086]
policies:[0, 4, 0]
qAverage:[0.0, 51.42851867675781]
ws:[-4.72382607460022, -0.18796932697296143]
memory len:10000
memory used:2723.0
now epsilon is 0.12042060695975983, the reward is 247.25 with loss [36.08742141723633, 29.000566005706787] in episode 1613
Report: 
rewardSum:247.25
loss:[36.08742141723633, 29.000566005706787]
policies:[0, 4, 0]
qAverage:[0.0, 50.869075775146484]
ws:[-1.8759121894836426, 5.236275494098663]
memory len:10000
memory used:2723.0
now epsilon is 0.12030023150300187, the reward is 247.25 with loss [25.494852542877197, 22.812665939331055] in episode 1614
Report: 
rewardSum:247.25
loss:[25.494852542877197, 22.812665939331055]
policies:[1, 3, 0]
qAverage:[0.0, 48.92619037628174]
ws:[-4.526124509051442, 2.2830910682678223]
memory len:10000
memory used:2723.0
now epsilon is 0.1201799763765674, the reward is 247.25 with loss [16.780648469924927, 31.588597297668457] in episode 1615
Report: 
rewardSum:247.25
loss:[16.780648469924927, 31.588597297668457]
policies:[1, 2, 1]
qAverage:[0.0, 42.339186350504555]
ws:[0.27952059110005695, 7.353993892669678]
memory len:10000
memory used:2723.0
now epsilon is 0.12005984146017121, the reward is 247.25 with loss [36.408942222595215, 30.05526041984558] in episode 1616
Report: 
rewardSum:247.25
loss:[36.408942222595215, 30.05526041984558]
policies:[0, 4, 0]
qAverage:[0.0, 51.475518798828126]
ws:[-2.4924342393875123, 6.881262671947479]
memory len:10000
memory used:2723.0
now epsilon is 0.11993982663364833, the reward is 247.25 with loss [21.77289390563965, 27.70304822921753] in episode 1617
Report: 
rewardSum:247.25
loss:[21.77289390563965, 27.70304822921753]
policies:[0, 3, 1]
qAverage:[0.0, 50.43828868865967]
ws:[-2.46288725733757, 5.145839184522629]
memory len:10000
memory used:2723.0
now epsilon is 0.11981993177695391, the reward is 247.25 with loss [31.314558029174805, 21.079553604125977] in episode 1618
Report: 
rewardSum:247.25
loss:[31.314558029174805, 21.079553604125977]
policies:[0, 4, 0]
qAverage:[0.0, 51.23393783569336]
ws:[-0.06237761974334717, 8.329114437103271]
memory len:10000
memory used:2723.0
now epsilon is 0.1197001567701631, the reward is 247.25 with loss [24.952970504760742, 21.531464099884033] in episode 1619
Report: 
rewardSum:247.25
loss:[24.952970504760742, 21.531464099884033]
policies:[1, 3, 0]
qAverage:[0.0, 45.40967082977295]
ws:[2.5469306111335754, 7.679373741149902]
memory len:10000
memory used:2724.0
now epsilon is 0.11952071871650556, the reward is 245.25 with loss [34.30442810058594, 35.15219557285309] in episode 1620
Report: 
rewardSum:245.25
loss:[34.30442810058594, 35.15219557285309]
policies:[0, 4, 2]
qAverage:[0.0, 45.607460021972656]
ws:[-0.4021042473614216, 2.263958752155304]
memory len:10000
memory used:2723.0
now epsilon is 0.11940124281058902, the reward is 247.25 with loss [28.106950759887695, 35.59008979797363] in episode 1621
Report: 
rewardSum:247.25
loss:[28.106950759887695, 35.59008979797363]
policies:[0, 4, 0]
qAverage:[0.0, 51.09265289306641]
ws:[-3.5079073190689085, 2.8404812648892404]
memory len:10000
memory used:2723.0
now epsilon is 0.11928188633578238, the reward is 59.6875 with loss [34.688395977020264, 22.810969352722168] in episode 1622
Report: 
rewardSum:59.6875
loss:[34.688395977020264, 22.810969352722168]
policies:[0, 3, 1]
qAverage:[0.0, 38.56441116333008]
ws:[-2.41306742032369, -0.41877440611521405]
memory len:10000
memory used:2723.0
now epsilon is 0.11916264917269934, the reward is 247.25 with loss [26.447250604629517, 23.70089101791382] in episode 1623
Report: 
rewardSum:247.25
loss:[26.447250604629517, 23.70089101791382]
policies:[0, 3, 1]
qAverage:[0.0, 49.82738494873047]
ws:[-4.200177878141403, 5.009975627064705]
memory len:10000
memory used:2723.0
now epsilon is 0.11910307529577857, the reward is -1.0 with loss [14.117579936981201, 20.680358171463013] in episode 1624
Report: 
rewardSum:-1.0
loss:[14.117579936981201, 20.680358171463013]
policies:[0, 1, 1]
qAverage:[0.0, 27.103771209716797]
ws:[-0.4359082281589508, 1.9536162614822388]
memory len:10000
memory used:2723.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19*		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.11898401687669256, the reward is 247.25 with loss [24.98320770263672, 28.44732666015625] in episode 1625
Report: 
rewardSum:247.25
loss:[24.98320770263672, 28.44732666015625]
policies:[0, 4, 0]
qAverage:[0.0, 50.49386138916016]
ws:[-2.5365361988544466, 4.776461613178253]
memory len:10000
memory used:2723.0
now epsilon is 0.1188650774713862, the reward is 247.25 with loss [22.77927017211914, 23.41783046722412] in episode 1626
Report: 
rewardSum:247.25
loss:[22.77927017211914, 23.41783046722412]
policies:[0, 4, 0]
qAverage:[0.0, 42.59130986531576]
ws:[-4.367481579383214, 2.420113960901896]
memory len:10000
memory used:2723.0
now epsilon is 0.11874625696089026, the reward is 247.25 with loss [27.628082275390625, 23.15486478805542] in episode 1627
Report: 
rewardSum:247.25
loss:[27.628082275390625, 23.15486478805542]
policies:[0, 3, 1]
qAverage:[0.0, 49.297950744628906]
ws:[-0.23248058557510376, 7.725477695465088]
memory len:10000
memory used:2723.0
now epsilon is 0.11862755522635456, the reward is 247.25 with loss [44.11146688461304, 30.44671630859375] in episode 1628
Report: 
rewardSum:247.25
loss:[44.11146688461304, 30.44671630859375]
policies:[1, 3, 0]
qAverage:[15.830915451049805, 34.795597076416016]
ws:[-3.578788012266159, 1.520579993724823]
memory len:10000
memory used:2723.0
now epsilon is 0.1184497250697839, the reward is 245.25 with loss [31.04438328742981, 41.82204818725586] in episode 1629
Report: 
rewardSum:245.25
loss:[31.04438328742981, 41.82204818725586]
policies:[0, 5, 1]
qAverage:[0.0, 50.04712982177735]
ws:[-4.215306794643402, 0.9065848350524902]
memory len:10000
memory used:2724.0
now epsilon is 0.11833131975595838, the reward is 247.25 with loss [27.546467304229736, 31.560364961624146] in episode 1630
Report: 
rewardSum:247.25
loss:[27.546467304229736, 31.560364961624146]
policies:[0, 3, 1]
qAverage:[0.0, 44.931840896606445]
ws:[0.38968226313591003, 1.98227509111166]
memory len:10000
memory used:2724.0
now epsilon is 0.11821303280305209, the reward is 247.25 with loss [25.857503414154053, 22.28280258178711] in episode 1631
Report: 
rewardSum:247.25
loss:[25.857503414154053, 22.28280258178711]
policies:[1, 3, 0]
qAverage:[12.687618255615234, 39.785292053222655]
ws:[-1.2288057327270507, 2.9864294290542603]
memory len:10000
memory used:2723.0
now epsilon is 0.11809486409274851, the reward is 247.25 with loss [21.544161796569824, 31.443415641784668] in episode 1632
Report: 
rewardSum:247.25
loss:[21.544161796569824, 31.443415641784668]
policies:[0, 4, 0]
qAverage:[0.0, 50.732819366455075]
ws:[-1.0257933616638184, 4.048787641525268]
memory len:10000
memory used:2723.0
now epsilon is 0.11797681350684933, the reward is 247.25 with loss [27.71011781692505, 22.905738830566406] in episode 1633
Report: 
rewardSum:247.25
loss:[27.71011781692505, 22.905738830566406]
policies:[0, 4, 0]
qAverage:[0.0, 50.639061737060544]
ws:[-2.7706587269902228, 2.328465294837952]
memory len:10000
memory used:2723.0
now epsilon is 0.11785888092727446, the reward is 247.25 with loss [26.79560422897339, 25.388339519500732] in episode 1634
Report: 
rewardSum:247.25
loss:[26.79560422897339, 25.388339519500732]
policies:[0, 4, 0]
qAverage:[0.0, 50.686920166015625]
ws:[-5.855029058456421, 1.0206841319799422]
memory len:10000
memory used:2723.0
now epsilon is 0.11774106623606184, the reward is 247.25 with loss [28.432361125946045, 29.67991876602173] in episode 1635
Report: 
rewardSum:247.25
loss:[28.432361125946045, 29.67991876602173]
policies:[0, 3, 1]
qAverage:[0.0, 46.5149507522583]
ws:[-5.573567733168602, 2.844203896820545]
memory len:10000
memory used:2723.0
now epsilon is 0.1176233693153673, the reward is 247.25 with loss [31.37453269958496, 22.758208751678467] in episode 1636
Report: 
rewardSum:247.25
loss:[31.37453269958496, 22.758208751678467]
policies:[0, 3, 1]
qAverage:[0.0, 44.68761189778646]
ws:[-6.377122084299724, 1.5173786282539368]
memory len:10000
memory used:2723.0
now epsilon is 0.11750579004746443, the reward is 247.25 with loss [30.81786823272705, 29.03172492980957] in episode 1637
Report: 
rewardSum:247.25
loss:[30.81786823272705, 29.03172492980957]
policies:[0, 2, 2]
qAverage:[0.0, 43.572714487711586]
ws:[-5.380741635958354, 3.2395923137664795]
memory len:10000
memory used:2723.0
now epsilon is 0.11730030907698591, the reward is 244.25 with loss [38.03562045097351, 55.35583257675171] in episode 1638
Report: 
rewardSum:244.25
loss:[38.03562045097351, 55.35583257675171]
policies:[1, 4, 2]
qAverage:[0.0, 50.6398193359375]
ws:[-3.4608080208301546, 2.7515884399414063]
memory len:10000
memory used:2723.0
now epsilon is 0.11718305274819403, the reward is 247.25 with loss [33.394102573394775, 31.299126625061035] in episode 1639
Report: 
rewardSum:247.25
loss:[33.394102573394775, 31.299126625061035]
policies:[0, 3, 1]
qAverage:[0.0, 46.678070068359375]
ws:[-7.057835549116135, -1.6054100282490253]
memory len:10000
memory used:2723.0
now epsilon is 0.11706591363176716, the reward is 247.25 with loss [27.42325210571289, 20.573573350906372] in episode 1640
Report: 
rewardSum:247.25
loss:[27.42325210571289, 20.573573350906372]
policies:[0, 4, 0]
qAverage:[0.0, 50.12351531982422]
ws:[-7.4692775249481205, -2.914981818199158]
memory len:10000
memory used:2723.0
now epsilon is 0.11694889161053684, the reward is 247.25 with loss [23.97138261795044, 37.53787851333618] in episode 1641
Report: 
rewardSum:247.25
loss:[23.97138261795044, 37.53787851333618]
policies:[0, 4, 0]
qAverage:[0.0, 50.957176208496094]
ws:[-4.223861545324326, -0.4761643886566162]
memory len:10000
memory used:2723.0
now epsilon is 0.11683198656745182, the reward is 247.25 with loss [27.7645206451416, 28.105799913406372] in episode 1642
Report: 
rewardSum:247.25
loss:[27.7645206451416, 28.105799913406372]
policies:[0, 3, 1]
qAverage:[0.0, 44.0979528427124]
ws:[0.8063339448999614, 2.131460189819336]
memory len:10000
memory used:2723.0
now epsilon is 0.11665684808108492, the reward is 57.6875 with loss [49.48471784591675, 33.875179052352905] in episode 1643
Report: 
rewardSum:57.6875
loss:[49.48471784591675, 33.875179052352905]
policies:[0, 4, 2]
qAverage:[0.0, 48.69739990234375]
ws:[2.987760329246521, 6.033253788948059]
memory len:10000
memory used:2723.0
now epsilon is 0.11654023497203128, the reward is 247.25 with loss [24.96561622619629, 32.0691397190094] in episode 1644
Report: 
rewardSum:247.25
loss:[24.96561622619629, 32.0691397190094]
policies:[1, 2, 1]
qAverage:[0.0, 41.97534942626953]
ws:[2.8017043272654214, 4.898239453633626]
memory len:10000
memory used:2723.0
now epsilon is 0.11642373843236406, the reward is 247.25 with loss [32.4664568901062, 27.50180149078369] in episode 1645
Report: 
rewardSum:247.25
loss:[32.4664568901062, 27.50180149078369]
policies:[2, 2, 0]
qAverage:[27.940987396240235, 26.859146118164062]
ws:[-0.8879643440246582, 1.1079029083251952]
memory len:10000
memory used:2723.0
now epsilon is 0.11630735834555758, the reward is 247.25 with loss [23.359548568725586, 29.279475688934326] in episode 1646
Report: 
rewardSum:247.25
loss:[23.359548568725586, 29.279475688934326]
policies:[1, 3, 0]
qAverage:[15.564632415771484, 36.39753532409668]
ws:[-5.339863151311874, -2.8220100104808807]
memory len:10000
memory used:2723.0
now epsilon is 0.11619109459520266, the reward is 247.25 with loss [15.340972781181335, 23.526456594467163] in episode 1647
Report: 
rewardSum:247.25
loss:[15.340972781181335, 23.526456594467163]
policies:[1, 3, 0]
qAverage:[12.380284881591797, 40.415609741210936]
ws:[-3.192638170719147, 0.7154194593429566]
memory len:10000
memory used:2723.0
now epsilon is 0.11607494706500644, the reward is 247.25 with loss [36.10613441467285, 22.520472764968872] in episode 1648
Report: 
rewardSum:247.25
loss:[36.10613441467285, 22.520472764968872]
policies:[1, 3, 0]
qAverage:[0.0, 47.61542320251465]
ws:[-3.0236927270889282, 1.5947922468185425]
memory len:10000
memory used:2723.0
now epsilon is 0.11595891563879238, the reward is 59.6875 with loss [24.663697242736816, 22.619107723236084] in episode 1649
Report: 
rewardSum:59.6875
loss:[24.663697242736816, 22.619107723236084]
policies:[2, 1, 1]
qAverage:[0.0, 33.91167068481445]
ws:[-0.03890349343419075, 0.7356400489807129]
memory len:10000
memory used:2723.0
now epsilon is 0.1158430002005, the reward is 247.25 with loss [27.185277700424194, 25.490670204162598] in episode 1650
Report: 
rewardSum:247.25
loss:[27.185277700424194, 25.490670204162598]
policies:[0, 4, 0]
qAverage:[0.0, 51.50293045043945]
ws:[-4.812682110071182, 0.9362477540969849]
memory len:10000
memory used:2723.0
now epsilon is 0.11572720063418485, the reward is 247.25 with loss [31.660484313964844, 31.251057147979736] in episode 1651
Report: 
rewardSum:247.25
loss:[31.660484313964844, 31.251057147979736]
policies:[1, 3, 0]
qAverage:[0.0, 47.73442554473877]
ws:[-3.7677454948425293, 1.781562089920044]
memory len:10000
memory used:2724.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2*		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.1156115168240184, the reward is 247.25 with loss [24.503443479537964, 28.703580498695374] in episode 1652
Report: 
rewardSum:247.25
loss:[24.503443479537964, 28.703580498695374]
policies:[0, 4, 0]
qAverage:[0.0, 51.68081893920898]
ws:[-0.2736271858215332, 6.154601812362671]
memory len:10000
memory used:2724.0
now epsilon is 0.11549594865428794, the reward is 59.6875 with loss [27.593358039855957, 36.17023420333862] in episode 1653
Report: 
rewardSum:59.6875
loss:[27.593358039855957, 36.17023420333862]
policies:[0, 3, 1]
qAverage:[0.0, 40.28633117675781]
ws:[2.049843152364095, 3.9160598119099936]
memory len:10000
memory used:2723.0
now epsilon is 0.11538049600939637, the reward is 247.25 with loss [26.56901979446411, 29.481220245361328] in episode 1654
Report: 
rewardSum:247.25
loss:[26.56901979446411, 29.481220245361328]
policies:[0, 4, 0]
qAverage:[0.0, 44.934396743774414]
ws:[1.2405595034360886, 5.733292400836945]
memory len:10000
memory used:2723.0
now epsilon is 0.11526515877386216, the reward is 247.25 with loss [21.074910163879395, 30.010541439056396] in episode 1655
Report: 
rewardSum:247.25
loss:[21.074910163879395, 30.010541439056396]
policies:[1, 3, 0]
qAverage:[0.0, 52.39901161193848]
ws:[-2.8365111649036407, 5.222009375691414]
memory len:10000
memory used:2723.0
now epsilon is 0.11512114934811114, the reward is 246.25 with loss [33.57957935333252, 28.839421272277832] in episode 1656
Report: 
rewardSum:246.25
loss:[33.57957935333252, 28.839421272277832]
policies:[0, 3, 2]
qAverage:[0.0, 49.961710929870605]
ws:[-3.138678327202797, 3.3013078570365906]
memory len:10000
memory used:2723.0
now epsilon is 0.11500607136199942, the reward is 247.25 with loss [21.5704448223114, 24.802188873291016] in episode 1657
Report: 
rewardSum:247.25
loss:[21.5704448223114, 24.802188873291016]
policies:[0, 3, 1]
qAverage:[0.0, 48.33907127380371]
ws:[-1.6653203070163727, 6.098926544189453]
memory len:10000
memory used:2723.0
now epsilon is 0.11489110841072675, the reward is 247.25 with loss [38.61525249481201, 12.792540550231934] in episode 1658
Report: 
rewardSum:247.25
loss:[38.61525249481201, 12.792540550231934]
policies:[1, 3, 0]
qAverage:[0.0, 51.61972141265869]
ws:[-1.5401750802993774, 4.5281935930252075]
memory len:10000
memory used:2723.0
now epsilon is 0.11477626037930143, the reward is 247.25 with loss [25.003342151641846, 28.097189903259277] in episode 1659
Report: 
rewardSum:247.25
loss:[25.003342151641846, 28.097189903259277]
policies:[0, 3, 1]
qAverage:[0.0, 50.98362731933594]
ws:[-3.737321058909098, 3.4233757654825845]
memory len:10000
memory used:2723.0
now epsilon is 0.1146615271528467, the reward is 247.25 with loss [19.747678518295288, 26.071166276931763] in episode 1660
Report: 
rewardSum:247.25
loss:[19.747678518295288, 26.071166276931763]
policies:[0, 4, 0]
qAverage:[0.0, 51.92014846801758]
ws:[-2.369633287191391, 2.206455183029175]
memory len:10000
memory used:2723.0
now epsilon is 0.11454690861660066, the reward is 247.25 with loss [32.84011650085449, 28.283995628356934] in episode 1661
Report: 
rewardSum:247.25
loss:[32.84011650085449, 28.283995628356934]
policies:[1, 3, 0]
qAverage:[15.73615550994873, 33.749878883361816]
ws:[0.5855743065476418, 1.5407711043953896]
memory len:10000
memory used:2723.0
now epsilon is 0.11437519560561342, the reward is 245.25 with loss [50.277222633361816, 51.74861478805542] in episode 1662
Report: 
rewardSum:245.25
loss:[50.277222633361816, 51.74861478805542]
policies:[1, 2, 3]
qAverage:[15.416534423828125, 35.60676670074463]
ws:[-1.0560745000839233, 4.868732489645481]
memory len:10000
memory used:2724.0
now epsilon is 0.11426086329355817, the reward is 247.25 with loss [26.91616916656494, 18.845139980316162] in episode 1663
Report: 
rewardSum:247.25
loss:[26.91616916656494, 18.845139980316162]
policies:[0, 3, 1]
qAverage:[0.0, 52.49115467071533]
ws:[0.05621790885925293, 8.561114072799683]
memory len:10000
memory used:2724.0
now epsilon is 0.1141466452709475, the reward is 247.25 with loss [33.2506217956543, 25.54594588279724] in episode 1664
Report: 
rewardSum:247.25
loss:[33.2506217956543, 25.54594588279724]
policies:[0, 3, 1]
qAverage:[0.0, 53.91999816894531]
ws:[-1.098505973815918, 8.381387710571289]
memory len:10000
memory used:2724.0
now epsilon is 0.11403254142353482, the reward is 247.25 with loss [24.876211881637573, 48.99393177032471] in episode 1665
Report: 
rewardSum:247.25
loss:[24.876211881637573, 48.99393177032471]
policies:[0, 4, 0]
qAverage:[0.0, 52.96375579833985]
ws:[-3.742087459564209, 5.3503358364105225]
memory len:10000
memory used:2724.0
now epsilon is 0.11391855163718775, the reward is 247.25 with loss [29.832231283187866, 37.01056480407715] in episode 1666
Report: 
rewardSum:247.25
loss:[29.832231283187866, 37.01056480407715]
policies:[1, 3, 0]
qAverage:[0.0, 51.90343952178955]
ws:[-6.141644537448883, 2.280038207769394]
memory len:10000
memory used:2724.0
now epsilon is 0.11380467579788797, the reward is 247.25 with loss [31.42341136932373, 27.05843210220337] in episode 1667
Report: 
rewardSum:247.25
loss:[31.42341136932373, 27.05843210220337]
policies:[1, 2, 1]
qAverage:[0.0, 43.89771016438802]
ws:[-0.7223840355873108, 5.898466110229492]
memory len:10000
memory used:2724.0
now epsilon is 0.11369091379173117, the reward is 247.25 with loss [29.648449420928955, 31.434348583221436] in episode 1668
Report: 
rewardSum:247.25
loss:[29.648449420928955, 31.434348583221436]
policies:[0, 4, 0]
qAverage:[0.0, 54.161163330078125]
ws:[-6.33614951968193, 2.1335690915584564]
memory len:10000
memory used:2724.0
now epsilon is 0.11357726550492689, the reward is 247.25 with loss [27.7373104095459, 28.365819454193115] in episode 1669
Report: 
rewardSum:247.25
loss:[27.7373104095459, 28.365819454193115]
policies:[0, 3, 1]
qAverage:[0.0, 49.121089935302734]
ws:[-6.4180106073617935, 3.109191596508026]
memory len:10000
memory used:2724.0
now epsilon is 0.11343536489109246, the reward is 246.25 with loss [45.51334190368652, 37.053096294403076] in episode 1670
Report: 
rewardSum:246.25
loss:[45.51334190368652, 37.053096294403076]
policies:[0, 4, 1]
qAverage:[0.0, 56.62891464233398]
ws:[-2.1875754475593565, 5.801643013954163]
memory len:10000
memory used:2724.0
now epsilon is 0.11332197205737396, the reward is 247.25 with loss [23.161164045333862, 33.96036195755005] in episode 1671
Report: 
rewardSum:247.25
loss:[23.161164045333862, 33.96036195755005]
policies:[1, 3, 0]
qAverage:[0.0, 48.5468495686849]
ws:[-4.250235398610433, 5.025222857793172]
memory len:10000
memory used:2724.0
now epsilon is 0.11315209530323023, the reward is 245.25 with loss [40.5384840965271, 43.54737901687622] in episode 1672
Report: 
rewardSum:245.25
loss:[40.5384840965271, 43.54737901687622]
policies:[0, 5, 1]
qAverage:[0.0, 56.76014391581217]
ws:[-1.0585683385531108, 4.846467374513547]
memory len:10000
memory used:2723.0
now epsilon is 0.11303898563289119, the reward is 247.25 with loss [27.298064708709717, 28.37812900543213] in episode 1673
Report: 
rewardSum:247.25
loss:[27.298064708709717, 28.37812900543213]
policies:[0, 4, 0]
qAverage:[0.0, 54.669717407226564]
ws:[-3.700350934267044, 1.5568826824426651]
memory len:10000
memory used:2723.0
now epsilon is 0.11286953309317285, the reward is 245.25 with loss [36.302122592926025, 37.26991844177246] in episode 1674
Report: 
rewardSum:245.25
loss:[36.302122592926025, 37.26991844177246]
policies:[0, 4, 2]
qAverage:[0.0, 57.46771011352539]
ws:[-1.2370232582092284, 7.136607646942139]
memory len:10000
memory used:2723.0
now epsilon is 0.11275670587910071, the reward is 247.25 with loss [25.962544918060303, 18.548778295516968] in episode 1675
Report: 
rewardSum:247.25
loss:[25.962544918060303, 18.548778295516968]
policies:[0, 4, 0]
qAverage:[0.0, 56.189107513427736]
ws:[-1.3441932201385498, 5.389683198928833]
memory len:10000
memory used:2724.0
now epsilon is 0.11258767649446397, the reward is 245.25 with loss [40.50410747528076, 47.72211503982544] in episode 1676
Report: 
rewardSum:245.25
loss:[40.50410747528076, 47.72211503982544]
policies:[0, 5, 1]
qAverage:[0.0, 57.344931284586586]
ws:[-0.11736716826756795, 7.688510139783223]
memory len:10000
memory used:2724.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26*		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.11236269807142549, the reward is 243.25 with loss [42.68711614608765, 58.009135007858276] in episode 1677
Report: 
rewardSum:243.25
loss:[42.68711614608765, 58.009135007858276]
policies:[0, 6, 2]
qAverage:[0.0, 58.52667018345424]
ws:[-0.612277912242072, 5.7715180941990445]
memory len:10000
memory used:2724.0
now epsilon is 0.11225037750234362, the reward is 247.25 with loss [36.531116008758545, 24.859455108642578] in episode 1678
Report: 
rewardSum:247.25
loss:[36.531116008758545, 24.859455108642578]
policies:[1, 3, 0]
qAverage:[0.0, 51.87881374359131]
ws:[-4.626846969127655, 3.6775566563010216]
memory len:10000
memory used:2724.0
now epsilon is 0.11208210713574739, the reward is 245.25 with loss [39.75107932090759, 36.24034309387207] in episode 1679
Report: 
rewardSum:245.25
loss:[39.75107932090759, 36.24034309387207]
policies:[0, 4, 2]
qAverage:[0.0, 59.99170608520508]
ws:[-0.19364473819732667, 7.9530261039733885]
memory len:10000
memory used:2724.0
now epsilon is 0.11197006705239712, the reward is 247.25 with loss [27.96694779396057, 27.326767921447754] in episode 1680
Report: 
rewardSum:247.25
loss:[27.96694779396057, 27.326767921447754]
policies:[0, 4, 0]
qAverage:[0.0, 57.267151832580566]
ws:[-1.2405730485916138, 6.546875834465027]
memory len:10000
memory used:2724.0
now epsilon is 0.11185813896712218, the reward is 247.25 with loss [23.96390676498413, 27.82243061065674] in episode 1681
Report: 
rewardSum:247.25
loss:[23.96390676498413, 27.82243061065674]
policies:[0, 4, 0]
qAverage:[0.0, 59.37169570922852]
ws:[1.4273283004760742, 8.348199081420898]
memory len:10000
memory used:2724.0
now epsilon is 0.11174632276796648, the reward is 247.25 with loss [24.966076850891113, 20.0686194896698] in episode 1682
Report: 
rewardSum:247.25
loss:[24.966076850891113, 20.0686194896698]
policies:[0, 4, 0]
qAverage:[0.0, 61.61711730957031]
ws:[0.9131733894348144, 7.762124729156494]
memory len:10000
memory used:2724.0
now epsilon is 0.11163461834308586, the reward is 247.25 with loss [23.70615863800049, 32.254255294799805] in episode 1683
Report: 
rewardSum:247.25
loss:[23.70615863800049, 32.254255294799805]
policies:[0, 4, 0]
qAverage:[0.0, 59.83648910522461]
ws:[0.032674741744995114, 7.040031623840332]
memory len:10000
memory used:2724.0
now epsilon is 0.11152302558074795, the reward is 247.25 with loss [37.73399829864502, 20.949760675430298] in episode 1684
Report: 
rewardSum:247.25
loss:[37.73399829864502, 20.949760675430298]
policies:[0, 4, 0]
qAverage:[0.0, 61.70168609619141]
ws:[1.039515733718872, 7.734005498886108]
memory len:10000
memory used:2723.0
now epsilon is 0.11141154436933205, the reward is 247.25 with loss [29.670210361480713, 23.140381813049316] in episode 1685
Report: 
rewardSum:247.25
loss:[29.670210361480713, 23.140381813049316]
policies:[0, 4, 0]
qAverage:[0.0, 58.4257755279541]
ws:[-0.36056238412857056, 6.185506582260132]
memory len:10000
memory used:2723.0
now epsilon is 0.11130017459732908, the reward is 247.25 with loss [24.09109354019165, 28.682234287261963] in episode 1686
Report: 
rewardSum:247.25
loss:[24.09109354019165, 28.682234287261963]
policies:[0, 4, 0]
qAverage:[0.0, 61.40118637084961]
ws:[1.14661865234375, 9.066283130645752]
memory len:10000
memory used:2723.0
now epsilon is 0.11118891615334141, the reward is 247.25 with loss [24.371140480041504, 24.873370885849] in episode 1687
Report: 
rewardSum:247.25
loss:[24.371140480041504, 24.873370885849]
policies:[0, 4, 0]
qAverage:[0.0, 59.430138397216794]
ws:[1.038684892654419, 7.888625431060791]
memory len:10000
memory used:2723.0
now epsilon is 0.11107776892608277, the reward is 59.6875 with loss [39.32613658905029, 21.012444972991943] in episode 1688
Report: 
rewardSum:59.6875
loss:[39.32613658905029, 21.012444972991943]
policies:[0, 3, 1]
qAverage:[0.0, 52.302449226379395]
ws:[4.094500184059143, 8.736418843269348]
memory len:10000
memory used:2723.0
now epsilon is 0.11091125637339674, the reward is 245.25 with loss [51.70974016189575, 39.53712606430054] in episode 1689
Report: 
rewardSum:245.25
loss:[51.70974016189575, 39.53712606430054]
policies:[1, 4, 1]
qAverage:[0.0, 57.63817596435547]
ws:[3.472375726699829, 10.316811084747314]
memory len:10000
memory used:2723.0
now epsilon is 0.11080038670181297, the reward is 247.25 with loss [19.601613521575928, 26.076701641082764] in episode 1690
Report: 
rewardSum:247.25
loss:[19.601613521575928, 26.076701641082764]
policies:[1, 3, 0]
qAverage:[13.60619659423828, 48.73651123046875]
ws:[2.28453506231308, 8.536180245876313]
memory len:10000
memory used:2723.0
now epsilon is 0.11068962785833159, the reward is 247.25 with loss [29.692277908325195, 25.86278533935547] in episode 1691
Report: 
rewardSum:247.25
loss:[29.692277908325195, 25.86278533935547]
policies:[2, 2, 0]
qAverage:[16.175945281982422, 46.6915340423584]
ws:[-0.6097670719027519, 4.519265711307526]
memory len:10000
memory used:2724.0
now epsilon is 0.11057897973216604, the reward is 247.25 with loss [24.4342098236084, 38.05602979660034] in episode 1692
Report: 
rewardSum:247.25
loss:[24.4342098236084, 38.05602979660034]
policies:[0, 4, 0]
qAverage:[0.0, 59.885698318481445]
ws:[0.7928698807954788, 8.74643424153328]
memory len:10000
memory used:2724.0
now epsilon is 0.11046844221264053, the reward is 247.25 with loss [25.697509288787842, 28.497899055480957] in episode 1693
Report: 
rewardSum:247.25
loss:[25.697509288787842, 28.497899055480957]
policies:[0, 4, 0]
qAverage:[0.0, 62.58562545776367]
ws:[1.4197456240653992, 7.817947196960449]
memory len:10000
memory used:2724.0
now epsilon is 0.11030284307897124, the reward is 245.25 with loss [43.40258550643921, 37.887542724609375] in episode 1694
Report: 
rewardSum:245.25
loss:[43.40258550643921, 37.887542724609375]
policies:[0, 5, 1]
qAverage:[0.0, 66.06912422180176]
ws:[1.3074926783641179, 9.441811243693033]
memory len:10000
memory used:2724.0
now epsilon is 0.11019258159256495, the reward is 247.25 with loss [34.737548828125, 26.066322326660156] in episode 1695
Report: 
rewardSum:247.25
loss:[34.737548828125, 26.066322326660156]
policies:[0, 4, 0]
qAverage:[0.0, 61.74701118469238]
ws:[-1.137718697078526, 6.210209459066391]
memory len:10000
memory used:2724.0
now epsilon is 0.1100824303263039, the reward is 247.25 with loss [24.330642700195312, 26.089234352111816] in episode 1696
Report: 
rewardSum:247.25
loss:[24.330642700195312, 26.089234352111816]
policies:[0, 4, 0]
qAverage:[0.0, 63.50026035308838]
ws:[-0.9096683971583843, 6.439290076494217]
memory len:10000
memory used:2723.0
now epsilon is 0.10988993049623641, the reward is 244.25 with loss [52.25308418273926, 42.90558958053589] in episode 1697
Report: 
rewardSum:244.25
loss:[52.25308418273926, 42.90558958053589]
policies:[0, 5, 2]
qAverage:[0.0, 66.02550188700359]
ws:[0.9636752009391785, 8.90461520353953]
memory len:10000
memory used:2724.0
now epsilon is 0.10978008176759645, the reward is 247.25 with loss [28.861552715301514, 22.351465702056885] in episode 1698
Report: 
rewardSum:247.25
loss:[28.861552715301514, 22.351465702056885]
policies:[0, 4, 0]
qAverage:[0.0, 63.91059036254883]
ws:[-0.12397086322307586, 8.02585220336914]
memory len:10000
memory used:2724.0
now epsilon is 0.10967034284649871, the reward is 247.25 with loss [30.321444988250732, 21.811288833618164] in episode 1699
Report: 
rewardSum:247.25
loss:[30.321444988250732, 21.811288833618164]
policies:[0, 4, 0]
qAverage:[0.0, 64.96280059814453]
ws:[-1.1111095070838928, 7.693147277832031]
memory len:10000
memory used:2724.0
now epsilon is 0.10956071362317682, the reward is 247.25 with loss [28.12034583091736, 23.898175716400146] in episode 1700
Report: 
rewardSum:247.25
loss:[28.12034583091736, 23.898175716400146]
policies:[0, 4, 0]
qAverage:[0.0, 64.57051315307618]
ws:[-0.5442675650119781, 7.802570152282715]
memory len:10000
memory used:2724.0
now epsilon is 0.1093964752316798, the reward is 245.25 with loss [40.442931175231934, 48.91926050186157] in episode 1701
Report: 
rewardSum:245.25
loss:[40.442931175231934, 48.91926050186157]
policies:[0, 5, 1]
qAverage:[0.0, 65.96262613932292]
ws:[2.7747261921564736, 11.262933492660522]
memory len:10000
memory used:2724.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2*		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.10928711977328949, the reward is 247.25 with loss [26.635340929031372, 28.987769603729248] in episode 1702
Report: 
rewardSum:247.25
loss:[26.635340929031372, 28.987769603729248]
policies:[0, 4, 0]
qAverage:[0.0, 71.3605354309082]
ws:[0.6685568332672119, 9.261222219467163]
memory len:10000
memory used:2724.0
now epsilon is 0.10917787362935612, the reward is 247.25 with loss [32.062010288238525, 34.281760692596436] in episode 1703
Report: 
rewardSum:247.25
loss:[32.062010288238525, 34.281760692596436]
policies:[0, 4, 0]
qAverage:[0.0, 73.84763946533204]
ws:[0.1498272180557251, 9.143463361263276]
memory len:10000
memory used:2724.0
now epsilon is 0.1090687366906062, the reward is 247.25 with loss [35.5171217918396, 28.552387237548828] in episode 1704
Report: 
rewardSum:247.25
loss:[35.5171217918396, 28.552387237548828]
policies:[0, 4, 0]
qAverage:[0.0, 62.73128414154053]
ws:[2.2589180087670684, 11.401734113693237]
memory len:10000
memory used:2724.0
now epsilon is 0.1089597088478755, the reward is 247.25 with loss [24.791789531707764, 29.712018966674805] in episode 1705
Report: 
rewardSum:247.25
loss:[24.791789531707764, 29.712018966674805]
policies:[0, 3, 1]
qAverage:[0.0, 66.4839916229248]
ws:[2.7310180068016052, 9.62749981880188]
memory len:10000
memory used:2724.0
now epsilon is 0.10879637140028721, the reward is 245.25 with loss [42.80453586578369, 52.40536880493164] in episode 1706
Report: 
rewardSum:245.25
loss:[42.80453586578369, 52.40536880493164]
policies:[0, 5, 1]
qAverage:[0.0, 71.70946248372395]
ws:[3.8522428969542184, 11.99586566289266]
memory len:10000
memory used:2724.0
now epsilon is 0.10866044391677168, the reward is 246.25 with loss [39.98816204071045, 33.042015075683594] in episode 1707
Report: 
rewardSum:246.25
loss:[39.98816204071045, 33.042015075683594]
policies:[0, 4, 1]
qAverage:[0.0, 72.25111541748046]
ws:[1.346231186389923, 6.780957555770874]
memory len:10000
memory used:2724.0
now epsilon is 0.1084975550861127, the reward is 245.25 with loss [34.83273506164551, 45.43794059753418] in episode 1708
Report: 
rewardSum:245.25
loss:[34.83273506164551, 45.43794059753418]
policies:[0, 5, 1]
qAverage:[0.0, 70.1483325958252]
ws:[1.9882425765196483, 8.21907539665699]
memory len:10000
memory used:2724.0
now epsilon is 0.10838909821082908, the reward is 247.25 with loss [34.83156728744507, 25.675010681152344] in episode 1709
Report: 
rewardSum:247.25
loss:[34.83156728744507, 25.675010681152344]
policies:[0, 4, 0]
qAverage:[0.0, 71.69877166748047]
ws:[2.536190617084503, 8.714195847511292]
memory len:10000
memory used:2723.0
now epsilon is 0.10825367956431826, the reward is 246.25 with loss [32.01034593582153, 33.36447095870972] in episode 1710
Report: 
rewardSum:246.25
loss:[32.01034593582153, 33.36447095870972]
policies:[1, 3, 1]
qAverage:[0.0, 64.74826622009277]
ws:[1.0962797701358795, 6.503527045249939]
memory len:10000
memory used:2723.0
now epsilon is 0.10814546647311837, the reward is 247.25 with loss [23.815587997436523, 31.702194213867188] in episode 1711
Report: 
rewardSum:247.25
loss:[23.815587997436523, 31.702194213867188]
policies:[1, 2, 1]
qAverage:[0.0, 47.74986775716146]
ws:[0.49265701572100323, 2.7516457637151084]
memory len:10000
memory used:2724.0
now epsilon is 0.10803736155443654, the reward is 247.25 with loss [23.351135730743408, 38.55155849456787] in episode 1712
Report: 
rewardSum:247.25
loss:[23.351135730743408, 38.55155849456787]
policies:[0, 4, 0]
qAverage:[0.0, 65.11645126342773]
ws:[3.298815608024597, 9.191907525062561]
memory len:10000
memory used:2723.0
now epsilon is 0.10787540676337602, the reward is 245.25 with loss [44.58771085739136, 41.54136323928833] in episode 1713
Report: 
rewardSum:245.25
loss:[44.58771085739136, 41.54136323928833]
policies:[0, 3, 3]
qAverage:[0.0, 73.79945182800293]
ws:[4.798347294330597, 11.82652485370636]
memory len:10000
memory used:2723.0
now epsilon is 0.1077675718031484, the reward is 247.25 with loss [35.6966290473938, 29.33421802520752] in episode 1714
Report: 
rewardSum:247.25
loss:[35.6966290473938, 29.33421802520752]
policies:[0, 4, 0]
qAverage:[0.0, 67.1798210144043]
ws:[2.068349063396454, 6.399077653884888]
memory len:10000
memory used:2723.0
now epsilon is 0.1076060214438712, the reward is 245.25 with loss [40.38223838806152, 49.14295434951782] in episode 1715
Report: 
rewardSum:245.25
loss:[40.38223838806152, 49.14295434951782]
policies:[0, 5, 1]
qAverage:[0.0, 81.00532786051433]
ws:[5.685494979222615, 12.45727825164795]
memory len:10000
memory used:2723.0
now epsilon is 0.10747158115401843, the reward is 246.25 with loss [40.201640129089355, 45.99110221862793] in episode 1716
Report: 
rewardSum:246.25
loss:[40.201640129089355, 45.99110221862793]
policies:[0, 4, 1]
qAverage:[0.0, 78.60691833496094]
ws:[6.277768874168396, 13.749835968017578]
memory len:10000
memory used:2724.0
now epsilon is 0.10736414986799081, the reward is 247.25 with loss [28.664998292922974, 21.838116884231567] in episode 1717
Report: 
rewardSum:247.25
loss:[28.664998292922974, 21.838116884231567]
policies:[1, 3, 0]
qAverage:[0.0, 77.57858848571777]
ws:[5.098503805696964, 11.096188187599182]
memory len:10000
memory used:2724.0
now epsilon is 0.10725682597296919, the reward is 247.25 with loss [32.75593376159668, 25.124850749969482] in episode 1718
Report: 
rewardSum:247.25
loss:[32.75593376159668, 25.124850749969482]
policies:[1, 3, 0]
qAverage:[0.0, 71.24921607971191]
ws:[5.825942367315292, 11.620299816131592]
memory len:10000
memory used:2724.0
now epsilon is 0.10714960936160284, the reward is 247.25 with loss [31.19817280769348, 24.24620294570923] in episode 1719
Report: 
rewardSum:247.25
loss:[31.19817280769348, 24.24620294570923]
policies:[0, 4, 0]
qAverage:[0.0, 77.0879135131836]
ws:[6.5127250254154205, 13.436835527420044]
memory len:10000
memory used:2724.0
now epsilon is 0.10704249992664833, the reward is 247.25 with loss [39.53412199020386, 21.5772066116333] in episode 1720
Report: 
rewardSum:247.25
loss:[39.53412199020386, 21.5772066116333]
policies:[0, 4, 0]
qAverage:[0.0, 78.71570434570313]
ws:[5.6463442087173465, 12.047740650177001]
memory len:10000
memory used:2724.0
now epsilon is 0.10693549756096941, the reward is 247.25 with loss [26.132818341255188, 21.47608494758606] in episode 1721
Report: 
rewardSum:247.25
loss:[26.132818341255188, 21.47608494758606]
policies:[1, 3, 0]
qAverage:[0.0, 75.79568481445312]
ws:[4.414108425378799, 10.66482675075531]
memory len:10000
memory used:2724.0
now epsilon is 0.10682860215753698, the reward is 247.25 with loss [25.478813648223877, 28.451782703399658] in episode 1722
Report: 
rewardSum:247.25
loss:[25.478813648223877, 28.451782703399658]
policies:[0, 4, 0]
qAverage:[0.0, 79.45170593261719]
ws:[5.100203001499176, 11.329871940612794]
memory len:10000
memory used:2724.0
now epsilon is 0.10672181360942888, the reward is 247.25 with loss [31.604958534240723, 24.357637405395508] in episode 1723
Report: 
rewardSum:247.25
loss:[31.604958534240723, 24.357637405395508]
policies:[0, 4, 0]
qAverage:[0.0, 78.40959014892579]
ws:[6.568027007579803, 13.221873569488526]
memory len:10000
memory used:2724.0
now epsilon is 0.10661513180982986, the reward is 247.25 with loss [28.754021644592285, 24.075838088989258] in episode 1724
Report: 
rewardSum:247.25
loss:[28.754021644592285, 24.075838088989258]
policies:[0, 4, 0]
qAverage:[0.0, 67.86939239501953]
ws:[3.0540504455566406, 7.6660730838775635]
memory len:10000
memory used:2724.0
now epsilon is 0.10650855665203145, the reward is 247.25 with loss [28.622135639190674, 28.60292100906372] in episode 1725
Report: 
rewardSum:247.25
loss:[28.622135639190674, 28.60292100906372]
policies:[0, 2, 2]
qAverage:[0.0, 84.53207397460938]
ws:[5.9286543528238935, 12.343000253041586]
memory len:10000
memory used:2724.0
now epsilon is 0.10640208802943181, the reward is 247.25 with loss [35.24615287780762, 31.054259777069092] in episode 1726
Report: 
rewardSum:247.25
loss:[35.24615287780762, 31.054259777069092]
policies:[0, 3, 1]
qAverage:[0.0, 74.5743293762207]
ws:[4.633491694927216, 11.057769924402237]
memory len:10000
memory used:2724.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19*		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.10629572583553569, the reward is 247.25 with loss [28.340500354766846, 27.54820942878723] in episode 1727
Report: 
rewardSum:247.25
loss:[28.340500354766846, 27.54820942878723]
policies:[0, 4, 0]
qAverage:[0.0, 81.34031867980957]
ws:[7.559009253978729, 13.786313772201538]
memory len:10000
memory used:2724.0
now epsilon is 0.10618946996395427, the reward is 247.25 with loss [29.508530378341675, 29.076364040374756] in episode 1728
Report: 
rewardSum:247.25
loss:[29.508530378341675, 29.076364040374756]
policies:[0, 4, 0]
qAverage:[0.0, 76.73136901855469]
ws:[7.793474078178406, 13.508527278900146]
memory len:10000
memory used:2724.0
now epsilon is 0.10608332030840514, the reward is 247.25 with loss [24.765822649002075, 25.43946146965027] in episode 1729
Report: 
rewardSum:247.25
loss:[24.765822649002075, 25.43946146965027]
policies:[0, 4, 0]
qAverage:[0.0, 71.28658103942871]
ws:[3.0062371492385864, 7.424505591392517]
memory len:10000
memory used:2724.0
now epsilon is 0.10597727676271207, the reward is 247.25 with loss [25.72263813018799, 26.57956838607788] in episode 1730
Report: 
rewardSum:247.25
loss:[25.72263813018799, 26.57956838607788]
policies:[0, 4, 0]
qAverage:[0.0, 83.62355194091796]
ws:[5.8385152339935305, 11.640967464447021]
memory len:10000
memory used:2724.0
now epsilon is 0.105871339220805, the reward is 247.25 with loss [17.810492515563965, 28.991939544677734] in episode 1731
Report: 
rewardSum:247.25
loss:[17.810492515563965, 28.991939544677734]
policies:[0, 4, 0]
qAverage:[0.0, 83.39068603515625]
ws:[6.955327939987183, 14.758069610595703]
memory len:10000
memory used:2724.0
now epsilon is 0.10576550757671986, the reward is 247.25 with loss [36.4752254486084, 39.04181241989136] in episode 1732
Report: 
rewardSum:247.25
loss:[36.4752254486084, 39.04181241989136]
policies:[0, 3, 1]
qAverage:[0.0, 76.80443954467773]
ws:[6.5172294825315475, 14.20048451423645]
memory len:10000
memory used:2724.0
now epsilon is 0.10565978172459857, the reward is 247.25 with loss [27.300405502319336, 21.201351165771484] in episode 1733
Report: 
rewardSum:247.25
loss:[27.300405502319336, 21.201351165771484]
policies:[0, 4, 0]
qAverage:[0.0, 84.47411155700684]
ws:[11.342400670051575, 21.637470245361328]
memory len:10000
memory used:2724.0
now epsilon is 0.1055541615586888, the reward is 59.6875 with loss [35.60128927230835, 28.95781183242798] in episode 1734
Report: 
rewardSum:59.6875
loss:[35.60128927230835, 28.95781183242798]
policies:[1, 2, 1]
qAverage:[0.0, 68.57420349121094]
ws:[5.987730503082275, 12.564230918884277]
memory len:10000
memory used:2724.0
now epsilon is 0.10539592924039776, the reward is 245.25 with loss [43.669851303100586, 37.48937129974365] in episode 1735
Report: 
rewardSum:245.25
loss:[43.669851303100586, 37.48937129974365]
policies:[0, 3, 3]
qAverage:[0.0, 79.90056419372559]
ws:[7.914051728323102, 14.737510561943054]
memory len:10000
memory used:2724.0
now epsilon is 0.10529057282804401, the reward is 247.25 with loss [27.247106552124023, 31.02189588546753] in episode 1736
Report: 
rewardSum:247.25
loss:[27.247106552124023, 31.02189588546753]
policies:[0, 4, 0]
qAverage:[0.0, 85.86461181640625]
ws:[6.7399779736995695, 13.700583267211915]
memory len:10000
memory used:2724.0
now epsilon is 0.10518532173260055, the reward is 247.25 with loss [29.561850547790527, 21.830957412719727] in episode 1737
Report: 
rewardSum:247.25
loss:[29.561850547790527, 21.830957412719727]
policies:[0, 3, 1]
qAverage:[0.0, 83.77803421020508]
ws:[7.920278660953045, 15.093727827072144]
memory len:10000
memory used:2724.0
now epsilon is 0.10508017584878994, the reward is 247.25 with loss [30.829506397247314, 28.46718406677246] in episode 1738
Report: 
rewardSum:247.25
loss:[30.829506397247314, 28.46718406677246]
policies:[0, 4, 0]
qAverage:[0.0, 86.43616485595703]
ws:[8.027729272842407, 14.733493041992187]
memory len:10000
memory used:2724.0
now epsilon is 0.10497513507143999, the reward is 247.25 with loss [40.92784833908081, 25.18017864227295] in episode 1739
Report: 
rewardSum:247.25
loss:[40.92784833908081, 25.18017864227295]
policies:[0, 4, 0]
qAverage:[0.0, 87.09962615966796]
ws:[8.320027780532836, 14.914732933044434]
memory len:10000
memory used:2724.0
now epsilon is 0.10487019929548368, the reward is 247.25 with loss [22.296486854553223, 32.461605072021484] in episode 1740
Report: 
rewardSum:247.25
loss:[22.296486854553223, 32.461605072021484]
policies:[0, 4, 0]
qAverage:[0.0, 85.58983154296875]
ws:[5.333296442031861, 10.488112831115723]
memory len:10000
memory used:2724.0
now epsilon is 0.10471299227958651, the reward is 245.25 with loss [36.855934619903564, 45.92796087265015] in episode 1741
Report: 
rewardSum:245.25
loss:[36.855934619903564, 45.92796087265015]
policies:[0, 4, 2]
qAverage:[0.0, 84.38906669616699]
ws:[8.297717452049255, 13.209594488143921]
memory len:10000
memory used:2724.0
now epsilon is 0.10460831854813488, the reward is 247.25 with loss [22.477736473083496, 26.595740795135498] in episode 1742
Report: 
rewardSum:247.25
loss:[22.477736473083496, 26.595740795135498]
policies:[0, 4, 0]
qAverage:[0.0, 87.26228942871094]
ws:[4.684972363710403, 9.080609130859376]
memory len:10000
memory used:2724.0
now epsilon is 0.10450374945116861, the reward is 247.25 with loss [34.263423919677734, 26.805264949798584] in episode 1743
Report: 
rewardSum:247.25
loss:[34.263423919677734, 26.805264949798584]
policies:[0, 4, 0]
qAverage:[0.0, 86.48661346435547]
ws:[4.601573753356933, 10.093372631072999]
memory len:10000
memory used:2724.0
now epsilon is 0.1043731850628714, the reward is 246.25 with loss [28.873616218566895, 27.37565803527832] in episode 1744
Report: 
rewardSum:246.25
loss:[28.873616218566895, 27.37565803527832]
policies:[0, 4, 1]
qAverage:[0.0, 86.91267395019531]
ws:[3.705989249050617, 9.429573249816894]
memory len:10000
memory used:2724.0
now epsilon is 0.10426885101123003, the reward is 247.25 with loss [30.269545555114746, 29.194268226623535] in episode 1745
Report: 
rewardSum:247.25
loss:[30.269545555114746, 29.194268226623535]
policies:[0, 4, 0]
qAverage:[0.0, 87.6729949951172]
ws:[4.512178301811218, 12.056241774559021]
memory len:10000
memory used:2724.0
now epsilon is 0.10416462125452154, the reward is 247.25 with loss [26.7542724609375, 22.822939157485962] in episode 1746
Report: 
rewardSum:247.25
loss:[26.7542724609375, 22.822939157485962]
policies:[0, 4, 0]
qAverage:[0.0, 87.45552978515624]
ws:[4.888213396072388, 12.236806154251099]
memory len:10000
memory used:2724.0
now epsilon is 0.10400847194442686, the reward is 245.25 with loss [41.991265296936035, 51.544968128204346] in episode 1747
Report: 
rewardSum:245.25
loss:[41.991265296936035, 51.544968128204346]
policies:[0, 5, 1]
qAverage:[0.0, 79.69798889160157]
ws:[3.6401722192764283, 9.687578296661377]
memory len:10000
memory used:2724.0
now epsilon is 0.1039045024691593, the reward is 247.25 with loss [30.661848545074463, 36.65488815307617] in episode 1748
Report: 
rewardSum:247.25
loss:[30.661848545074463, 36.65488815307617]
policies:[0, 4, 0]
qAverage:[0.0, 90.53661804199218]
ws:[5.028916567564011, 11.832876586914063]
memory len:10000
memory used:2724.0
now epsilon is 0.10380063692438495, the reward is 247.25 with loss [32.511579513549805, 28.767172813415527] in episode 1749
Report: 
rewardSum:247.25
loss:[32.511579513549805, 28.767172813415527]
policies:[0, 4, 0]
qAverage:[0.0, 89.04847869873046]
ws:[3.9780353188514708, 10.688376712799073]
memory len:10000
memory used:2724.0
now epsilon is 0.10369687520621228, the reward is 247.25 with loss [22.15765953063965, 41.736485958099365] in episode 1750
Report: 
rewardSum:247.25
loss:[22.15765953063965, 41.736485958099365]
policies:[0, 4, 0]
qAverage:[0.0, 90.77919311523438]
ws:[4.004566216468811, 11.318059635162353]
memory len:10000
memory used:2724.0
now epsilon is 0.10348966283462506, the reward is 243.25 with loss [44.587425231933594, 57.227051734924316] in episode 1751
Report: 
rewardSum:243.25
loss:[44.587425231933594, 57.227051734924316]
policies:[0, 6, 2]
qAverage:[0.0, 94.4639140537807]
ws:[2.9261425816054856, 8.492045657975334]
memory len:10000
memory used:2724.0
now epsilon is 0.10338621197394629, the reward is 247.25 with loss [30.347517013549805, 28.629798412322998] in episode 1752
Report: 
rewardSum:247.25
loss:[30.347517013549805, 28.629798412322998]
policies:[0, 4, 0]
qAverage:[0.0, 85.33322525024414]
ws:[5.958279922604561, 12.727873265743256]
memory len:10000
memory used:2724.0
############# STATE ###############
0-		8-		16-		24-		32-		
1*		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.1032828645253406, the reward is 247.25 with loss [29.725022315979004, 30.883347511291504] in episode 1753
Report: 
rewardSum:247.25
loss:[29.725022315979004, 30.883347511291504]
policies:[0, 4, 0]
qAverage:[0.0, 90.72088775634765]
ws:[6.953600656986237, 14.941968536376953]
memory len:10000
memory used:2724.0
now epsilon is 0.1031796203854347, the reward is 247.25 with loss [29.4062180519104, 30.652424335479736] in episode 1754
Report: 
rewardSum:247.25
loss:[29.4062180519104, 30.652424335479736]
policies:[0, 3, 1]
qAverage:[0.0, 73.47195688883464]
ws:[2.930662155151367, 8.076158205668131]
memory len:10000
memory used:2724.0
now epsilon is 0.1030764794509586, the reward is 247.25 with loss [38.530500411987305, 28.55948257446289] in episode 1755
Report: 
rewardSum:247.25
loss:[38.530500411987305, 28.55948257446289]
policies:[0, 4, 0]
qAverage:[0.0, 91.00079917907715]
ws:[9.343533992767334, 17.960429191589355]
memory len:10000
memory used:2724.0
now epsilon is 0.10297344161874557, the reward is 247.25 with loss [25.50390100479126, 28.681704998016357] in episode 1756
Report: 
rewardSum:247.25
loss:[25.50390100479126, 28.681704998016357]
policies:[0, 4, 0]
qAverage:[0.0, 88.62477111816406]
ws:[7.1216634750366214, 13.210537528991699]
memory len:10000
memory used:2724.0
now epsilon is 0.10287050678573201, the reward is 247.25 with loss [23.24897861480713, 16.730390548706055] in episode 1757
Report: 
rewardSum:247.25
loss:[23.24897861480713, 16.730390548706055]
policies:[0, 4, 0]
qAverage:[0.0, 90.32221069335938]
ws:[6.829391288757324, 12.942948818206787]
memory len:10000
memory used:2724.0
now epsilon is 0.10276767484895734, the reward is 247.25 with loss [23.411017894744873, 44.92763710021973] in episode 1758
Report: 
rewardSum:247.25
loss:[23.411017894744873, 44.92763710021973]
policies:[0, 4, 0]
qAverage:[0.0, 88.94404602050781]
ws:[4.707960593700409, 10.072923517227172]
memory len:10000
memory used:2724.0
now epsilon is 0.10266494570556386, the reward is 247.25 with loss [37.68364906311035, 33.318687915802] in episode 1759
Report: 
rewardSum:247.25
loss:[37.68364906311035, 33.318687915802]
policies:[0, 4, 0]
qAverage:[0.0, 95.23927001953125]
ws:[6.22637791633606, 12.649929761886597]
memory len:10000
memory used:2724.0
now epsilon is 0.1025623192527968, the reward is 247.25 with loss [26.138028144836426, 41.724303245544434] in episode 1760
Report: 
rewardSum:247.25
loss:[26.138028144836426, 41.724303245544434]
policies:[0, 4, 0]
qAverage:[0.0, 93.03189544677734]
ws:[6.169135493040085, 11.713118743896484]
memory len:10000
memory used:2724.0
now epsilon is 0.10245979538800398, the reward is 247.25 with loss [26.406893730163574, 32.99051332473755] in episode 1761
Report: 
rewardSum:247.25
loss:[26.406893730163574, 32.99051332473755]
policies:[0, 4, 0]
qAverage:[0.0, 94.9846435546875]
ws:[6.82616548538208, 12.996094036102296]
memory len:10000
memory used:2724.0
now epsilon is 0.10235737400863593, the reward is 247.25 with loss [26.44122552871704, 22.773369789123535] in episode 1762
Report: 
rewardSum:247.25
loss:[26.44122552871704, 22.773369789123535]
policies:[0, 4, 0]
qAverage:[0.0, 91.89907531738281]
ws:[8.039897251129151, 14.885119247436524]
memory len:10000
memory used:2724.0
now epsilon is 0.10225505501224562, the reward is 247.25 with loss [37.95749759674072, 31.249825954437256] in episode 1763
Report: 
rewardSum:247.25
loss:[37.95749759674072, 31.249825954437256]
policies:[0, 4, 0]
qAverage:[0.0, 94.85215911865234]
ws:[7.968437051773071, 14.948901176452637]
memory len:10000
memory used:2724.0
now epsilon is 0.10215283829648847, the reward is 247.25 with loss [35.652503967285156, 24.552584886550903] in episode 1764
Report: 
rewardSum:247.25
loss:[35.652503967285156, 24.552584886550903]
policies:[0, 4, 0]
qAverage:[0.0, 92.57878875732422]
ws:[6.1851472616195675, 13.043390083312989]
memory len:10000
memory used:2724.0
now epsilon is 0.1020507237591222, the reward is 247.25 with loss [33.10008096694946, 25.996132373809814] in episode 1765
Report: 
rewardSum:247.25
loss:[33.10008096694946, 25.996132373809814]
policies:[0, 4, 0]
qAverage:[0.0, 93.68256072998047]
ws:[6.0594193816185, 13.6987380027771]
memory len:10000
memory used:2724.0
now epsilon is 0.10192322412018223, the reward is 246.25 with loss [32.95703411102295, 43.37463045120239] in episode 1766
Report: 
rewardSum:246.25
loss:[32.95703411102295, 43.37463045120239]
policies:[0, 3, 2]
qAverage:[0.0, 90.00295066833496]
ws:[9.346183598041534, 17.041062355041504]
memory len:10000
memory used:2724.0
now epsilon is 0.1018213391109013, the reward is 247.25 with loss [25.486243724822998, 25.51500940322876] in episode 1767
Report: 
rewardSum:247.25
loss:[25.486243724822998, 25.51500940322876]
policies:[0, 4, 0]
qAverage:[0.0, 92.58243103027344]
ws:[7.641925191879272, 14.2885910987854]
memory len:10000
memory used:2724.0
now epsilon is 0.10171955594842913, the reward is 247.25 with loss [34.00317192077637, 23.006936073303223] in episode 1768
Report: 
rewardSum:247.25
loss:[34.00317192077637, 23.006936073303223]
policies:[0, 3, 1]
qAverage:[0.0, 88.27723503112793]
ws:[7.620380589738488, 13.716073513031006]
memory len:10000
memory used:2724.0
now epsilon is 0.10161787453095712, the reward is 247.25 with loss [22.751946449279785, 34.345885276794434] in episode 1769
Report: 
rewardSum:247.25
loss:[22.751946449279785, 34.345885276794434]
policies:[0, 4, 0]
qAverage:[0.0, 92.92035865783691]
ws:[8.525471210479736, 15.217644214630127]
memory len:10000
memory used:2725.0
now epsilon is 0.10146554295416846, the reward is 245.25 with loss [40.237650871276855, 37.286303997039795] in episode 1770
Report: 
rewardSum:245.25
loss:[40.237650871276855, 37.286303997039795]
policies:[0, 5, 1]
qAverage:[0.0, 96.37203852335612]
ws:[5.266493106881778, 11.35594912370046]
memory len:10000
memory used:2725.0
now epsilon is 0.10136411545445172, the reward is 247.25 with loss [32.54575157165527, 25.036941528320312] in episode 1771
Report: 
rewardSum:247.25
loss:[32.54575157165527, 25.036941528320312]
policies:[1, 3, 0]
qAverage:[0.0, 89.85031318664551]
ws:[4.759621672332287, 9.772287875413895]
memory len:10000
memory used:2725.0
now epsilon is 0.10126278934420571, the reward is 247.25 with loss [28.523049354553223, 23.321431159973145] in episode 1772
Report: 
rewardSum:247.25
loss:[28.523049354553223, 23.321431159973145]
policies:[1, 3, 0]
qAverage:[0.0, 91.52579307556152]
ws:[5.962929904460907, 11.643372178077698]
memory len:10000
memory used:2725.0
now epsilon is 0.10116156452207899, the reward is 247.25 with loss [32.91863441467285, 23.72788381576538] in episode 1773
Report: 
rewardSum:247.25
loss:[32.91863441467285, 23.72788381576538]
policies:[0, 4, 0]
qAverage:[0.0, 92.99730529785157]
ws:[5.858270454406738, 11.13222041130066]
memory len:10000
memory used:2724.0
now epsilon is 0.10106044088682142, the reward is 247.25 with loss [26.048120498657227, 33.5361123085022] in episode 1774
Report: 
rewardSum:247.25
loss:[26.048120498657227, 33.5361123085022]
policies:[0, 4, 0]
qAverage:[0.0, 94.22842559814453]
ws:[6.763893711566925, 11.849706602096557]
memory len:10000
memory used:2724.0
now epsilon is 0.10095941833728406, the reward is 247.25 with loss [30.215638637542725, 35.46561098098755] in episode 1775
Report: 
rewardSum:247.25
loss:[30.215638637542725, 35.46561098098755]
policies:[0, 3, 1]
qAverage:[0.0, 90.85408592224121]
ws:[8.702219247817993, 14.702247619628906]
memory len:10000
memory used:2724.0
now epsilon is 0.10080807382768894, the reward is 245.25 with loss [36.90686655044556, 41.966312885284424] in episode 1776
Report: 
rewardSum:245.25
loss:[36.90686655044556, 41.966312885284424]
policies:[0, 3, 3]
qAverage:[0.0, 83.51957893371582]
ws:[5.305378537625074, 8.706471860408783]
memory len:10000
memory used:2737.0
now epsilon is 0.10070730355058885, the reward is 247.25 with loss [30.894105911254883, 25.70246458053589] in episode 1777
Report: 
rewardSum:247.25
loss:[30.894105911254883, 25.70246458053589]
policies:[0, 4, 0]
qAverage:[0.0, 91.94582824707031]
ws:[4.3137706279754635, 7.948347723484039]
memory len:10000
memory used:2737.0
now epsilon is 0.1006066340059833, the reward is 247.25 with loss [30.25975513458252, 37.392505168914795] in episode 1778
Report: 
rewardSum:247.25
loss:[30.25975513458252, 37.392505168914795]
policies:[0, 4, 0]
qAverage:[0.0, 94.01766662597656]
ws:[4.084585976600647, 7.16343340575695]
memory len:10000
memory used:2737.0
now epsilon is 0.10050606509317755, the reward is 247.25 with loss [25.860989570617676, 36.598633766174316] in episode 1779
Report: 
rewardSum:247.25
loss:[25.860989570617676, 36.598633766174316]
policies:[0, 4, 0]
qAverage:[0.0, 93.17471466064453]
ws:[3.3022323489189147, 6.538647502660751]
memory len:10000
memory used:2725.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2*		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.10040559671157757, the reward is 247.25 with loss [22.569706439971924, 35.368515968322754] in episode 1780
Report: 
rewardSum:247.25
loss:[22.569706439971924, 35.368515968322754]
policies:[0, 4, 0]
qAverage:[0.0, 93.86659240722656]
ws:[4.473072338104248, 8.842940557003022]
memory len:10000
memory used:2725.0
now epsilon is 0.10030522876068983, the reward is 247.25 with loss [30.1294527053833, 29.048335552215576] in episode 1781
Report: 
rewardSum:247.25
loss:[30.1294527053833, 29.048335552215576]
policies:[0, 3, 1]
qAverage:[0.0, 78.53280385335286]
ws:[5.062841375668843, 8.741106708844503]
memory len:10000
memory used:2725.0
now epsilon is 0.10020496114012123, the reward is 247.25 with loss [29.814260005950928, 20.144083499908447] in episode 1782
Report: 
rewardSum:247.25
loss:[29.814260005950928, 20.144083499908447]
policies:[0, 4, 0]
qAverage:[0.0, 92.82609558105469]
ws:[5.718049645423889, 10.91962308883667]
memory len:10000
memory used:2725.0
now epsilon is 0.10010479374957913, the reward is 247.25 with loss [40.101057052612305, 30.991965293884277] in episode 1783
Report: 
rewardSum:247.25
loss:[40.101057052612305, 30.991965293884277]
policies:[0, 4, 0]
qAverage:[0.0, 92.59808959960938]
ws:[5.817384886741638, 10.913750553131104]
memory len:10000
memory used:2725.0
now epsilon is 0.10000472648887107, the reward is 247.25 with loss [20.159867763519287, 24.042818546295166] in episode 1784
Report: 
rewardSum:247.25
loss:[20.159867763519287, 24.042818546295166]
policies:[1, 3, 0]
qAverage:[0.0, 87.85563278198242]
ws:[6.866447478532791, 12.471564888954163]
memory len:10000
memory used:2725.0
now epsilon is 0.09990475925790475, the reward is 247.25 with loss [28.786474227905273, 42.290472984313965] in episode 1785
Report: 
rewardSum:247.25
loss:[28.786474227905273, 42.290472984313965]
policies:[0, 4, 0]
qAverage:[0.0, 93.09228820800782]
ws:[6.636707067489624, 13.059440803527831]
memory len:10000
memory used:2725.0
now epsilon is 0.09980489195668792, the reward is 247.25 with loss [35.39695405960083, 30.86135959625244] in episode 1786
Report: 
rewardSum:247.25
loss:[35.39695405960083, 30.86135959625244]
policies:[0, 4, 0]
qAverage:[0.0, 92.02779998779297]
ws:[6.8630270004272464, 13.420239067077636]
memory len:10000
memory used:2725.0
now epsilon is 0.09970512448532831, the reward is 247.25 with loss [27.227614402770996, 32.80959224700928] in episode 1787
Report: 
rewardSum:247.25
loss:[27.227614402770996, 32.80959224700928]
policies:[0, 4, 0]
qAverage:[0.0, 93.35348205566406]
ws:[5.919941520690918, 11.838579273223877]
memory len:10000
memory used:2725.0
now epsilon is 0.0996054567440335, the reward is 247.25 with loss [29.020375728607178, 35.139832496643066] in episode 1788
Report: 
rewardSum:247.25
loss:[29.020375728607178, 35.139832496643066]
policies:[0, 3, 1]
qAverage:[0.0, 88.52879333496094]
ws:[5.3840766083449125, 10.766018867492676]
memory len:10000
memory used:2725.0
now epsilon is 0.0995058886331108, the reward is 247.25 with loss [30.32629156112671, 33.288920402526855] in episode 1789
Report: 
rewardSum:247.25
loss:[30.32629156112671, 33.288920402526855]
policies:[0, 4, 0]
qAverage:[0.0, 88.95619010925293]
ws:[4.914113476872444, 10.169400930404663]
memory len:10000
memory used:2725.0
now epsilon is 0.0994064200529672, the reward is 247.25 with loss [19.75460684299469, 24.15945315361023] in episode 1790
Report: 
rewardSum:247.25
loss:[19.75460684299469, 24.15945315361023]
policies:[0, 4, 0]
qAverage:[0.0, 91.72794647216797]
ws:[4.226674318313599, 9.81635251045227]
memory len:10000
memory used:2725.0
now epsilon is 0.09925740358534788, the reward is 245.25 with loss [46.984822273254395, 45.37680721282959] in episode 1791
Report: 
rewardSum:245.25
loss:[46.984822273254395, 45.37680721282959]
policies:[0, 5, 1]
qAverage:[0.0, 95.46645863850911]
ws:[3.4353225598266968, 8.247728625933329]
memory len:10000
memory used:2725.0
now epsilon is 0.09915818339708568, the reward is 247.25 with loss [19.411324501037598, 26.37230634689331] in episode 1792
Report: 
rewardSum:247.25
loss:[19.411324501037598, 26.37230634689331]
policies:[0, 4, 0]
qAverage:[0.0, 91.96017265319824]
ws:[3.8117849603295326, 8.328129172325134]
memory len:10000
memory used:2725.0
now epsilon is 0.09900953905180589, the reward is 245.25 with loss [42.094629764556885, 37.20350122451782] in episode 1793
Report: 
rewardSum:245.25
loss:[42.094629764556885, 37.20350122451782]
policies:[0, 4, 2]
qAverage:[0.0, 91.44812774658203]
ws:[2.148734962940216, 6.73487322628498]
memory len:10000
memory used:2725.0
now epsilon is 0.09888583899348474, the reward is 246.25 with loss [36.63264179229736, 26.285816192626953] in episode 1794
Report: 
rewardSum:246.25
loss:[36.63264179229736, 26.285816192626953]
policies:[0, 4, 1]
qAverage:[0.0, 91.62664947509765]
ws:[3.6158423244953157, 9.271257972717285]
memory len:10000
memory used:2725.0
now epsilon is 0.09878699023050092, the reward is 247.25 with loss [24.468169689178467, 43.860578536987305] in episode 1795
Report: 
rewardSum:247.25
loss:[24.468169689178467, 43.860578536987305]
policies:[0, 4, 0]
qAverage:[0.0, 75.76216634114583]
ws:[3.243166128794352, 8.978974024454752]
memory len:10000
memory used:2725.0
now epsilon is 0.09868824027921796, the reward is 247.25 with loss [32.407063245773315, 30.196884155273438] in episode 1796
Report: 
rewardSum:247.25
loss:[32.407063245773315, 30.196884155273438]
policies:[0, 4, 0]
qAverage:[0.0, 80.51589584350586]
ws:[1.6973564326763153, 5.964488744735718]
memory len:10000
memory used:2725.0
now epsilon is 0.09858958904086124, the reward is 247.25 with loss [24.820762157440186, 28.16514778137207] in episode 1797
Report: 
rewardSum:247.25
loss:[24.820762157440186, 28.16514778137207]
policies:[0, 4, 0]
qAverage:[0.0, 80.47118949890137]
ws:[2.7166599482297897, 7.88818097114563]
memory len:10000
memory used:2725.0
now epsilon is 0.09849103641675482, the reward is 247.25 with loss [26.029300689697266, 28.778934478759766] in episode 1798
Report: 
rewardSum:247.25
loss:[26.029300689697266, 28.778934478759766]
policies:[0, 4, 0]
qAverage:[0.0, 90.22128677368164]
ws:[6.7042770981788635, 12.944495439529419]
memory len:10000
memory used:2725.0
now epsilon is 0.09839258230832143, the reward is 247.25 with loss [31.5962553024292, 33.06900358200073] in episode 1799
Report: 
rewardSum:247.25
loss:[31.5962553024292, 33.06900358200073]
policies:[0, 4, 0]
qAverage:[0.0, 90.61836395263671]
ws:[3.6911784648895263, 7.824399328231811]
memory len:10000
memory used:2725.0
now epsilon is 0.09829422661708233, the reward is 247.25 with loss [26.804550170898438, 30.412001609802246] in episode 1800
Report: 
rewardSum:247.25
loss:[26.804550170898438, 30.412001609802246]
policies:[0, 4, 0]
qAverage:[0.0, 91.98714599609374]
ws:[3.9049565315246584, 8.493432557582855]
memory len:10000
memory used:2726.0
now epsilon is 0.09819596924465723, the reward is 247.25 with loss [22.71823215484619, 18.232685565948486] in episode 1801
Report: 
rewardSum:247.25
loss:[22.71823215484619, 18.232685565948486]
policies:[1, 3, 0]
qAverage:[0.0, 90.20627403259277]
ws:[5.231994330883026, 10.366026163101196]
memory len:10000
memory used:2725.0
now epsilon is 0.09809781009276419, the reward is 247.25 with loss [30.959128856658936, 26.832260608673096] in episode 1802
Report: 
rewardSum:247.25
loss:[30.959128856658936, 26.832260608673096]
policies:[0, 3, 1]
qAverage:[0.0, 88.985107421875]
ws:[4.74617537856102, 9.290200591087341]
memory len:10000
memory used:2725.0
now epsilon is 0.0979997490632195, the reward is 247.25 with loss [26.783206462860107, 29.125064373016357] in episode 1803
Report: 
rewardSum:247.25
loss:[26.783206462860107, 29.125064373016357]
policies:[0, 4, 0]
qAverage:[0.0, 91.3975830078125]
ws:[5.479565522074699, 11.172482538223267]
memory len:10000
memory used:2725.0
now epsilon is 0.09790178605793758, the reward is 247.25 with loss [26.500999450683594, 24.87127947807312] in episode 1804
Report: 
rewardSum:247.25
loss:[26.500999450683594, 24.87127947807312]
policies:[0, 4, 0]
qAverage:[0.0, 89.2245086669922]
ws:[5.156543278694153, 10.730712890625]
memory len:10000
memory used:2725.0
now epsilon is 0.09775502513118654, the reward is 57.6875 with loss [58.39844512939453, 41.97759675979614] in episode 1805
Report: 
rewardSum:57.6875
loss:[58.39844512939453, 41.97759675979614]
policies:[0, 4, 2]
qAverage:[0.0, 86.60010833740235]
ws:[2.9796470165252686, 9.371041774749756]
memory len:10000
memory used:2725.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20*		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.09765730675808049, the reward is 247.25 with loss [29.109453439712524, 21.13341760635376] in episode 1806
Report: 
rewardSum:247.25
loss:[29.109453439712524, 21.13341760635376]
policies:[0, 4, 0]
qAverage:[0.0, 87.65291900634766]
ws:[4.541238689422608, 10.721439361572266]
memory len:10000
memory used:2725.0
now epsilon is 0.09755968606670926, the reward is 247.25 with loss [31.441826343536377, 25.623558044433594] in episode 1807
Report: 
rewardSum:247.25
loss:[31.441826343536377, 25.623558044433594]
policies:[0, 4, 0]
qAverage:[0.0, 89.26297302246094]
ws:[3.72205069065094, 9.629480361938477]
memory len:10000
memory used:2725.0
now epsilon is 0.09746216295942774, the reward is 247.25 with loss [25.27219867706299, 26.26835298538208] in episode 1808
Report: 
rewardSum:247.25
loss:[25.27219867706299, 26.26835298538208]
policies:[0, 4, 0]
qAverage:[0.0, 89.0463851928711]
ws:[3.131661522388458, 8.840372037887573]
memory len:10000
memory used:2725.0
now epsilon is 0.09736473733868843, the reward is 247.25 with loss [23.77388572692871, 24.243908882141113] in episode 1809
Report: 
rewardSum:247.25
loss:[23.77388572692871, 24.243908882141113]
policies:[0, 3, 1]
qAverage:[0.0, 78.41653633117676]
ws:[1.3245854675769806, 6.8539886474609375]
memory len:10000
memory used:2725.0
now epsilon is 0.09726740910704135, the reward is 247.25 with loss [38.203444480895996, 26.37490463256836] in episode 1810
Report: 
rewardSum:247.25
loss:[38.203444480895996, 26.37490463256836]
policies:[0, 4, 0]
qAverage:[0.0, 89.13075256347656]
ws:[4.830407911539078, 11.71494951248169]
memory len:10000
memory used:2725.0
now epsilon is 0.09717017816713391, the reward is 247.25 with loss [25.829171180725098, 23.359620094299316] in episode 1811
Report: 
rewardSum:247.25
loss:[25.829171180725098, 23.359620094299316]
policies:[0, 4, 0]
qAverage:[0.0, 87.30932006835937]
ws:[7.517755031585693, 15.94112138748169]
memory len:10000
memory used:2725.0
now epsilon is 0.09707304442171084, the reward is 247.25 with loss [32.09778451919556, 32.30642914772034] in episode 1812
Report: 
rewardSum:247.25
loss:[32.09778451919556, 32.30642914772034]
policies:[1, 3, 0]
qAverage:[0.0, 84.49610710144043]
ws:[7.393875479698181, 13.697343349456787]
memory len:10000
memory used:2725.0
now epsilon is 0.09697600777361413, the reward is 247.25 with loss [30.8004150390625, 30.35424566268921] in episode 1813
Report: 
rewardSum:247.25
loss:[30.8004150390625, 30.35424566268921]
policies:[0, 3, 1]
qAverage:[0.0, 77.41708755493164]
ws:[2.9893464893102646, 8.70933473110199]
memory len:10000
memory used:2725.0
now epsilon is 0.09687906812578283, the reward is 247.25 with loss [31.624812841415405, 44.472777366638184] in episode 1814
Report: 
rewardSum:247.25
loss:[31.624812841415405, 44.472777366638184]
policies:[1, 3, 0]
qAverage:[0.0, 79.56182861328125]
ws:[1.7941712141036987, 6.791810572147369]
memory len:10000
memory used:2725.0
now epsilon is 0.0967338403174515, the reward is 245.25 with loss [45.24788284301758, 57.85792684555054] in episode 1815
Report: 
rewardSum:245.25
loss:[45.24788284301758, 57.85792684555054]
policies:[0, 5, 1]
qAverage:[0.0, 91.6286226908366]
ws:[2.1975462461511293, 6.303724527359009]
memory len:10000
memory used:2725.0
now epsilon is 0.09663714274627869, the reward is 247.25 with loss [40.88546800613403, 26.177632808685303] in episode 1816
Report: 
rewardSum:247.25
loss:[40.88546800613403, 26.177632808685303]
policies:[0, 4, 0]
qAverage:[0.0, 87.45103607177734]
ws:[2.625612473487854, 7.690825363993644]
memory len:10000
memory used:2725.0
now epsilon is 0.0965405418364215, the reward is 247.25 with loss [23.89521312713623, 21.712475776672363] in episode 1817
Report: 
rewardSum:247.25
loss:[23.89521312713623, 21.712475776672363]
policies:[0, 4, 0]
qAverage:[0.0, 88.86177825927734]
ws:[2.5237855076789857, 7.790840649604798]
memory len:10000
memory used:2732.0
now epsilon is 0.0963958215002616, the reward is 245.25 with loss [44.66171073913574, 44.027159690856934] in episode 1818
Report: 
rewardSum:245.25
loss:[44.66171073913574, 44.027159690856934]
policies:[0, 4, 2]
qAverage:[0.0, 86.65169067382813]
ws:[2.4906449106056243, 7.743192863464356]
memory len:10000
memory used:2726.0
now epsilon is 0.09629946182117005, the reward is 247.25 with loss [28.72215700149536, 26.14727783203125] in episode 1819
Report: 
rewardSum:247.25
loss:[28.72215700149536, 26.14727783203125]
policies:[0, 4, 0]
qAverage:[0.0, 86.57572174072266]
ws:[3.0993023246526716, 8.076355158537627]
memory len:10000
memory used:2726.0
now epsilon is 0.09620319846562873, the reward is 247.25 with loss [26.020472526550293, 38.91481018066406] in episode 1820
Report: 
rewardSum:247.25
loss:[26.020472526550293, 38.91481018066406]
policies:[0, 4, 0]
qAverage:[0.0, 86.77703247070312]
ws:[2.4960363030433657, 6.154215931892395]
memory len:10000
memory used:2726.0
now epsilon is 0.09610703133735021, the reward is 247.25 with loss [36.20110750198364, 26.463755130767822] in episode 1821
Report: 
rewardSum:247.25
loss:[36.20110750198364, 26.463755130767822]
policies:[0, 4, 0]
qAverage:[0.0, 86.27144775390624]
ws:[3.2038796842098236, 7.857023376226425]
memory len:10000
memory used:2726.0
now epsilon is 0.09596296086065827, the reward is 245.25 with loss [44.90813112258911, 38.22314167022705] in episode 1822
Report: 
rewardSum:245.25
loss:[44.90813112258911, 38.22314167022705]
policies:[1, 4, 1]
qAverage:[0.0, 83.6886474609375]
ws:[2.187520271539688, 5.566180244088173]
memory len:10000
memory used:2726.0
now epsilon is 0.09581910635466029, the reward is 245.25 with loss [40.03433656692505, 33.96268033981323] in episode 1823
Report: 
rewardSum:245.25
loss:[40.03433656692505, 33.96268033981323]
policies:[0, 5, 1]
qAverage:[0.0, 90.33563486735027]
ws:[2.6385324696699777, 6.275166749954224]
memory len:10000
memory used:2726.0
now epsilon is 0.09572332317448222, the reward is 247.25 with loss [31.894954681396484, 20.35645055770874] in episode 1824
Report: 
rewardSum:247.25
loss:[31.894954681396484, 20.35645055770874]
policies:[1, 3, 0]
qAverage:[21.402723693847655, 68.41609191894531]
ws:[3.0058890521526336, 6.282500910758972]
memory len:10000
memory used:2731.0
now epsilon is 0.0956276357415716, the reward is 247.25 with loss [37.462016582489014, 29.533525466918945] in episode 1825
Report: 
rewardSum:247.25
loss:[37.462016582489014, 29.533525466918945]
policies:[0, 4, 0]
qAverage:[0.0, 76.07099914550781]
ws:[0.980625843629241, 2.9060780107975006]
memory len:10000
memory used:2731.0
now epsilon is 0.09553204396021708, the reward is 247.25 with loss [36.20473003387451, 32.62629795074463] in episode 1826
Report: 
rewardSum:247.25
loss:[36.20473003387451, 32.62629795074463]
policies:[1, 3, 0]
qAverage:[0.0, 85.18536376953125]
ws:[4.705444097518921, 9.235249042510986]
memory len:10000
memory used:2732.0
now epsilon is 0.09543654773480298, the reward is 247.25 with loss [23.95715045928955, 24.62433671951294] in episode 1827
Report: 
rewardSum:247.25
loss:[23.95715045928955, 24.62433671951294]
policies:[0, 4, 0]
qAverage:[0.0, 86.15402069091797]
ws:[4.294713473320007, 8.297769117355347]
memory len:10000
memory used:2731.0
now epsilon is 0.09534114696980918, the reward is 247.25 with loss [30.05297613143921, 25.295331478118896] in episode 1828
Report: 
rewardSum:247.25
loss:[30.05297613143921, 25.295331478118896]
policies:[0, 4, 0]
qAverage:[0.0, 86.38060607910157]
ws:[4.833018517494201, 9.91392650604248]
memory len:10000
memory used:2737.0
now epsilon is 0.09524584156981104, the reward is 247.25 with loss [28.035929203033447, 31.04513645172119] in episode 1829
Report: 
rewardSum:247.25
loss:[28.035929203033447, 31.04513645172119]
policies:[0, 3, 1]
qAverage:[0.0, 81.00459098815918]
ws:[6.135169982910156, 11.967815160751343]
memory len:10000
memory used:2737.0
now epsilon is 0.09515063143947934, the reward is 247.25 with loss [17.350846767425537, 27.55518889427185] in episode 1830
Report: 
rewardSum:247.25
loss:[17.350846767425537, 27.55518889427185]
policies:[0, 2, 2]
qAverage:[0.0, 63.790181477864586]
ws:[1.823113039135933, 5.297522147496541]
memory len:10000
memory used:2736.0
now epsilon is 0.09505551648358013, the reward is 247.25 with loss [30.18775701522827, 35.707987785339355] in episode 1831
Report: 
rewardSum:247.25
loss:[30.18775701522827, 35.707987785339355]
policies:[1, 3, 0]
qAverage:[0.0, 79.74799156188965]
ws:[4.64544312749058, 8.628499984741211]
memory len:10000
memory used:2737.0
now epsilon is 0.09496049660697464, the reward is 59.6875 with loss [26.51656436920166, 26.72737407684326] in episode 1832
Report: 
rewardSum:59.6875
loss:[26.51656436920166, 26.72737407684326]
policies:[1, 2, 1]
qAverage:[0.0, 67.1373774210612]
ws:[0.5280981262524923, 2.953166961669922]
memory len:10000
memory used:2737.0
############# STATE ###############
0-		8*		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.09486557171461925, the reward is 247.25 with loss [30.52639865875244, 27.162518978118896] in episode 1833
Report: 
rewardSum:247.25
loss:[30.52639865875244, 27.162518978118896]
policies:[0, 4, 0]
qAverage:[0.0, 83.18146362304688]
ws:[4.416725993156433, 8.569496440887452]
memory len:10000
memory used:2737.0
now epsilon is 0.09477074171156531, the reward is 247.25 with loss [23.540397882461548, 23.49415874481201] in episode 1834
Report: 
rewardSum:247.25
loss:[23.540397882461548, 23.49415874481201]
policies:[0, 4, 0]
qAverage:[0.0, 82.61683959960938]
ws:[5.083589020371437, 10.037945818901061]
memory len:10000
memory used:2737.0
now epsilon is 0.09467600650295911, the reward is 247.25 with loss [32.01276683807373, 39.70083808898926] in episode 1835
Report: 
rewardSum:247.25
loss:[32.01276683807373, 39.70083808898926]
policies:[0, 4, 0]
qAverage:[0.0, 83.95767517089844]
ws:[5.467064419388771, 10.496815395355224]
memory len:10000
memory used:2737.0
now epsilon is 0.09458136599404172, the reward is 247.25 with loss [20.593405961990356, 26.37527847290039] in episode 1836
Report: 
rewardSum:247.25
loss:[20.593405961990356, 26.37527847290039]
policies:[0, 3, 1]
qAverage:[0.0, 78.85630798339844]
ws:[5.150733560323715, 9.167111992835999]
memory len:10000
memory used:2737.0
now epsilon is 0.09443958258553015, the reward is 245.25 with loss [57.944435596466064, 31.156744480133057] in episode 1837
Report: 
rewardSum:245.25
loss:[57.944435596466064, 31.156744480133057]
policies:[1, 4, 1]
qAverage:[0.0, 79.58364868164062]
ws:[2.634608668088913, 7.273179912567139]
memory len:10000
memory used:2737.0
now epsilon is 0.09434517841188599, the reward is 247.25 with loss [33.463237285614014, 31.887707233428955] in episode 1838
Report: 
rewardSum:247.25
loss:[33.463237285614014, 31.887707233428955]
policies:[0, 4, 0]
qAverage:[0.0, 83.621533203125]
ws:[3.5094834208488463, 6.987104749679565]
memory len:10000
memory used:2737.0
now epsilon is 0.09425086860701981, the reward is 247.25 with loss [24.554311752319336, 28.984929084777832] in episode 1839
Report: 
rewardSum:247.25
loss:[24.554311752319336, 28.984929084777832]
policies:[1, 3, 0]
qAverage:[0.0, 74.38068199157715]
ws:[1.2912136763334274, 4.722323298454285]
memory len:10000
memory used:2737.0
now epsilon is 0.0942037490633956, the reward is -1.0 with loss [12.425696611404419, 16.15585708618164] in episode 1840
Report: 
rewardSum:-1.0
loss:[12.425696611404419, 16.15585708618164]
policies:[0, 1, 1]
qAverage:[0.0, 44.04631042480469]
ws:[-0.298267662525177, 0.597237765789032]
memory len:10000
memory used:2737.0
now epsilon is 0.09410958063485074, the reward is 247.25 with loss [30.785569190979004, 29.208232879638672] in episode 1841
Report: 
rewardSum:247.25
loss:[30.785569190979004, 29.208232879638672]
policies:[2, 1, 1]
qAverage:[0.0, 60.31146240234375]
ws:[8.181181907653809, 12.711897850036621]
memory len:10000
memory used:2737.0
now epsilon is 0.09401550633942715, the reward is 247.25 with loss [34.94523048400879, 34.16303825378418] in episode 1842
Report: 
rewardSum:247.25
loss:[34.94523048400879, 34.16303825378418]
policies:[1, 2, 1]
qAverage:[0.0, 66.47522735595703]
ws:[0.6696797907352448, 2.7408432165781655]
memory len:10000
memory used:2737.0
now epsilon is 0.09392152608302701, the reward is 247.25 with loss [28.83635902404785, 27.901273250579834] in episode 1843
Report: 
rewardSum:247.25
loss:[28.83635902404785, 27.901273250579834]
policies:[0, 3, 1]
qAverage:[0.0, 81.93473815917969]
ws:[5.190980792045593, 10.313230633735657]
memory len:10000
memory used:2737.0
now epsilon is 0.09382763977164654, the reward is 247.25 with loss [34.48319911956787, 35.769859790802] in episode 1844
Report: 
rewardSum:247.25
loss:[34.48319911956787, 35.769859790802]
policies:[0, 4, 0]
qAverage:[0.0, 84.48303833007813]
ws:[3.739068794250488, 8.204686999320984]
memory len:10000
memory used:2737.0
now epsilon is 0.09373384731137596, the reward is 247.25 with loss [21.44445037841797, 30.899465560913086] in episode 1845
Report: 
rewardSum:247.25
loss:[21.44445037841797, 30.899465560913086]
policies:[1, 3, 0]
qAverage:[0.0, 77.05855560302734]
ws:[4.680431524912517, 9.25285784403483]
memory len:10000
memory used:2737.0
now epsilon is 0.09364014860839934, the reward is 247.25 with loss [34.27845096588135, 33.28394031524658] in episode 1846
Report: 
rewardSum:247.25
loss:[34.27845096588135, 33.28394031524658]
policies:[0, 4, 0]
qAverage:[0.0, 83.7253173828125]
ws:[3.5278056383132936, 8.287101602554321]
memory len:10000
memory used:2737.0
now epsilon is 0.09354654356899454, the reward is 247.25 with loss [28.44673252105713, 25.601574182510376] in episode 1847
Report: 
rewardSum:247.25
loss:[28.44673252105713, 25.601574182510376]
policies:[0, 3, 1]
qAverage:[0.0, 82.98047637939453]
ws:[4.790283590555191, 10.294023275375366]
memory len:10000
memory used:2737.0
now epsilon is 0.0934530320995331, the reward is 247.25 with loss [39.32257652282715, 37.2621750831604] in episode 1848
Report: 
rewardSum:247.25
loss:[39.32257652282715, 37.2621750831604]
policies:[1, 3, 0]
qAverage:[0.0, 63.69085439046224]
ws:[1.4562999506791432, 4.757592995961507]
memory len:10000
memory used:2737.0
now epsilon is 0.09335961410648015, the reward is 247.25 with loss [33.61311054229736, 23.30397367477417] in episode 1849
Report: 
rewardSum:247.25
loss:[33.61311054229736, 23.30397367477417]
policies:[0, 4, 0]
qAverage:[0.0, 84.05098266601563]
ws:[3.7389871001243593, 8.653255414962768]
memory len:10000
memory used:2738.0
now epsilon is 0.09326628949639437, the reward is 247.25 with loss [28.197484016418457, 31.25088930130005] in episode 1850
Report: 
rewardSum:247.25
loss:[28.197484016418457, 31.25088930130005]
policies:[0, 4, 0]
qAverage:[0.0, 78.71822929382324]
ws:[3.5862665362656116, 7.6487573981285095]
memory len:10000
memory used:2738.0
now epsilon is 0.09317305817592776, the reward is 247.25 with loss [39.11546850204468, 25.486011505126953] in episode 1851
Report: 
rewardSum:247.25
loss:[39.11546850204468, 25.486011505126953]
policies:[0, 4, 0]
qAverage:[0.0, 83.17365264892578]
ws:[4.186381220817566, 9.072985410690308]
memory len:10000
memory used:2738.0
now epsilon is 0.09307992005182572, the reward is 247.25 with loss [31.941233158111572, 23.036380767822266] in episode 1852
Report: 
rewardSum:247.25
loss:[31.941233158111572, 23.036380767822266]
policies:[0, 4, 0]
qAverage:[0.0, 82.0034164428711]
ws:[3.894395571947098, 8.595267152786255]
memory len:10000
memory used:2737.0
now epsilon is 0.0929868750309268, the reward is 247.25 with loss [29.467313766479492, 27.584433555603027] in episode 1853
Report: 
rewardSum:247.25
loss:[29.467313766479492, 27.584433555603027]
policies:[1, 3, 0]
qAverage:[0.0, 72.96883392333984]
ws:[5.812392234802246, 11.00660753250122]
memory len:10000
memory used:2738.0
now epsilon is 0.0928939230201627, the reward is 247.25 with loss [27.50815439224243, 29.866333484649658] in episode 1854
Report: 
rewardSum:247.25
loss:[27.50815439224243, 29.866333484649658]
policies:[0, 4, 0]
qAverage:[0.0, 81.59830169677734]
ws:[3.7115589678287506, 8.8865656375885]
memory len:10000
memory used:2739.0
now epsilon is 0.09280106392655817, the reward is 247.25 with loss [29.738520622253418, 19.519564151763916] in episode 1855
Report: 
rewardSum:247.25
loss:[29.738520622253418, 19.519564151763916]
policies:[0, 4, 0]
qAverage:[0.0, 76.87421989440918]
ws:[3.860405260697007, 9.221110045909882]
memory len:10000
memory used:2739.0
now epsilon is 0.09270829765723088, the reward is 247.25 with loss [20.246742844581604, 27.688029289245605] in episode 1856
Report: 
rewardSum:247.25
loss:[20.246742844581604, 27.688029289245605]
policies:[0, 4, 0]
qAverage:[0.0, 81.81892852783203]
ws:[3.935456508398056, 10.564081048965454]
memory len:10000
memory used:2737.0
now epsilon is 0.09261562411939137, the reward is 247.25 with loss [24.377851009368896, 33.81036376953125] in episode 1857
Report: 
rewardSum:247.25
loss:[24.377851009368896, 33.81036376953125]
policies:[0, 3, 1]
qAverage:[0.0, 81.35834884643555]
ws:[4.656456887722015, 11.688202977180481]
memory len:10000
memory used:2737.0
now epsilon is 0.09252304322034291, the reward is 59.6875 with loss [32.90543460845947, 33.35843515396118] in episode 1858
Report: 
rewardSum:59.6875
loss:[32.90543460845947, 33.35843515396118]
policies:[0, 3, 1]
qAverage:[0.0, 67.86416117350261]
ws:[0.37592120965321857, 4.090349038441976]
memory len:10000
memory used:2737.0
now epsilon is 0.0924074472287646, the reward is 246.25 with loss [44.56808853149414, 37.108437061309814] in episode 1859
Report: 
rewardSum:246.25
loss:[44.56808853149414, 37.108437061309814]
policies:[0, 4, 1]
qAverage:[0.0, 82.75009002685547]
ws:[2.369043219089508, 7.680982542037964]
memory len:10000
memory used:2737.0
now epsilon is 0.09231507442855344, the reward is 247.25 with loss [22.955662488937378, 33.14062786102295] in episode 1860
Report: 
rewardSum:247.25
loss:[22.955662488937378, 33.14062786102295]
policies:[1, 3, 0]
qAverage:[0.0, 74.54117965698242]
ws:[-0.08625240623950958, 3.694208838045597]
memory len:10000
memory used:2737.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10*		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.0922227939665085, the reward is 247.25 with loss [32.12135076522827, 15.354939222335815] in episode 1861
Report: 
rewardSum:247.25
loss:[32.12135076522827, 15.354939222335815]
policies:[0, 4, 0]
qAverage:[0.0, 82.05454711914062]
ws:[1.938866674900055, 6.684210336208343]
memory len:10000
memory used:2737.0
now epsilon is 0.09213060575032617, the reward is 247.25 with loss [47.53402900695801, 37.70501708984375] in episode 1862
Report: 
rewardSum:247.25
loss:[47.53402900695801, 37.70501708984375]
policies:[0, 4, 0]
qAverage:[0.0, 84.26001739501953]
ws:[2.107249844074249, 6.087079548835755]
memory len:10000
memory used:2737.0
now epsilon is 0.09199249618535815, the reward is 245.25 with loss [36.678319454193115, 43.591464042663574] in episode 1863
Report: 
rewardSum:245.25
loss:[36.678319454193115, 43.591464042663574]
policies:[0, 5, 1]
qAverage:[0.0, 86.46095275878906]
ws:[3.1896422654390335, 8.738988896210989]
memory len:10000
memory used:2737.0
now epsilon is 0.09190053818060971, the reward is 247.25 with loss [35.79792022705078, 26.736068725585938] in episode 1864
Report: 
rewardSum:247.25
loss:[35.79792022705078, 26.736068725585938]
policies:[0, 4, 0]
qAverage:[0.0, 82.29081878662109]
ws:[2.9553637981414793, 6.855344200134278]
memory len:10000
memory used:2737.0
now epsilon is 0.0918086720993875, the reward is 247.25 with loss [27.786644339561462, 23.156102418899536] in episode 1865
Report: 
rewardSum:247.25
loss:[27.786644339561462, 23.156102418899536]
policies:[0, 4, 0]
qAverage:[0.0, 83.89594268798828]
ws:[3.3409444987773895, 7.903503942489624]
memory len:10000
memory used:2737.0
now epsilon is 0.09171689784980248, the reward is 247.25 with loss [29.23484182357788, 33.435749530792236] in episode 1866
Report: 
rewardSum:247.25
loss:[29.23484182357788, 33.435749530792236]
policies:[0, 4, 0]
qAverage:[0.0, 83.78172302246094]
ws:[3.3127310425043106, 7.8970658540725704]
memory len:10000
memory used:2737.0
now epsilon is 0.09162521534005742, the reward is 247.25 with loss [26.781930923461914, 33.023359298706055] in episode 1867
Report: 
rewardSum:247.25
loss:[26.781930923461914, 33.023359298706055]
policies:[0, 4, 0]
qAverage:[0.0, 83.26689910888672]
ws:[3.096224069595337, 6.954026460647583]
memory len:10000
memory used:2737.0
now epsilon is 0.09153362447844691, the reward is 247.25 with loss [27.265333652496338, 30.05379629135132] in episode 1868
Report: 
rewardSum:247.25
loss:[27.265333652496338, 30.05379629135132]
policies:[0, 4, 0]
qAverage:[0.0, 83.10609130859375]
ws:[3.3452712655067445, 7.2354542255401615]
memory len:10000
memory used:2737.0
now epsilon is 0.0913964098259033, the reward is 245.25 with loss [46.59083652496338, 42.260775566101074] in episode 1869
Report: 
rewardSum:245.25
loss:[46.59083652496338, 42.260775566101074]
policies:[0, 5, 1]
qAverage:[0.0, 82.29827575683593]
ws:[3.206903946399689, 7.164964723587036]
memory len:10000
memory used:2737.0
now epsilon is 0.09125940086674265, the reward is 245.25 with loss [37.07261514663696, 40.445396423339844] in episode 1870
Report: 
rewardSum:245.25
loss:[37.07261514663696, 40.445396423339844]
policies:[0, 5, 1]
qAverage:[0.0, 83.30984649658203]
ws:[2.885955719649792, 5.820310783386231]
memory len:10000
memory used:2737.0
now epsilon is 0.0911681756824479, the reward is 247.25 with loss [23.348350763320923, 16.09002661705017] in episode 1871
Report: 
rewardSum:247.25
loss:[23.348350763320923, 16.09002661705017]
policies:[0, 4, 0]
qAverage:[0.0, 84.21604614257812]
ws:[2.8884991645812987, 6.382441782951355]
memory len:10000
memory used:2737.0
now epsilon is 0.09107704168913368, the reward is 247.25 with loss [26.6732439994812, 27.59500503540039] in episode 1872
Report: 
rewardSum:247.25
loss:[26.6732439994812, 27.59500503540039]
policies:[0, 4, 0]
qAverage:[0.0, 82.98338775634765]
ws:[3.924011754989624, 8.721975374221802]
memory len:10000
memory used:2737.0
now epsilon is 0.09098599879564322, the reward is 247.25 with loss [30.2807354927063, 35.13940906524658] in episode 1873
Report: 
rewardSum:247.25
loss:[30.2807354927063, 35.13940906524658]
policies:[0, 4, 0]
qAverage:[0.0, 83.68820648193359]
ws:[3.6447311878204345, 7.913986921310425]
memory len:10000
memory used:2737.0
now epsilon is 0.09089504691091085, the reward is 247.25 with loss [20.963265419006348, 29.61431360244751] in episode 1874
Report: 
rewardSum:247.25
loss:[20.963265419006348, 29.61431360244751]
policies:[0, 3, 1]
qAverage:[0.0, 62.286336263020836]
ws:[1.2947457234064739, 3.57212503751119]
memory len:10000
memory used:2737.0
now epsilon is 0.09080418594396196, the reward is 247.25 with loss [27.419273376464844, 31.575528144836426] in episode 1875
Report: 
rewardSum:247.25
loss:[27.419273376464844, 31.575528144836426]
policies:[0, 4, 0]
qAverage:[0.0, 83.3611053466797]
ws:[4.275319576263428, 8.938294315338135]
memory len:10000
memory used:2736.0
now epsilon is 0.09071341580391284, the reward is 247.25 with loss [19.646426916122437, 24.96244466304779] in episode 1876
Report: 
rewardSum:247.25
loss:[19.646426916122437, 24.96244466304779]
policies:[0, 3, 1]
qAverage:[0.0, 82.89509773254395]
ws:[4.229641258716583, 8.634065508842468]
memory len:10000
memory used:2737.0
now epsilon is 0.09062273639997064, the reward is 59.6875 with loss [26.68136692047119, 26.501118659973145] in episode 1877
Report: 
rewardSum:59.6875
loss:[26.68136692047119, 26.501118659973145]
policies:[0, 3, 1]
qAverage:[0.0, 73.41092681884766]
ws:[0.755702530965209, 3.245854079723358]
memory len:10000
memory used:2737.0
now epsilon is 0.09053214764143325, the reward is 247.25 with loss [33.436686515808105, 23.86797285079956] in episode 1878
Report: 
rewardSum:247.25
loss:[33.436686515808105, 23.86797285079956]
policies:[0, 4, 0]
qAverage:[0.0, 83.43473968505859]
ws:[3.545213431119919, 7.615717649459839]
memory len:10000
memory used:2737.0
now epsilon is 0.09039643426557353, the reward is 245.25 with loss [38.587892055511475, 40.83330726623535] in episode 1879
Report: 
rewardSum:245.25
loss:[38.587892055511475, 40.83330726623535]
policies:[0, 5, 1]
qAverage:[0.0, 80.36900329589844]
ws:[4.6897641062736515, 9.03701958656311]
memory len:10000
memory used:2738.0
now epsilon is 0.09026092433258871, the reward is 245.25 with loss [47.01274394989014, 49.89622449874878] in episode 1880
Report: 
rewardSum:245.25
loss:[47.01274394989014, 49.89622449874878]
policies:[0, 5, 1]
qAverage:[0.0, 84.89246877034505]
ws:[4.829436798890431, 9.339844266573587]
memory len:10000
memory used:2738.0
now epsilon is 0.0901706972504618, the reward is 247.25 with loss [20.324856758117676, 27.309606790542603] in episode 1881
Report: 
rewardSum:247.25
loss:[20.324856758117676, 27.309606790542603]
policies:[0, 4, 0]
qAverage:[0.0, 83.4627182006836]
ws:[5.470910668373108, 11.086636734008788]
memory len:10000
memory used:2738.0
now epsilon is 0.0900805603615875, the reward is 247.25 with loss [30.701599597930908, 23.96589708328247] in episode 1882
Report: 
rewardSum:247.25
loss:[30.701599597930908, 23.96589708328247]
policies:[0, 4, 0]
qAverage:[0.0, 81.65015869140625]
ws:[5.54723744392395, 10.779046440124512]
memory len:10000
memory used:2738.0
now epsilon is 0.08999051357580638, the reward is 247.25 with loss [29.553861618041992, 30.999940872192383] in episode 1883
Report: 
rewardSum:247.25
loss:[29.553861618041992, 30.999940872192383]
policies:[0, 4, 0]
qAverage:[0.0, 83.86631927490234]
ws:[4.864954113960266, 9.704382085800171]
memory len:10000
memory used:2738.0
now epsilon is 0.08987808166384836, the reward is 246.25 with loss [35.547014474868774, 32.099613904953] in episode 1884
Report: 
rewardSum:246.25
loss:[35.547014474868774, 32.099613904953]
policies:[0, 4, 1]
qAverage:[0.0, 82.7263900756836]
ws:[4.277572792768479, 9.196496486663818]
memory len:10000
memory used:2738.0
now epsilon is 0.08978823728084812, the reward is 247.25 with loss [28.108510494232178, 29.602237224578857] in episode 1885
Report: 
rewardSum:247.25
loss:[28.108510494232178, 29.602237224578857]
policies:[0, 4, 0]
qAverage:[0.0, 82.34552001953125]
ws:[3.327718675136566, 6.758474040031433]
memory len:10000
memory used:2738.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20*		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.08969848270854484, the reward is 247.25 with loss [30.581180572509766, 19.271501541137695] in episode 1886
Report: 
rewardSum:247.25
loss:[30.581180572509766, 19.271501541137695]
policies:[0, 4, 0]
qAverage:[0.0, 82.86487426757813]
ws:[4.127076244354248, 8.431125116348266]
memory len:10000
memory used:2738.0
now epsilon is 0.08960881785716153, the reward is 247.25 with loss [36.01705360412598, 30.516034603118896] in episode 1887
Report: 
rewardSum:247.25
loss:[36.01705360412598, 30.516034603118896]
policies:[0, 4, 0]
qAverage:[0.0, 82.32704467773438]
ws:[4.0158151388168335, 8.507788622379303]
memory len:10000
memory used:2737.0
now epsilon is 0.08951924263701087, the reward is 247.25 with loss [32.88451814651489, 27.757089138031006] in episode 1888
Report: 
rewardSum:247.25
loss:[32.88451814651489, 27.757089138031006]
policies:[0, 4, 0]
qAverage:[0.0, 75.59494018554688]
ws:[4.545444028452039, 8.351304829120636]
memory len:10000
memory used:2738.0
now epsilon is 0.08942975695849525, the reward is 247.25 with loss [26.72592854499817, 32.947429180145264] in episode 1889
Report: 
rewardSum:247.25
loss:[26.72592854499817, 32.947429180145264]
policies:[0, 4, 0]
qAverage:[0.0, 81.67131042480469]
ws:[5.807027077674865, 11.4999258518219]
memory len:10000
memory used:2738.0
now epsilon is 0.08934036073210662, the reward is 247.25 with loss [21.362009525299072, 34.1184606552124] in episode 1890
Report: 
rewardSum:247.25
loss:[21.362009525299072, 34.1184606552124]
policies:[0, 4, 0]
qAverage:[0.0, 82.22809448242188]
ws:[6.103696632385254, 12.097844886779786]
memory len:10000
memory used:2738.0
now epsilon is 0.08925105386842636, the reward is 247.25 with loss [27.069369792938232, 25.164473056793213] in episode 1891
Report: 
rewardSum:247.25
loss:[27.069369792938232, 25.164473056793213]
policies:[0, 4, 0]
qAverage:[0.0, 81.85351104736328]
ws:[5.834320688247681, 12.26029863357544]
memory len:10000
memory used:2738.0
now epsilon is 0.0891618362781253, the reward is 59.6875 with loss [23.869706869125366, 27.951152801513672] in episode 1892
Report: 
rewardSum:59.6875
loss:[23.869706869125366, 27.951152801513672]
policies:[0, 3, 1]
qAverage:[0.0, 73.45304298400879]
ws:[2.8635319471359253, 8.394297122955322]
memory len:10000
memory used:2738.0
now epsilon is 0.08907270787196352, the reward is 247.25 with loss [33.350815296173096, 26.37933921813965] in episode 1893
Report: 
rewardSum:247.25
loss:[33.350815296173096, 26.37933921813965]
policies:[0, 4, 0]
qAverage:[0.0, 82.36710510253906]
ws:[6.362133932113648, 12.743335056304932]
memory len:10000
memory used:2738.0
now epsilon is 0.08898366856079032, the reward is 247.25 with loss [30.617064476013184, 24.3018741607666] in episode 1894
Report: 
rewardSum:247.25
loss:[30.617064476013184, 24.3018741607666]
policies:[1, 3, 0]
qAverage:[0.0, 75.79706001281738]
ws:[7.319794669747353, 12.943974137306213]
memory len:10000
memory used:2737.0
now epsilon is 0.08889471825554411, the reward is 247.25 with loss [26.80195951461792, 20.5028178691864] in episode 1895
Report: 
rewardSum:247.25
loss:[26.80195951461792, 20.5028178691864]
policies:[0, 4, 0]
qAverage:[0.0, 82.52602081298828]
ws:[6.320721888542176, 10.9878399848938]
memory len:10000
memory used:2737.0
now epsilon is 0.08880585686725236, the reward is 247.25 with loss [32.1816463470459, 32.04370880126953] in episode 1896
Report: 
rewardSum:247.25
loss:[32.1816463470459, 32.04370880126953]
policies:[0, 4, 0]
qAverage:[0.0, 81.1804412841797]
ws:[6.774302387237549, 12.409984970092774]
memory len:10000
memory used:2738.0
now epsilon is 0.08871708430703142, the reward is 247.25 with loss [26.770880222320557, 17.906383514404297] in episode 1897
Report: 
rewardSum:247.25
loss:[26.770880222320557, 17.906383514404297]
policies:[0, 4, 0]
qAverage:[0.0, 82.66603240966796]
ws:[6.25337268859148, 11.988750696182251]
memory len:10000
memory used:2738.0
now epsilon is 0.08862840048608654, the reward is 247.25 with loss [20.94711971282959, 31.510330200195312] in episode 1898
Report: 
rewardSum:247.25
loss:[20.94711971282959, 31.510330200195312]
policies:[0, 3, 1]
qAverage:[0.0, 79.94329452514648]
ws:[7.512291610240936, 14.012189626693726]
memory len:10000
memory used:2738.0
now epsilon is 0.08853980531571172, the reward is 247.25 with loss [19.486051559448242, 27.257572650909424] in episode 1899
Report: 
rewardSum:247.25
loss:[19.486051559448242, 27.257572650909424]
policies:[0, 4, 0]
qAverage:[0.0, 82.57933197021484]
ws:[5.164262878894806, 10.371331024169923]
memory len:10000
memory used:2738.0
now epsilon is 0.08845129870728961, the reward is 247.25 with loss [39.02989673614502, 36.50624132156372] in episode 1900
Report: 
rewardSum:247.25
loss:[39.02989673614502, 36.50624132156372]
policies:[1, 3, 0]
qAverage:[0.0, 75.52258491516113]
ws:[4.509333789348602, 8.901451382320374]
memory len:10000
memory used:2738.0
now epsilon is 0.08836288057229148, the reward is 247.25 with loss [20.946253776550293, 29.405869007110596] in episode 1901
Report: 
rewardSum:247.25
loss:[20.946253776550293, 29.405869007110596]
policies:[0, 4, 0]
qAverage:[0.0, 82.32440185546875]
ws:[2.7061285614967345, 7.303944039344787]
memory len:10000
memory used:2738.0
now epsilon is 0.08823041906402536, the reward is 245.25 with loss [45.98444056510925, 56.2903151512146] in episode 1902
Report: 
rewardSum:245.25
loss:[45.98444056510925, 56.2903151512146]
policies:[0, 5, 1]
qAverage:[0.0, 84.40155665079753]
ws:[2.308512898782889, 6.714329749345779]
memory len:10000
memory used:2739.0
now epsilon is 0.08814222172585442, the reward is 247.25 with loss [27.35043478012085, 39.27969264984131] in episode 1903
Report: 
rewardSum:247.25
loss:[27.35043478012085, 39.27969264984131]
policies:[0, 4, 0]
qAverage:[0.0, 80.93372955322266]
ws:[2.5354539155960083, 6.268178081512451]
memory len:10000
memory used:2739.0
now epsilon is 0.08805411255195318, the reward is 247.25 with loss [25.272467136383057, 32.03089380264282] in episode 1904
Report: 
rewardSum:247.25
loss:[25.272467136383057, 32.03089380264282]
policies:[1, 3, 0]
qAverage:[0.0, 81.52217864990234]
ws:[1.9802973866462708, 4.529016479849815]
memory len:10000
memory used:2739.0
now epsilon is 0.08794409993132686, the reward is 246.25 with loss [29.74196720123291, 33.161113262176514] in episode 1905
Report: 
rewardSum:246.25
loss:[29.74196720123291, 33.161113262176514]
policies:[0, 4, 1]
qAverage:[0.0, 81.19183349609375]
ws:[2.2501295804977417, 5.710097980499268]
memory len:10000
memory used:2738.0
now epsilon is 0.08785618880493687, the reward is 247.25 with loss [26.243537187576294, 41.40341663360596] in episode 1906
Report: 
rewardSum:247.25
loss:[26.243537187576294, 41.40341663360596]
policies:[0, 4, 0]
qAverage:[0.0, 82.75764923095703]
ws:[3.190941858291626, 6.752062177658081]
memory len:10000
memory used:2738.0
now epsilon is 0.08776836555671208, the reward is 247.25 with loss [21.91866970062256, 25.200160026550293] in episode 1907
Report: 
rewardSum:247.25
loss:[21.91866970062256, 25.200160026550293]
policies:[0, 3, 1]
qAverage:[0.0, 78.27066802978516]
ws:[4.182909071445465, 7.18034004420042]
memory len:10000
memory used:2738.0
now epsilon is 0.08768063009880728, the reward is 247.25 with loss [29.49874258041382, 30.57373046875] in episode 1908
Report: 
rewardSum:247.25
loss:[29.49874258041382, 30.57373046875]
policies:[0, 4, 0]
qAverage:[0.0, 76.12613487243652]
ws:[5.508966818451881, 9.525689989328384]
memory len:10000
memory used:2738.0
now epsilon is 0.08759298234346508, the reward is 247.25 with loss [19.46147084236145, 21.22450304031372] in episode 1909
Report: 
rewardSum:247.25
loss:[19.46147084236145, 21.22450304031372]
policies:[2, 2, 0]
qAverage:[0.0, 63.31770579020182]
ws:[1.5965806245803833, 3.9262640476226807]
memory len:10000
memory used:2738.0
now epsilon is 0.08750542220301578, the reward is 247.25 with loss [18.33764362335205, 38.36769199371338] in episode 1910
Report: 
rewardSum:247.25
loss:[18.33764362335205, 38.36769199371338]
policies:[2, 1, 1]
qAverage:[0.0, 44.424198150634766]
ws:[1.0894047021865845, 2.247783660888672]
memory len:10000
memory used:2738.0
now epsilon is 0.08741794958987736, the reward is 247.25 with loss [30.609939575195312, 28.740286350250244] in episode 1911
Report: 
rewardSum:247.25
loss:[30.609939575195312, 28.740286350250244]
policies:[0, 3, 1]
qAverage:[0.0, 78.09288597106934]
ws:[8.14368212223053, 15.188509941101074]
memory len:10000
memory used:2738.0
now epsilon is 0.08733056441655532, the reward is 247.25 with loss [32.93142890930176, 28.950926780700684] in episode 1912
Report: 
rewardSum:247.25
loss:[32.93142890930176, 28.950926780700684]
policies:[0, 4, 0]
qAverage:[0.0, 80.97527503967285]
ws:[7.789215445518494, 13.911351919174194]
memory len:10000
memory used:2738.0
now epsilon is 0.08724326659564262, the reward is 247.25 with loss [27.815098762512207, 26.006463289260864] in episode 1913
Report: 
rewardSum:247.25
loss:[27.815098762512207, 26.006463289260864]
policies:[0, 4, 0]
qAverage:[0.0, 81.70859069824219]
ws:[6.9853075504302975, 12.75320119857788]
memory len:10000
memory used:2738.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2*		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.08713426702580963, the reward is 246.25 with loss [49.476884841918945, 33.99162769317627] in episode 1914
Report: 
rewardSum:246.25
loss:[49.476884841918945, 33.99162769317627]
policies:[0, 4, 1]
qAverage:[0.0, 82.26981506347656]
ws:[6.5405686378479, 13.155395984649658]
memory len:10000
memory used:2739.0
now epsilon is 0.08704716542868841, the reward is 247.25 with loss [31.58699941635132, 19.628820419311523] in episode 1915
Report: 
rewardSum:247.25
loss:[31.58699941635132, 19.628820419311523]
policies:[0, 4, 0]
qAverage:[0.0, 82.24744262695313]
ws:[4.8501335978508, 10.958970737457275]
memory len:10000
memory used:2739.0
now epsilon is 0.08696015090050667, the reward is 247.25 with loss [40.188629150390625, 31.46119213104248] in episode 1916
Report: 
rewardSum:247.25
loss:[40.188629150390625, 31.46119213104248]
policies:[0, 4, 0]
qAverage:[0.0, 82.10926971435546]
ws:[4.458885180950165, 10.281263661384582]
memory len:10000
memory used:2739.0
now epsilon is 0.08682979217212745, the reward is 245.25 with loss [34.46533679962158, 47.25940990447998] in episode 1917
Report: 
rewardSum:245.25
loss:[34.46533679962158, 47.25940990447998]
policies:[0, 5, 1]
qAverage:[0.0, 85.45293680826823]
ws:[4.296604077021281, 9.953595379988352]
memory len:10000
memory used:2739.0
now epsilon is 0.08674299493570088, the reward is 247.25 with loss [28.419214725494385, 31.29701805114746] in episode 1918
Report: 
rewardSum:247.25
loss:[28.419214725494385, 31.29701805114746]
policies:[0, 4, 0]
qAverage:[0.0, 72.36465072631836]
ws:[0.16756971180438995, 2.4472103628795594]
memory len:10000
memory used:2739.0
now epsilon is 0.08665628446396718, the reward is 247.25 with loss [33.37712621688843, 24.260743379592896] in episode 1919
Report: 
rewardSum:247.25
loss:[33.37712621688843, 24.260743379592896]
policies:[1, 3, 0]
qAverage:[0.0, 61.433990478515625]
ws:[1.4022653500239055, 4.810441469152768]
memory len:10000
memory used:2739.0
now epsilon is 0.08656966067019423, the reward is 247.25 with loss [32.624483585357666, 33.59209394454956] in episode 1920
Report: 
rewardSum:247.25
loss:[32.624483585357666, 33.59209394454956]
policies:[0, 4, 0]
qAverage:[0.0, 82.21351776123046]
ws:[3.8710214495658875, 6.934642314910889]
memory len:10000
memory used:2739.0
now epsilon is 0.08648312346773653, the reward is 247.25 with loss [27.983964443206787, 32.45723533630371] in episode 1921
Report: 
rewardSum:247.25
loss:[27.983964443206787, 32.45723533630371]
policies:[1, 3, 0]
qAverage:[0.0, 72.24658203125]
ws:[1.4454944878816605, 5.146871353499591]
memory len:10000
memory used:2738.0
now epsilon is 0.08639667277003525, the reward is 247.25 with loss [32.808297634124756, 28.721456050872803] in episode 1922
Report: 
rewardSum:247.25
loss:[32.808297634124756, 28.721456050872803]
policies:[1, 3, 0]
qAverage:[0.0, 80.37284088134766]
ws:[5.557301729917526, 9.554961442947388]
memory len:10000
memory used:2738.0
now epsilon is 0.08631030849061806, the reward is 247.25 with loss [34.8211030960083, 31.605549812316895] in episode 1923
Report: 
rewardSum:247.25
loss:[34.8211030960083, 31.605549812316895]
policies:[0, 4, 0]
qAverage:[0.0, 75.27187728881836]
ws:[6.6511882692575455, 11.345705449581146]
memory len:10000
memory used:2738.0
now epsilon is 0.08622403054309907, the reward is 247.25 with loss [32.530580043792725, 33.383633613586426] in episode 1924
Report: 
rewardSum:247.25
loss:[32.530580043792725, 33.383633613586426]
policies:[0, 4, 0]
qAverage:[0.0, 80.44532623291016]
ws:[4.903275954723358, 8.06771366596222]
memory len:10000
memory used:2738.0
now epsilon is 0.08613783884117877, the reward is 247.25 with loss [24.659317016601562, 26.87436056137085] in episode 1925
Report: 
rewardSum:247.25
loss:[24.659317016601562, 26.87436056137085]
policies:[0, 4, 0]
qAverage:[0.0, 71.41946411132812]
ws:[0.8935787826776505, 3.091851830482483]
memory len:10000
memory used:2738.0
now epsilon is 0.0860517332986439, the reward is 247.25 with loss [32.49456286430359, 32.8598051071167] in episode 1926
Report: 
rewardSum:247.25
loss:[32.49456286430359, 32.8598051071167]
policies:[0, 4, 0]
qAverage:[0.0, 80.49092864990234]
ws:[3.94097980260849, 8.641793406009674]
memory len:10000
memory used:2739.0
now epsilon is 0.08596571382936737, the reward is 59.6875 with loss [28.77311897277832, 30.710983276367188] in episode 1927
Report: 
rewardSum:59.6875
loss:[28.77311897277832, 30.710983276367188]
policies:[0, 2, 2]
qAverage:[0.0, 66.77821604410808]
ws:[-0.5170511802037557, 2.570678234100342]
memory len:10000
memory used:2739.0
now epsilon is 0.08587978034730818, the reward is 247.25 with loss [37.94721555709839, 29.080899238586426] in episode 1928
Report: 
rewardSum:247.25
loss:[37.94721555709839, 29.080899238586426]
policies:[1, 3, 0]
qAverage:[0.0, 76.26585006713867]
ws:[2.661643162369728, 7.233060270547867]
memory len:10000
memory used:2739.0
now epsilon is 0.08579393276651136, the reward is 247.25 with loss [22.933157444000244, 20.774145126342773] in episode 1929
Report: 
rewardSum:247.25
loss:[22.933157444000244, 20.774145126342773]
policies:[0, 4, 0]
qAverage:[0.0, 81.1789337158203]
ws:[3.3110396027565003, 9.618650197982788]
memory len:10000
memory used:2739.0
now epsilon is 0.08570817100110785, the reward is 247.25 with loss [45.256351470947266, 29.440248489379883] in episode 1930
Report: 
rewardSum:247.25
loss:[45.256351470947266, 29.440248489379883]
policies:[1, 3, 0]
qAverage:[0.0, 78.14058685302734]
ws:[4.647709131240845, 11.830493330955505]
memory len:10000
memory used:2739.0
now epsilon is 0.08562249496531445, the reward is 247.25 with loss [33.91042423248291, 29.047781467437744] in episode 1931
Report: 
rewardSum:247.25
loss:[33.91042423248291, 29.047781467437744]
policies:[1, 3, 0]
qAverage:[0.0, 77.84821701049805]
ws:[3.5128336288034916, 8.969290971755981]
memory len:10000
memory used:2739.0
now epsilon is 0.08553690457343367, the reward is 247.25 with loss [35.8619270324707, 37.171786308288574] in episode 1932
Report: 
rewardSum:247.25
loss:[35.8619270324707, 37.171786308288574]
policies:[0, 4, 0]
qAverage:[0.0, 80.06895599365234]
ws:[3.1870319098234177, 9.175844097137452]
memory len:10000
memory used:2739.0
now epsilon is 0.08545139973985375, the reward is 247.25 with loss [26.864513635635376, 26.393003463745117] in episode 1933
Report: 
rewardSum:247.25
loss:[26.864513635635376, 26.393003463745117]
policies:[0, 4, 0]
qAverage:[0.0, 82.3569549560547]
ws:[2.0168363571166994, 7.191775393486023]
memory len:10000
memory used:2739.0
now epsilon is 0.08536598037904841, the reward is 247.25 with loss [32.50543403625488, 29.68094539642334] in episode 1934
Report: 
rewardSum:247.25
loss:[32.50543403625488, 29.68094539642334]
policies:[1, 3, 0]
qAverage:[0.0, 70.95595741271973]
ws:[-0.013326466083526611, 3.6085060983896255]
memory len:10000
memory used:2739.0
now epsilon is 0.08528064640557698, the reward is 247.25 with loss [35.01153564453125, 31.662453174591064] in episode 1935
Report: 
rewardSum:247.25
loss:[35.01153564453125, 31.662453174591064]
policies:[0, 3, 1]
qAverage:[0.0, 81.21473693847656]
ws:[1.9837640523910522, 7.467753052711487]
memory len:10000
memory used:2739.0
now epsilon is 0.0851953977340841, the reward is 247.25 with loss [32.66434049606323, 27.51531744003296] in episode 1936
Report: 
rewardSum:247.25
loss:[32.66434049606323, 27.51531744003296]
policies:[0, 4, 0]
qAverage:[0.0, 80.34210815429688]
ws:[2.205555388331413, 6.980966925621033]
memory len:10000
memory used:2739.0
now epsilon is 0.08511023427929981, the reward is 247.25 with loss [29.737546920776367, 34.47080945968628] in episode 1937
Report: 
rewardSum:247.25
loss:[29.737546920776367, 34.47080945968628]
policies:[0, 4, 0]
qAverage:[0.0, 80.36501617431641]
ws:[2.919852378964424, 7.55291006565094]
memory len:10000
memory used:2739.0
now epsilon is 0.08502515595603932, the reward is 247.25 with loss [29.082422256469727, 26.603157997131348] in episode 1938
Report: 
rewardSum:247.25
loss:[29.082422256469727, 26.603157997131348]
policies:[0, 3, 1]
qAverage:[0.0, 78.2851791381836]
ws:[4.263954937458038, 9.957705855369568]
memory len:10000
memory used:2739.0
now epsilon is 0.08494016267920304, the reward is 247.25 with loss [28.441545963287354, 21.348906993865967] in episode 1939
Report: 
rewardSum:247.25
loss:[28.441545963287354, 21.348906993865967]
policies:[0, 4, 0]
qAverage:[0.0, 79.50113677978516]
ws:[3.510395336151123, 7.578975248336792]
memory len:10000
memory used:2739.0
now epsilon is 0.0848340405501855, the reward is 246.25 with loss [38.56829309463501, 41.55556392669678] in episode 1940
Report: 
rewardSum:246.25
loss:[38.56829309463501, 41.55556392669678]
policies:[1, 3, 1]
qAverage:[0.0, 77.50985527038574]
ws:[5.096841216087341, 11.199511051177979]
memory len:10000
memory used:2739.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21*		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.08474923831709874, the reward is 247.25 with loss [20.03207540512085, 19.663448333740234] in episode 1941
Report: 
rewardSum:247.25
loss:[20.03207540512085, 19.663448333740234]
policies:[0, 4, 0]
qAverage:[0.0, 79.66400756835938]
ws:[3.423944914340973, 7.516957712173462]
memory len:10000
memory used:2739.0
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.08462219388555486, the reward is 245.25 with loss [37.67897963523865, 41.72835159301758] in episode 1942
Report: 
rewardSum:245.25
loss:[37.67897963523865, 41.72835159301758]
policies:[3, 1, 2]
qAverage:[0.0, 52.77238464355469]
ws:[0.3471963405609131, 3.255230188369751]
memory len:10000
memory used:2738.0
now epsilon is 0.08453760341970347, the reward is 247.25 with loss [23.03008770942688, 41.29475975036621] in episode 1943
Report: 
rewardSum:247.25
loss:[23.03008770942688, 41.29475975036621]
policies:[0, 3, 1]
qAverage:[0.0, 73.10270309448242]
ws:[3.4416266679763794, 7.8573514223098755]
memory len:10000
memory used:2739.0
now epsilon is 0.08445309751260179, the reward is 247.25 with loss [37.46641540527344, 25.756788730621338] in episode 1944
Report: 
rewardSum:247.25
loss:[37.46641540527344, 25.756788730621338]
policies:[0, 4, 0]
qAverage:[0.0, 79.90081481933593]
ws:[3.9103121131658556, 9.808062601089478]
memory len:10000
memory used:2738.0
now epsilon is 0.08436867607972277, the reward is 247.25 with loss [31.646428108215332, 28.99835777282715] in episode 1945
Report: 
rewardSum:247.25
loss:[31.646428108215332, 28.99835777282715]
policies:[0, 4, 0]
qAverage:[0.0, 73.03067016601562]
ws:[3.716001346707344, 8.11846148967743]
memory len:10000
memory used:2738.0
now epsilon is 0.08428433903662388, the reward is 247.25 with loss [18.32476282119751, 21.951098680496216] in episode 1946
Report: 
rewardSum:247.25
loss:[18.32476282119751, 21.951098680496216]
policies:[0, 4, 0]
qAverage:[0.0, 78.80479888916015]
ws:[3.9658913195133207, 9.669752645492554]
memory len:10000
memory used:2738.0
now epsilon is 0.08420008629894696, the reward is 247.25 with loss [26.152801036834717, 23.570500373840332] in episode 1947
Report: 
rewardSum:247.25
loss:[26.152801036834717, 23.570500373840332]
policies:[0, 3, 1]
qAverage:[0.0, 76.32988548278809]
ws:[3.9377378821372986, 8.394397377967834]
memory len:10000
memory used:2739.0
now epsilon is 0.08403183340284805, the reward is 243.25 with loss [54.79702663421631, 60.91734313964844] in episode 1948
Report: 
rewardSum:243.25
loss:[54.79702663421631, 60.91734313964844]
policies:[0, 6, 2]
qAverage:[0.0, 79.91697801862445]
ws:[3.536380112171173, 8.499281678880964]
memory len:10000
memory used:2739.0
now epsilon is 0.08394783307613107, the reward is 247.25 with loss [26.556710243225098, 27.06337070465088] in episode 1949
Report: 
rewardSum:247.25
loss:[26.556710243225098, 27.06337070465088]
policies:[0, 3, 1]
qAverage:[0.0, 76.72511672973633]
ws:[3.1459855809807777, 8.092736959457397]
memory len:10000
memory used:2739.0
now epsilon is 0.08386391671824595, the reward is 247.25 with loss [26.591479778289795, 32.56915092468262] in episode 1950
Report: 
rewardSum:247.25
loss:[26.591479778289795, 32.56915092468262]
policies:[0, 4, 0]
qAverage:[0.0, 76.58365631103516]
ws:[2.9363776326179503, 8.6087726354599]
memory len:10000
memory used:2739.0
now epsilon is 0.0837800842452553, the reward is 247.25 with loss [39.749505043029785, 21.0414879322052] in episode 1951
Report: 
rewardSum:247.25
loss:[39.749505043029785, 21.0414879322052]
policies:[1, 3, 0]
qAverage:[0.0, 71.9052791595459]
ws:[3.935569107532501, 9.71995609998703]
memory len:10000
memory used:2739.0
now epsilon is 0.08369633557330572, the reward is 247.25 with loss [42.516849517822266, 27.621224403381348] in episode 1952
Report: 
rewardSum:247.25
loss:[42.516849517822266, 27.621224403381348]
policies:[0, 3, 1]
qAverage:[0.0, 57.56298319498698]
ws:[1.4710696935653687, 6.195563793182373]
memory len:10000
memory used:2739.0
now epsilon is 0.08361267061862757, the reward is 247.25 with loss [35.49336814880371, 30.42690420150757] in episode 1953
Report: 
rewardSum:247.25
loss:[35.49336814880371, 30.42690420150757]
policies:[0, 4, 0]
qAverage:[0.0, 77.39757995605468]
ws:[4.129427787661553, 9.914131927490235]
memory len:10000
memory used:2739.0
now epsilon is 0.08352908929753496, the reward is 247.25 with loss [31.0994234085083, 29.59858465194702] in episode 1954
Report: 
rewardSum:247.25
loss:[31.0994234085083, 29.59858465194702]
policies:[0, 4, 0]
qAverage:[0.0, 76.8657211303711]
ws:[4.204156429320574, 10.124958753585815]
memory len:10000
memory used:2738.0
now epsilon is 0.08348732997345427, the reward is -1.0 with loss [21.408766746520996, 15.41288709640503] in episode 1955
Report: 
rewardSum:-1.0
loss:[21.408766746520996, 15.41288709640503]
policies:[0, 1, 1]
qAverage:[0.0, 41.53863525390625]
ws:[0.02887856401503086, 1.6178202629089355]
memory len:10000
memory used:2738.0
now epsilon is 0.08336217722178108, the reward is 245.25 with loss [40.37592363357544, 35.57922410964966] in episode 1956
Report: 
rewardSum:245.25
loss:[40.37592363357544, 35.57922410964966]
policies:[0, 5, 1]
qAverage:[0.0, 79.43858210245769]
ws:[3.7112738291422525, 8.856633822123209]
memory len:10000
memory used:2739.0
now epsilon is 0.08323721208194376, the reward is 245.25 with loss [63.531721115112305, 50.30695819854736] in episode 1957
Report: 
rewardSum:245.25
loss:[63.531721115112305, 50.30695819854736]
policies:[0, 5, 1]
qAverage:[0.0, 79.75913111368816]
ws:[4.701120535532634, 11.20610523223877]
memory len:10000
memory used:2739.0
now epsilon is 0.08315400607861437, the reward is 247.25 with loss [27.571381330490112, 31.844621658325195] in episode 1958
Report: 
rewardSum:247.25
loss:[27.571381330490112, 31.844621658325195]
policies:[0, 4, 0]
qAverage:[0.0, 75.99770965576172]
ws:[4.649607422947883, 10.734918022155762]
memory len:10000
memory used:2739.0
now epsilon is 0.08307088325009125, the reward is 247.25 with loss [29.95502281188965, 30.327210426330566] in episode 1959
Report: 
rewardSum:247.25
loss:[29.95502281188965, 30.327210426330566]
policies:[0, 4, 0]
qAverage:[0.0, 77.23719177246093]
ws:[3.936845624446869, 10.228941631317138]
memory len:10000
memory used:2739.0
now epsilon is 0.08298784351323078, the reward is 247.25 with loss [18.095040798187256, 26.64418649673462] in episode 1960
Report: 
rewardSum:247.25
loss:[18.095040798187256, 26.64418649673462]
policies:[0, 4, 0]
qAverage:[0.0, 74.53109588623047]
ws:[2.6879504024982452, 8.00665740966797]
memory len:10000
memory used:2739.0
now epsilon is 0.08290488678497246, the reward is 247.25 with loss [29.267925262451172, 29.204895496368408] in episode 1961
Report: 
rewardSum:247.25
loss:[29.267925262451172, 29.204895496368408]
policies:[0, 4, 0]
qAverage:[0.0, 76.74180603027344]
ws:[2.9351895451545715, 8.263487291336059]
memory len:10000
memory used:2739.0
now epsilon is 0.0828220129823388, the reward is 247.25 with loss [37.960376262664795, 34.8258843421936] in episode 1962
Report: 
rewardSum:247.25
loss:[37.960376262664795, 34.8258843421936]
policies:[0, 4, 0]
qAverage:[0.0, 68.32474899291992]
ws:[0.5011528879404068, 4.19961953163147]
memory len:10000
memory used:2739.0
now epsilon is 0.08273922202243529, the reward is 247.25 with loss [35.93790245056152, 42.81502151489258] in episode 1963
Report: 
rewardSum:247.25
loss:[35.93790245056152, 42.81502151489258]
policies:[1, 3, 0]
qAverage:[0.0, 68.08020210266113]
ws:[0.3572886064648628, 3.541123926639557]
memory len:10000
memory used:2739.0
now epsilon is 0.08265651382245025, the reward is 247.25 with loss [28.459450721740723, 25.508732557296753] in episode 1964
Report: 
rewardSum:247.25
loss:[28.459450721740723, 25.508732557296753]
policies:[0, 3, 1]
qAverage:[0.0, 73.3586483001709]
ws:[3.7376278191804886, 8.273566842079163]
memory len:10000
memory used:2739.0
now epsilon is 0.08257388829965479, the reward is 247.25 with loss [21.742851495742798, 26.801509380340576] in episode 1965
Report: 
rewardSum:247.25
loss:[21.742851495742798, 26.801509380340576]
policies:[0, 4, 0]
qAverage:[0.0, 76.4052734375]
ws:[2.6442833989858627, 6.618266177177429]
memory len:10000
memory used:2739.0
now epsilon is 0.08249134537140271, the reward is 247.25 with loss [30.926448822021484, 27.913068771362305] in episode 1966
Report: 
rewardSum:247.25
loss:[30.926448822021484, 27.913068771362305]
policies:[0, 4, 0]
qAverage:[0.0, 75.27438354492188]
ws:[2.7896236002445223, 6.099614548683166]
memory len:10000
memory used:2739.0
now epsilon is 0.08240888495513043, the reward is 247.25 with loss [23.14883017539978, 23.815550565719604] in episode 1967
Report: 
rewardSum:247.25
loss:[23.14883017539978, 23.815550565719604]
policies:[0, 3, 1]
qAverage:[0.0, 68.68498039245605]
ws:[-0.18661090172827244, 1.4418530315160751]
memory len:10000
memory used:2739.0
############# STATE ###############
0-		8-		16*		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.08232650696835693, the reward is 247.25 with loss [27.549057006835938, 34.99012327194214] in episode 1968
Report: 
rewardSum:247.25
loss:[27.549057006835938, 34.99012327194214]
policies:[0, 4, 0]
qAverage:[0.0, 76.36128082275391]
ws:[2.5894223153591156, 5.016889876127243]
memory len:10000
memory used:2739.0
now epsilon is 0.08224421132868362, the reward is 247.25 with loss [36.28055143356323, 27.930875301361084] in episode 1969
Report: 
rewardSum:247.25
loss:[36.28055143356323, 27.930875301361084]
policies:[0, 4, 0]
qAverage:[0.0, 71.38052558898926]
ws:[2.8186216354370117, 5.020099364221096]
memory len:10000
memory used:2739.0
now epsilon is 0.08216199795379425, the reward is 247.25 with loss [43.922383308410645, 34.6303129196167] in episode 1970
Report: 
rewardSum:247.25
loss:[43.922383308410645, 34.6303129196167]
policies:[0, 4, 0]
qAverage:[0.0, 74.57857131958008]
ws:[2.993869200348854, 5.132391333580017]
memory len:10000
memory used:2739.0
now epsilon is 0.0820798667614549, the reward is 247.25 with loss [34.83166742324829, 29.58301591873169] in episode 1971
Report: 
rewardSum:247.25
loss:[34.83166742324829, 29.58301591873169]
policies:[1, 3, 0]
qAverage:[0.0, 67.7664852142334]
ws:[0.40780482441186905, 1.4199745506048203]
memory len:10000
memory used:2739.0
now epsilon is 0.0819978176695138, the reward is 247.25 with loss [37.566243171691895, 35.17793273925781] in episode 1972
Report: 
rewardSum:247.25
loss:[37.566243171691895, 35.17793273925781]
policies:[0, 4, 0]
qAverage:[0.0, 74.89220123291015]
ws:[2.6828620970249175, 4.949542737007141]
memory len:10000
memory used:2739.0
now epsilon is 0.0819158505959014, the reward is 247.25 with loss [35.99606132507324, 28.378817558288574] in episode 1973
Report: 
rewardSum:247.25
loss:[35.99606132507324, 28.378817558288574]
policies:[1, 3, 0]
qAverage:[0.0, 71.90604591369629]
ws:[3.0383282601833344, 5.175683438777924]
memory len:10000
memory used:2739.0
now epsilon is 0.08183396545863006, the reward is 247.25 with loss [37.1928653717041, 29.454589366912842] in episode 1974
Report: 
rewardSum:247.25
loss:[37.1928653717041, 29.454589366912842]
policies:[0, 4, 0]
qAverage:[0.0, 74.76190338134765]
ws:[2.697950094938278, 5.425731563568116]
memory len:10000
memory used:2739.0
now epsilon is 0.08175216217579419, the reward is 247.25 with loss [31.833802700042725, 28.945591926574707] in episode 1975
Report: 
rewardSum:247.25
loss:[31.833802700042725, 28.945591926574707]
policies:[0, 4, 0]
qAverage:[0.0, 73.8708282470703]
ws:[1.8104524940252305, 3.799000844359398]
memory len:10000
memory used:2739.0
now epsilon is 0.08167044066557004, the reward is 247.25 with loss [25.422505378723145, 26.313176155090332] in episode 1976
Report: 
rewardSum:247.25
loss:[25.422505378723145, 26.313176155090332]
policies:[0, 4, 0]
qAverage:[0.0, 73.97891693115234]
ws:[2.531161594390869, 4.9425124168396]
memory len:10000
memory used:2740.0
now epsilon is 0.08158880084621566, the reward is 247.25 with loss [31.868206024169922, 25.91304874420166] in episode 1977
Report: 
rewardSum:247.25
loss:[31.868206024169922, 25.91304874420166]
policies:[0, 4, 0]
qAverage:[0.0, 72.58287811279297]
ws:[3.334666907787323, 6.456740379333496]
memory len:10000
memory used:2739.0
now epsilon is 0.08150724263607079, the reward is 247.25 with loss [28.399888515472412, 24.43478488922119] in episode 1978
Report: 
rewardSum:247.25
loss:[28.399888515472412, 24.43478488922119]
policies:[0, 3, 1]
qAverage:[0.0, 70.48889350891113]
ws:[3.168915033340454, 5.83703476190567]
memory len:10000
memory used:2739.0
now epsilon is 0.08138505815969044, the reward is 245.25 with loss [57.71005868911743, 41.48860263824463] in episode 1979
Report: 
rewardSum:245.25
loss:[57.71005868911743, 41.48860263824463]
policies:[0, 5, 1]
qAverage:[0.0, 77.7487424214681]
ws:[2.805822640657425, 6.615235010782878]
memory len:10000
memory used:2739.0
now epsilon is 0.08130370361584133, the reward is 247.25 with loss [22.522811889648438, 24.68147563934326] in episode 1980
Report: 
rewardSum:247.25
loss:[22.522811889648438, 24.68147563934326]
policies:[0, 4, 0]
qAverage:[0.0, 74.90547485351563]
ws:[2.310090261697769, 5.909890675544739]
memory len:10000
memory used:2739.0
now epsilon is 0.08122243039603319, the reward is 247.25 with loss [42.569148540496826, 33.164809703826904] in episode 1981
Report: 
rewardSum:247.25
loss:[42.569148540496826, 33.164809703826904]
policies:[0, 4, 0]
qAverage:[0.0, 74.86417236328126]
ws:[2.205996584892273, 5.794741892814637]
memory len:10000
memory used:2739.0
now epsilon is 0.08114123841897249, the reward is 247.25 with loss [32.01019048690796, 29.31990385055542] in episode 1982
Report: 
rewardSum:247.25
loss:[32.01019048690796, 29.31990385055542]
policies:[0, 4, 0]
qAverage:[0.0, 74.42689208984375]
ws:[2.448971521854401, 6.590046238899231]
memory len:10000
memory used:2739.0
now epsilon is 0.08106012760344691, the reward is 59.6875 with loss [33.10072994232178, 33.91493797302246] in episode 1983
Report: 
rewardSum:59.6875
loss:[33.10072994232178, 33.91493797302246]
policies:[0, 3, 1]
qAverage:[0.0, 67.30574607849121]
ws:[1.9320506006479263, 5.78951358795166]
memory len:10000
memory used:2739.0
now epsilon is 0.0809790978683254, the reward is 247.25 with loss [27.99267339706421, 44.69163799285889] in episode 1984
Report: 
rewardSum:247.25
loss:[27.99267339706421, 44.69163799285889]
policies:[0, 4, 0]
qAverage:[0.0, 69.88986396789551]
ws:[3.373231053352356, 7.706262946128845]
memory len:10000
memory used:2739.0
now epsilon is 0.0808981491325579, the reward is 247.25 with loss [28.785515785217285, 23.895856380462646] in episode 1985
Report: 
rewardSum:247.25
loss:[28.785515785217285, 23.895856380462646]
policies:[0, 4, 0]
qAverage:[0.0, 74.0044189453125]
ws:[4.340423965454102, 9.504416847229004]
memory len:10000
memory used:2739.0
now epsilon is 0.08081728131517546, the reward is 247.25 with loss [39.26873826980591, 27.83961296081543] in episode 1986
Report: 
rewardSum:247.25
loss:[39.26873826980591, 27.83961296081543]
policies:[0, 3, 1]
qAverage:[0.0, 61.074851989746094]
ws:[2.7678140004475913, 7.8892927169799805]
memory len:10000
memory used:2739.0
now epsilon is 0.08073649433529004, the reward is 247.25 with loss [35.76683807373047, 30.455795288085938] in episode 1987
Report: 
rewardSum:247.25
loss:[35.76683807373047, 30.455795288085938]
policies:[0, 4, 0]
qAverage:[0.0, 73.82028045654297]
ws:[3.3602886736392974, 7.931192922592163]
memory len:10000
memory used:2739.0
now epsilon is 0.08065578811209442, the reward is 247.25 with loss [26.5327091217041, 28.212952375411987] in episode 1988
Report: 
rewardSum:247.25
loss:[26.5327091217041, 28.212952375411987]
policies:[0, 4, 0]
qAverage:[0.0, 73.50625152587891]
ws:[2.9023964762687684, 7.268768548965454]
memory len:10000
memory used:2739.0
now epsilon is 0.08057516256486222, the reward is 247.25 with loss [23.657028675079346, 28.222833156585693] in episode 1989
Report: 
rewardSum:247.25
loss:[23.657028675079346, 28.222833156585693]
policies:[0, 4, 0]
qAverage:[0.0, 73.92222900390625]
ws:[2.311840298771858, 6.989455473423004]
memory len:10000
memory used:2740.0
now epsilon is 0.0804946176129477, the reward is 247.25 with loss [31.010623455047607, 29.861773014068604] in episode 1990
Report: 
rewardSum:247.25
loss:[31.010623455047607, 29.861773014068604]
policies:[0, 4, 0]
qAverage:[0.0, 72.93096466064453]
ws:[0.9495704293251037, 4.651143116876483]
memory len:10000
memory used:2740.0
now epsilon is 0.08041415317578576, the reward is 247.25 with loss [33.12667465209961, 23.551782608032227] in episode 1991
Report: 
rewardSum:247.25
loss:[33.12667465209961, 23.551782608032227]
policies:[0, 4, 0]
qAverage:[0.0, 74.15119934082031]
ws:[2.5067604541778565, 8.025839018821717]
memory len:10000
memory used:2740.0
now epsilon is 0.08033376917289185, the reward is 247.25 with loss [26.82830238342285, 34.52423572540283] in episode 1992
Report: 
rewardSum:247.25
loss:[26.82830238342285, 34.52423572540283]
policies:[0, 4, 0]
qAverage:[0.0, 73.47571563720703]
ws:[3.637240195274353, 9.591877269744874]
memory len:10000
memory used:2740.0
now epsilon is 0.08025346552386187, the reward is 247.25 with loss [28.41481351852417, 21.935322999954224] in episode 1993
Report: 
rewardSum:247.25
loss:[28.41481351852417, 21.935322999954224]
policies:[0, 4, 0]
qAverage:[0.0, 73.57272033691406]
ws:[3.122253696620464, 8.040735101699829]
memory len:10000
memory used:2740.0
now epsilon is 0.08017324214837206, the reward is 247.25 with loss [31.96978759765625, 40.12372970581055] in episode 1994
Report: 
rewardSum:247.25
loss:[31.96978759765625, 40.12372970581055]
policies:[1, 3, 0]
qAverage:[0.0, 72.81373405456543]
ws:[4.159247487783432, 9.137103915214539]
memory len:10000
memory used:2740.0
now epsilon is 0.08009309896617899, the reward is 247.25 with loss [20.742329835891724, 31.23204755783081] in episode 1995
Report: 
rewardSum:247.25
loss:[20.742329835891724, 31.23204755783081]
policies:[0, 4, 0]
qAverage:[0.0, 73.26130828857421]
ws:[3.9038533478975297, 9.910711097717286]
memory len:10000
memory used:2740.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2*		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.08001303589711943, the reward is 247.25 with loss [23.96653461456299, 34.12568283081055] in episode 1996
Report: 
rewardSum:247.25
loss:[23.96653461456299, 34.12568283081055]
policies:[0, 4, 0]
qAverage:[0.0, 74.15100860595703]
ws:[3.627716875076294, 9.789380311965942]
memory len:10000
memory used:2740.0
now epsilon is 0.07993305286111028, the reward is 247.25 with loss [32.35626554489136, 30.008833408355713] in episode 1997
Report: 
rewardSum:247.25
loss:[32.35626554489136, 30.008833408355713]
policies:[1, 3, 0]
qAverage:[0.0, 61.976531982421875]
ws:[3.591898481051127, 7.622238477071126]
memory len:10000
memory used:2740.0
now epsilon is 0.07985314977814852, the reward is 247.25 with loss [23.178534030914307, 31.064002513885498] in episode 1998
Report: 
rewardSum:247.25
loss:[23.178534030914307, 31.064002513885498]
policies:[0, 4, 0]
qAverage:[0.0, 74.23810424804688]
ws:[3.1199867606163023, 9.947090864181519]
memory len:10000
memory used:2740.0
now epsilon is 0.0797334448908598, the reward is 245.25 with loss [49.30842423439026, 40.459508419036865] in episode 1999
Report: 
rewardSum:245.25
loss:[49.30842423439026, 40.459508419036865]
policies:[0, 5, 1]
qAverage:[0.0, 76.59412002563477]
ws:[3.6786962747573853, 11.618080417315165]
memory len:10000
memory used:2740.0
Traceback (most recent call last):
  File "agentRunner.py", line 57, in <module>
    agent.train()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 234, in train
    self.env.close()
AttributeError: 'DeepSeaTreasure' object has no attribute 'close'
pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)
Hello from the pygame community. https://www.pygame.org/contribute.html
2021-03-25 14:17:44.892075: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
2021-03-25 14:17:46.678878: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 14:17:46.679695: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-25 14:17:47.661873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0001:00:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 149.31GiB/s
2021-03-25 14:17:47.661949: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 14:17:47.667486: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 14:17:47.667557: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 14:17:47.668699: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-25 14:17:47.669475: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-25 14:17:47.670521: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-25 14:17:47.671393: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-25 14:17:47.671562: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 14:17:47.672767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-25 14:17:47.673387: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 14:17:47.674006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0001:00:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 149.31GiB/s
2021-03-25 14:17:47.674045: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 14:17:47.674077: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 14:17:47.674107: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 14:17:47.674137: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-25 14:17:47.674167: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-25 14:17:47.674197: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-25 14:17:47.674227: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-25 14:17:47.674257: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 14:17:47.676651: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-25 14:17:47.676718: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 14:17:48.233171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-25 14:17:48.233240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-25 14:17:48.233253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-25 14:17:48.235116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7468 MB memory) -> physical GPU (device: 0, name: Tesla M60, pci bus id: 0001:00:00.0, compute capability: 5.2)
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.9990003749375039, the reward is -3.0 with loss [0, 0] in episode 0
No handles with labels found to put in legend.
Report: 
rewardSum:-3.0
loss:[0, 0]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:8
memory used:2000.0
now epsilon is 0.996754870534042, the reward is -8.0 with loss [0, 0] in episode 1
Report: 
rewardSum:-8.0
loss:[0, 0]
policies:[0, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:26
memory used:2001.0
2021-03-25 14:17:49.071674: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 14:17:49.851887: W tensorflow/stream_executor/gpu/asm_compiler.cc:98] *** WARNING *** You are using ptxas 9.1.108, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You do not need to update to CUDA 9.2.88; cherry-picking the ptxas binary is sufficient.
2021-03-25 14:17:49.902449: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 14:17:50.131535: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 14:17:50.540615: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-03-25 14:17:50.543118: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2596990000 Hz
now epsilon is 0.9940172184164668, the reward is 187.33333333333334 with loss [6.576150983572006, 1.9429068230092525] in episode 2
Report: 
rewardSum:187.33333333333334
loss:[6.576150983572006, 1.9429068230092525]
policies:[0, 0, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:48
memory used:2705.0
now epsilon is 0.9925271241694124, the reward is 44.33333333333334 with loss [9.165148347616196, 9.933386132121086] in episode 3
Report: 
rewardSum:44.33333333333334
loss:[9.165148347616196, 9.933386132121086]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:60
memory used:2702.0
now epsilon is 0.9912870854385903, the reward is -4.0 with loss [4.125437945127487, 9.920855522155762] in episode 4
Report: 
rewardSum:-4.0
loss:[4.125437945127487, 9.920855522155762]
policies:[1, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:70
memory used:2702.0
now epsilon is 0.9890589185917195, the reward is 436.0 with loss [16.988567356020212, 21.687040392309427] in episode 5
Report: 
rewardSum:436.0
loss:[16.988567356020212, 21.687040392309427]
policies:[0, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:88
memory used:2705.0
now epsilon is 0.9875762571475453, the reward is 192.33333333333334 with loss [23.47190484404564, 24.613416075706482] in episode 6
Report: 
rewardSum:192.33333333333334
loss:[23.47190484404564, 24.613416075706482]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:100
memory used:2703.0
now epsilon is 0.9848638146889512, the reward is 187.33333333333334 with loss [31.406845927238464, 45.334065556526184] in episode 7
Report: 
rewardSum:187.33333333333334
loss:[31.406845927238464, 45.334065556526184]
policies:[0, 0, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:122
memory used:2721.0
now epsilon is 0.9843714443355952, the reward is -1.0 with loss [9.79931354522705, 8.193555444478989] in episode 8
Report: 
rewardSum:-1.0
loss:[9.79931354522705, 8.193555444478989]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:126
memory used:2721.0
now epsilon is 0.9814223871529224, the reward is 988.0 with loss [71.90636284276843, 49.314468182623386] in episode 9
Report: 
rewardSum:988.0
loss:[71.90636284276843, 49.314468182623386]
policies:[0, 0, 12]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:150
memory used:2726.0
now epsilon is 0.9760387097218384, the reward is 176.33333333333334 with loss [174.79469096660614, 153.6990498751402] in episode 10
Report: 
rewardSum:176.33333333333334
loss:[174.79469096660614, 153.6990498751402]
policies:[0, 1, 21]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:194
memory used:2727.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20*		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.9728713377450043, the reward is -12.0 with loss [84.8212111145258, 98.2036337275058] in episode 11
Report: 
rewardSum:-12.0
loss:[84.8212111145258, 98.2036337275058]
policies:[0, 0, 13]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:220
memory used:2727.0
now epsilon is 0.9721418666398703, the reward is -2.0 with loss [25.413888931274414, 19.51859676837921] in episode 12
Report: 
rewardSum:-2.0
loss:[25.413888931274414, 19.51859676837921]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:226
memory used:2727.0
now epsilon is 0.969471815739227, the reward is -10.0 with loss [45.15927104651928, 48.196825575083494] in episode 13
Report: 
rewardSum:-10.0
loss:[45.15927104651928, 48.196825575083494]
policies:[0, 0, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:248
memory used:2727.0
now epsilon is 0.9680185165925426, the reward is 192.33333333333334 with loss [24.687346659600735, 36.68366551399231] in episode 14
Report: 
rewardSum:192.33333333333334
loss:[24.687346659600735, 36.68366551399231]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:260
memory used:2726.0
now epsilon is 0.9663257541835559, the reward is 438.0 with loss [61.67744195461273, 23.887616395950317] in episode 15
Report: 
rewardSum:438.0
loss:[61.67744195461273, 23.887616395950317]
policies:[0, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:274
memory used:2726.0
now epsilon is 0.9653597907411388, the reward is 194.33333333333334 with loss [43.239638328552246, 25.807148218154907] in episode 16
Report: 
rewardSum:194.33333333333334
loss:[43.239638328552246, 25.807148218154907]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:282
memory used:2726.0
now epsilon is 0.9615055833365661, the reward is 429.0 with loss [138.3625196069479, 132.80331411212683] in episode 17
Report: 
rewardSum:429.0
loss:[138.3625196069479, 132.80331411212683]
policies:[0, 0, 16]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:314
memory used:2727.0
now epsilon is 0.9607846344163371, the reward is -2.0 with loss [12.598486073315144, 25.816625595092773] in episode 18
Report: 
rewardSum:-2.0
loss:[12.598486073315144, 25.816625595092773]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:320
memory used:2727.0
now epsilon is 0.9586250294937709, the reward is 41.33333333333334 with loss [55.78203308582306, 75.23836863040924] in episode 19
Report: 
rewardSum:41.33333333333334
loss:[55.78203308582306, 75.23836863040924]
policies:[0, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:338
memory used:2727.0
now epsilon is 0.9581457768930884, the reward is -1.0 with loss [17.245591163635254, 10.24790096282959] in episode 20
Report: 
rewardSum:-1.0
loss:[17.245591163635254, 10.24790096282959]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:342
memory used:2727.0
now epsilon is 0.9567094561900504, the reward is -5.0 with loss [19.208119191229343, 55.705040007829666] in episode 21
Report: 
rewardSum:-5.0
loss:[19.208119191229343, 55.705040007829666]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:354
memory used:2727.0
now epsilon is 0.9550364697998095, the reward is 191.33333333333334 with loss [40.21124917268753, 56.15068030357361] in episode 22
Report: 
rewardSum:191.33333333333334
loss:[40.21124917268753, 56.15068030357361]
policies:[0, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:368
memory used:2727.0
now epsilon is 0.954559011254689, the reward is -1.0 with loss [15.64592456817627, 9.446901455521584] in episode 23
Report: 
rewardSum:-1.0
loss:[15.64592456817627, 9.446901455521584]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:372
memory used:2727.0
now epsilon is 0.9528897853218017, the reward is 438.0 with loss [67.05530190467834, 33.2682282589376] in episode 24
Report: 
rewardSum:438.0
loss:[67.05530190467834, 33.2682282589376]
policies:[0, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:386
memory used:2727.0
now epsilon is 0.949560085217993, the reward is 986.0 with loss [92.7594038657844, 113.14398367330432] in episode 25
Report: 
rewardSum:986.0
loss:[92.7594038657844, 113.14398367330432]
policies:[0, 0, 14]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:414
memory used:2727.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20*		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.9478996008473125, the reward is -6.0 with loss [86.14085841178894, 78.17233419418335] in episode 26
Report: 
rewardSum:-6.0
loss:[86.14085841178894, 78.17233419418335]
policies:[0, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:428
memory used:2727.0
now epsilon is 0.9457689582758565, the reward is 189.33333333333334 with loss [76.52995479106903, 77.29064631462097] in episode 27
Report: 
rewardSum:189.33333333333334
loss:[76.52995479106903, 77.29064631462097]
policies:[0, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:446
memory used:2726.0
now epsilon is 0.943407194082821, the reward is 435.0 with loss [58.53115935251117, 90.0027756690979] in episode 28
Report: 
rewardSum:435.0
loss:[58.53115935251117, 90.0027756690979]
policies:[0, 0, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:466
memory used:2732.0
now epsilon is 0.9422285245723249, the reward is -4.0 with loss [42.59883439540863, 40.53670966811478] in episode 29
Report: 
rewardSum:-4.0
loss:[42.59883439540863, 40.53670966811478]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:476
memory used:2732.0
now epsilon is 0.9405808608141095, the reward is 438.0 with loss [56.58586394786835, 39.40651798248291] in episode 30
Report: 
rewardSum:438.0
loss:[56.58586394786835, 39.40651798248291]
policies:[0, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:490
memory used:2732.0
now epsilon is 0.9389360783058134, the reward is 43.33333333333334 with loss [47.273961782455444, 41.85882902145386] in episode 31
Report: 
rewardSum:43.33333333333334
loss:[47.273961782455444, 41.85882902145386]
policies:[0, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:504
memory used:2732.0
now epsilon is 0.9379974942698571, the reward is 194.33333333333334 with loss [26.555185079574585, 14.066897928714752] in episode 32
Report: 
rewardSum:194.33333333333334
loss:[26.555185079574585, 14.066897928714752]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:512
memory used:2732.0
now epsilon is 0.9368255835039101, the reward is 45.33333333333334 with loss [15.272944211959839, 30.092720694839954] in episode 33
Report: 
rewardSum:45.33333333333334
loss:[15.272944211959839, 30.092720694839954]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:522
memory used:2732.0
now epsilon is 0.9333186288284107, the reward is 985.0 with loss [137.67554716020823, 143.86717700958252] in episode 34
Report: 
rewardSum:985.0
loss:[137.67554716020823, 143.86717700958252]
policies:[0, 0, 15]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:552
memory used:2733.0
now epsilon is 0.9309879554657764, the reward is 40.33333333333334 with loss [92.56375332921743, 74.20921463519335] in episode 35
Report: 
rewardSum:40.33333333333334
loss:[92.56375332921743, 74.20921463519335]
policies:[0, 0, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:572
memory used:2734.0
now epsilon is 0.9305225196747907, the reward is -1.0 with loss [0.8988039828836918, 6.643113017082214] in episode 36
Report: 
rewardSum:-1.0
loss:[0.8988039828836918, 6.643113017082214]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:576
memory used:2734.0
now epsilon is 0.9298248022434678, the reward is -2.0 with loss [34.75465536117554, 38.2059166431427] in episode 37
Report: 
rewardSum:-2.0
loss:[34.75465536117554, 38.2059166431427]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:582
memory used:2734.0
now epsilon is 0.9270391601695713, the reward is 988.0 with loss [120.10806775093079, 113.44522007927299] in episode 38
Report: 
rewardSum:988.0
loss:[120.10806775093079, 113.44522007927299]
policies:[0, 0, 12]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:606
memory used:2734.0
now epsilon is 0.9251867033568567, the reward is 42.33333333333334 with loss [66.54839762672782, 51.571382999420166] in episode 39
Report: 
rewardSum:42.33333333333334
loss:[66.54839762672782, 51.571382999420166]
policies:[1, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:622
memory used:2733.0
now epsilon is 0.9242618635406932, the reward is -3.0 with loss [32.388264656066895, 35.89704418182373] in episode 40
Report: 
rewardSum:-3.0
loss:[32.388264656066895, 35.89704418182373]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:630
memory used:2733.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39*		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.9228763369521017, the reward is 994.0 with loss [42.214701771736145, 33.25554096698761] in episode 41
Report: 
rewardSum:994.0
loss:[42.214701771736145, 33.25554096698761]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:642
memory used:2733.0
now epsilon is 0.9224149564633968, the reward is -1.0 with loss [10.994020462036133, 12.581926167011261] in episode 42
Report: 
rewardSum:-1.0
loss:[10.994020462036133, 12.581926167011261]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:646
memory used:2733.0
now epsilon is 0.9219538066360998, the reward is -1.0 with loss [23.751660346984863, 37.567813873291016] in episode 43
Report: 
rewardSum:-1.0
loss:[23.751660346984863, 37.567813873291016]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:650
memory used:2733.0
now epsilon is 0.9210321985045228, the reward is -3.0 with loss [37.49287506937981, 45.65855550765991] in episode 44
Report: 
rewardSum:-3.0
loss:[37.49287506937981, 45.65855550765991]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:658
memory used:2733.0
now epsilon is 0.920571739969783, the reward is -1.0 with loss [16.823758602142334, 9.843567229807377] in episode 45
Report: 
rewardSum:-1.0
loss:[16.823758602142334, 9.843567229807377]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:662
memory used:2733.0
now epsilon is 0.9198814837576231, the reward is -2.0 with loss [10.182357907295227, 34.08666944503784] in episode 46
Report: 
rewardSum:-2.0
loss:[10.182357907295227, 34.08666944503784]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:668
memory used:2733.0
now epsilon is 0.9173549693995063, the reward is 187.33333333333334 with loss [102.87525677680969, 99.32884812355042] in episode 47
Report: 
rewardSum:187.33333333333334
loss:[102.87525677680969, 99.32884812355042]
policies:[0, 2, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:690
memory used:2733.0
now epsilon is 0.9141494392491258, the reward is 431.0 with loss [128.4281667508185, 98.08066450431943] in episode 48
Report: 
rewardSum:431.0
loss:[128.4281667508185, 98.08066450431943]
policies:[0, 0, 14]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:718
memory used:2733.0
now epsilon is 0.9136924216638413, the reward is -1.0 with loss [12.452973365783691, 10.640189409255981] in episode 49
Report: 
rewardSum:-1.0
loss:[12.452973365783691, 10.640189409255981]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:722
memory used:2733.0
now epsilon is 0.9125508770517786, the reward is -4.0 with loss [47.41874122619629, 28.37838751077652] in episode 50
Report: 
rewardSum:-4.0
loss:[47.41874122619629, 28.37838751077652]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:732
memory used:2733.0
now epsilon is 0.911638668324275, the reward is 194.33333333333334 with loss [36.63254940509796, 36.776716232299805] in episode 51
Report: 
rewardSum:194.33333333333334
loss:[36.63254940509796, 36.776716232299805]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:740
memory used:2733.0
now epsilon is 0.9111829059675296, the reward is -1.0 with loss [9.854532852768898, 32.166114807128906] in episode 52
Report: 
rewardSum:-1.0
loss:[9.854532852768898, 32.166114807128906]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:744
memory used:2733.0
now epsilon is 0.909589531311472, the reward is 43.33333333333334 with loss [65.11516427993774, 79.62048079818487] in episode 53
Report: 
rewardSum:43.33333333333334
loss:[65.11516427993774, 79.62048079818487]
policies:[1, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:758
memory used:2734.0
now epsilon is 0.9070912844705161, the reward is 434.0 with loss [174.2863163948059, 76.26142621040344] in episode 54
Report: 
rewardSum:434.0
loss:[174.2863163948059, 76.26142621040344]
policies:[0, 0, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:780
memory used:2734.0
now epsilon is 0.9055050647840622, the reward is 43.33333333333334 with loss [58.945186495780945, 63.153454184532166] in episode 55
Report: 
rewardSum:43.33333333333334
loss:[58.945186495780945, 63.153454184532166]
policies:[0, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:794
memory used:2734.0
now epsilon is 0.9050523688457367, the reward is -1.0 with loss [11.447928234934807, 19.855796813964844] in episode 56
Report: 
rewardSum:-1.0
loss:[11.447928234934807, 19.855796813964844]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:798
memory used:2734.0
now epsilon is 0.902792281687179, the reward is 188.33333333333334 with loss [63.6664674282074, 90.46445578336716] in episode 57
Report: 
rewardSum:188.33333333333334
loss:[63.6664674282074, 90.46445578336716]
policies:[0, 0, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:818
memory used:2733.0
now epsilon is 0.9018898278961766, the reward is 46.33333333333334 with loss [24.541983485221863, 34.17046892642975] in episode 58
Report: 
rewardSum:46.33333333333334
loss:[24.541983485221863, 34.17046892642975]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:826
memory used:2733.0
now epsilon is 0.9014389393503428, the reward is -1.0 with loss [5.536375284194946, 25.86457347869873] in episode 59
Report: 
rewardSum:-1.0
loss:[5.536375284194946, 25.86457347869873]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:830
memory used:2732.0
now epsilon is 0.8998626038522365, the reward is -6.0 with loss [37.62518307566643, 73.96222066879272] in episode 60
Report: 
rewardSum:-6.0
loss:[37.62518307566643, 73.96222066879272]
policies:[0, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:844
memory used:2732.0
now epsilon is 0.8991878756095252, the reward is -2.0 with loss [20.309014916419983, 22.160752534866333] in episode 61
Report: 
rewardSum:-2.0
loss:[20.309014916419983, 22.160752534866333]
policies:[0, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:850
memory used:2733.0
now epsilon is 0.8987383378709627, the reward is -1.0 with loss [17.9367356300354, 12.170906007289886] in episode 62
Report: 
rewardSum:-1.0
loss:[17.9367356300354, 12.170906007289886]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:854
memory used:2733.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20*		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.8962698945389524, the reward is -10.0 with loss [62.45339998602867, 77.80795346945524] in episode 63
Report: 
rewardSum:-10.0
loss:[62.45339998602867, 77.80795346945524]
policies:[1, 1, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:876
memory used:2733.0
now epsilon is 0.8944789224381992, the reward is 992.0 with loss [58.0328773856163, 86.83975386619568] in episode 64
Report: 
rewardSum:992.0
loss:[58.0328773856163, 86.83975386619568]
policies:[1, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:892
memory used:2732.0
now epsilon is 0.8935847788894555, the reward is -3.0 with loss [28.106814861297607, 40.25066964328289] in episode 65
Report: 
rewardSum:-3.0
loss:[28.106814861297607, 40.25066964328289]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:900
memory used:2732.0
now epsilon is 0.8922452391776589, the reward is 192.33333333333334 with loss [64.32330513000488, 32.74260139465332] in episode 66
Report: 
rewardSum:192.33333333333334
loss:[64.32330513000488, 32.74260139465332]
policies:[0, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:912
memory used:2739.0
now epsilon is 0.8917991723233974, the reward is -1.0 with loss [14.584857940673828, 7.131185173988342] in episode 67
Report: 
rewardSum:-1.0
loss:[14.584857940673828, 7.131185173988342]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:916
memory used:2739.0
now epsilon is 0.8904623093480015, the reward is -5.0 with loss [8.954040989279747, 84.14420437812805] in episode 68
Report: 
rewardSum:-5.0
loss:[8.954040989279747, 84.14420437812805]
policies:[1, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:928
memory used:2738.0
now epsilon is 0.8897946295637601, the reward is -2.0 with loss [41.850951328873634, 37.12415969371796] in episode 69
Report: 
rewardSum:-2.0
loss:[41.850951328873634, 37.12415969371796]
policies:[1, 0, 2]
qAverage:[-0.12935678660869598, 0.0]
ws:[1.6104736328125, 0.7970904111862183]
memory len:934
memory used:2738.0
now epsilon is 0.8893497878611426, the reward is -1.0 with loss [21.898467540740967, 15.78290605545044] in episode 70
Report: 
rewardSum:-1.0
loss:[21.898467540740967, 15.78290605545044]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:938
memory used:2738.0
now epsilon is 0.8875726438696112, the reward is 437.0 with loss [45.365465082228184, 72.93611168861389] in episode 71
Report: 
rewardSum:437.0
loss:[45.365465082228184, 72.93611168861389]
policies:[0, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:954
memory used:2738.0
now epsilon is 0.8871289130209667, the reward is -1.0 with loss [30.786497592926025, 3.4587316513061523] in episode 72
Report: 
rewardSum:-1.0
loss:[30.786497592926025, 3.4587316513061523]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:958
memory used:2738.0
now epsilon is 0.8862421167258462, the reward is -3.0 with loss [35.19176959991455, 32.70391368865967] in episode 73
Report: 
rewardSum:-3.0
loss:[35.19176959991455, 32.70391368865967]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:966
memory used:2738.0
now epsilon is 0.8846923557298118, the reward is 191.33333333333334 with loss [49.87477779388428, 73.50245642662048] in episode 74
Report: 
rewardSum:191.33333333333334
loss:[49.87477779388428, 73.50245642662048]
policies:[0, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:980
memory used:2739.0
now epsilon is 0.8840290023290079, the reward is -2.0 with loss [24.055773735046387, 30.223418712615967] in episode 75
Report: 
rewardSum:-2.0
loss:[24.055773735046387, 30.223418712615967]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:986
memory used:2739.0
now epsilon is 0.8816009591443008, the reward is 989.0 with loss [74.03774404525757, 84.67738059163094] in episode 76
Report: 
rewardSum:989.0
loss:[74.03774404525757, 84.67738059163094]
policies:[0, 1, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1008
memory used:2739.0
now epsilon is 0.8796193394317169, the reward is 991.0 with loss [71.8411503881216, 84.001962184906] in episode 77
Report: 
rewardSum:991.0
loss:[71.8411503881216, 84.001962184906]
policies:[1, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1026
memory used:2740.0
now epsilon is 0.8787400498945648, the reward is -3.0 with loss [17.82214280962944, 60.71598148345947] in episode 78
Report: 
rewardSum:-3.0
loss:[17.82214280962944, 60.71598148345947]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1034
memory used:2740.0
now epsilon is 0.8774227633639651, the reward is 994.0 with loss [51.55218994617462, 60.51714897155762] in episode 79
Report: 
rewardSum:994.0
loss:[51.55218994617462, 60.51714897155762]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1046
memory used:2740.0
now epsilon is 0.8758884246657346, the reward is 191.33333333333334 with loss [61.550254106521606, 69.84946286678314] in episode 80
Report: 
rewardSum:191.33333333333334
loss:[61.550254106521606, 69.84946286678314]
policies:[1, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1060
memory used:2740.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2*		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.8743567690472454, the reward is -6.0 with loss [42.432582676410675, 25.74391284584999] in episode 81
Report: 
rewardSum:-6.0
loss:[42.432582676410675, 25.74391284584999]
policies:[0, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1074
memory used:2740.0
now epsilon is 0.8734827401073427, the reward is -3.0 with loss [32.269935965538025, 48.752041190862656] in episode 82
Report: 
rewardSum:-3.0
loss:[32.269935965538025, 48.752041190862656]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1082
memory used:2740.0
now epsilon is 0.8715193681322503, the reward is 436.0 with loss [113.32150316238403, 84.41830557584763] in episode 83
Report: 
rewardSum:436.0
loss:[113.32150316238403, 84.41830557584763]
policies:[1, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1100
memory used:2740.0
now epsilon is 0.8704305134855324, the reward is -4.0 with loss [53.88227987289429, 26.68678492307663] in episode 84
Report: 
rewardSum:-4.0
loss:[53.88227987289429, 26.68678492307663]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1110
memory used:2740.0
now epsilon is 0.8699953526306968, the reward is -1.0 with loss [22.7183780670166, 2.089550018310547] in episode 85
Report: 
rewardSum:-1.0
loss:[22.7183780670166, 2.089550018310547]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1114
memory used:2740.0
now epsilon is 0.8684740021568338, the reward is 191.33333333333334 with loss [62.424471378326416, 53.1688557267189] in episode 86
Report: 
rewardSum:191.33333333333334
loss:[62.424471378326416, 53.1688557267189]
policies:[0, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1128
memory used:2746.0
now epsilon is 0.8680398194353806, the reward is -1.0 with loss [9.550286769866943, 50.87748622894287] in episode 87
Report: 
rewardSum:-1.0
loss:[9.550286769866943, 50.87748622894287]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1132
memory used:2746.0
now epsilon is 0.8676058537781516, the reward is -1.0 with loss [21.55772304534912, 28.233882427215576] in episode 88
Report: 
rewardSum:-1.0
loss:[21.55772304534912, 28.233882427215576]
policies:[2, 0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1136
memory used:2746.0
now epsilon is 0.8660886817923698, the reward is 191.33333333333334 with loss [55.98823094367981, 34.5667684674263] in episode 89
Report: 
rewardSum:191.33333333333334
loss:[55.98823094367981, 34.5667684674263]
policies:[0, 1, 6]
qAverage:[0.0, 8.889304161071777]
ws:[2.3707969188690186, 2.6164321899414062]
memory len:1150
memory used:2746.0
now epsilon is 0.862630815158192, the reward is 984.0 with loss [110.0620388686657, 114.06970989704132] in episode 90
Report: 
rewardSum:984.0
loss:[110.0620388686657, 114.06970989704132]
policies:[2, 0, 14]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1182
memory used:2746.0
now epsilon is 0.8615530656487346, the reward is 45.33333333333334 with loss [67.73073387145996, 28.35159981250763] in episode 91
Report: 
rewardSum:45.33333333333334
loss:[67.73073387145996, 28.35159981250763]
policies:[1, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1192
memory used:2746.0
now epsilon is 0.860046478101204, the reward is 43.33333333333334 with loss [75.37323820590973, 55.26651358604431] in episode 92
Report: 
rewardSum:43.33333333333334
loss:[75.37323820590973, 55.26651358604431]
policies:[0, 2, 5]
qAverage:[0.0, 7.617030223210652]
ws:[-1.3200275500615437, 0.26357237497965497]
memory len:1206
memory used:2747.0
now epsilon is 0.8585425251053093, the reward is 191.33333333333334 with loss [67.18998920917511, 78.69474446773529] in episode 93
Report: 
rewardSum:191.33333333333334
loss:[67.18998920917511, 78.69474446773529]
policies:[1, 0, 6]
qAverage:[17.51320457458496, 0.0]
ws:[-9.565372467041016, -10.439584732055664]
memory len:1220
memory used:2747.0
now epsilon is 0.8576843044799954, the reward is 46.33333333333334 with loss [40.27569270133972, 34.78078579902649] in episode 94
Report: 
rewardSum:46.33333333333334
loss:[40.27569270133972, 34.78078579902649]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1228
memory used:2747.0
now epsilon is 0.855756443459312, the reward is 41.33333333333334 with loss [80.90813720226288, 97.03500628471375] in episode 95
Report: 
rewardSum:41.33333333333334
loss:[80.90813720226288, 97.03500628471375]
policies:[0, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1246
memory used:2747.0
now epsilon is 0.8527661581582067, the reward is 184.33333333333334 with loss [104.64382362365723, 91.2315956056118] in episode 96
Report: 
rewardSum:184.33333333333334
loss:[104.64382362365723, 91.2315956056118]
policies:[0, 1, 13]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1274
memory used:2748.0
now epsilon is 0.8514878081228032, the reward is 44.33333333333334 with loss [34.37454271316528, 75.44821345806122] in episode 97
Report: 
rewardSum:44.33333333333334
loss:[34.37454271316528, 75.44821345806122]
policies:[1, 2, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1286
memory used:2748.0
############# STATE ###############
0-		9-		18*		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.849786321865403, the reward is -7.0 with loss [41.561209201812744, 91.2448303103447] in episode 98
Report: 
rewardSum:-7.0
loss:[41.561209201812744, 91.2448303103447]
policies:[0, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1302
memory used:2752.0
now epsilon is 0.8485124387917734, the reward is 192.33333333333334 with loss [53.827632427215576, 34.84656047821045] in episode 99
Report: 
rewardSum:192.33333333333334
loss:[53.827632427215576, 34.84656047821045]
policies:[1, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1314
memory used:2758.0
now epsilon is 0.8466051938442242, the reward is 189.33333333333334 with loss [84.29048991203308, 72.28414559364319] in episode 100
Report: 
rewardSum:189.33333333333334
loss:[84.29048991203308, 72.28414559364319]
policies:[1, 2, 6]
qAverage:[9.153849283854166, 1.4250741004943848]
ws:[-2.841547648111979, -3.1900383631388345]
memory len:1332
memory used:2759.0
now epsilon is 0.8455474663478997, the reward is -4.0 with loss [19.83068096637726, 22.282372653484344] in episode 101
Report: 
rewardSum:-4.0
loss:[19.83068096637726, 22.282372653484344]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1342
memory used:2772.0
now epsilon is 0.8404887405382303, the reward is 421.0 with loss [207.57654082775116, 200.44956374168396] in episode 102
Report: 
rewardSum:421.0
loss:[207.57654082775116, 200.44956374168396]
policies:[3, 0, 21]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1390
memory used:2772.0
now epsilon is 0.8396485669284426, the reward is 194.33333333333334 with loss [17.633836030960083, 22.612093448638916] in episode 103
Report: 
rewardSum:194.33333333333334
loss:[17.633836030960083, 22.612093448638916]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1398
memory used:2772.0
now epsilon is 0.8390189879242331, the reward is -2.0 with loss [8.127505421638489, 29.205087661743164] in episode 104
Report: 
rewardSum:-2.0
loss:[8.127505421638489, 29.205087661743164]
policies:[0, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1404
memory used:2772.0
now epsilon is 0.838180283515994, the reward is -3.0 with loss [24.2559974193573, 33.36690616607666] in episode 105
Report: 
rewardSum:-3.0
loss:[24.2559974193573, 33.36690616607666]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1412
memory used:2772.0
now epsilon is 0.8365053890312798, the reward is -7.0 with loss [42.43498992919922, 80.46265625953674] in episode 106
Report: 
rewardSum:-7.0
loss:[42.43498992919922, 80.46265625953674]
policies:[1, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1428
memory used:2773.0
now epsilon is 0.8350426020564489, the reward is 191.33333333333334 with loss [32.49084138870239, 63.157981872558594] in episode 107
Report: 
rewardSum:191.33333333333334
loss:[32.49084138870239, 63.157981872558594]
policies:[1, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1442
memory used:2773.0
now epsilon is 0.8335823730397156, the reward is 43.33333333333334 with loss [65.35168766975403, 45.33387339115143] in episode 108
Report: 
rewardSum:43.33333333333334
loss:[65.35168766975403, 45.33387339115143]
policies:[0, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1456
memory used:2773.0
now epsilon is 0.8321246975080095, the reward is 438.0 with loss [48.39045059680939, 39.443526327610016] in episode 109
Report: 
rewardSum:438.0
loss:[48.39045059680939, 39.443526327610016]
policies:[1, 1, 5]
qAverage:[0.0, 22.67908477783203]
ws:[5.061371326446533, 5.1131463050842285]
memory len:1470
memory used:2773.0
now epsilon is 0.8298392128742612, the reward is 187.33333333333334 with loss [71.21506762504578, 87.67336106300354] in episode 110
Report: 
rewardSum:187.33333333333334
loss:[71.21506762504578, 87.67336106300354]
policies:[1, 0, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1492
memory used:2779.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31*		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.8273531154714232, the reward is 186.33333333333334 with loss [60.12501120567322, 96.30796492099762] in episode 111
Report: 
rewardSum:186.33333333333334
loss:[60.12501120567322, 96.30796492099762]
policies:[1, 1, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1516
memory used:2779.0
now epsilon is 0.8267327557506016, the reward is -2.0 with loss [26.382441997528076, 17.219722986221313] in episode 112
Report: 
rewardSum:-2.0
loss:[26.382441997528076, 17.219722986221313]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1522
memory used:2779.0
now epsilon is 0.825906332967967, the reward is 194.33333333333334 with loss [28.510557413101196, 29.897894382476807] in episode 113
Report: 
rewardSum:194.33333333333334
loss:[28.510557413101196, 29.897894382476807]
policies:[1, 0, 3]
qAverage:[26.106287002563477, 0.0]
ws:[5.41526985168457, 5.0154619216918945]
memory len:1530
memory used:2779.0
now epsilon is 0.8240499009244427, the reward is 41.33333333333334 with loss [46.85753881931305, 83.07485806941986] in episode 114
Report: 
rewardSum:41.33333333333334
loss:[46.85753881931305, 83.07485806941986]
policies:[0, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1548
memory used:2779.0
now epsilon is 0.8232261599907313, the reward is 194.33333333333334 with loss [30.530901432037354, 7.482545852661133] in episode 115
Report: 
rewardSum:194.33333333333334
loss:[30.530901432037354, 7.482545852661133]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1556
memory used:2779.0
now epsilon is 0.8215811475964321, the reward is 992.0 with loss [66.92190313339233, 41.61936020851135] in episode 116
Report: 
rewardSum:992.0
loss:[66.92190313339233, 41.61936020851135]
policies:[0, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1572
memory used:2779.0
now epsilon is 0.8209651157693628, the reward is -2.0 with loss [7.533427715301514, 15.252307176589966] in episode 117
Report: 
rewardSum:-2.0
loss:[7.533427715301514, 15.252307176589966]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1578
memory used:2779.0
now epsilon is 0.8205546845217979, the reward is -1.0 with loss [9.598514318466187, 6.034941673278809] in episode 118
Report: 
rewardSum:-1.0
loss:[9.598514318466187, 6.034941673278809]
policies:[0, 1, 1]
qAverage:[0.0, 11.765850067138672]
ws:[2.547110080718994, 2.9004878997802734]
memory len:1582
memory used:2779.0
now epsilon is 0.819529503884628, the reward is -4.0 with loss [28.77845525741577, 25.445505261421204] in episode 119
Report: 
rewardSum:-4.0
loss:[28.77845525741577, 25.445505261421204]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1592
memory used:2780.0
now epsilon is 0.8172786127698216, the reward is 187.33333333333334 with loss [71.53908383846283, 61.900670409202576] in episode 120
Report: 
rewardSum:187.33333333333334
loss:[71.53908383846283, 61.900670409202576]
policies:[1, 0, 10]
qAverage:[12.952216148376465, 0.0]
ws:[3.1982686519622803, 3.0755958557128906]
memory len:1614
memory used:2779.0
now epsilon is 0.81687002454335, the reward is -1.0 with loss [10.518747806549072, 10.541462898254395] in episode 121
Report: 
rewardSum:-1.0
loss:[10.518747806549072, 10.541462898254395]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1618
memory used:2780.0
now epsilon is 0.8162575251753086, the reward is -2.0 with loss [32.75389862060547, 24.00292181968689] in episode 122
Report: 
rewardSum:-2.0
loss:[32.75389862060547, 24.00292181968689]
policies:[1, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1624
memory used:2780.0
now epsilon is 0.8150339038739429, the reward is 192.33333333333334 with loss [52.12979197502136, 26.37074875831604] in episode 123
Report: 
rewardSum:192.33333333333334
loss:[52.12979197502136, 26.37074875831604]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1636
memory used:2780.0
now epsilon is 0.8142191755568466, the reward is 194.33333333333334 with loss [24.275089263916016, 28.061393976211548] in episode 124
Report: 
rewardSum:194.33333333333334
loss:[24.275089263916016, 28.061393976211548]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1644
memory used:2780.0
now epsilon is 0.8138121168577667, the reward is -1.0 with loss [13.161694765090942, 8.693972706794739] in episode 125
Report: 
rewardSum:-1.0
loss:[13.161694765090942, 8.693972706794739]
policies:[1, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1648
memory used:2780.0
now epsilon is 0.8101577340438908, the reward is 180.33333333333334 with loss [121.7901759147644, 68.83955186605453] in episode 126
Report: 
rewardSum:180.33333333333334
loss:[121.7901759147644, 68.83955186605453]
policies:[2, 2, 14]
qAverage:[0.0, 16.05423355102539]
ws:[-0.5015043616294861, -0.22251766920089722]
memory len:1684
memory used:2780.0
now epsilon is 0.8089432567125739, the reward is 192.33333333333334 with loss [50.733527302742004, 57.31185507774353] in episode 127
Report: 
rewardSum:192.33333333333334
loss:[50.733527302742004, 57.31185507774353]
policies:[0, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1696
memory used:2780.0
now epsilon is 0.8085388356431713, the reward is -1.0 with loss [16.94916820526123, 12.1496102809906] in episode 128
Report: 
rewardSum:-1.0
loss:[16.94916820526123, 12.1496102809906]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1700
memory used:2780.0
now epsilon is 0.807730599959061, the reward is 46.33333333333334 with loss [11.826033353805542, 69.40181922912598] in episode 129
Report: 
rewardSum:46.33333333333334
loss:[11.826033353805542, 69.40181922912598]
policies:[2, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1708
memory used:2780.0
now epsilon is 0.8071249534459586, the reward is -2.0 with loss [46.045350313186646, 11.646302223205566] in episode 130
Report: 
rewardSum:-2.0
loss:[46.045350313186646, 11.646302223205566]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1714
memory used:2780.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22*		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.8055121153017217, the reward is 190.33333333333334 with loss [69.57496213912964, 80.33472871780396] in episode 131
Report: 
rewardSum:190.33333333333334
loss:[69.57496213912964, 80.33472871780396]
policies:[3, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1730
memory used:2779.0
now epsilon is 0.804304602044702, the reward is 192.33333333333334 with loss [27.821980953216553, 20.620498836040497] in episode 132
Report: 
rewardSum:192.33333333333334
loss:[27.821980953216553, 20.620498836040497]
policies:[2, 1, 3]
qAverage:[20.468109130859375, 0.0]
ws:[4.86810827255249, 4.522549152374268]
memory len:1742
memory used:2779.0
now epsilon is 0.8026973996701197, the reward is 42.33333333333334 with loss [37.600523352622986, 45.66832756996155] in episode 133
Report: 
rewardSum:42.33333333333334
loss:[37.600523352622986, 45.66832756996155]
policies:[0, 1, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1758
memory used:2780.0
now epsilon is 0.8008931355368658, the reward is 189.33333333333334 with loss [49.65639638900757, 53.18122547864914] in episode 134
Report: 
rewardSum:189.33333333333334
loss:[49.65639638900757, 53.18122547864914]
policies:[3, 0, 6]
qAverage:[32.30461883544922, 0.0]
ws:[4.232317845026652, 3.605407476425171]
memory len:1776
memory used:2780.0
now epsilon is 0.8000925426862021, the reward is -3.0 with loss [33.75133037567139, 15.745121002197266] in episode 135
Report: 
rewardSum:-3.0
loss:[33.75133037567139, 15.745121002197266]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1784
memory used:2779.0
now epsilon is 0.7974961388004379, the reward is 987.0 with loss [117.46128356456757, 65.44302916526794] in episode 136
Report: 
rewardSum:987.0
loss:[117.46128356456757, 65.44302916526794]
policies:[1, 1, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1810
memory used:2779.0
now epsilon is 0.796499766937431, the reward is 193.33333333333334 with loss [55.08290147781372, 22.02286434173584] in episode 137
Report: 
rewardSum:193.33333333333334
loss:[55.08290147781372, 22.02286434173584]
policies:[1, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1820
memory used:2779.0
now epsilon is 0.7959025414434889, the reward is -2.0 with loss [15.188846588134766, 37.739333152770996] in episode 138
Report: 
rewardSum:-2.0
loss:[15.188846588134766, 37.739333152770996]
policies:[0, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1826
memory used:2779.0
now epsilon is 0.7941135504617292, the reward is -8.0 with loss [57.746007561683655, 51.446981370449066] in episode 139
Report: 
rewardSum:-8.0
loss:[57.746007561683655, 51.446981370449066]
policies:[0, 1, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1844
memory used:2779.0
now epsilon is 0.79331973465422, the reward is 194.33333333333334 with loss [54.91234874725342, 33.461127281188965] in episode 140
Report: 
rewardSum:194.33333333333334
loss:[54.91234874725342, 33.461127281188965]
policies:[0, 3, 1]
qAverage:[0.0, 51.16167068481445]
ws:[1.2289714813232422, 1.9080170392990112]
memory len:1852
memory used:2780.0
now epsilon is 0.792526712364887, the reward is -3.0 with loss [17.74902641773224, 43.66602039337158] in episode 141
Report: 
rewardSum:-3.0
loss:[17.74902641773224, 43.66602039337158]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1860
memory used:2780.0
now epsilon is 0.789954860578033, the reward is 37.33333333333334 with loss [83.09693109989166, 79.79666674137115] in episode 142
Report: 
rewardSum:37.33333333333334
loss:[83.09693109989166, 79.79666674137115]
policies:[0, 2, 11]
qAverage:[0.0, 45.15219497680664]
ws:[5.89708948135376, 7.019491672515869]
memory len:1886
memory used:2780.0
now epsilon is 0.7895599325199228, the reward is -1.0 with loss [20.874380111694336, 6.075350522994995] in episode 143
Report: 
rewardSum:-1.0
loss:[20.874380111694336, 6.075350522994995]
policies:[1, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1890
memory used:2780.0
now epsilon is 0.7881792385037418, the reward is 438.0 with loss [46.51228046417236, 79.82351970672607] in episode 144
Report: 
rewardSum:438.0
loss:[46.51228046417236, 79.82351970672607]
policies:[3, 0, 4]
qAverage:[27.443798065185547, 0.0]
ws:[-6.189823150634766, -6.916074752807617]
memory len:1904
memory used:2780.0
now epsilon is 0.7877851981456924, the reward is -1.0 with loss [16.58850383758545, 4.164432883262634] in episode 145
Report: 
rewardSum:-1.0
loss:[16.58850383758545, 4.164432883262634]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1908
memory used:2786.0
now epsilon is 0.7868009588906832, the reward is 193.33333333333334 with loss [14.04768681526184, 31.615088939666748] in episode 146
Report: 
rewardSum:193.33333333333334
loss:[14.04768681526184, 31.615088939666748]
policies:[0, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1918
memory used:2786.0
now epsilon is 0.785425089458709, the reward is 191.33333333333334 with loss [23.12244999408722, 41.049264907836914] in episode 147
Report: 
rewardSum:191.33333333333334
loss:[23.12244999408722, 41.049264907836914]
policies:[0, 2, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1932
memory used:2786.0
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.7836596491833945, the reward is 991.0 with loss [45.25118017196655, 43.526234805583954] in episode 148
Report: 
rewardSum:991.0
loss:[45.25118017196655, 43.526234805583954]
policies:[0, 2, 7]
qAverage:[0.0, 39.4122314453125]
ws:[6.701857089996338, 6.777689456939697]
memory len:1950
memory used:2786.0
now epsilon is 0.7830720513704464, the reward is -2.0 with loss [4.655598759651184, 12.83759593963623] in episode 149
Report: 
rewardSum:-2.0
loss:[4.655598759651184, 12.83759593963623]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1956
memory used:2786.0
now epsilon is 0.7820937006039258, the reward is 45.33333333333334 with loss [32.06858777999878, 21.779206037521362] in episode 150
Report: 
rewardSum:45.33333333333334
loss:[32.06858777999878, 21.779206037521362]
policies:[0, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1966
memory used:2786.0
now epsilon is 0.780921293021506, the reward is 192.33333333333334 with loss [19.104251980781555, 41.936201989650726] in episode 151
Report: 
rewardSum:192.33333333333334
loss:[19.104251980781555, 41.936201989650726]
policies:[2, 0, 4]
qAverage:[46.07622146606445, 0.0]
ws:[8.346955299377441, 8.004521369934082]
memory len:1978
memory used:2786.0
now epsilon is 0.7793608163646335, the reward is 190.33333333333334 with loss [41.47698676586151, 37.006378531455994] in episode 152
Report: 
rewardSum:190.33333333333334
loss:[41.47698676586151, 37.006378531455994]
policies:[1, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1994
memory used:2786.0
now epsilon is 0.7785817477598681, the reward is 46.33333333333334 with loss [39.46322250366211, 32.731950998306274] in episode 153
Report: 
rewardSum:46.33333333333334
loss:[39.46322250366211, 32.731950998306274]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2002
memory used:2785.0
now epsilon is 0.7770259461013608, the reward is 190.33333333333334 with loss [61.53640007972717, 29.648582935333252] in episode 154
Report: 
rewardSum:190.33333333333334
loss:[61.53640007972717, 29.648582935333252]
policies:[2, 1, 5]
qAverage:[23.810335795084637, 33.480211893717446]
ws:[8.814430554707846, 8.81960646311442]
memory len:2018
memory used:2786.0
now epsilon is 0.7758611354012582, the reward is 192.33333333333334 with loss [27.402543544769287, 38.07685661315918] in episode 155
Report: 
rewardSum:192.33333333333334
loss:[27.402543544769287, 38.07685661315918]
policies:[0, 3, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2030
memory used:2786.0
now epsilon is 0.7741171925162244, the reward is 189.33333333333334 with loss [43.49878633022308, 69.9988944530487] in episode 156
Report: 
rewardSum:189.33333333333334
loss:[43.49878633022308, 69.9988944530487]
policies:[0, 2, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2048
memory used:2787.0
now epsilon is 0.772570312159138, the reward is 992.0 with loss [36.5350558757782, 41.49383020401001] in episode 157
Report: 
rewardSum:992.0
loss:[36.5350558757782, 41.49383020401001]
policies:[0, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2064
memory used:2788.0
now epsilon is 0.7717980315125633, the reward is 194.33333333333334 with loss [35.45280480384827, 16.194020748138428] in episode 158
Report: 
rewardSum:194.33333333333334
loss:[35.45280480384827, 16.194020748138428]
policies:[0, 2, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2072
memory used:2788.0
now epsilon is 0.7706410577848074, the reward is 44.33333333333334 with loss [34.171242237091064, 36.32773530483246] in episode 159
Report: 
rewardSum:44.33333333333334
loss:[34.171242237091064, 36.32773530483246]
policies:[1, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2084
memory used:2788.0
now epsilon is 0.7698707056692573, the reward is 194.33333333333334 with loss [7.95685088634491, 28.822816610336304] in episode 160
Report: 
rewardSum:194.33333333333334
loss:[7.95685088634491, 28.822816610336304]
policies:[1, 1, 2]
qAverage:[0.0, 60.579837799072266]
ws:[5.627425670623779, 5.6775383949279785]
memory len:2092
memory used:2789.0
now epsilon is 0.7691011236169888, the reward is 194.33333333333334 with loss [20.551368832588196, 33.585678815841675] in episode 161
Report: 
rewardSum:194.33333333333334
loss:[20.551368832588196, 33.585678815841675]
policies:[3, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2100
memory used:2789.0
now epsilon is 0.7683323108582273, the reward is 194.33333333333334 with loss [27.964524269104004, 21.33205485343933] in episode 162
Report: 
rewardSum:194.33333333333334
loss:[27.964524269104004, 21.33205485343933]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2108
memory used:2789.0
now epsilon is 0.7656475132933545, the reward is 36.33333333333334 with loss [130.01592588424683, 108.98534858226776] in episode 163
Report: 
rewardSum:36.33333333333334
loss:[130.01592588424683, 108.98534858226776]
policies:[3, 0, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2136
memory used:2788.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40*		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.7635446125721764, the reward is 434.0 with loss [78.11081087589264, 97.2922831773758] in episode 164
Report: 
rewardSum:434.0
loss:[78.11081087589264, 97.2922831773758]
policies:[2, 1, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2158
memory used:2794.0
now epsilon is 0.7620188588822114, the reward is 190.33333333333334 with loss [40.46810066699982, 27.16321873664856] in episode 165
Report: 
rewardSum:190.33333333333334
loss:[40.46810066699982, 27.16321873664856]
policies:[2, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2174
memory used:2794.0
now epsilon is 0.7603060299923845, the reward is 189.33333333333334 with loss [46.677637815475464, 51.4273955821991] in episode 166
Report: 
rewardSum:189.33333333333334
loss:[46.677637815475464, 51.4273955821991]
policies:[1, 1, 7]
qAverage:[0.0, 41.57331466674805]
ws:[7.122769355773926, 7.432037830352783]
memory len:2192
memory used:2794.0
now epsilon is 0.7580282455526989, the reward is 988.0 with loss [70.01535367965698, 92.15336775779724] in episode 167
Report: 
rewardSum:988.0
loss:[70.01535367965698, 92.15336775779724]
policies:[3, 1, 8]
qAverage:[70.77144241333008, 0.0]
ws:[9.391778707504272, 8.379053592681885]
memory len:2216
memory used:2794.0
now epsilon is 0.7576492788066879, the reward is -1.0 with loss [7.157549142837524, 6.561293125152588] in episode 168
Report: 
rewardSum:-1.0
loss:[7.157549142837524, 6.561293125152588]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2220
memory used:2794.0
now epsilon is 0.756324386569219, the reward is 191.33333333333334 with loss [16.346049547195435, 51.739211678504944] in episode 169
Report: 
rewardSum:191.33333333333334
loss:[16.346049547195435, 51.739211678504944]
policies:[2, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2234
memory used:2794.0
now epsilon is 0.7540585306492028, the reward is 433.0 with loss [74.25375318527222, 99.70141887664795] in episode 170
Report: 
rewardSum:433.0
loss:[74.25375318527222, 99.70141887664795]
policies:[1, 1, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2258
memory used:2794.0
now epsilon is 0.753304754843377, the reward is -3.0 with loss [14.606961131095886, 14.066798686981201] in episode 171
Report: 
rewardSum:-3.0
loss:[14.606961131095886, 14.066798686981201]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2266
memory used:2794.0
now epsilon is 0.7523635945976055, the reward is 193.33333333333334 with loss [35.91381049156189, 26.39749836921692] in episode 172
Report: 
rewardSum:193.33333333333334
loss:[35.91381049156189, 26.39749836921692]
policies:[2, 1, 2]
qAverage:[56.84297180175781, 0.0]
ws:[11.028993606567383, 10.966598510742188]
memory len:2276
memory used:2794.0
now epsilon is 0.7512357543115096, the reward is 44.33333333333334 with loss [48.52659821510315, 68.66855049133301] in episode 173
Report: 
rewardSum:44.33333333333334
loss:[48.52659821510315, 68.66855049133301]
policies:[1, 0, 5]
qAverage:[48.24889373779297, 0.0]
ws:[7.1218695640563965, 6.530293941497803]
memory len:2288
memory used:2794.0
now epsilon is 0.7502971790236009, the reward is 193.33333333333334 with loss [25.112985372543335, 29.75384211540222] in episode 174
Report: 
rewardSum:193.33333333333334
loss:[25.112985372543335, 29.75384211540222]
policies:[1, 1, 3]
qAverage:[0.0, 58.45621871948242]
ws:[7.6254496574401855, 8.170636177062988]
memory len:2298
memory used:2795.0
now epsilon is 0.7484235448806662, the reward is 40.33333333333334 with loss [59.965620279312134, 79.43517184257507] in episode 175
Report: 
rewardSum:40.33333333333334
loss:[59.965620279312134, 79.43517184257507]
policies:[1, 2, 7]
qAverage:[0.0, 44.540184020996094]
ws:[4.721221446990967, 4.828756809234619]
memory len:2318
memory used:2796.0
now epsilon is 0.7471147855738359, the reward is 993.0 with loss [44.0159717798233, 27.028428077697754] in episode 176
Report: 
rewardSum:993.0
loss:[44.0159717798233, 27.028428077697754]
policies:[2, 2, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2332
memory used:2796.0
now epsilon is 0.7461813589218876, the reward is 193.33333333333334 with loss [25.378952980041504, 22.563610315322876] in episode 177
Report: 
rewardSum:193.33333333333334
loss:[25.378952980041504, 22.563610315322876]
policies:[1, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2342
memory used:2802.0
now epsilon is 0.744876520498842, the reward is -6.0 with loss [61.10658884048462, 32.432982206344604] in episode 178
Report: 
rewardSum:-6.0
loss:[61.10658884048462, 32.432982206344604]
policies:[1, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2356
memory used:2801.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23*		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.7441319232604865, the reward is 194.33333333333334 with loss [25.514369249343872, 16.347150564193726] in episode 179
Report: 
rewardSum:194.33333333333334
loss:[25.514369249343872, 16.347150564193726]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2364
memory used:2802.0
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.743202223322607, the reward is 193.33333333333334 with loss [54.5659966468811, 74.60905170440674] in episode 180
Report: 
rewardSum:193.33333333333334
loss:[54.5659966468811, 74.60905170440674]
policies:[2, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2374
memory used:2802.0
now epsilon is 0.7426449609939194, the reward is -2.0 with loss [13.806251883506775, 12.83931589126587] in episode 181
Report: 
rewardSum:-2.0
loss:[13.806251883506775, 12.83931589126587]
policies:[1, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2380
memory used:2801.0
now epsilon is 0.7419025944783736, the reward is 194.33333333333334 with loss [15.718974590301514, 39.93970060348511] in episode 182
Report: 
rewardSum:194.33333333333334
loss:[15.718974590301514, 39.93970060348511]
policies:[2, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2388
memory used:2801.0
now epsilon is 0.7406052382795654, the reward is 191.33333333333334 with loss [52.48744869232178, 48.821675539016724] in episode 183
Report: 
rewardSum:191.33333333333334
loss:[52.48744869232178, 48.821675539016724]
policies:[1, 1, 5]
qAverage:[0.0, 73.53970336914062]
ws:[6.1119065284729, 7.442531585693359]
memory len:2402
memory used:2802.0
now epsilon is 0.7393101507520342, the reward is 43.33333333333334 with loss [51.50281238555908, 32.758843183517456] in episode 184
Report: 
rewardSum:43.33333333333334
loss:[51.50281238555908, 32.758843183517456]
policies:[0, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2416
memory used:2802.0
now epsilon is 0.7380173279285819, the reward is -6.0 with loss [42.74292469024658, 52.79664635658264] in episode 185
Report: 
rewardSum:-6.0
loss:[42.74292469024658, 52.79664635658264]
policies:[0, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2430
memory used:2802.0
now epsilon is 0.737279587311028, the reward is 194.33333333333334 with loss [29.223954439163208, 24.92009687423706] in episode 186
Report: 
rewardSum:194.33333333333334
loss:[29.223954439163208, 24.92009687423706]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2438
memory used:2802.0
now epsilon is 0.735806317730766, the reward is 190.33333333333334 with loss [34.509273290634155, 57.40695381164551] in episode 187
Report: 
rewardSum:190.33333333333334
loss:[34.509273290634155, 57.40695381164551]
policies:[1, 2, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2454
memory used:2802.0
now epsilon is 0.7354384605597956, the reward is -1.0 with loss [9.642451763153076, 13.435061693191528] in episode 188
Report: 
rewardSum:-1.0
loss:[9.642451763153076, 13.435061693191528]
policies:[1, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2458
memory used:2802.0
now epsilon is 0.734519622018236, the reward is 193.33333333333334 with loss [47.23225784301758, 34.426955461502075] in episode 189
Report: 
rewardSum:193.33333333333334
loss:[47.23225784301758, 34.426955461502075]
policies:[1, 3, 1]
qAverage:[0.0, 91.85809135437012]
ws:[9.586456060409546, 11.132439613342285]
memory len:2468
memory used:2802.0
now epsilon is 0.7334185309678599, the reward is 192.33333333333334 with loss [22.95240819454193, 25.297836184501648] in episode 190
Report: 
rewardSum:192.33333333333334
loss:[22.95240819454193, 25.297836184501648]
policies:[1, 1, 4]
qAverage:[0.0, 52.51676559448242]
ws:[7.22568941116333, 9.055765151977539]
memory len:2480
memory used:2803.0
now epsilon is 0.7328686045741489, the reward is -2.0 with loss [11.916450500488281, 21.03365182876587] in episode 191
Report: 
rewardSum:-2.0
loss:[11.916450500488281, 21.03365182876587]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2486
memory used:2803.0
now epsilon is 0.7325022160761496, the reward is -1.0 with loss [15.785460948944092, 27.61021614074707] in episode 192
Report: 
rewardSum:-1.0
loss:[15.785460948944092, 27.61021614074707]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2490
memory used:2803.0
now epsilon is 0.731769988502626, the reward is 194.33333333333334 with loss [10.784653544425964, 25.684534072875977] in episode 193
Report: 
rewardSum:194.33333333333334
loss:[10.784653544425964, 25.684534072875977]
policies:[0, 2, 2]
qAverage:[0.0, 49.43412780761719]
ws:[8.008475303649902, 8.907054901123047]
memory len:2498
memory used:2803.0
now epsilon is 0.731404149243999, the reward is -1.0 with loss [20.057415008544922, 6.158350944519043] in episode 194
Report: 
rewardSum:-1.0
loss:[20.057415008544922, 6.158350944519043]
policies:[0, 1, 1]
qAverage:[0.0, 45.22838592529297]
ws:[7.7142839431762695, 8.778433799743652]
memory len:2502
memory used:2803.0
now epsilon is 0.7306730193256011, the reward is 194.33333333333334 with loss [28.524378299713135, 33.15784811973572] in episode 195
Report: 
rewardSum:194.33333333333334
loss:[28.524378299713135, 33.15784811973572]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2510
memory used:2803.0
now epsilon is 0.728666178328135, the reward is 39.33333333333334 with loss [51.42174243927002, 60.3921058177948] in episode 196
Report: 
rewardSum:39.33333333333334
loss:[51.42174243927002, 60.3921058177948]
policies:[0, 3, 8]
qAverage:[0.0, 54.458370208740234]
ws:[7.626845836639404, 8.664796829223633]
memory len:2532
memory used:2803.0
now epsilon is 0.7275738619575195, the reward is 44.33333333333334 with loss [49.149128675460815, 21.979606986045837] in episode 197
Report: 
rewardSum:44.33333333333334
loss:[49.149128675460815, 21.979606986045837]
policies:[0, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2544
memory used:2803.0
now epsilon is 0.7264831830377547, the reward is 994.0 with loss [21.364089608192444, 35.422834157943726] in episode 198
Report: 
rewardSum:994.0
loss:[21.364089608192444, 35.422834157943726]
policies:[1, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2556
memory used:2803.0
now epsilon is 0.7261199868514348, the reward is -1.0 with loss [6.553654074668884, 9.195234775543213] in episode 199
Report: 
rewardSum:-1.0
loss:[6.553654074668884, 9.195234775543213]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2560
memory used:2803.0
now epsilon is 0.7253941391141988, the reward is 194.33333333333334 with loss [27.199378490447998, 43.431108474731445] in episode 200
Report: 
rewardSum:194.33333333333334
loss:[27.199378490447998, 43.431108474731445]
policies:[2, 1, 1]
qAverage:[39.03686014811198, 32.139208475748696]
ws:[9.797918955485025, 9.918074607849121]
memory len:2568
memory used:2803.0
now epsilon is 0.7248502295099299, the reward is -2.0 with loss [24.887238264083862, 17.691105365753174] in episode 201
Report: 
rewardSum:-2.0
loss:[24.887238264083862, 17.691105365753174]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2574
memory used:2803.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21*		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.7232209464555399, the reward is 189.33333333333334 with loss [50.10152208805084, 51.611969232559204] in episode 202
Report: 
rewardSum:189.33333333333334
loss:[50.10152208805084, 51.611969232559204]
policies:[3, 2, 4]
qAverage:[84.86243438720703, 0.0]
ws:[11.140781164169312, 10.59737777709961]
memory len:2592
memory used:2803.0
now epsilon is 0.7228593811836213, the reward is -1.0 with loss [4.068056225776672, 11.315276622772217] in episode 203
Report: 
rewardSum:-1.0
loss:[4.068056225776672, 11.315276622772217]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2596
memory used:2803.0
now epsilon is 0.7208740008517988, the reward is -10.0 with loss [57.53180706501007, 71.42727136611938] in episode 204
Report: 
rewardSum:-10.0
loss:[57.53180706501007, 71.42727136611938]
policies:[0, 2, 9]
qAverage:[0.0, 51.76204299926758]
ws:[5.785661697387695, 5.949410915374756]
memory len:2618
memory used:2802.0
now epsilon is 0.720513608905998, the reward is -1.0 with loss [18.85462760925293, 14.193650722503662] in episode 205
Report: 
rewardSum:-1.0
loss:[18.85462760925293, 14.193650722503662]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2622
memory used:2802.0
now epsilon is 0.7194335137490293, the reward is 192.33333333333334 with loss [49.636510610580444, 35.69666409492493] in episode 206
Report: 
rewardSum:192.33333333333334
loss:[49.636510610580444, 35.69666409492493]
policies:[2, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2634
memory used:2802.0
now epsilon is 0.7187143499778862, the reward is 194.33333333333334 with loss [23.3573579788208, 18.975870370864868] in episode 207
Report: 
rewardSum:194.33333333333334
loss:[23.3573579788208, 18.975870370864868]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2642
memory used:2802.0
now epsilon is 0.7176369520230665, the reward is 44.33333333333334 with loss [38.4784722328186, 18.73431658744812] in episode 208
Report: 
rewardSum:44.33333333333334
loss:[38.4784722328186, 18.73431658744812]
policies:[2, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2654
memory used:2802.0
now epsilon is 0.7170988588547648, the reward is -2.0 with loss [25.475137948989868, 12.809807181358337] in episode 209
Report: 
rewardSum:-2.0
loss:[25.475137948989868, 12.809807181358337]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2660
memory used:2802.0
now epsilon is 0.716740354244016, the reward is -1.0 with loss [14.139151334762573, 15.300715565681458] in episode 210
Report: 
rewardSum:-1.0
loss:[14.139151334762573, 15.300715565681458]
policies:[1, 0, 1]
qAverage:[49.25059127807617, 0.0]
ws:[6.591125965118408, 6.18601131439209]
memory len:2664
memory used:2802.0
now epsilon is 0.7153081272041962, the reward is 190.33333333333334 with loss [52.87812638282776, 60.08995044231415] in episode 211
Report: 
rewardSum:190.33333333333334
loss:[52.87812638282776, 60.08995044231415]
policies:[0, 1, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2680
memory used:2802.0
now epsilon is 0.7145930872728358, the reward is 46.33333333333334 with loss [16.39850401878357, 22.89002513885498] in episode 212
Report: 
rewardSum:46.33333333333334
loss:[16.39850401878357, 22.89002513885498]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2688
memory used:2802.0
now epsilon is 0.7135218673496775, the reward is 439.0 with loss [71.11650240421295, 39.295247077941895] in episode 213
Report: 
rewardSum:439.0
loss:[71.11650240421295, 39.295247077941895]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2700
memory used:2803.0
now epsilon is 0.7119180476361961, the reward is 41.33333333333334 with loss [54.14828610420227, 48.814152240753174] in episode 214
Report: 
rewardSum:41.33333333333334
loss:[54.14828610420227, 48.814152240753174]
policies:[2, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2718
memory used:2808.0
now epsilon is 0.7110285949142076, the reward is -4.0 with loss [57.05245518684387, 16.123612642288208] in episode 215
Report: 
rewardSum:-4.0
loss:[57.05245518684387, 16.123612642288208]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2728
memory used:2808.0
now epsilon is 0.7097852277093921, the reward is 191.33333333333334 with loss [38.30120503902435, 35.36583876609802] in episode 216
Report: 
rewardSum:191.33333333333334
loss:[38.30120503902435, 35.36583876609802]
policies:[0, 2, 5]
qAverage:[0.0, 53.465579986572266]
ws:[4.35656213760376, 5.081273078918457]
memory len:2742
memory used:2808.0
now epsilon is 0.708721215069713, the reward is 44.33333333333334 with loss [36.63866639137268, 25.760021924972534] in episode 217
Report: 
rewardSum:44.33333333333334
loss:[36.63866639137268, 25.760021924972534]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2754
memory used:2815.0
now epsilon is 0.7078357563909113, the reward is -4.0 with loss [30.596349716186523, 32.93515729904175] in episode 218
Report: 
rewardSum:-4.0
loss:[30.596349716186523, 32.93515729904175]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2764
memory used:2821.0
now epsilon is 0.7065979724646567, the reward is 438.0 with loss [37.54571306705475, 49.553762912750244] in episode 219
Report: 
rewardSum:438.0
loss:[37.54571306705475, 49.553762912750244]
policies:[0, 3, 4]
qAverage:[0.0, 61.568450927734375]
ws:[5.207833290100098, 6.060819625854492]
memory len:2778
memory used:2821.0
now epsilon is 0.7057151665124166, the reward is 193.33333333333334 with loss [25.094717264175415, 20.71132457256317] in episode 220
Report: 
rewardSum:193.33333333333334
loss:[25.094717264175415, 20.71132457256317]
policies:[0, 2, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2788
memory used:2821.0
now epsilon is 0.7053623530363584, the reward is -1.0 with loss [23.4034481048584, 4.256997108459473] in episode 221
Report: 
rewardSum:-1.0
loss:[23.4034481048584, 4.256997108459473]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2792
memory used:2821.0
now epsilon is 0.7044810908363346, the reward is 193.33333333333334 with loss [17.344810724258423, 37.46970558166504] in episode 222
Report: 
rewardSum:193.33333333333334
loss:[17.344810724258423, 37.46970558166504]
policies:[0, 1, 4]
qAverage:[0.0, 83.28266906738281]
ws:[17.456186294555664, 17.545541763305664]
memory len:2802
memory used:2821.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20*		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.70377687388188, the reward is 194.33333333333334 with loss [28.643847227096558, 14.181967973709106] in episode 223
Report: 
rewardSum:194.33333333333334
loss:[28.643847227096558, 14.181967973709106]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2810
memory used:2821.0
now epsilon is 0.7028975925401225, the reward is -4.0 with loss [27.973485469818115, 37.6976523399353] in episode 224
Report: 
rewardSum:-4.0
loss:[27.973485469818115, 37.6976523399353]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2820
memory used:2821.0
now epsilon is 0.7011423241408947, the reward is 40.33333333333334 with loss [49.07538080215454, 65.1720232963562] in episode 225
Report: 
rewardSum:40.33333333333334
loss:[49.07538080215454, 65.1720232963562]
policies:[1, 1, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2840
memory used:2823.0
now epsilon is 0.6995663305619029, the reward is 189.33333333333334 with loss [52.894020795822144, 45.17800235748291] in episode 226
Report: 
rewardSum:189.33333333333334
loss:[52.894020795822144, 45.17800235748291]
policies:[1, 2, 6]
qAverage:[0.0, 72.42781829833984]
ws:[11.749473571777344, 12.34688663482666]
memory len:2858
memory used:2823.0
now epsilon is 0.6988670265249949, the reward is 194.33333333333334 with loss [26.67715096473694, 36.72774863243103] in episode 227
Report: 
rewardSum:194.33333333333334
loss:[26.67715096473694, 36.72774863243103]
policies:[0, 2, 2]
qAverage:[0.0, 90.60821024576823]
ws:[10.100153605143229, 11.559665362040201]
memory len:2866
memory used:2823.0
now epsilon is 0.6983430072817488, the reward is -2.0 with loss [22.685104846954346, 31.175549030303955] in episode 228
Report: 
rewardSum:-2.0
loss:[22.685104846954346, 31.175549030303955]
policies:[0, 1, 2]
qAverage:[0.0, 57.02566909790039]
ws:[5.246575355529785, 6.043034553527832]
memory len:2872
memory used:2823.0
now epsilon is 0.6978193809546899, the reward is -2.0 with loss [22.50703477859497, 17.165828704833984] in episode 229
Report: 
rewardSum:-2.0
loss:[22.50703477859497, 17.165828704833984]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2878
memory used:2823.0
now epsilon is 0.6965991125444321, the reward is 438.0 with loss [36.30116260051727, 65.22750806808472] in episode 230
Report: 
rewardSum:438.0
loss:[36.30116260051727, 65.22750806808472]
policies:[2, 1, 4]
qAverage:[0.0, 72.91471099853516]
ws:[10.345025062561035, 10.463860511779785]
memory len:2892
memory used:2823.0
now epsilon is 0.6955548667196372, the reward is 439.0 with loss [56.96033716201782, 34.96076774597168] in episode 231
Report: 
rewardSum:439.0
loss:[56.96033716201782, 34.96076774597168]
policies:[0, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2904
memory used:2823.0
now epsilon is 0.6948595726425232, the reward is 46.33333333333334 with loss [22.257110595703125, 14.675920486450195] in episode 232
Report: 
rewardSum:46.33333333333334
loss:[22.257110595703125, 14.675920486450195]
policies:[0, 3, 1]
qAverage:[0.0, 60.47449493408203]
ws:[3.0833897590637207, 3.380154848098755]
memory len:2912
memory used:2823.0
now epsilon is 0.6941649735987945, the reward is 46.33333333333334 with loss [16.14262068271637, 24.992857933044434] in episode 233
Report: 
rewardSum:46.33333333333334
loss:[16.14262068271637, 24.992857933044434]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2920
memory used:2823.0
now epsilon is 0.6931243767011733, the reward is 192.33333333333334 with loss [53.262309193611145, 25.169848561286926] in episode 234
Report: 
rewardSum:192.33333333333334
loss:[53.262309193611145, 25.169848561286926]
policies:[1, 2, 3]
qAverage:[0.0, 90.89498901367188]
ws:[9.343295256296793, 10.573004722595215]
memory len:2932
memory used:2821.0
now epsilon is 0.691739340309136, the reward is -7.0 with loss [36.87847876548767, 45.80450928211212] in episode 235
Report: 
rewardSum:-7.0
loss:[36.87847876548767, 45.80450928211212]
policies:[0, 1, 7]
qAverage:[0.0, 59.364830017089844]
ws:[3.1210856437683105, 4.844698905944824]
memory len:2948
memory used:2821.0
now epsilon is 0.6907023795881758, the reward is 192.33333333333334 with loss [25.480865716934204, 30.96441340446472] in episode 236
Report: 
rewardSum:192.33333333333334
loss:[25.480865716934204, 30.96441340446472]
policies:[2, 0, 4]
qAverage:[92.88335673014323, 0.0]
ws:[12.323208808898926, 12.106949170430502]
memory len:2960
memory used:2821.0
now epsilon is 0.6891498524082497, the reward is 41.33333333333334 with loss [47.08136248588562, 64.51728963851929] in episode 237
Report: 
rewardSum:41.33333333333334
loss:[47.08136248588562, 64.51728963851929]
policies:[2, 2, 5]
qAverage:[64.71893310546875, 0.0]
ws:[7.1397504806518555, 6.824141979217529]
memory len:2978
memory used:2822.0
now epsilon is 0.6884609609439669, the reward is 194.33333333333334 with loss [16.66228747367859, 17.123151302337646] in episode 238
Report: 
rewardSum:194.33333333333334
loss:[16.66228747367859, 17.123151302337646]
policies:[1, 2, 1]
qAverage:[0.0, 105.97394307454427]
ws:[17.839163462320965, 18.564241727193195]
memory len:2986
memory used:2822.0
now epsilon is 0.6877727581128573, the reward is 194.33333333333334 with loss [13.393009424209595, 17.625694274902344] in episode 239
Report: 
rewardSum:194.33333333333334
loss:[13.393009424209595, 17.625694274902344]
policies:[2, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2994
memory used:2822.0
now epsilon is 0.6870852432265457, the reward is 46.33333333333334 with loss [13.816513895988464, 20.832951307296753] in episode 240
Report: 
rewardSum:46.33333333333334
loss:[13.816513895988464, 20.832951307296753]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3002
memory used:2822.0
now epsilon is 0.685540846469622, the reward is 189.33333333333334 with loss [32.50553685426712, 88.94841194152832] in episode 241
Report: 
rewardSum:189.33333333333334
loss:[32.50553685426712, 88.94841194152832]
policies:[1, 1, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3020
memory used:2822.0
############# STATE ###############
0-		9-		18-		27-		36-		
1*		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.6845131776802698, the reward is 44.33333333333334 with loss [37.077969908714294, 26.94536590576172] in episode 242
Report: 
rewardSum:44.33333333333334
loss:[37.077969908714294, 26.94536590576172]
policies:[0, 2, 4]
qAverage:[0.0, 100.54349772135417]
ws:[10.17674477895101, 10.82062816619873]
memory len:3032
memory used:2822.0
now epsilon is 0.6836579639219638, the reward is 193.33333333333334 with loss [24.208626747131348, 30.136866211891174] in episode 243
Report: 
rewardSum:193.33333333333334
loss:[24.208626747131348, 30.136866211891174]
policies:[1, 2, 2]
qAverage:[0.0, 96.3659159342448]
ws:[15.963656107584635, 16.939385096232098]
memory len:3042
memory used:2822.0
now epsilon is 0.6822918437975428, the reward is -7.0 with loss [70.38735926151276, 67.28965032100677] in episode 244
Report: 
rewardSum:-7.0
loss:[70.38735926151276, 67.28965032100677]
policies:[2, 2, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3058
memory used:2821.0
now epsilon is 0.6804178848471583, the reward is 39.33333333333334 with loss [48.60146951675415, 57.78215134143829] in episode 245
Report: 
rewardSum:39.33333333333334
loss:[48.60146951675415, 57.78215134143829]
policies:[2, 3, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3080
memory used:2821.0
now epsilon is 0.6795677876459754, the reward is 193.33333333333334 with loss [32.503530740737915, 29.369006156921387] in episode 246
Report: 
rewardSum:193.33333333333334
loss:[32.503530740737915, 29.369006156921387]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3090
memory used:2833.0
now epsilon is 0.6785490728469824, the reward is 192.33333333333334 with loss [35.107173919677734, 45.45644211769104] in episode 247
Report: 
rewardSum:192.33333333333334
loss:[35.107173919677734, 45.45644211769104]
policies:[0, 2, 4]
qAverage:[0.0, 69.34528350830078]
ws:[9.486372947692871, 10.45175838470459]
memory len:3102
memory used:2833.0
now epsilon is 0.6770238632782292, the reward is 189.33333333333334 with loss [53.28584671020508, 58.70236158370972] in episode 248
Report: 
rewardSum:189.33333333333334
loss:[53.28584671020508, 58.70236158370972]
policies:[1, 3, 5]
qAverage:[0.0, 106.68414688110352]
ws:[9.228522300720215, 11.100788354873657]
memory len:3120
memory used:2821.0
now epsilon is 0.6756709997512228, the reward is 42.33333333333334 with loss [43.26091682910919, 59.11164051294327] in episode 249
Report: 
rewardSum:42.33333333333334
loss:[43.26091682910919, 59.11164051294327]
policies:[1, 1, 6]
qAverage:[0.0, 78.57887268066406]
ws:[6.863264560699463, 8.487911224365234]
memory len:3136
memory used:2821.0
now epsilon is 0.6749955820858697, the reward is 194.33333333333334 with loss [23.77280366420746, 46.57099151611328] in episode 250
Report: 
rewardSum:194.33333333333334
loss:[23.77280366420746, 46.57099151611328]
policies:[0, 2, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3144
memory used:2821.0
now epsilon is 0.6721325811588962, the reward is 33.33333333333334 with loss [77.51175272464752, 111.96630334854126] in episode 251
Report: 
rewardSum:33.33333333333334
loss:[77.51175272464752, 111.96630334854126]
policies:[2, 1, 14]
qAverage:[0.0, 61.24380874633789]
ws:[5.8528876304626465, 6.555225372314453]
memory len:3178
memory used:2821.0
now epsilon is 0.6714607005854496, the reward is 46.33333333333334 with loss [16.075440406799316, 23.15907621383667] in episode 252
Report: 
rewardSum:46.33333333333334
loss:[16.075440406799316, 23.15907621383667]
policies:[0, 2, 2]
qAverage:[0.0, 64.07331848144531]
ws:[2.556199312210083, 2.8388383388519287]
memory len:3186
memory used:2821.0
now epsilon is 0.6702865252844815, the reward is 43.33333333333334 with loss [35.66040802001953, 37.75015687942505] in episode 253
Report: 
rewardSum:43.33333333333334
loss:[35.66040802001953, 37.75015687942505]
policies:[0, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3200
memory used:2821.0
now epsilon is 0.6692817236807471, the reward is 192.33333333333334 with loss [46.016027212142944, 42.821528911590576] in episode 254
Report: 
rewardSum:192.33333333333334
loss:[46.016027212142944, 42.821528911590576]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3212
memory used:2821.0
now epsilon is 0.6682784283377308, the reward is 192.33333333333334 with loss [51.669663190841675, 32.431362986564636] in episode 255
Report: 
rewardSum:192.33333333333334
loss:[51.669663190841675, 32.431362986564636]
policies:[2, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3224
memory used:2821.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21*		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.6667763046236481, the reward is 189.33333333333334 with loss [27.74952644109726, 37.289608001708984] in episode 256
Report: 
rewardSum:189.33333333333334
loss:[27.74952644109726, 37.289608001708984]
policies:[3, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3242
memory used:2822.0
now epsilon is 0.666109778318468, the reward is -3.0 with loss [17.897085189819336, 15.778884887695312] in episode 257
Report: 
rewardSum:-3.0
loss:[17.897085189819336, 15.778884887695312]
policies:[0, 1, 3]
qAverage:[0.0, 69.0755386352539]
ws:[15.207385063171387, 16.5443058013916]
memory len:3250
memory used:2822.0
now epsilon is 0.6646125291903113, the reward is 189.33333333333334 with loss [36.79178035259247, 46.58498537540436] in episode 258
Report: 
rewardSum:189.33333333333334
loss:[36.79178035259247, 46.58498537540436]
policies:[1, 4, 4]
qAverage:[0.0, 110.78503926595052]
ws:[24.390064875284832, 25.099738438924152]
memory len:3268
memory used:2822.0
now epsilon is 0.6621245890041327, the reward is 985.0 with loss [72.64100313186646, 68.27776682376862] in episode 259
Report: 
rewardSum:985.0
loss:[72.64100313186646, 68.27776682376862]
policies:[2, 0, 13]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3298
memory used:2821.0
now epsilon is 0.6617935680924175, the reward is -1.0 with loss [6.433879137039185, 3.2518155574798584] in episode 260
Report: 
rewardSum:-1.0
loss:[6.433879137039185, 3.2518155574798584]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3302
memory used:2821.0
now epsilon is 0.6611320226555537, the reward is 194.33333333333334 with loss [28.800411701202393, 15.575254082679749] in episode 261
Report: 
rewardSum:194.33333333333334
loss:[28.800411701202393, 15.575254082679749]
policies:[0, 2, 2]
qAverage:[0.0, 64.86339569091797]
ws:[2.0424892902374268, 2.921283721923828]
memory len:3310
memory used:2825.0
now epsilon is 0.6601409442262767, the reward is 44.33333333333334 with loss [67.73733842372894, 20.269357562065125] in episode 262
Report: 
rewardSum:44.33333333333334
loss:[67.73733842372894, 20.269357562065125]
policies:[0, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3322
memory used:2825.0
now epsilon is 0.6588218170070337, the reward is 190.33333333333334 with loss [52.49949222803116, 33.91374754905701] in episode 263
Report: 
rewardSum:190.33333333333334
loss:[52.49949222803116, 33.91374754905701]
policies:[1, 3, 4]
qAverage:[0.0, 115.02156575520833]
ws:[22.567970911661785, 23.857850074768066]
memory len:3338
memory used:2825.0
now epsilon is 0.6579987013964825, the reward is -4.0 with loss [27.069793105125427, 32.22080874443054] in episode 264
Report: 
rewardSum:-4.0
loss:[27.069793105125427, 32.22080874443054]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3348
memory used:2825.0
now epsilon is 0.656027417276338, the reward is 186.33333333333334 with loss [87.79311454296112, 95.95949751138687] in episode 265
Report: 
rewardSum:186.33333333333334
loss:[87.79311454296112, 95.95949751138687]
policies:[2, 3, 7]
qAverage:[0.0, 68.72384643554688]
ws:[8.126113891601562, 8.888815879821777]
memory len:3372
memory used:2825.0
now epsilon is 0.6538985233740583, the reward is 185.33333333333334 with loss [63.241734743118286, 63.59190225601196] in episode 266
Report: 
rewardSum:185.33333333333334
loss:[63.241734743118286, 63.59190225601196]
policies:[0, 3, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3398
memory used:2825.0
now epsilon is 0.651939522888833, the reward is 186.33333333333334 with loss [61.24002468585968, 75.48110175132751] in episode 267
Report: 
rewardSum:186.33333333333334
loss:[61.24002468585968, 75.48110175132751]
policies:[1, 1, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3422
memory used:2810.0
now epsilon is 0.6516135938736087, the reward is -1.0 with loss [5.755445718765259, 9.940199613571167] in episode 268
Report: 
rewardSum:-1.0
loss:[5.755445718765259, 9.940199613571167]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3426
memory used:2809.0
now epsilon is 0.651125005845571, the reward is -2.0 with loss [18.603328227996826, 8.678211688995361] in episode 269
Report: 
rewardSum:-2.0
loss:[18.603328227996826, 8.678211688995361]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3432
memory used:2810.0
now epsilon is 0.6494990233997107, the reward is 188.33333333333334 with loss [73.07710409164429, 63.25107502937317] in episode 270
Report: 
rewardSum:188.33333333333334
loss:[73.07710409164429, 63.25107502937317]
policies:[1, 2, 7]
qAverage:[0.0, 63.08837890625]
ws:[6.816556930541992, 7.391839504241943]
memory len:3452
memory used:2809.0
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.6486875554558792, the reward is -4.0 with loss [26.27062487602234, 29.287052631378174] in episode 271
Report: 
rewardSum:-4.0
loss:[26.27062487602234, 29.287052631378174]
policies:[0, 1, 4]
qAverage:[0.0, 64.1750259399414]
ws:[-0.33309298753738403, 0.32561808824539185]
memory len:3462
memory used:2805.0
now epsilon is 0.6475532032815858, the reward is 191.33333333333334 with loss [31.86191725730896, 28.50682771205902] in episode 272
Report: 
rewardSum:191.33333333333334
loss:[31.86191725730896, 28.50682771205902]
policies:[2, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3476
memory used:2805.0
now epsilon is 0.6462592295266966, the reward is 190.33333333333334 with loss [46.858076095581055, 34.590553402900696] in episode 273
Report: 
rewardSum:190.33333333333334
loss:[46.858076095581055, 34.590553402900696]
policies:[1, 2, 5]
qAverage:[0.0, 71.36170959472656]
ws:[12.529963493347168, 14.922464370727539]
memory len:3492
memory used:2805.0
now epsilon is 0.6451291237369292, the reward is 43.33333333333334 with loss [42.31131935119629, 36.240984082221985] in episode 274
Report: 
rewardSum:43.33333333333334
loss:[42.31131935119629, 36.240984082221985]
policies:[1, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3506
memory used:2805.0
now epsilon is 0.6440009941496478, the reward is 191.33333333333334 with loss [53.45149493217468, 50.36977446079254] in episode 275
Report: 
rewardSum:191.33333333333334
loss:[53.45149493217468, 50.36977446079254]
policies:[0, 3, 4]
qAverage:[0.0, 74.24866485595703]
ws:[12.82955265045166, 14.711318016052246]
memory len:3520
memory used:2806.0
now epsilon is 0.6433572346156236, the reward is -3.0 with loss [20.046631336212158, 20.261688709259033] in episode 276
Report: 
rewardSum:-3.0
loss:[20.046631336212158, 20.261688709259033]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3528
memory used:2806.0
now epsilon is 0.6420716454587914, the reward is 42.33333333333334 with loss [33.06008982658386, 50.322808623313904] in episode 277
Report: 
rewardSum:42.33333333333334
loss:[33.06008982658386, 50.322808623313904]
policies:[1, 2, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3544
memory used:2805.0
now epsilon is 0.641109139732161, the reward is 192.33333333333334 with loss [45.9606990814209, 30.306245863437653] in episode 278
Report: 
rewardSum:192.33333333333334
loss:[45.9606990814209, 30.306245863437653]
policies:[1, 2, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3556
memory used:2805.0
now epsilon is 0.6401480768620723, the reward is -5.0 with loss [39.54164147377014, 40.50773286819458] in episode 279
Report: 
rewardSum:-5.0
loss:[39.54164147377014, 40.50773286819458]
policies:[1, 2, 3]
qAverage:[0.0, 73.8338623046875]
ws:[4.110722064971924, 5.071253299713135]
memory len:3568
memory used:2805.0
now epsilon is 0.6383898685101589, the reward is 39.33333333333334 with loss [76.96787738800049, 67.14914917945862] in episode 280
Report: 
rewardSum:39.33333333333334
loss:[76.96787738800049, 67.14914917945862]
policies:[1, 4, 6]
qAverage:[0.0, 122.92453308105469]
ws:[17.767878532409668, 19.908007431030274]
memory len:3590
memory used:2811.0
now epsilon is 0.6367956881139313, the reward is 188.33333333333334 with loss [52.350022196769714, 43.628151297569275] in episode 281
Report: 
rewardSum:188.33333333333334
loss:[52.350022196769714, 43.628151297569275]
policies:[2, 2, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3610
memory used:2811.0
now epsilon is 0.6348879256440491, the reward is 988.0 with loss [74.30072069168091, 60.67817533016205] in episode 282
Report: 
rewardSum:988.0
loss:[74.30072069168091, 60.67817533016205]
policies:[2, 2, 8]
qAverage:[51.346075439453124, 53.85074157714844]
ws:[0.33028359413146974, 0.31704235076904297]
memory len:3634
memory used:2811.0
now epsilon is 0.633302490262336, the reward is 188.33333333333334 with loss [75.24753606319427, 63.292720675468445] in episode 283
Report: 
rewardSum:188.33333333333334
loss:[75.24753606319427, 63.292720675468445]
policies:[1, 3, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3654
memory used:2811.0
now epsilon is 0.6326694252209285, the reward is 194.33333333333334 with loss [21.71742343902588, 32.58757948875427] in episode 284
Report: 
rewardSum:194.33333333333334
loss:[21.71742343902588, 32.58757948875427]
policies:[0, 1, 3]
qAverage:[0.0, 73.6530532836914]
ws:[5.028332233428955, 5.037908554077148]
memory len:3662
memory used:2811.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20*		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.6309317574724348, the reward is 989.0 with loss [76.69214779138565, 60.99977517127991] in episode 285
Report: 
rewardSum:989.0
loss:[76.69214779138565, 60.99977517127991]
policies:[3, 2, 6]
qAverage:[85.739990234375, 0.0]
ws:[-17.855129877726238, -20.581616083780926]
memory len:3684
memory used:2811.0
now epsilon is 0.6296709975361728, the reward is 42.33333333333334 with loss [37.52624535560608, 43.20190215110779] in episode 286
Report: 
rewardSum:42.33333333333334
loss:[37.52624535560608, 43.20190215110779]
policies:[2, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3700
memory used:2811.0
now epsilon is 0.6280985898118965, the reward is 188.33333333333334 with loss [80.8557203412056, 68.97283661365509] in episode 287
Report: 
rewardSum:188.33333333333334
loss:[80.8557203412056, 68.97283661365509]
policies:[2, 2, 6]
qAverage:[0.0, 86.35418701171875]
ws:[6.016845226287842, 6.482071399688721]
memory len:3720
memory used:2811.0
now epsilon is 0.6268434912553905, the reward is 992.0 with loss [41.12520891427994, 53.77878415584564] in episode 288
Report: 
rewardSum:992.0
loss:[41.12520891427994, 53.77878415584564]
policies:[3, 0, 5]
qAverage:[64.76091766357422, 0.0]
ws:[-8.941222190856934, -9.544602394104004]
memory len:3736
memory used:2817.0
now epsilon is 0.6257473375350565, the reward is 43.33333333333334 with loss [38.44450879096985, 29.771551609039307] in episode 289
Report: 
rewardSum:43.33333333333334
loss:[38.44450879096985, 29.771551609039307]
policies:[0, 2, 5]
qAverage:[0.0, 85.36602020263672]
ws:[6.5682268142700195, 7.82081413269043]
memory len:3750
memory used:2817.0
now epsilon is 0.6232490358191048, the reward is 182.33333333333334 with loss [91.62219715118408, 77.44195300340652] in episode 290
Report: 
rewardSum:182.33333333333334
loss:[91.62219715118408, 77.44195300340652]
policies:[2, 6, 8]
qAverage:[0.0, 109.15348815917969]
ws:[9.80353832244873, 12.056569735209147]
memory len:3782
memory used:2817.0
now epsilon is 0.6226260204627236, the reward is -3.0 with loss [30.326082468032837, 31.402109146118164] in episode 291
Report: 
rewardSum:-3.0
loss:[30.326082468032837, 31.402109146118164]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3790
memory used:2817.0
now epsilon is 0.622003627888107, the reward is 194.33333333333334 with loss [21.466895818710327, 26.136805057525635] in episode 292
Report: 
rewardSum:194.33333333333334
loss:[21.466895818710327, 26.136805057525635]
policies:[1, 2, 1]
qAverage:[0.0, 68.14034271240234]
ws:[3.1137146949768066, 3.4565062522888184]
memory len:3798
memory used:2817.0
now epsilon is 0.6207607085945966, the reward is 190.33333333333334 with loss [60.77507400512695, 63.197444677352905] in episode 293
Report: 
rewardSum:190.33333333333334
loss:[60.77507400512695, 63.197444677352905]
policies:[2, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3814
memory used:2817.0
now epsilon is 0.6196751917635926, the reward is 191.33333333333334 with loss [30.445933938026428, 42.43456196784973] in episode 294
Report: 
rewardSum:191.33333333333334
loss:[30.445933938026428, 42.43456196784973]
policies:[1, 2, 4]
qAverage:[0.0, 97.10811360677083]
ws:[5.092723687489827, 6.162285486857097]
memory len:3828
memory used:2817.0
now epsilon is 0.6185915731628957, the reward is 43.33333333333334 with loss [16.534322023391724, 34.75590133666992] in episode 295
Report: 
rewardSum:43.33333333333334
loss:[16.534322023391724, 34.75590133666992]
policies:[1, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3842
memory used:2818.0
now epsilon is 0.6170468328594363, the reward is -9.0 with loss [90.73549556732178, 56.126474380493164] in episode 296
Report: 
rewardSum:-9.0
loss:[90.73549556732178, 56.126474380493164]
policies:[0, 1, 9]
qAverage:[0.0, 61.46791076660156]
ws:[7.215636253356934, 8.153656959533691]
memory len:3862
memory used:2817.0
now epsilon is 0.6159678104385373, the reward is 438.0 with loss [67.81498861312866, 46.92907178401947] in episode 297
Report: 
rewardSum:438.0
loss:[67.81498861312866, 46.92907178401947]
policies:[2, 0, 5]
qAverage:[75.70523071289062, 0.0]
ws:[4.584670543670654, 4.243377208709717]
memory len:3876
memory used:2817.0
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.6148906748912482, the reward is -6.0 with loss [37.8434294462204, 32.36117434501648] in episode 298
Report: 
rewardSum:-6.0
loss:[37.8434294462204, 32.36117434501648]
policies:[1, 1, 5]
qAverage:[0.0, 64.87875366210938]
ws:[2.584099531173706, 2.871800184249878]
memory len:3890
memory used:2817.0
now epsilon is 0.6136619690622857, the reward is 190.33333333333334 with loss [31.914746284484863, 51.61428499221802] in episode 299
Report: 
rewardSum:190.33333333333334
loss:[31.914746284484863, 51.61428499221802]
policies:[1, 3, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3906
memory used:2817.0
now epsilon is 0.612742051225055, the reward is 44.33333333333334 with loss [35.06907594203949, 26.059820652008057] in episode 300
Report: 
rewardSum:44.33333333333334
loss:[35.06907594203949, 26.059820652008057]
policies:[1, 1, 4]
qAverage:[0.0, 76.84228515625]
ws:[5.071168422698975, 6.343935012817383]
memory len:3918
memory used:2830.0
now epsilon is 0.6124357184958207, the reward is -1.0 with loss [7.992320537567139, 27.833413124084473] in episode 301
Report: 
rewardSum:-1.0
loss:[7.992320537567139, 27.833413124084473]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3922
memory used:2836.0
now epsilon is 0.6099905635352653, the reward is 34.33333333333334 with loss [93.9843401312828, 93.09821599721909] in episode 302
Report: 
rewardSum:34.33333333333334
loss:[93.9843401312828, 93.09821599721909]
policies:[1, 5, 10]
qAverage:[0.0, 107.1119384765625]
ws:[13.609867413838705, 16.15448570251465]
memory len:3954
memory used:2836.0
now epsilon is 0.6095331849763135, the reward is -2.0 with loss [20.908427715301514, 10.240733027458191] in episode 303
Report: 
rewardSum:-2.0
loss:[20.908427715301514, 10.240733027458191]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3960
memory used:2836.0
now epsilon is 0.6089238803281883, the reward is 194.33333333333334 with loss [5.475767374038696, 13.552089214324951] in episode 304
Report: 
rewardSum:194.33333333333334
loss:[5.475767374038696, 13.552089214324951]
policies:[1, 2, 1]
qAverage:[0.0, 86.26758575439453]
ws:[5.775996685028076, 7.449436187744141]
memory len:3968
memory used:2836.0
now epsilon is 0.6074032820845485, the reward is 188.33333333333334 with loss [57.48638296127319, 21.536731004714966] in episode 305
Report: 
rewardSum:188.33333333333334
loss:[57.48638296127319, 21.536731004714966]
policies:[1, 2, 7]
qAverage:[0.0, 105.19300333658855]
ws:[18.421773463487625, 20.500622193018597]
memory len:3988
memory used:2836.0
now epsilon is 0.6069478435016099, the reward is -2.0 with loss [18.602360010147095, 12.752498626708984] in episode 306
Report: 
rewardSum:-2.0
loss:[18.602360010147095, 12.752498626708984]
policies:[0, 1, 2]
qAverage:[0.0, 67.44619750976562]
ws:[1.9683319330215454, 3.114673137664795]
memory len:3994
memory used:2836.0
now epsilon is 0.606341123225618, the reward is 46.33333333333334 with loss [26.323662042617798, 41.451520919799805] in episode 307
Report: 
rewardSum:46.33333333333334
loss:[26.323662042617798, 41.451520919799805]
policies:[2, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4002
memory used:2836.0
now epsilon is 0.6036183785844871, the reward is 180.33333333333334 with loss [124.20873296260834, 95.41221016645432] in episode 308
Report: 
rewardSum:180.33333333333334
loss:[124.20873296260834, 95.41221016645432]
policies:[1, 3, 14]
qAverage:[0.0, 72.59180450439453]
ws:[-0.5973685383796692, -0.300784170627594]
memory len:4038
memory used:2836.0
now epsilon is 0.6024121976314798, the reward is 190.33333333333334 with loss [44.09577864408493, 30.415347158908844] in episode 309
Report: 
rewardSum:190.33333333333334
loss:[44.09577864408493, 30.415347158908844]
policies:[1, 2, 5]
qAverage:[68.76570129394531, 0.0]
ws:[1.6953983306884766, 1.5103880167007446]
memory len:4054
memory used:2837.0
now epsilon is 0.601810011300774, the reward is 194.33333333333334 with loss [29.975996017456055, 23.084361791610718] in episode 310
Report: 
rewardSum:194.33333333333334
loss:[29.975996017456055, 23.084361791610718]
policies:[1, 2, 1]
qAverage:[0.0, 118.51702880859375]
ws:[29.522120157877605, 30.123301188151043]
memory len:4062
memory used:2837.0
now epsilon is 0.6007576333276051, the reward is 43.33333333333334 with loss [29.478646993637085, 50.756017446517944] in episode 311
Report: 
rewardSum:43.33333333333334
loss:[29.478646993637085, 50.756017446517944]
policies:[1, 2, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4076
memory used:2837.0
now epsilon is 0.5998570599001936, the reward is 44.33333333333334 with loss [32.16043400764465, 65.88578128814697] in episode 312
Report: 
rewardSum:44.33333333333334
loss:[32.16043400764465, 65.88578128814697]
policies:[2, 2, 2]
qAverage:[0.0, 100.04482014973958]
ws:[4.161484082539876, 4.295333703358968]
memory len:4088
memory used:2837.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2*		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.5989578364889169, the reward is 44.33333333333334 with loss [27.10493278503418, 24.141634702682495] in episode 313
Report: 
rewardSum:44.33333333333334
loss:[27.10493278503418, 24.141634702682495]
policies:[1, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4100
memory used:2837.0
now epsilon is 0.5983591032241842, the reward is 194.33333333333334 with loss [19.14496397972107, 23.18717932701111] in episode 314
Report: 
rewardSum:194.33333333333334
loss:[19.14496397972107, 23.18717932701111]
policies:[2, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4108
memory used:2838.0
now epsilon is 0.597462125344055, the reward is 192.33333333333334 with loss [42.83527231216431, 38.312148451805115] in episode 315
Report: 
rewardSum:192.33333333333334
loss:[42.83527231216431, 38.312148451805115]
policies:[1, 2, 3]
qAverage:[0.0, 105.9376729329427]
ws:[5.099369049072266, 5.200467427571614]
memory len:4120
memory used:2838.0
now epsilon is 0.5959701492731709, the reward is 188.33333333333334 with loss [55.1333441734314, 54.4492689371109] in episode 316
Report: 
rewardSum:188.33333333333334
loss:[55.1333441734314, 54.4492689371109]
policies:[0, 2, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4140
memory used:2838.0
now epsilon is 0.5946305565882248, the reward is 189.33333333333334 with loss [41.30454123020172, 46.474297642707825] in episode 317
Report: 
rewardSum:189.33333333333334
loss:[41.30454123020172, 46.474297642707825]
policies:[0, 4, 5]
qAverage:[0.0, 99.24135335286458]
ws:[9.776591142018637, 11.229608535766602]
memory len:4158
memory used:2838.0
now epsilon is 0.5937391680337022, the reward is 44.33333333333334 with loss [40.26797795295715, 25.388989806175232] in episode 318
Report: 
rewardSum:44.33333333333334
loss:[40.26797795295715, 25.388989806175232]
policies:[1, 2, 3]
qAverage:[0.0, 80.97274017333984]
ws:[3.9597249031066895, 5.004659652709961]
memory len:4170
memory used:2838.0
now epsilon is 0.5921084247700311, the reward is 187.33333333333334 with loss [52.33097326755524, 64.87429994344711] in episode 319
Report: 
rewardSum:187.33333333333334
loss:[52.33097326755524, 64.87429994344711]
policies:[3, 3, 5]
qAverage:[0.0, 107.92325846354167]
ws:[4.533852577209473, 6.078853607177734]
memory len:4192
memory used:2837.0
now epsilon is 0.5912208170495252, the reward is 44.33333333333334 with loss [39.16812777519226, 20.270188570022583] in episode 320
Report: 
rewardSum:44.33333333333334
loss:[39.16812777519226, 20.270188570022583]
policies:[0, 2, 4]
qAverage:[0.0, 80.73042297363281]
ws:[4.128650665283203, 5.035386562347412]
memory len:4204
memory used:2843.0
now epsilon is 0.5906298179033331, the reward is 194.33333333333334 with loss [20.59058952331543, 15.075778722763062] in episode 321
Report: 
rewardSum:194.33333333333334
loss:[20.59058952331543, 15.075778722763062]
policies:[1, 0, 3]
qAverage:[91.95977783203125, 0.0]
ws:[27.59848976135254, 27.55718994140625]
memory len:4212
memory used:2843.0
now epsilon is 0.58930222895523, the reward is 189.33333333333334 with loss [44.312754571437836, 36.71258747577667] in episode 322
Report: 
rewardSum:189.33333333333334
loss:[44.312754571437836, 36.71258747577667]
policies:[1, 3, 5]
qAverage:[0.0, 104.85234069824219]
ws:[3.5897600650787354, 4.681336482365926]
memory len:4230
memory used:2844.0
now epsilon is 0.5884188278985143, the reward is 44.33333333333334 with loss [22.990476608276367, 40.23915982246399] in episode 323
Report: 
rewardSum:44.33333333333334
loss:[22.990476608276367, 40.23915982246399]
policies:[0, 2, 4]
qAverage:[0.0, 84.20787048339844]
ws:[7.287107944488525, 8.697127342224121]
memory len:4242
memory used:2844.0
now epsilon is 0.5878306296909024, the reward is 194.33333333333334 with loss [12.53850644826889, 18.78698205947876] in episode 324
Report: 
rewardSum:194.33333333333334
loss:[12.53850644826889, 18.78698205947876]
policies:[1, 3, 0]
qAverage:[0.0, 103.16135660807292]
ws:[6.205461343129476, 7.422101974487305]
memory len:4250
memory used:2843.0
now epsilon is 0.5872430194609606, the reward is 194.33333333333334 with loss [32.96160686016083, 27.720420598983765] in episode 325
Report: 
rewardSum:194.33333333333334
loss:[32.96160686016083, 27.720420598983765]
policies:[0, 2, 2]
qAverage:[0.0, 115.23496500651042]
ws:[27.019529660542805, 28.642567316691082]
memory len:4258
memory used:2844.0
now epsilon is 0.5860695605836458, the reward is 190.33333333333334 with loss [43.53999400138855, 64.99974763393402] in episode 326
Report: 
rewardSum:190.33333333333334
loss:[43.53999400138855, 64.99974763393402]
policies:[1, 2, 5]
qAverage:[0.0, 89.88178253173828]
ws:[4.753359317779541, 6.355093002319336]
memory len:4274
memory used:2844.0
now epsilon is 0.5846060339044262, the reward is 188.33333333333334 with loss [46.28698492050171, 68.30051898956299] in episode 327
Report: 
rewardSum:188.33333333333334
loss:[46.28698492050171, 68.30051898956299]
policies:[3, 3, 4]
qAverage:[0.0, 102.65889994303386]
ws:[12.961209774017334, 14.867864608764648]
memory len:4294
memory used:2844.0
now epsilon is 0.5840216470612489, the reward is 194.33333333333334 with loss [25.13495445251465, 14.06386137008667] in episode 328
Report: 
rewardSum:194.33333333333334
loss:[25.13495445251465, 14.06386137008667]
policies:[0, 3, 1]
qAverage:[0.0, 87.11946105957031]
ws:[5.318488597869873, 6.311829566955566]
memory len:4302
memory used:2844.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20*		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.5832919849247098, the reward is 193.33333333333334 with loss [27.58968472480774, 43.46930432319641] in episode 329
Report: 
rewardSum:193.33333333333334
loss:[27.58968472480774, 43.46930432319641]
policies:[1, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4312
memory used:2844.0
now epsilon is 0.5821264212056131, the reward is 190.33333333333334 with loss [34.78076684474945, 66.51751685142517] in episode 330
Report: 
rewardSum:190.33333333333334
loss:[34.78076684474945, 66.51751685142517]
policies:[1, 4, 3]
qAverage:[0.0, 102.49866739908855]
ws:[3.6083181699117026, 5.384867032368978]
memory len:4328
memory used:2844.0
now epsilon is 0.5813991269171735, the reward is 193.33333333333334 with loss [32.48421949148178, 18.535499930381775] in episode 331
Report: 
rewardSum:193.33333333333334
loss:[32.48421949148178, 18.535499930381775]
policies:[0, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4338
memory used:2844.0
now epsilon is 0.580237345603246, the reward is 190.33333333333334 with loss [42.41636145114899, 50.081775426864624] in episode 332
Report: 
rewardSum:190.33333333333334
loss:[42.41636145114899, 50.081775426864624]
policies:[2, 3, 3]
qAverage:[0.0, 89.87491607666016]
ws:[3.331491470336914, 4.325137615203857]
memory len:4354
memory used:2844.0
now epsilon is 0.5793675333760627, the reward is 192.33333333333334 with loss [48.5994336605072, 26.679540038108826] in episode 333
Report: 
rewardSum:192.33333333333334
loss:[48.5994336605072, 26.679540038108826]
policies:[0, 2, 4]
qAverage:[0.0, 85.97026062011719]
ws:[4.050015926361084, 5.678889751434326]
memory len:4366
memory used:2844.0
now epsilon is 0.578499025052043, the reward is 44.33333333333334 with loss [16.997148513793945, 25.635077238082886] in episode 334
Report: 
rewardSum:44.33333333333334
loss:[16.997148513793945, 25.635077238082886]
policies:[1, 2, 3]
qAverage:[0.0, 100.639892578125]
ws:[3.1049728393554688, 4.092502117156982]
memory len:4378
memory used:2844.0
now epsilon is 0.5777762627422395, the reward is 193.33333333333334 with loss [36.72679567337036, 26.761804938316345] in episode 335
Report: 
rewardSum:193.33333333333334
loss:[36.72679567337036, 26.761804938316345]
policies:[1, 2, 2]
qAverage:[0.0, 81.4247055053711]
ws:[2.500488519668579, 3.6527976989746094]
memory len:4388
memory used:2844.0
now epsilon is 0.5771987031094871, the reward is -3.0 with loss [16.62069547176361, 36.983108043670654] in episode 336
Report: 
rewardSum:-3.0
loss:[16.62069547176361, 36.983108043670654]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4396
memory used:2844.0
now epsilon is 0.5766217208198187, the reward is 194.33333333333334 with loss [23.584991931915283, 33.799396991729736] in episode 337
Report: 
rewardSum:194.33333333333334
loss:[23.584991931915283, 33.799396991729736]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4404
memory used:2844.0
now epsilon is 0.5760453152961076, the reward is 194.33333333333334 with loss [19.481829404830933, 19.545799493789673] in episode 338
Report: 
rewardSum:194.33333333333334
loss:[19.481829404830933, 19.545799493789673]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4412
memory used:2844.0
now epsilon is 0.5748942322409352, the reward is 190.33333333333334 with loss [45.33117651939392, 40.557703495025635] in episode 339
Report: 
rewardSum:190.33333333333334
loss:[45.33117651939392, 40.557703495025635]
policies:[0, 4, 4]
qAverage:[0.0, 95.27510579427083]
ws:[11.553863922754923, 13.147072950998941]
memory len:4428
memory used:2844.0
now epsilon is 0.57360201297615, the reward is 189.33333333333334 with loss [55.18747282028198, 36.32199168205261] in episode 340
Report: 
rewardSum:189.33333333333334
loss:[55.18747282028198, 36.32199168205261]
policies:[1, 3, 5]
qAverage:[0.0, 93.31107584635417]
ws:[1.8063043355941772, 2.9157091776529946]
memory len:4446
memory used:2844.0
now epsilon is 0.5730286260280809, the reward is 194.33333333333334 with loss [20.28162455558777, 14.966334223747253] in episode 341
Report: 
rewardSum:194.33333333333334
loss:[20.28162455558777, 14.966334223747253]
policies:[0, 1, 3]
qAverage:[0.0, 79.67086791992188]
ws:[3.2703890800476074, 4.75729513168335]
memory len:4454
memory used:2844.0
now epsilon is 0.5724558122519756, the reward is 194.33333333333334 with loss [22.67294752597809, 13.073653936386108] in episode 342
Report: 
rewardSum:194.33333333333334
loss:[22.67294752597809, 13.073653936386108]
policies:[1, 2, 1]
qAverage:[0.0, 93.21086120605469]
ws:[30.597084045410156, 30.702621459960938]
memory len:4462
memory used:2845.0
now epsilon is 0.571311901924401, the reward is 190.33333333333334 with loss [44.10536789894104, 30.21414291858673] in episode 343
Report: 
rewardSum:190.33333333333334
loss:[44.10536789894104, 30.21414291858673]
policies:[2, 2, 4]
qAverage:[0.0, 99.26667785644531]
ws:[4.5365424156188965, 5.46063248316447]
memory len:4478
memory used:2845.0
now epsilon is 0.5707408042287351, the reward is 46.33333333333334 with loss [27.24652862548828, 10.255258083343506] in episode 344
Report: 
rewardSum:46.33333333333334
loss:[27.24652862548828, 10.255258083343506]
policies:[1, 1, 2]
qAverage:[0.0, 85.9332504272461]
ws:[3.887054204940796, 5.293947219848633]
memory len:4486
memory used:2845.0
now epsilon is 0.5688886764275076, the reward is 37.33333333333334 with loss [63.46297973394394, 59.93760359287262] in episode 345
Report: 
rewardSum:37.33333333333334
loss:[63.46297973394394, 59.93760359287262]
policies:[0, 5, 8]
qAverage:[0.0, 122.04717102050782]
ws:[8.642906951904298, 9.90958833694458]
memory len:4512
memory used:2844.0
now epsilon is 0.5683200010487806, the reward is 194.33333333333334 with loss [29.923389673233032, 15.697051048278809] in episode 346
Report: 
rewardSum:194.33333333333334
loss:[29.923389673233032, 15.697051048278809]
policies:[1, 1, 2]
qAverage:[0.0, 83.2073745727539]
ws:[3.155931234359741, 4.019382953643799]
memory len:4520
memory used:2844.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22*		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.5677518941322144, the reward is 194.33333333333334 with loss [13.745324850082397, 11.981118321418762] in episode 347
Report: 
rewardSum:194.33333333333334
loss:[13.745324850082397, 11.981118321418762]
policies:[1, 3, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4528
memory used:2844.0
now epsilon is 0.5669007983810278, the reward is -5.0 with loss [25.949416875839233, 52.27890419960022] in episode 348
Report: 
rewardSum:-5.0
loss:[25.949416875839233, 52.27890419960022]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4540
memory used:2844.0
now epsilon is 0.5666173834131373, the reward is -1.0 with loss [21.008474826812744, 6.994431138038635] in episode 349
Report: 
rewardSum:-1.0
loss:[21.008474826812744, 6.994431138038635]
policies:[1, 0, 1]
qAverage:[66.40070343017578, 0.0]
ws:[3.3966376781463623, 2.991847038269043]
memory len:4544
memory used:2844.0
now epsilon is 0.5650611318959265, the reward is 187.33333333333334 with loss [62.099517822265625, 82.56750893592834] in episode 350
Report: 
rewardSum:187.33333333333334
loss:[62.099517822265625, 82.56750893592834]
policies:[0, 5, 6]
qAverage:[0.0, 114.45657730102539]
ws:[9.281617760658264, 10.30569338798523]
memory len:4566
memory used:2844.0
now epsilon is 0.5642140697663454, the reward is 192.33333333333334 with loss [27.11479425430298, 32.35921597480774] in episode 351
Report: 
rewardSum:192.33333333333334
loss:[27.11479425430298, 32.35921597480774]
policies:[0, 3, 3]
qAverage:[0.0, 119.91765213012695]
ws:[18.071672677993774, 18.904817581176758]
memory len:4578
memory used:2844.0
now epsilon is 0.5622425262829227, the reward is 986.0 with loss [68.1106071472168, 91.27938032150269] in episode 352
Report: 
rewardSum:986.0
loss:[68.1106071472168, 91.27938032150269]
policies:[4, 3, 7]
qAverage:[32.61531066894531, 72.72002410888672]
ws:[2.4306941628456116, 3.1171810030937195]
memory len:4606
memory used:2844.0
now epsilon is 0.5616804945624492, the reward is 194.33333333333334 with loss [33.73955464363098, 17.613818883895874] in episode 353
Report: 
rewardSum:194.33333333333334
loss:[33.73955464363098, 17.613818883895874]
policies:[1, 2, 1]
qAverage:[56.731404622395836, 50.20586140950521]
ws:[22.926164944966633, 22.890350977579754]
memory len:4614
memory used:2844.0
now epsilon is 0.5611190246629695, the reward is 194.33333333333334 with loss [27.433042287826538, 17.911829233169556] in episode 354
Report: 
rewardSum:194.33333333333334
loss:[27.433042287826538, 17.911829233169556]
policies:[0, 3, 1]
qAverage:[0.0, 98.34988403320312]
ws:[4.3024327754974365, 4.818132956822713]
memory len:4622
memory used:2844.0
now epsilon is 0.5599977680811111, the reward is 190.33333333333334 with loss [43.61235237121582, 42.09286403656006] in episode 355
Report: 
rewardSum:190.33333333333334
loss:[43.61235237121582, 42.09286403656006]
policies:[1, 4, 3]
qAverage:[0.0, 107.53083801269531]
ws:[5.014133900403976, 5.623145401477814]
memory len:4638
memory used:2844.0
now epsilon is 0.5594379802771954, the reward is 194.33333333333334 with loss [21.62886357307434, 26.713980674743652] in episode 356
Report: 
rewardSum:194.33333333333334
loss:[21.62886357307434, 26.713980674743652]
policies:[1, 1, 2]
qAverage:[0.0, 79.35657501220703]
ws:[5.82573127746582, 6.101178169250488]
memory len:4646
memory used:2844.0
now epsilon is 0.5585993476050944, the reward is 192.33333333333334 with loss [33.00898319482803, 30.807939052581787] in episode 357
Report: 
rewardSum:192.33333333333334
loss:[33.00898319482803, 30.807939052581787]
policies:[1, 3, 2]
qAverage:[0.0, 71.42496490478516]
ws:[13.927094459533691, 15.434869766235352]
memory len:4658
memory used:2844.0
now epsilon is 0.5580409576973344, the reward is -3.0 with loss [21.611825048923492, 21.57234251499176] in episode 358
Report: 
rewardSum:-3.0
loss:[21.611825048923492, 21.57234251499176]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4666
memory used:2851.0
now epsilon is 0.5569258518654828, the reward is 190.33333333333334 with loss [23.27304381132126, 40.0688282251358] in episode 359
Report: 
rewardSum:190.33333333333334
loss:[23.27304381132126, 40.0688282251358]
policies:[0, 5, 3]
qAverage:[0.0, 123.68731689453125]
ws:[14.137980341911316, 16.091363509496052]
memory len:4682
memory used:2851.0
now epsilon is 0.5555351025459984, the reward is 188.33333333333334 with loss [70.46239256858826, 38.02715528011322] in episode 360
Report: 
rewardSum:188.33333333333334
loss:[70.46239256858826, 38.02715528011322]
policies:[3, 3, 4]
qAverage:[0.0, 99.64465840657552]
ws:[10.48489507039388, 12.420377572377523]
memory len:4702
memory used:2851.0
now epsilon is 0.5544250040413945, the reward is 190.33333333333334 with loss [43.13467335700989, 56.594846367836] in episode 361
Report: 
rewardSum:190.33333333333334
loss:[43.13467335700989, 56.594846367836]
policies:[0, 4, 4]
qAverage:[0.0, 94.71346537272136]
ws:[14.764546553293863, 15.700780391693115]
memory len:4718
memory used:2851.0
now epsilon is 0.5537323192153522, the reward is -4.0 with loss [15.45666790008545, 13.870399475097656] in episode 362
Report: 
rewardSum:-4.0
loss:[15.45666790008545, 13.870399475097656]
policies:[1, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4728
memory used:2851.0
now epsilon is 0.5529022396875696, the reward is 192.33333333333334 with loss [12.71418023109436, 24.61482560634613] in episode 363
Report: 
rewardSum:192.33333333333334
loss:[12.71418023109436, 24.61482560634613]
policies:[0, 2, 4]
qAverage:[0.0, 65.823974609375]
ws:[0.610965371131897, 0.8861719965934753]
memory len:4740
memory used:2851.0
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.5515215380896618, the reward is -9.0 with loss [40.74632120132446, 50.24614429473877] in episode 364
Report: 
rewardSum:-9.0
loss:[40.74632120132446, 50.24614429473877]
policies:[0, 2, 8]
qAverage:[0.0, 94.66147867838542]
ws:[16.038874944051106, 17.846320470174152]
memory len:4760
memory used:2849.0
now epsilon is 0.5505570989684857, the reward is 191.33333333333334 with loss [44.25024902820587, 23.089745193719864] in episode 365
Report: 
rewardSum:191.33333333333334
loss:[44.25024902820587, 23.089745193719864]
policies:[2, 3, 2]
qAverage:[0.0, 100.52378336588542]
ws:[13.266254583994547, 13.398037751515707]
memory len:4774
memory used:2849.0
now epsilon is 0.5491822536310624, the reward is 188.33333333333334 with loss [67.5655996799469, 45.58776891231537] in episode 366
Report: 
rewardSum:188.33333333333334
loss:[67.5655996799469, 45.58776891231537]
policies:[1, 2, 7]
qAverage:[0.0, 98.61895243326823]
ws:[5.9798173904418945, 6.816950798034668]
memory len:4794
memory used:2849.0
now epsilon is 0.5486332772864547, the reward is 194.33333333333334 with loss [10.05674260854721, 22.161635637283325] in episode 367
Report: 
rewardSum:194.33333333333334
loss:[10.05674260854721, 22.161635637283325]
policies:[2, 2, 0]
qAverage:[53.84215799967448, 51.34893290201823]
ws:[21.14570077260335, 21.2420867284139]
memory len:4802
memory used:2849.0
now epsilon is 0.5480848497123598, the reward is 194.33333333333334 with loss [15.93910300731659, 20.349926471710205] in episode 368
Report: 
rewardSum:194.33333333333334
loss:[15.93910300731659, 20.349926471710205]
policies:[0, 2, 2]
qAverage:[0.0, 88.38008626302083]
ws:[2.918077230453491, 3.514912207921346]
memory len:4810
memory used:2855.0
now epsilon is 0.547536970360213, the reward is 46.33333333333334 with loss [12.376820683479309, 13.905930995941162] in episode 369
Report: 
rewardSum:46.33333333333334
loss:[12.376820683479309, 13.905930995941162]
policies:[2, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4818
memory used:2855.0
now epsilon is 0.5468528912723272, the reward is 193.33333333333334 with loss [21.10735821723938, 15.264091968536377] in episode 370
Report: 
rewardSum:193.33333333333334
loss:[21.10735821723938, 15.264091968536377]
policies:[1, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4828
memory used:2855.0
now epsilon is 0.5463062434167131, the reward is 194.33333333333334 with loss [17.905791997909546, 27.582536220550537] in episode 371
Report: 
rewardSum:194.33333333333334
loss:[17.905791997909546, 27.582536220550537]
policies:[1, 2, 1]
qAverage:[0.0, 65.64171600341797]
ws:[0.2936963737010956, 0.50347900390625]
memory len:4836
memory used:2855.0
now epsilon is 0.5454872960430026, the reward is 44.33333333333334 with loss [26.732167720794678, 20.527031302452087] in episode 372
Report: 
rewardSum:44.33333333333334
loss:[26.732167720794678, 20.527031302452087]
policies:[1, 1, 4]
qAverage:[0.0, 69.9308853149414]
ws:[3.2006945610046387, 3.2199337482452393]
memory len:4848
memory used:2856.0
now epsilon is 0.5449420132706049, the reward is -3.0 with loss [24.75439417362213, 22.373919367790222] in episode 373
Report: 
rewardSum:-3.0
loss:[24.75439417362213, 22.373919367790222]
policies:[0, 1, 3]
qAverage:[0.0, 63.55131912231445]
ws:[2.131636619567871, 2.3216514587402344]
memory len:4856
memory used:2856.0
now epsilon is 0.5438530824159117, the reward is 42.33333333333334 with loss [22.709992319345474, 47.62059462070465] in episode 374
Report: 
rewardSum:42.33333333333334
loss:[22.709992319345474, 47.62059462070465]
policies:[0, 4, 4]
qAverage:[0.0, 107.41436386108398]
ws:[9.858035802841187, 10.58866536617279]
memory len:4872
memory used:2927.0
now epsilon is 0.543309433244413, the reward is 194.33333333333334 with loss [30.27865695953369, 17.475511074066162] in episode 375
Report: 
rewardSum:194.33333333333334
loss:[30.27865695953369, 17.475511074066162]
policies:[1, 1, 2]
qAverage:[0.0, 74.23595428466797]
ws:[2.5720014572143555, 2.9968101978302]
memory len:4880
memory used:2944.0
now epsilon is 0.5423593545328185, the reward is 191.33333333333334 with loss [47.576261043548584, 31.54933524131775] in episode 376
Report: 
rewardSum:191.33333333333334
loss:[47.576261043548584, 31.54933524131775]
policies:[0, 3, 4]
qAverage:[0.0, 98.45074971516927]
ws:[14.517165184020996, 15.123263994852701]
memory len:4894
memory used:2944.0
now epsilon is 0.5399239168401455, the reward is 180.33333333333334 with loss [97.19277340173721, 82.03850483894348] in episode 377
Report: 
rewardSum:180.33333333333334
loss:[97.19277340173721, 82.03850483894348]
policies:[1, 5, 12]
qAverage:[0.0, 101.42381286621094]
ws:[8.0521290153265, 9.152493119239807]
memory len:4930
memory used:2943.0
now epsilon is 0.5387103021476836, the reward is 189.33333333333334 with loss [66.42380648851395, 61.31042659282684] in episode 378
Report: 
rewardSum:189.33333333333334
loss:[66.42380648851395, 61.31042659282684]
policies:[2, 2, 5]
qAverage:[0.0, 78.95222727457683]
ws:[1.2541567087173462, 2.3500578800837197]
memory len:4948
memory used:2889.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21*		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.537902741567055, the reward is 192.33333333333334 with loss [24.931236028671265, 29.824623942375183] in episode 379
Report: 
rewardSum:192.33333333333334
loss:[24.931236028671265, 29.824623942375183]
policies:[2, 1, 3]
qAverage:[0.0, 61.588802337646484]
ws:[0.3445494472980499, 0.5349313616752625]
memory len:4960
memory used:2889.0
now epsilon is 0.5370963915704616, the reward is 44.33333333333334 with loss [25.07216167449951, 21.698240756988525] in episode 380
Report: 
rewardSum:44.33333333333334
loss:[25.07216167449951, 21.698240756988525]
policies:[2, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4972
memory used:2889.0
now epsilon is 0.5361571775305761, the reward is 993.0 with loss [18.718081027269363, 48.057722091674805] in episode 381
Report: 
rewardSum:993.0
loss:[18.718081027269363, 48.057722091674805]
policies:[2, 1, 4]
qAverage:[40.13981374104818, 39.1821543375651]
ws:[-4.059705893198649, -4.385036627451579]
memory len:4986
memory used:2885.0
now epsilon is 0.5349520295313395, the reward is 991.0 with loss [65.64247369766235, 37.307230830192566] in episode 382
Report: 
rewardSum:991.0
loss:[65.64247369766235, 37.307230830192566]
policies:[2, 1, 6]
qAverage:[38.612876892089844, 42.95508321126302]
ws:[-1.482065200805664, -1.3546772003173828]
memory len:5004
memory used:2885.0
now epsilon is 0.5338830611703919, the reward is 190.33333333333334 with loss [35.29960525035858, 46.20082902908325] in episode 383
Report: 
rewardSum:190.33333333333334
loss:[35.29960525035858, 46.20082902908325]
policies:[1, 4, 3]
qAverage:[0.0, 102.7050277709961]
ws:[7.584751486778259, 8.638621711730957]
memory len:5020
memory used:2884.0
now epsilon is 0.533082736927199, the reward is 994.0 with loss [23.47296917438507, 29.094109773635864] in episode 384
Report: 
rewardSum:994.0
loss:[23.47296917438507, 29.094109773635864]
policies:[2, 2, 2]
qAverage:[39.15323384602865, 43.62348937988281]
ws:[-3.784606615702311, -3.7374583880106607]
memory len:5032
memory used:2884.0
now epsilon is 0.5313528144298032, the reward is 185.33333333333334 with loss [49.285272657871246, 76.45160555839539] in episode 385
Report: 
rewardSum:185.33333333333334
loss:[49.285272657871246, 76.45160555839539]
policies:[1, 5, 7]
qAverage:[0.0, 79.50189208984375]
ws:[0.4821822231945892, 1.1870331366856892]
memory len:5058
memory used:2878.0
now epsilon is 0.5309543994393312, the reward is -2.0 with loss [13.469590187072754, 7.940710783004761] in episode 386
Report: 
rewardSum:-2.0
loss:[13.469590187072754, 7.940710783004761]
policies:[0, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5064
memory used:2878.0
now epsilon is 0.5304236441146092, the reward is 194.33333333333334 with loss [17.697974920272827, 14.835025787353516] in episode 387
Report: 
rewardSum:194.33333333333334
loss:[17.697974920272827, 14.835025787353516]
policies:[2, 2, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5072
memory used:2878.0
now epsilon is 0.5294960986284387, the reward is 191.33333333333334 with loss [26.14262592792511, 50.78587472438812] in episode 388
Report: 
rewardSum:191.33333333333334
loss:[26.14262592792511, 50.78587472438812]
policies:[0, 5, 2]
qAverage:[0.0, 65.5655517578125]
ws:[2.121945858001709, 2.6514317989349365]
memory len:5086
memory used:2878.0
now epsilon is 0.5290990758267127, the reward is -2.0 with loss [5.3102816343307495, 10.237749099731445] in episode 389
Report: 
rewardSum:-2.0
loss:[5.3102816343307495, 10.237749099731445]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5092
memory used:2877.0
now epsilon is 0.5281738465972744, the reward is 43.33333333333334 with loss [28.57215976715088, 30.65079426765442] in episode 390
Report: 
rewardSum:43.33333333333334
loss:[28.57215976715088, 30.65079426765442]
policies:[0, 2, 5]
qAverage:[0.0, 59.01060104370117]
ws:[1.4391140937805176, 1.7325681447982788]
memory len:5106
memory used:2877.0
now epsilon is 0.52777781523667, the reward is -2.0 with loss [14.729577541351318, 9.03233551979065] in episode 391
Report: 
rewardSum:-2.0
loss:[14.729577541351318, 9.03233551979065]
policies:[0, 1, 2]
qAverage:[0.0, 56.255218505859375]
ws:[1.975389838218689, 2.291311264038086]
memory len:5112
memory used:2877.0
now epsilon is 0.5269866431406173, the reward is 192.33333333333334 with loss [37.47233438491821, 15.493270993232727] in episode 392
Report: 
rewardSum:192.33333333333334
loss:[37.47233438491821, 15.493270993232727]
policies:[1, 2, 3]
qAverage:[0.0, 64.92927551269531]
ws:[2.107349395751953, 2.8264241218566895]
memory len:5124
memory used:2877.0
now epsilon is 0.5260651078969666, the reward is 191.33333333333334 with loss [43.24751567840576, 46.80747562646866] in episode 393
Report: 
rewardSum:191.33333333333334
loss:[43.24751567840576, 46.80747562646866]
policies:[0, 2, 5]
qAverage:[0.0, 60.71903610229492]
ws:[1.7021312713623047, 2.5113160610198975]
memory len:5138
memory used:2882.0
now epsilon is 0.5251451841309811, the reward is 43.33333333333334 with loss [30.44778800010681, 40.98099362850189] in episode 394
Report: 
rewardSum:43.33333333333334
loss:[30.44778800010681, 40.98099362850189]
policies:[0, 2, 5]
qAverage:[0.0, 68.58600616455078]
ws:[-0.05438109114766121, 0.8714953660964966]
memory len:5152
memory used:2882.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40*		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.5237028387079783, the reward is 187.33333333333334 with loss [48.82448327541351, 79.2637848854065] in episode 395
Report: 
rewardSum:187.33333333333334
loss:[48.82448327541351, 79.2637848854065]
policies:[2, 4, 5]
qAverage:[26.278915405273438, 75.23871154785157]
ws:[7.574787759780884, 8.00772728919983]
memory len:5174
memory used:2882.0
now epsilon is 0.5227870458138869, the reward is 191.33333333333334 with loss [43.60082495212555, 50.863003969192505] in episode 396
Report: 
rewardSum:191.33333333333334
loss:[43.60082495212555, 50.863003969192505]
policies:[1, 3, 3]
qAverage:[29.30295181274414, 63.39238357543945]
ws:[6.929992198944092, 7.267273098230362]
memory len:5188
memory used:2882.0
now epsilon is 0.5222644547805431, the reward is 46.33333333333334 with loss [13.462603330612183, 16.604533672332764] in episode 397
Report: 
rewardSum:46.33333333333334
loss:[13.462603330612183, 16.604533672332764]
policies:[0, 3, 1]
qAverage:[0.0, 83.31205749511719]
ws:[1.6622437884410222, 1.7501524488131206]
memory len:5196
memory used:2882.0
now epsilon is 0.5217423861422936, the reward is 194.33333333333334 with loss [39.28493022918701, 12.32532000541687] in episode 398
Report: 
rewardSum:194.33333333333334
loss:[39.28493022918701, 12.32532000541687]
policies:[1, 2, 1]
qAverage:[0.0, 90.5828857421875]
ws:[13.442081610361734, 14.45645554860433]
memory len:5204
memory used:2895.0
now epsilon is 0.5212208393769392, the reward is 46.33333333333334 with loss [23.31351137161255, 21.25735092163086] in episode 399
Report: 
rewardSum:46.33333333333334
loss:[23.31351137161255, 21.25735092163086]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5212
memory used:2907.0
now epsilon is 0.5209602615335532, the reward is -1.0 with loss [3.2515318393707275, 18.58998954296112] in episode 400
Report: 
rewardSum:-1.0
loss:[3.2515318393707275, 18.58998954296112]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5216
memory used:2908.0
now epsilon is 0.5206998139628027, the reward is -1.0 with loss [6.6847217082977295, 11.803227424621582] in episode 401
Report: 
rewardSum:-1.0
loss:[6.6847217082977295, 11.803227424621582]
policies:[0, 1, 1]
qAverage:[0.0, 57.559661865234375]
ws:[1.8377193212509155, 2.2180891036987305]
memory len:5220
memory used:2908.0
now epsilon is 0.5197892724221872, the reward is 191.33333333333334 with loss [39.5081467628479, 23.440265357494354] in episode 402
Report: 
rewardSum:191.33333333333334
loss:[39.5081467628479, 23.440265357494354]
policies:[2, 1, 4]
qAverage:[0.0, 57.97796630859375]
ws:[14.397285461425781, 15.96124267578125]
memory len:5234
memory used:2908.0
now epsilon is 0.5192696780382574, the reward is 194.33333333333334 with loss [9.360410451889038, 22.986258506774902] in episode 403
Report: 
rewardSum:194.33333333333334
loss:[9.360410451889038, 22.986258506774902]
policies:[3, 1, 0]
qAverage:[0.0, 56.948246002197266]
ws:[-1.671568751335144, -1.4636167287826538]
memory len:5242
memory used:2908.0
now epsilon is 0.5183616373592383, the reward is 43.33333333333334 with loss [30.510216236114502, 28.94322896003723] in episode 404
Report: 
rewardSum:43.33333333333334
loss:[30.510216236114502, 28.94322896003723]
policies:[1, 2, 4]
qAverage:[0.0, 80.55708312988281]
ws:[3.0068652431170144, 4.192823211352031]
memory len:5256
memory used:2881.0
now epsilon is 0.5178434700750976, the reward is 194.33333333333334 with loss [26.609304547309875, 29.11418652534485] in episode 405
Report: 
rewardSum:194.33333333333334
loss:[26.609304547309875, 29.11418652534485]
policies:[0, 3, 1]
qAverage:[0.0, 76.57644399007161]
ws:[11.64784828821818, 12.98820956548055]
memory len:5264
memory used:2887.0
now epsilon is 0.5173258207639606, the reward is 194.33333333333334 with loss [9.112077623605728, 34.66376233100891] in episode 406
Report: 
rewardSum:194.33333333333334
loss:[9.112077623605728, 34.66376233100891]
policies:[1, 2, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5272
memory used:2898.0
now epsilon is 0.5162920739901004, the reward is 42.33333333333334 with loss [41.191500663757324, 32.36499607563019] in episode 407
Report: 
rewardSum:42.33333333333334
loss:[41.191500663757324, 32.36499607563019]
policies:[1, 3, 4]
qAverage:[0.0, 82.58537038167317]
ws:[2.7991484800974527, 4.025362968444824]
memory len:5288
memory used:2898.0
now epsilon is 0.5147453256991793, the reward is 186.33333333333334 with loss [73.06059062480927, 79.37350896000862] in episode 408
Report: 
rewardSum:186.33333333333334
loss:[73.06059062480927, 79.37350896000862]
policies:[1, 5, 6]
qAverage:[0.0, 80.56356048583984]
ws:[3.683537801106771, 4.451546351114909]
memory len:5312
memory used:2898.0
now epsilon is 0.5139736901235457, the reward is 192.33333333333334 with loss [32.640201926231384, 27.208697855472565] in episode 409
Report: 
rewardSum:192.33333333333334
loss:[32.640201926231384, 27.208697855472565]
policies:[0, 4, 2]
qAverage:[0.0, 81.82432810465495]
ws:[0.9015901287396749, 1.519289255142212]
memory len:5324
memory used:2898.0
now epsilon is 0.5133315441641494, the reward is 45.33333333333334 with loss [31.176310539245605, 31.887553989887238] in episode 410
Report: 
rewardSum:45.33333333333334
loss:[31.176310539245605, 31.887553989887238]
policies:[2, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5334
memory used:2898.0
now epsilon is 0.5128184050872331, the reward is 194.33333333333334 with loss [27.422049522399902, 17.413820326328278] in episode 411
Report: 
rewardSum:194.33333333333334
loss:[27.422049522399902, 17.413820326328278]
policies:[1, 2, 1]
qAverage:[0.0, 86.1745122273763]
ws:[14.518104553222656, 15.854821840922037]
memory len:5342
memory used:2900.0
now epsilon is 0.5121777025122595, the reward is 45.33333333333334 with loss [36.12632644176483, 11.578829884529114] in episode 412
Report: 
rewardSum:45.33333333333334
loss:[36.12632644176483, 11.578829884529114]
policies:[0, 2, 3]
qAverage:[0.0, 62.174591064453125]
ws:[3.4755609035491943, 4.126822471618652]
memory len:5352
memory used:2900.0
now epsilon is 0.5114099159650617, the reward is 192.33333333333334 with loss [54.99162995815277, 25.53161758184433] in episode 413
Report: 
rewardSum:192.33333333333334
loss:[54.99162995815277, 25.53161758184433]
policies:[1, 2, 3]
qAverage:[0.0, 57.766170501708984]
ws:[11.20067310333252, 12.80561637878418]
memory len:5364
memory used:2899.0
now epsilon is 0.511154242970199, the reward is -1.0 with loss [6.855211019515991, 18.38508892059326] in episode 414
Report: 
rewardSum:-1.0
loss:[6.855211019515991, 18.38508892059326]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5368
memory used:2899.0
now epsilon is 0.5106432803781248, the reward is 194.33333333333334 with loss [14.785083770751953, 19.560009241104126] in episode 415
Report: 
rewardSum:194.33333333333334
loss:[14.785083770751953, 19.560009241104126]
policies:[1, 2, 1]
qAverage:[0.0, 61.57568359375]
ws:[1.4233533143997192, 1.9907225370407104]
memory len:5376
memory used:2899.0
now epsilon is 0.509877794026087, the reward is 192.33333333333334 with loss [37.25551974773407, 41.41128873825073] in episode 416
Report: 
rewardSum:192.33333333333334
loss:[37.25551974773407, 41.41128873825073]
policies:[0, 2, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5388
memory used:2899.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20*		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.5091134551861729, the reward is 192.33333333333334 with loss [34.82829165458679, 29.78192439675331] in episode 417
Report: 
rewardSum:192.33333333333334
loss:[34.82829165458679, 29.78192439675331]
policies:[1, 3, 2]
qAverage:[0.0, 93.61155128479004]
ws:[14.478786647319794, 15.780660033226013]
memory len:5400
memory used:2899.0
now epsilon is 0.5083502621381898, the reward is 192.33333333333334 with loss [21.76773428916931, 39.724445104599] in episode 418
Report: 
rewardSum:192.33333333333334
loss:[21.76773428916931, 39.724445104599]
policies:[2, 2, 2]
qAverage:[42.83187357584635, 36.53874206542969]
ws:[18.590693791707356, 19.539328893025715]
memory len:5412
memory used:2899.0
now epsilon is 0.5073344507822047, the reward is 190.33333333333334 with loss [33.16934788227081, 35.71827030181885] in episode 419
Report: 
rewardSum:190.33333333333334
loss:[33.16934788227081, 35.71827030181885]
policies:[2, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5428
memory used:2899.0
now epsilon is 0.5070808152652169, the reward is -1.0 with loss [7.7835774421691895, 19.091785430908203] in episode 420
Report: 
rewardSum:-1.0
loss:[7.7835774421691895, 19.091785430908203]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5432
memory used:2899.0
now epsilon is 0.506573924573567, the reward is 194.33333333333334 with loss [35.11728119850159, 6.478992521762848] in episode 421
Report: 
rewardSum:194.33333333333334
loss:[35.11728119850159, 6.478992521762848]
policies:[1, 3, 0]
qAverage:[0.0, 65.65335845947266]
ws:[14.358368873596191, 14.924553871154785]
memory len:5440
memory used:2900.0
now epsilon is 0.5060675405825563, the reward is 194.33333333333334 with loss [13.790939092636108, 14.702246189117432] in episode 422
Report: 
rewardSum:194.33333333333334
loss:[13.790939092636108, 14.702246189117432]
policies:[0, 3, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5448
memory used:2899.0
now epsilon is 0.5042991795733732, the reward is 184.33333333333334 with loss [80.3377137184143, 109.44464421272278] in episode 423
Report: 
rewardSum:184.33333333333334
loss:[80.3377137184143, 109.44464421272278]
policies:[2, 4, 8]
qAverage:[0.0, 85.29565048217773]
ws:[5.807151526212692, 7.2806603610515594]
memory len:5476
memory used:2900.0
now epsilon is 0.5030398490207351, the reward is 990.0 with loss [80.07038402557373, 35.47543531656265] in episode 424
Report: 
rewardSum:990.0
loss:[80.07038402557373, 35.47543531656265]
policies:[0, 2, 8]
qAverage:[0.0, 53.57429504394531]
ws:[0.9203226566314697, 1.4466884136199951]
memory len:5496
memory used:2912.0
now epsilon is 0.5024113635307749, the reward is 193.33333333333334 with loss [47.281381607055664, 30.586320400238037] in episode 425
Report: 
rewardSum:193.33333333333334
loss:[47.281381607055664, 30.586320400238037]
policies:[1, 1, 3]
qAverage:[0.0, 56.60394287109375]
ws:[2.224848985671997, 3.583031177520752]
memory len:5506
memory used:2911.0
now epsilon is 0.5017836632549717, the reward is 193.33333333333334 with loss [21.52351176738739, 42.662211894989014] in episode 426
Report: 
rewardSum:193.33333333333334
loss:[21.52351176738739, 42.662211894989014]
policies:[0, 3, 2]
qAverage:[0.0, 84.79848861694336]
ws:[9.632285729050636, 11.327558845281601]
memory len:5516
memory used:2912.0
now epsilon is 0.5012820677292309, the reward is 194.33333333333334 with loss [9.795838117599487, 10.922028958797455] in episode 427
Report: 
rewardSum:194.33333333333334
loss:[9.795838117599487, 10.922028958797455]
policies:[0, 4, 0]
qAverage:[0.0, 89.1590404510498]
ws:[10.53513091802597, 12.42035448551178]
memory len:5524
memory used:2912.0
now epsilon is 0.5006557783675463, the reward is 193.33333333333334 with loss [61.744314670562744, 21.343096673488617] in episode 428
Report: 
rewardSum:193.33333333333334
loss:[61.744314670562744, 21.343096673488617]
policies:[0, 2, 3]
qAverage:[0.0, 83.47123209635417]
ws:[11.73226797580719, 13.337733586629232]
memory len:5534
memory used:2912.0
now epsilon is 0.5000302714762307, the reward is 193.33333333333334 with loss [38.481560945510864, 13.542036533355713] in episode 429
Report: 
rewardSum:193.33333333333334
loss:[38.481560945510864, 13.542036533355713]
policies:[1, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5544
memory used:2912.0
now epsilon is 0.4997802875923846, the reward is -1.0 with loss [17.596101760864258, 4.862900018692017] in episode 430
Report: 
rewardSum:-1.0
loss:[17.596101760864258, 4.862900018692017]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5548
memory used:2912.0
now epsilon is 0.4986569057952332, the reward is -8.0 with loss [43.380898237228394, 51.010045886039734] in episode 431
Report: 
rewardSum:-8.0
loss:[43.380898237228394, 51.010045886039734]
policies:[1, 1, 7]
qAverage:[0.0, 55.68648147583008]
ws:[9.790908813476562, 12.190753936767578]
memory len:5566
memory used:2912.0
now epsilon is 0.49815843585461356, the reward is 194.33333333333334 with loss [16.50178349018097, 10.792048871517181] in episode 432
Report: 
rewardSum:194.33333333333334
loss:[16.50178349018097, 10.792048871517181]
policies:[1, 2, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5574
memory used:2912.0
now epsilon is 0.49766046419703946, the reward is 194.33333333333334 with loss [7.391679883003235, 11.65510368347168] in episode 433
Report: 
rewardSum:194.33333333333334
loss:[7.391679883003235, 11.65510368347168]
policies:[0, 3, 1]
qAverage:[0.0, 78.88738505045573]
ws:[16.874862670898438, 18.875537236531574]
memory len:5582
memory used:2912.0
now epsilon is 0.49741166506872, the reward is -1.0 with loss [6.323357105255127, 6.627069354057312] in episode 434
Report: 
rewardSum:-1.0
loss:[6.323357105255127, 6.627069354057312]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5586
memory used:2912.0
now epsilon is 0.4969144399019394, the reward is 194.33333333333334 with loss [11.99580717086792, 23.864912509918213] in episode 435
Report: 
rewardSum:194.33333333333334
loss:[11.99580717086792, 23.864912509918213]
policies:[1, 3, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5594
memory used:2912.0
now epsilon is 0.49666601373914093, the reward is -1.0 with loss [8.063504815101624, 11.454894542694092] in episode 436
Report: 
rewardSum:-1.0
loss:[8.063504815101624, 11.454894542694092]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5598
memory used:2912.0
now epsilon is 0.49616953394411734, the reward is 194.33333333333334 with loss [20.945165038108826, 25.870687246322632] in episode 437
Report: 
rewardSum:194.33333333333334
loss:[20.945165038108826, 25.870687246322632]
policies:[1, 2, 1]
qAverage:[0.0, 74.47915649414062]
ws:[4.31182066599528, 5.23727019627889]
memory len:5606
memory used:2912.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38*		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.4954257446471154, the reward is 192.33333333333334 with loss [38.910139083862305, 35.673863530159] in episode 438
Report: 
rewardSum:192.33333333333334
loss:[38.910139083862305, 35.673863530159]
policies:[2, 2, 2]
qAverage:[0.0, 47.1667594909668]
ws:[0.8660886883735657, 1.117713451385498]
memory len:5618
memory used:2913.0
now epsilon is 0.4949305046561604, the reward is 194.33333333333334 with loss [27.238487601280212, 19.849552631378174] in episode 439
Report: 
rewardSum:194.33333333333334
loss:[27.238487601280212, 19.849552631378174]
policies:[1, 3, 0]
qAverage:[0.0, 77.71998596191406]
ws:[11.283998171488443, 12.368141651153564]
memory len:5626
memory used:2913.0
now epsilon is 0.4944357597195123, the reward is 46.33333333333334 with loss [21.544391632080078, 25.240596413612366] in episode 440
Report: 
rewardSum:46.33333333333334
loss:[21.544391632080078, 25.240596413612366]
policies:[0, 1, 3]
qAverage:[0.0, 57.207191467285156]
ws:[2.810680627822876, 3.4994380474090576]
memory len:5634
memory used:2912.0
now epsilon is 0.49394150934230246, the reward is 194.33333333333334 with loss [17.57520294189453, 26.8305823802948] in episode 441
Report: 
rewardSum:194.33333333333334
loss:[17.57520294189453, 26.8305823802948]
policies:[0, 3, 1]
qAverage:[0.0, 71.96299997965495]
ws:[3.5977020263671875, 4.8328750928243]
memory len:5642
memory used:2912.0
now epsilon is 0.49320105999412633, the reward is 192.33333333333334 with loss [22.658162593841553, 21.875523805618286] in episode 442
Report: 
rewardSum:192.33333333333334
loss:[22.658162593841553, 21.875523805618286]
policies:[2, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5654
memory used:2913.0
now epsilon is 0.4927080438537066, the reward is 194.33333333333334 with loss [23.875427722930908, 13.673239707946777] in episode 443
Report: 
rewardSum:194.33333333333334
loss:[23.875427722930908, 13.673239707946777]
policies:[0, 4, 0]
qAverage:[0.0, 57.667335510253906]
ws:[4.132200717926025, 5.189985275268555]
memory len:5662
memory used:2913.0
now epsilon is 0.4917234895740912, the reward is 190.33333333333334 with loss [37.52411341667175, 58.758699893951416] in episode 444
Report: 
rewardSum:190.33333333333334
loss:[37.52411341667175, 58.758699893951416]
policies:[0, 3, 5]
qAverage:[0.0, 66.61726633707683]
ws:[1.9292895793914795, 3.277467886606852]
memory len:5678
memory used:2913.0
now epsilon is 0.4912319504500949, the reward is 46.33333333333334 with loss [12.641945600509644, 22.83432698249817] in episode 445
Report: 
rewardSum:46.33333333333334
loss:[12.641945600509644, 22.83432698249817]
policies:[0, 2, 2]
qAverage:[0.0, 55.90851974487305]
ws:[2.216184377670288, 3.3922603130340576]
memory len:5686
memory used:2912.0
now epsilon is 0.4898827499303626, the reward is 989.0 with loss [61.81253433227539, 87.43088960647583] in episode 446
Report: 
rewardSum:989.0
loss:[61.81253433227539, 87.43088960647583]
policies:[1, 1, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5708
memory used:2913.0
now epsilon is 0.4889038412968009, the reward is 190.33333333333334 with loss [34.45099091529846, 57.4597350358963] in episode 447
Report: 
rewardSum:190.33333333333334
loss:[34.45099091529846, 57.4597350358963]
policies:[1, 3, 4]
qAverage:[0.0, 52.460723876953125]
ws:[1.5490599870681763, 1.866774082183838]
memory len:5724
memory used:2913.0
now epsilon is 0.48817094372945324, the reward is 192.33333333333334 with loss [24.22568106651306, 18.41929841041565] in episode 448
Report: 
rewardSum:192.33333333333334
loss:[24.22568106651306, 18.41929841041565]
policies:[1, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5736
memory used:2913.0
now epsilon is 0.487439144821594, the reward is 44.33333333333334 with loss [19.173220574855804, 38.81342601776123] in episode 449
Report: 
rewardSum:44.33333333333334
loss:[19.173220574855804, 38.81342601776123]
policies:[1, 3, 2]
qAverage:[0.0, 64.82158915201823]
ws:[5.36139610906442, 6.531345049540202]
memory len:5748
memory used:2913.0
now epsilon is 0.48658676581553223, the reward is 191.33333333333334 with loss [37.21375334262848, 44.62557053565979] in episode 450
Report: 
rewardSum:191.33333333333334
loss:[37.21375334262848, 44.62557053565979]
policies:[1, 1, 5]
qAverage:[0.0, 50.695350646972656]
ws:[1.3780179023742676, 1.551714539527893]
memory len:5762
memory used:2913.0
now epsilon is 0.4858573416898721, the reward is 44.33333333333334 with loss [32.55453824996948, 18.16818904876709] in episode 451
Report: 
rewardSum:44.33333333333334
loss:[32.55453824996948, 18.16818904876709]
policies:[2, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5774
memory used:2914.0
now epsilon is 0.485007728764039, the reward is 43.33333333333334 with loss [23.152586340904236, 33.51850402355194] in episode 452
Report: 
rewardSum:43.33333333333334
loss:[23.152586340904236, 33.51850402355194]
policies:[2, 1, 4]
qAverage:[30.700169881184895, 34.795997619628906]
ws:[2.58577561378479, 2.511930227279663]
memory len:5788
memory used:2914.0
now epsilon is 0.4842806717141023, the reward is 192.33333333333334 with loss [43.73615372180939, 38.476014375686646] in episode 453
Report: 
rewardSum:192.33333333333334
loss:[43.73615372180939, 38.476014375686646]
policies:[3, 1, 2]
qAverage:[47.11347198486328, 29.338054656982422]
ws:[8.804238140583038, 8.587063670158386]
memory len:5800
memory used:2914.0
now epsilon is 0.48403856164578724, the reward is -1.0 with loss [15.071200609207153, 16.06913924217224] in episode 454
Report: 
rewardSum:-1.0
loss:[15.071200609207153, 16.06913924217224]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5804
memory used:2914.0
now epsilon is 0.48379657261737447, the reward is -1.0 with loss [4.406158924102783, 5.0559022426605225] in episode 455
Report: 
rewardSum:-1.0
loss:[4.406158924102783, 5.0559022426605225]
policies:[1, 0, 1]
qAverage:[43.178436279296875, 0.0]
ws:[-0.5093896985054016, -0.9141069054603577]
memory len:5808
memory used:2914.0
now epsilon is 0.48295056333378567, the reward is 191.33333333333334 with loss [31.9216947555542, 42.9341801404953] in episode 456
Report: 
rewardSum:191.33333333333334
loss:[31.9216947555542, 42.9341801404953]
policies:[3, 2, 2]
qAverage:[28.528477986653645, 33.291297912597656]
ws:[5.112991174062093, 6.177799979845683]
memory len:5822
memory used:2914.0
now epsilon is 0.48246779384673066, the reward is 194.33333333333334 with loss [19.035160779953003, 11.286262154579163] in episode 457
Report: 
rewardSum:194.33333333333334
loss:[19.035160779953003, 11.286262154579163]
policies:[1, 1, 2]
qAverage:[29.947227478027344, 34.59297434488932]
ws:[1.9088438600301743, 2.1440869569778442]
memory len:5830
memory used:2914.0
############# STATE ###############
0-		9*		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.4816241081826948, the reward is -6.0 with loss [43.847783863544464, 26.671074211597443] in episode 458
Report: 
rewardSum:-6.0
loss:[43.847783863544464, 26.671074211597443]
policies:[0, 1, 6]
qAverage:[0.0, 45.903411865234375]
ws:[0.4158118665218353, 0.4402085244655609]
memory len:5844
memory used:2914.0
now epsilon is 0.48054153696163265, the reward is 189.33333333333334 with loss [42.369937002658844, 29.347859740257263] in episode 459
Report: 
rewardSum:189.33333333333334
loss:[42.369937002658844, 29.347859740257263]
policies:[0, 4, 5]
qAverage:[0.0, 50.00262451171875]
ws:[9.754941940307617, 10.358782768249512]
memory len:5862
memory used:2914.0
now epsilon is 0.47982117501374005, the reward is 192.33333333333334 with loss [35.9914813041687, 32.364444732666016] in episode 460
Report: 
rewardSum:192.33333333333334
loss:[35.9914813041687, 32.364444732666016]
policies:[1, 3, 2]
qAverage:[0.0, 66.5037358601888]
ws:[7.59106985727946, 8.266144275665283]
memory len:5874
memory used:2914.0
now epsilon is 0.47934153374168004, the reward is 46.33333333333334 with loss [15.31605339050293, 8.338457226753235] in episode 461
Report: 
rewardSum:46.33333333333334
loss:[15.31605339050293, 8.338457226753235]
policies:[0, 2, 2]
qAverage:[0.0, 52.71583557128906]
ws:[2.6555492877960205, 3.141406297683716]
memory len:5882
memory used:2914.0
now epsilon is 0.4788623719310566, the reward is 46.33333333333334 with loss [18.65679407119751, 31.963635683059692] in episode 462
Report: 
rewardSum:46.33333333333334
loss:[18.65679407119751, 31.963635683059692]
policies:[1, 1, 2]
qAverage:[45.39231872558594, 0.0]
ws:[0.5256744027137756, 0.4202321767807007]
memory len:5890
memory used:2915.0
now epsilon is 0.47838368910258805, the reward is 194.33333333333334 with loss [22.063900470733643, 14.33407211303711] in episode 463
Report: 
rewardSum:194.33333333333334
loss:[22.063900470733643, 14.33407211303711]
policies:[0, 3, 1]
qAverage:[0.0, 71.8527323404948]
ws:[9.782241026560465, 10.280680656433105]
memory len:5898
memory used:2915.0
now epsilon is 0.477427758477384, the reward is 42.33333333333334 with loss [48.25343382358551, 54.54144859313965] in episode 464
Report: 
rewardSum:42.33333333333334
loss:[48.25343382358551, 54.54144859313965]
policies:[1, 2, 5]
qAverage:[29.836402893066406, 31.028757731119793]
ws:[0.4432135025660197, 0.3858428696791331]
memory len:5914
memory used:2915.0
now epsilon is 0.4762355309519758, the reward is -9.0 with loss [52.513526916503906, 41.210678696632385] in episode 465
Report: 
rewardSum:-9.0
loss:[52.513526916503906, 41.210678696632385]
policies:[0, 2, 8]
qAverage:[0.0, 62.65399169921875]
ws:[5.458820660909017, 7.2471678256988525]
memory len:5934
memory used:2915.0
now epsilon is 0.47528389288567513, the reward is 190.33333333333334 with loss [38.34492349624634, 39.11537420749664] in episode 466
Report: 
rewardSum:190.33333333333334
loss:[38.34492349624634, 39.11537420749664]
policies:[1, 3, 4]
qAverage:[0.0, 61.52453867594401]
ws:[0.37646178404490155, 0.9791959325472513]
memory len:5950
memory used:2915.0
now epsilon is 0.47433415643097293, the reward is 190.33333333333334 with loss [39.53049635887146, 38.99600565433502] in episode 467
Report: 
rewardSum:190.33333333333334
loss:[39.53049635887146, 38.99600565433502]
policies:[0, 5, 3]
qAverage:[0.0, 79.72319030761719]
ws:[2.233075884791712, 3.1482238272825875]
memory len:5966
memory used:2915.0
now epsilon is 0.4738600001202066, the reward is 194.33333333333334 with loss [25.14221954345703, 12.040651679039001] in episode 468
Report: 
rewardSum:194.33333333333334
loss:[25.14221954345703, 12.040651679039001]
policies:[0, 2, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5974
memory used:2915.0
now epsilon is 0.47303136680216906, the reward is 191.33333333333334 with loss [51.90828800201416, 28.307854413986206] in episode 469
Report: 
rewardSum:191.33333333333334
loss:[51.90828800201416, 28.307854413986206]
policies:[2, 3, 2]
qAverage:[0.0, 67.20718129475911]
ws:[12.626515706380209, 15.81767463684082]
memory len:5988
memory used:2914.0
now epsilon is 0.47196810992681876, the reward is 189.33333333333334 with loss [63.472811698913574, 46.26451766490936] in episode 470
Report: 
rewardSum:189.33333333333334
loss:[63.472811698913574, 46.26451766490936]
policies:[1, 2, 6]
qAverage:[0.0, 43.741188049316406]
ws:[0.7719547152519226, 1.7929185628890991]
memory len:6006
memory used:2914.0
now epsilon is 0.4710249992383146, the reward is 437.0 with loss [54.71097433567047, 40.260841727256775] in episode 471
Report: 
rewardSum:437.0
loss:[54.71097433567047, 40.260841727256775]
policies:[0, 3, 5]
qAverage:[0.0, 62.30071767171224]
ws:[1.6461623509724934, 2.9400078455607095]
memory len:6022
memory used:2914.0
now epsilon is 0.4699662521782882, the reward is 41.33333333333334 with loss [62.4356484413147, 41.2872668504715] in episode 472
Report: 
rewardSum:41.33333333333334
loss:[62.4356484413147, 41.2872668504715]
policies:[0, 4, 5]
qAverage:[0.0, 67.93607139587402]
ws:[-0.9007699806243181, 0.7529858946800232]
memory len:6040
memory used:2914.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20*		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.4691444278107337, the reward is 191.33333333333334 with loss [38.09970259666443, 45.16060495376587] in episode 473
Report: 
rewardSum:191.33333333333334
loss:[38.09970259666443, 45.16060495376587]
policies:[1, 2, 4]
qAverage:[0.0, 64.79626719156902]
ws:[11.328069686889648, 14.630301793416342]
memory len:6054
memory used:2915.0
now epsilon is 0.4683240405576273, the reward is 191.33333333333334 with loss [29.586102962493896, 29.145193338394165] in episode 474
Report: 
rewardSum:191.33333333333334
loss:[29.586102962493896, 29.145193338394165]
policies:[1, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6068
memory used:2915.0
now epsilon is 0.4678558921093165, the reward is 194.33333333333334 with loss [25.663934350013733, 26.130764484405518] in episode 475
Report: 
rewardSum:194.33333333333334
loss:[25.663934350013733, 26.130764484405518]
policies:[0, 2, 2]
qAverage:[0.0, 52.3423957824707]
ws:[9.222737312316895, 11.079276084899902]
memory len:6076
memory used:2915.0
now epsilon is 0.46692099866366316, the reward is 190.33333333333334 with loss [41.38248252868652, 75.8715290427208] in episode 476
Report: 
rewardSum:190.33333333333334
loss:[41.38248252868652, 75.8715290427208]
policies:[1, 5, 2]
qAverage:[0.0, 75.61117045084636]
ws:[1.3476469417413075, 2.6276094367106757]
memory len:6092
memory used:2915.0
now epsilon is 0.4662210547582185, the reward is 192.33333333333334 with loss [47.02342700958252, 19.087661027908325] in episode 477
Report: 
rewardSum:192.33333333333334
loss:[47.02342700958252, 19.087661027908325]
policies:[0, 5, 1]
qAverage:[0.0, 76.43690236409505]
ws:[8.426582256952921, 10.285194367170334]
memory len:6104
memory used:2915.0
now epsilon is 0.4657550085072189, the reward is 46.33333333333334 with loss [20.197481215000153, 12.244530200958252] in episode 478
Report: 
rewardSum:46.33333333333334
loss:[20.197481215000153, 12.244530200958252]
policies:[0, 3, 1]
qAverage:[0.0, 56.01999155680338]
ws:[3.0544636249542236, 3.3647358417510986]
memory len:6112
memory used:2916.0
now epsilon is 0.46494054829113396, the reward is 191.33333333333334 with loss [22.298784732818604, 23.061702251434326] in episode 479
Report: 
rewardSum:191.33333333333334
loss:[22.298784732818604, 23.061702251434326]
policies:[0, 4, 3]
qAverage:[0.0, 64.87295786539714]
ws:[7.659029324849446, 9.125877062479654]
memory len:6126
memory used:2916.0
now epsilon is 0.4642435732051947, the reward is 44.33333333333334 with loss [32.548275887966156, 27.26399827003479] in episode 480
Report: 
rewardSum:44.33333333333334
loss:[32.548275887966156, 27.26399827003479]
policies:[1, 2, 3]
qAverage:[0.0, 56.05847676595052]
ws:[2.1743357181549072, 3.205028216044108]
memory len:6138
memory used:2916.0
now epsilon is 0.4637795036943161, the reward is -3.0 with loss [18.191580772399902, 23.58248996734619] in episode 481
Report: 
rewardSum:-3.0
loss:[18.191580772399902, 23.58248996734619]
policies:[0, 2, 2]
qAverage:[0.0, 43.376312255859375]
ws:[1.3935903310775757, 2.9214529991149902]
memory len:6146
memory used:2916.0
now epsilon is 0.4628527558953787, the reward is 190.33333333333334 with loss [44.53117549419403, 21.11224329471588] in episode 482
Report: 
rewardSum:190.33333333333334
loss:[44.53117549419403, 21.11224329471588]
policies:[0, 5, 3]
qAverage:[0.0, 70.49474029541015]
ws:[7.289521408081055, 9.597768878936767]
memory len:6162
memory used:2921.0
now epsilon is 0.46158150068031906, the reward is 187.33333333333334 with loss [47.24224936962128, 43.00520968437195] in episode 483
Report: 
rewardSum:187.33333333333334
loss:[47.24224936962128, 43.00520968437195]
policies:[1, 2, 8]
qAverage:[0.0, 38.97263717651367]
ws:[0.9007550477981567, 1.3771708011627197]
memory len:6184
memory used:2922.0
now epsilon is 0.4599685880492825, the reward is 986.0 with loss [79.0985985994339, 57.1812539100647] in episode 484
Report: 
rewardSum:986.0
loss:[79.0985985994339, 57.1812539100647]
policies:[2, 2, 10]
qAverage:[0.0, 58.9339853922526]
ws:[2.0318025747934976, 3.1075785160064697]
memory len:6212
memory used:2924.0
now epsilon is 0.45950879192070754, the reward is 194.33333333333334 with loss [18.164599299430847, 18.183711171150208] in episode 485
Report: 
rewardSum:194.33333333333334
loss:[18.164599299430847, 18.183711171150208]
policies:[1, 2, 1]
qAverage:[0.0, 60.09535217285156]
ws:[3.136729598045349, 4.986820300420125]
memory len:6220
memory used:2924.0
now epsilon is 0.45904945541586634, the reward is 194.33333333333334 with loss [17.67415452003479, 24.13964867591858] in episode 486
Report: 
rewardSum:194.33333333333334
loss:[17.67415452003479, 24.13964867591858]
policies:[1, 2, 1]
qAverage:[0.0, 45.42610549926758]
ws:[2.679913282394409, 3.950258731842041]
memory len:6228
memory used:2924.0
now epsilon is 0.4585905780753075, the reward is 194.33333333333334 with loss [13.780252456665039, 17.487418472766876] in episode 487
Report: 
rewardSum:194.33333333333334
loss:[13.780252456665039, 17.487418472766876]
policies:[0, 2, 2]
qAverage:[0.0, 61.98053487141927]
ws:[8.67659060160319, 10.940544128417969]
memory len:6236
memory used:2924.0
now epsilon is 0.45813215944003893, the reward is 194.33333333333334 with loss [24.820252180099487, 18.088386297225952] in episode 488
Report: 
rewardSum:194.33333333333334
loss:[24.820252180099487, 18.088386297225952]
policies:[0, 3, 1]
qAverage:[0.0, 68.26265907287598]
ws:[7.375859975814819, 9.525469779968262]
memory len:6244
memory used:2923.0
now epsilon is 0.45721669645169766, the reward is 992.0 with loss [40.418979331851006, 32.83854639530182] in episode 489
Report: 
rewardSum:992.0
loss:[40.418979331851006, 32.83854639530182]
policies:[2, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6260
memory used:2923.0
############# STATE ###############
0-		9*		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.456759651182933, the reward is 194.33333333333334 with loss [26.51215088367462, 10.069273948669434] in episode 490
Report: 
rewardSum:194.33333333333334
loss:[26.51215088367462, 10.069273948669434]
policies:[0, 3, 1]
qAverage:[0.0, 59.334920247395836]
ws:[1.7383633852005005, 3.0756091276804605]
memory len:6268
memory used:2924.0
now epsilon is 0.4560749397756211, the reward is 192.33333333333334 with loss [28.883931398391724, 25.998412370681763] in episode 491
Report: 
rewardSum:192.33333333333334
loss:[28.883931398391724, 25.998412370681763]
policies:[0, 2, 4]
qAverage:[0.0, 59.92532094319662]
ws:[4.501042127609253, 6.106414874394734]
memory len:6280
memory used:2924.0
now epsilon is 0.4550497967313666, the reward is 41.33333333333334 with loss [36.35551553964615, 35.69614851474762] in episode 492
Report: 
rewardSum:41.33333333333334
loss:[36.35551553964615, 35.69614851474762]
policies:[1, 3, 5]
qAverage:[0.0, 63.6370906829834]
ws:[5.911518156528473, 7.872485280036926]
memory len:6298
memory used:2924.0
now epsilon is 0.45459491754987025, the reward is 46.33333333333334 with loss [31.593639612197876, 20.304999470710754] in episode 493
Report: 
rewardSum:46.33333333333334
loss:[31.593639612197876, 20.304999470710754]
policies:[1, 1, 2]
qAverage:[0.0, 42.65476608276367]
ws:[2.372828245162964, 3.108992338180542]
memory len:6306
memory used:2924.0
now epsilon is 0.4541404930770041, the reward is 194.33333333333334 with loss [20.70758628845215, 8.465938448905945] in episode 494
Report: 
rewardSum:194.33333333333334
loss:[20.70758628845215, 8.465938448905945]
policies:[0, 2, 2]
qAverage:[0.0, 62.26992543538412]
ws:[5.209240277608235, 6.883963902791341]
memory len:6314
memory used:2931.0
now epsilon is 0.4536865228582301, the reward is 194.33333333333334 with loss [13.187909126281738, 9.261062026023865] in episode 495
Report: 
rewardSum:194.33333333333334
loss:[13.187909126281738, 9.261062026023865]
policies:[0, 2, 2]
qAverage:[0.0, 57.89366912841797]
ws:[7.550124009450276, 9.922234853108725]
memory len:6322
memory used:2931.0
now epsilon is 0.45311969818785447, the reward is -4.0 with loss [27.026309490203857, 35.75489926338196] in episode 496
Report: 
rewardSum:-4.0
loss:[27.026309490203857, 35.75489926338196]
policies:[0, 2, 3]
qAverage:[0.0, 41.55908203125]
ws:[2.6309211254119873, 3.588013172149658]
memory len:6332
memory used:2931.0
now epsilon is 0.45244044329871635, the reward is 192.33333333333334 with loss [23.512206077575684, 26.0385103225708] in episode 497
Report: 
rewardSum:192.33333333333334
loss:[23.512206077575684, 26.0385103225708]
policies:[2, 2, 2]
qAverage:[0.0, 41.1995735168457]
ws:[2.072643756866455, 2.7291064262390137]
memory len:6344
memory used:2931.0
now epsilon is 0.4510849867312594, the reward is 186.33333333333334 with loss [70.25138854980469, 28.58110213279724] in episode 498
Report: 
rewardSum:186.33333333333334
loss:[70.25138854980469, 28.58110213279724]
policies:[0, 5, 7]
qAverage:[0.0, 72.05875905354817]
ws:[4.551192012925942, 6.100576971968015]
memory len:6368
memory used:2931.0
now epsilon is 0.45018360576194777, the reward is 437.0 with loss [50.67628622055054, 35.72167122364044] in episode 499
Report: 
rewardSum:437.0
loss:[50.67628622055054, 35.72167122364044]
policies:[2, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6384
memory used:2935.0
now epsilon is 0.44939637507171437, the reward is 191.33333333333334 with loss [27.94540572166443, 37.43183517456055] in episode 500
Report: 
rewardSum:191.33333333333334
loss:[27.94540572166443, 37.43183517456055]
policies:[0, 4, 3]
qAverage:[0.0, 66.6665153503418]
ws:[7.7099305391311646, 9.938486218452454]
memory len:6398
memory used:2936.0
now epsilon is 0.44872270167779843, the reward is 192.33333333333334 with loss [24.53964865207672, 29.125926971435547] in episode 501
Report: 
rewardSum:192.33333333333334
loss:[24.53964865207672, 29.125926971435547]
policies:[1, 3, 2]
qAverage:[0.0, 39.45429992675781]
ws:[1.7490068674087524, 2.461918830871582]
memory len:6410
memory used:2936.0
now epsilon is 0.448050038162615, the reward is 192.33333333333334 with loss [42.90648818016052, 40.65034055709839] in episode 502
Report: 
rewardSum:192.33333333333334
loss:[42.90648818016052, 40.65034055709839]
policies:[1, 3, 2]
qAverage:[0.0, 53.50943247477213]
ws:[2.764285604159037, 3.6495207945505777]
memory len:6422
memory used:2936.0
now epsilon is 0.4469311723682145, the reward is 40.33333333333334 with loss [45.76880991458893, 33.059697687625885] in episode 503
Report: 
rewardSum:40.33333333333334
loss:[45.76880991458893, 33.059697687625885]
policies:[1, 1, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6442
memory used:2943.0
now epsilon is 0.4464844087671045, the reward is 46.33333333333334 with loss [16.600337266921997, 37.40666437149048] in episode 504
Report: 
rewardSum:46.33333333333334
loss:[16.600337266921997, 37.40666437149048]
policies:[1, 2, 1]
qAverage:[0.0, 51.10453796386719]
ws:[2.0633533000946045, 2.709049622217814]
memory len:6450
memory used:2945.0
now epsilon is 0.4460380917620872, the reward is 194.33333333333334 with loss [16.72650444507599, 27.4773006439209] in episode 505
Report: 
rewardSum:194.33333333333334
loss:[16.72650444507599, 27.4773006439209]
policies:[0, 2, 2]
qAverage:[0.0, 37.359764099121094]
ws:[0.26975199580192566, 0.5940049886703491]
memory len:6458
memory used:2948.0
now epsilon is 0.44525811028263285, the reward is 438.0 with loss [25.784412384033203, 39.25910186767578] in episode 506
Report: 
rewardSum:438.0
loss:[25.784412384033203, 39.25910186767578]
policies:[2, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6472
memory used:2948.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22*		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.44481301911631477, the reward is 194.33333333333334 with loss [9.383111298084259, 28.217844247817993] in episode 507
Report: 
rewardSum:194.33333333333334
loss:[9.383111298084259, 28.217844247817993]
policies:[0, 4, 0]
qAverage:[0.0, 64.22599029541016]
ws:[4.057478345930576, 5.214308857917786]
memory len:6480
memory used:2947.0
now epsilon is 0.44436837287428155, the reward is 194.33333333333334 with loss [22.751178741455078, 18.585309982299805] in episode 508
Report: 
rewardSum:194.33333333333334
loss:[22.751178741455078, 18.585309982299805]
policies:[0, 4, 0]
qAverage:[0.0, 56.4349365234375]
ws:[3.6106189489364624, 4.913480073213577]
memory len:6488
memory used:2947.0
now epsilon is 0.44392417111177584, the reward is 194.33333333333334 with loss [24.617087841033936, 17.520779609680176] in episode 509
Report: 
rewardSum:194.33333333333334
loss:[24.617087841033936, 17.520779609680176]
policies:[2, 2, 0]
qAverage:[0.0, 55.76093292236328]
ws:[5.863276878992717, 7.830226103464763]
memory len:6496
memory used:2948.0
now epsilon is 0.44336954328113864, the reward is 193.33333333333334 with loss [27.19443079829216, 18.317534923553467] in episode 510
Report: 
rewardSum:193.33333333333334
loss:[27.19443079829216, 18.317534923553467]
policies:[0, 3, 2]
qAverage:[0.0, 57.0274658203125]
ws:[10.42234738667806, 12.080443700154623]
memory len:6506
memory used:2948.0
now epsilon is 0.4429263399737274, the reward is 194.33333333333334 with loss [30.79596781730652, 23.397727489471436] in episode 511
Report: 
rewardSum:194.33333333333334
loss:[30.79596781730652, 23.397727489471436]
policies:[1, 3, 0]
qAverage:[0.0, 60.552581787109375]
ws:[7.717070773243904, 9.12911581993103]
memory len:6514
memory used:2948.0
now epsilon is 0.44248357970345004, the reward is 46.33333333333334 with loss [16.34831440448761, 23.038311004638672] in episode 512
Report: 
rewardSum:46.33333333333334
loss:[16.34831440448761, 23.038311004638672]
policies:[1, 2, 1]
qAverage:[0.0, 42.42789840698242]
ws:[1.4189397096633911, 2.148963451385498]
memory len:6522
memory used:2948.0
now epsilon is 0.4420412620274355, the reward is 194.33333333333334 with loss [25.50653100013733, 26.42409086227417] in episode 513
Report: 
rewardSum:194.33333333333334
loss:[25.50653100013733, 26.42409086227417]
policies:[0, 2, 2]
qAverage:[0.0, 42.514747619628906]
ws:[1.4829806089401245, 2.0391318798065186]
memory len:6530
memory used:2948.0
now epsilon is 0.4410476632007518, the reward is 189.33333333333334 with loss [30.767403483390808, 34.515127539634705] in episode 514
Report: 
rewardSum:189.33333333333334
loss:[30.767403483390808, 34.515127539634705]
policies:[0, 5, 4]
qAverage:[0.0, 62.35326385498047]
ws:[3.4745965719223024, 4.5677845001220705]
memory len:6548
memory used:2947.0
now epsilon is 0.4406067809028611, the reward is 194.33333333333334 with loss [24.085052490234375, 34.43574857711792] in episode 515
Report: 
rewardSum:194.33333333333334
loss:[24.085052490234375, 34.43574857711792]
policies:[1, 3, 0]
qAverage:[0.0, 62.30801963806152]
ws:[3.7118682116270065, 4.441398739814758]
memory len:6556
memory used:2947.0
now epsilon is 0.4403865050503335, the reward is -1.0 with loss [16.321495056152344, 14.937541961669922] in episode 516
Report: 
rewardSum:-1.0
loss:[16.321495056152344, 14.937541961669922]
policies:[0, 1, 1]
qAverage:[0.0, 36.06851577758789]
ws:[-0.45493996143341064, -0.26462894678115845]
memory len:6560
memory used:2947.0
now epsilon is 0.4399462836627002, the reward is 194.33333333333334 with loss [18.91173768043518, 13.40192699432373] in episode 517
Report: 
rewardSum:194.33333333333334
loss:[18.91173768043518, 13.40192699432373]
policies:[0, 2, 2]
qAverage:[0.0, 45.803070068359375]
ws:[5.50096321105957, 6.744709014892578]
memory len:6568
memory used:2947.0
now epsilon is 0.4395065023313991, the reward is 194.33333333333334 with loss [23.878614902496338, 17.93449866771698] in episode 518
Report: 
rewardSum:194.33333333333334
loss:[23.878614902496338, 17.93449866771698]
policies:[0, 3, 1]
qAverage:[0.0, 35.5445556640625]
ws:[-0.11650400608778, 0.4106420874595642]
memory len:6576
memory used:2947.0
now epsilon is 0.43928677654938986, the reward is -1.0 with loss [24.505173683166504, 6.157037734985352] in episode 519
Report: 
rewardSum:-1.0
loss:[24.505173683166504, 6.157037734985352]
policies:[0, 1, 1]
qAverage:[0.0, 36.29161071777344]
ws:[-0.026990484446287155, 0.7623746991157532]
memory len:6580
memory used:2947.0
now epsilon is 0.4384089713638943, the reward is 190.33333333333334 with loss [32.621961295604706, 33.30732220411301] in episode 520
Report: 
rewardSum:190.33333333333334
loss:[32.621961295604706, 33.30732220411301]
policies:[1, 4, 3]
qAverage:[0.0, 62.34710884094238]
ws:[9.021421432495117, 11.73220682144165]
memory len:6596
memory used:2944.0
now epsilon is 0.43797072676849585, the reward is -3.0 with loss [28.632922768592834, 10.774848699569702] in episode 521
Report: 
rewardSum:-3.0
loss:[28.632922768592834, 10.774848699569702]
policies:[0, 2, 2]
qAverage:[0.0, 49.54150136311849]
ws:[4.448049416144689, 6.533602039019267]
memory len:6604
memory used:2944.0
now epsilon is 0.43731418113905934, the reward is 192.33333333333334 with loss [40.009444653987885, 35.504358887672424] in episode 522
Report: 
rewardSum:192.33333333333334
loss:[40.009444653987885, 35.504358887672424]
policies:[1, 3, 2]
qAverage:[0.0, 58.837755839029946]
ws:[5.218845685323079, 7.0730821291605634]
memory len:6616
memory used:2944.0
now epsilon is 0.4366586197127606, the reward is 192.33333333333334 with loss [17.268439531326294, 36.92149239778519] in episode 523
Report: 
rewardSum:192.33333333333334
loss:[17.268439531326294, 36.92149239778519]
policies:[0, 5, 1]
qAverage:[0.0, 60.745527267456055]
ws:[4.105339370667934, 6.095143616199493]
memory len:6628
memory used:2943.0
now epsilon is 0.4360040410142173, the reward is 192.33333333333334 with loss [21.57707953453064, 21.904106736183167] in episode 524
Report: 
rewardSum:192.33333333333334
loss:[21.57707953453064, 21.904106736183167]
policies:[0, 4, 2]
qAverage:[0.0, 56.55936241149902]
ws:[4.438493728637695, 6.7168275117874146]
memory len:6640
memory used:2943.0
now epsilon is 0.43535044357025887, the reward is 994.0 with loss [46.35701131820679, 30.695858240127563] in episode 525
Report: 
rewardSum:994.0
loss:[46.35701131820679, 30.695858240127563]
policies:[1, 2, 3]
qAverage:[16.778169631958008, 38.3150577545166]
ws:[-4.755772605538368, -3.677956759929657]
memory len:6652
memory used:2943.0
now epsilon is 0.43458915145344545, the reward is 191.33333333333334 with loss [34.84069788455963, 44.78870666027069] in episode 526
Report: 
rewardSum:191.33333333333334
loss:[34.84069788455963, 44.78870666027069]
policies:[1, 1, 5]
qAverage:[0.0, 37.7746696472168]
ws:[0.12177805602550507, 1.6322702169418335]
memory len:6666
memory used:2943.0
now epsilon is 0.43393767501781116, the reward is 192.33333333333334 with loss [42.18478465080261, 22.339791476726532] in episode 527
Report: 
rewardSum:192.33333333333334
loss:[42.18478465080261, 22.339791476726532]
policies:[1, 4, 1]
qAverage:[0.0, 55.60298538208008]
ws:[0.2349756956100464, 2.6492858827114105]
memory len:6678
memory used:2943.0
now epsilon is 0.4331788533924782, the reward is 191.33333333333334 with loss [56.391316413879395, 44.827175974845886] in episode 528
Report: 
rewardSum:191.33333333333334
loss:[56.391316413879395, 44.827175974845886]
policies:[0, 4, 3]
qAverage:[0.0, 63.32560577392578]
ws:[2.045603317022324, 4.648893165588379]
memory len:6692
memory used:2942.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11*		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.4325294910822216, the reward is 192.33333333333334 with loss [25.425030171871185, 34.85908555984497] in episode 529
Report: 
rewardSum:192.33333333333334
loss:[25.425030171871185, 34.85908555984497]
policies:[0, 3, 3]
qAverage:[0.0, 58.287275314331055]
ws:[4.841013729572296, 8.123884685337543]
memory len:6704
memory used:2942.0
now epsilon is 0.4320971237626672, the reward is 194.33333333333334 with loss [31.775709629058838, 20.893849849700928] in episode 530
Report: 
rewardSum:194.33333333333334
loss:[31.775709629058838, 20.893849849700928]
policies:[0, 3, 1]
qAverage:[0.0, 53.835558573404946]
ws:[3.9351888497670493, 6.61104154586792]
memory len:6712
memory used:2942.0
now epsilon is 0.4314493830330718, the reward is 192.33333333333334 with loss [27.19142782688141, 42.84067153930664] in episode 531
Report: 
rewardSum:192.33333333333334
loss:[27.19142782688141, 42.84067153930664]
policies:[2, 3, 1]
qAverage:[0.0, 53.22843933105469]
ws:[7.233759880065918, 9.96248467763265]
memory len:6724
memory used:2942.0
now epsilon is 0.4310180954165934, the reward is -3.0 with loss [26.238705277442932, 18.203203201293945] in episode 532
Report: 
rewardSum:-3.0
loss:[26.238705277442932, 18.203203201293945]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6732
memory used:2942.0
now epsilon is 0.4305872389260257, the reward is 194.33333333333334 with loss [16.540936708450317, 19.112193822860718] in episode 533
Report: 
rewardSum:194.33333333333334
loss:[16.540936708450317, 19.112193822860718]
policies:[0, 4, 0]
qAverage:[0.0, 61.75444030761719]
ws:[3.9261106491088866, 6.615794438868761]
memory len:6740
memory used:2942.0
now epsilon is 0.4299417616086399, the reward is 192.33333333333334 with loss [24.275270819664, 21.633434295654297] in episode 534
Report: 
rewardSum:192.33333333333334
loss:[24.275270819664, 21.633434295654297]
policies:[3, 2, 1]
qAverage:[0.0, 54.045555114746094]
ws:[3.111847162246704, 5.128687699635823]
memory len:6752
memory used:2942.0
now epsilon is 0.42876089858113225, the reward is 187.33333333333334 with loss [51.807977735996246, 44.778130769729614] in episode 535
Report: 
rewardSum:187.33333333333334
loss:[51.807977735996246, 44.778130769729614]
policies:[0, 4, 7]
qAverage:[0.0, 51.90432993570963]
ws:[1.7529939413070679, 3.012285073598226]
memory len:6774
memory used:2942.0
now epsilon is 0.42779715070880886, the reward is 991.0 with loss [37.517661571502686, 38.69732713699341] in episode 536
Report: 
rewardSum:991.0
loss:[37.517661571502686, 38.69732713699341]
policies:[2, 3, 4]
qAverage:[0.0, 44.95701599121094]
ws:[0.8797550996144613, 1.8222286701202393]
memory len:6792
memory used:2960.0
now epsilon is 0.42715585590891303, the reward is 44.33333333333334 with loss [14.721579670906067, 37.211861193180084] in episode 537
Report: 
rewardSum:44.33333333333334
loss:[14.721579670906067, 37.211861193180084]
policies:[0, 2, 4]
qAverage:[0.0, 34.19602584838867]
ws:[-0.599238932132721, -0.1463034600019455]
memory len:6804
memory used:2960.0
now epsilon is 0.42662217799470215, the reward is -4.0 with loss [19.385259866714478, 34.73944902420044] in episode 538
Report: 
rewardSum:-4.0
loss:[19.385259866714478, 34.73944902420044]
policies:[1, 1, 3]
qAverage:[0.0, 33.352848052978516]
ws:[1.0034246444702148, 1.4390166997909546]
memory len:6814
memory used:2960.0
now epsilon is 0.42640889356959094, the reward is -1.0 with loss [15.362596988677979, 12.835508823394775] in episode 539
Report: 
rewardSum:-1.0
loss:[15.362596988677979, 12.835508823394775]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6818
memory used:2961.0
now epsilon is 0.4254504324196181, the reward is 991.0 with loss [31.29878368973732, 33.502041190862656] in episode 540
Report: 
rewardSum:991.0
loss:[31.29878368973732, 33.502041190862656]
policies:[1, 3, 5]
qAverage:[16.498180389404297, 36.37697792053223]
ws:[-8.204296528361738, -9.37537788413465]
memory len:6836
memory used:2961.0
now epsilon is 0.4250251415045217, the reward is 194.33333333333334 with loss [36.27574825286865, 20.826277375221252] in episode 541
Report: 
rewardSum:194.33333333333334
loss:[36.27574825286865, 20.826277375221252]
policies:[2, 1, 1]
qAverage:[21.962870279947918, 25.756202697753906]
ws:[1.694198449452718, 1.537071205675602]
memory len:6844
memory used:2961.0
now epsilon is 0.4241758346437296, the reward is 42.33333333333334 with loss [27.238114774227142, 40.9546577334404] in episode 542
Report: 
rewardSum:42.33333333333334
loss:[27.238114774227142, 40.9546577334404]
policies:[1, 1, 6]
qAverage:[32.471492767333984, 0.0]
ws:[0.7918033599853516, 0.5729771852493286]
memory len:6860
memory used:2961.0
now epsilon is 0.4235399684240789, the reward is 192.33333333333334 with loss [14.241925060749054, 19.872849941253662] in episode 543
Report: 
rewardSum:192.33333333333334
loss:[14.241925060749054, 19.872849941253662]
policies:[2, 2, 2]
qAverage:[21.200650533040363, 29.40516408284505]
ws:[4.291660308837891, 4.347880244255066]
memory len:6872
memory used:2961.0
now epsilon is 0.42311658725667345, the reward is 194.33333333333334 with loss [28.532222509384155, 18.601704336702824] in episode 544
Report: 
rewardSum:194.33333333333334
loss:[28.532222509384155, 18.601704336702824]
policies:[0, 3, 1]
qAverage:[0.0, 53.20364888509115]
ws:[8.01251252492269, 8.892896016438803]
memory len:6880
memory used:2973.0
now epsilon is 0.42237668833816105, the reward is 191.33333333333334 with loss [44.37282395362854, 24.655065059661865] in episode 545
Report: 
rewardSum:191.33333333333334
loss:[44.37282395362854, 24.655065059661865]
policies:[0, 3, 4]
qAverage:[0.0, 44.474853515625]
ws:[4.441141128540039, 5.277823448181152]
memory len:6894
memory used:2973.0
now epsilon is 0.42195447001468417, the reward is 194.33333333333334 with loss [35.8044673204422, 18.174220085144043] in episode 546
Report: 
rewardSum:194.33333333333334
loss:[35.8044673204422, 18.174220085144043]
policies:[0, 3, 1]
qAverage:[0.0, 53.06884765625]
ws:[7.62598196665446, 9.359188079833984]
memory len:6902
memory used:2973.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31*		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.42111129912588263, the reward is 190.33333333333334 with loss [37.881380558013916, 61.225377559661865] in episode 547
Report: 
rewardSum:190.33333333333334
loss:[37.881380558013916, 61.225377559661865]
policies:[0, 3, 5]
qAverage:[0.0, 46.310290018717446]
ws:[4.338451234002908, 5.679383595784505]
memory len:6918
memory used:2973.0
now epsilon is 0.4206903457171762, the reward is 194.33333333333334 with loss [37.47583290934563, 17.65685772895813] in episode 548
Report: 
rewardSum:194.33333333333334
loss:[37.47583290934563, 17.65685772895813]
policies:[0, 3, 1]
qAverage:[0.0, 53.090532302856445]
ws:[0.9994740933179855, 2.3460384979844093]
memory len:6926
memory used:2973.0
now epsilon is 0.42026981310404715, the reward is 194.33333333333334 with loss [17.650054454803467, 19.58464813232422] in episode 549
Report: 
rewardSum:194.33333333333334
loss:[17.650054454803467, 19.58464813232422]
policies:[1, 2, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6934
memory used:2972.0
now epsilon is 0.4198497008658578, the reward is 194.33333333333334 with loss [32.7871470451355, 28.514972448349] in episode 550
Report: 
rewardSum:194.33333333333334
loss:[32.7871470451355, 28.514972448349]
policies:[0, 2, 2]
qAverage:[0.0, 51.82277170817057]
ws:[3.961031993230184, 5.945518970489502]
memory len:6942
memory used:2972.0
now epsilon is 0.4194300085823908, the reward is 194.33333333333334 with loss [16.919131010770798, 18.63936412334442] in episode 551
Report: 
rewardSum:194.33333333333334
loss:[16.919131010770798, 18.63936412334442]
policies:[1, 1, 2]
qAverage:[0.0, 35.55648422241211]
ws:[1.572842001914978, 2.863459348678589]
memory len:6950
memory used:2972.0
now epsilon is 0.419010735833849, the reward is 194.33333333333334 with loss [18.212072372436523, 44.653783321380615] in episode 552
Report: 
rewardSum:194.33333333333334
loss:[18.212072372436523, 44.653783321380615]
policies:[0, 4, 0]
qAverage:[0.0, 55.28123474121094]
ws:[4.496453921000163, 5.715655326843262]
memory len:6958
memory used:2972.0
now epsilon is 0.4185918822008546, the reward is 194.33333333333334 with loss [24.93273639678955, 32.973743200302124] in episode 553
Report: 
rewardSum:194.33333333333334
loss:[24.93273639678955, 32.973743200302124]
policies:[1, 2, 1]
qAverage:[0.0, 43.317848205566406]
ws:[4.4853997230529785, 5.973578453063965]
memory len:6966
memory used:2972.0
now epsilon is 0.4179643866766575, the reward is 439.0 with loss [45.174219727516174, 35.054728984832764] in episode 554
Report: 
rewardSum:439.0
loss:[45.174219727516174, 35.054728984832764]
policies:[0, 3, 3]
qAverage:[0.0, 53.008907318115234]
ws:[2.3146577812731266, 3.545984521508217]
memory len:6978
memory used:2972.0
now epsilon is 0.41744219235575464, the reward is 193.33333333333334 with loss [23.859851837158203, 30.56146478652954] in episode 555
Report: 
rewardSum:193.33333333333334
loss:[23.859851837158203, 30.56146478652954]
policies:[0, 3, 2]
qAverage:[0.0, 52.00723775227865]
ws:[3.10780922571818, 3.9137969811757407]
memory len:6988
memory used:2972.0
now epsilon is 0.4161915862935803, the reward is 186.33333333333334 with loss [58.46456778049469, 58.47112828493118] in episode 556
Report: 
rewardSum:186.33333333333334
loss:[58.46456778049469, 58.47112828493118]
policies:[0, 4, 8]
qAverage:[0.0, 48.553995768229164]
ws:[6.653568426767985, 8.514781951904297]
memory len:7012
memory used:2972.0
now epsilon is 0.4153599310922154, the reward is -7.0 with loss [47.512725591659546, 57.36137318611145] in episode 557
Report: 
rewardSum:-7.0
loss:[47.512725591659546, 57.36137318611145]
policies:[1, 1, 6]
qAverage:[0.0, 33.58694839477539]
ws:[5.347041606903076, 6.9796600341796875]
memory len:7028
memory used:2972.0
now epsilon is 0.41494472689513906, the reward is 194.33333333333334 with loss [36.32757520675659, 27.218690991401672] in episode 558
Report: 
rewardSum:194.33333333333334
loss:[36.32757520675659, 27.218690991401672]
policies:[0, 3, 1]
qAverage:[0.0, 54.908114433288574]
ws:[3.6334027349948883, 5.512155935168266]
memory len:7036
memory used:2973.0
now epsilon is 0.41452993774658414, the reward is 194.33333333333334 with loss [19.7867648601532, 26.648839235305786] in episode 559
Report: 
rewardSum:194.33333333333334
loss:[19.7867648601532, 26.648839235305786]
policies:[0, 1, 3]
qAverage:[0.0, 41.882530212402344]
ws:[3.574631690979004, 5.261468410491943]
memory len:7044
memory used:2973.0
now epsilon is 0.41411556323165777, the reward is 194.33333333333334 with loss [20.67979621887207, 24.559743285179138] in episode 560
Report: 
rewardSum:194.33333333333334
loss:[20.67979621887207, 24.559743285179138]
policies:[2, 2, 0]
qAverage:[0.0, 46.87755584716797]
ws:[0.6215608914693197, 2.3143444657325745]
memory len:7052
memory used:2972.0
now epsilon is 0.4134947779907641, the reward is 44.33333333333334 with loss [23.021826446056366, 44.671788930892944] in episode 561
Report: 
rewardSum:44.33333333333334
loss:[23.021826446056366, 44.671788930892944]
policies:[1, 2, 3]
qAverage:[0.0, 48.644004821777344]
ws:[6.62413231531779, 8.44984769821167]
memory len:7064
memory used:2974.0
now epsilon is 0.41328805644519234, the reward is -1.0 with loss [7.512481689453125, 15.680310249328613] in episode 562
Report: 
rewardSum:-1.0
loss:[7.512481689453125, 15.680310249328613]
policies:[1, 0, 1]
qAverage:[30.682355880737305, 0.0]
ws:[-1.1025924682617188, -1.1924859285354614]
memory len:7068
memory used:2974.0
now epsilon is 0.41246220322488675, the reward is 190.33333333333334 with loss [38.53572106361389, 53.456907510757446] in episode 563
Report: 
rewardSum:190.33333333333334
loss:[38.53572106361389, 53.456907510757446]
policies:[2, 2, 4]
qAverage:[20.288490295410156, 28.542996724446613]
ws:[1.5921825567881267, 2.1627474625905356]
memory len:7084
memory used:2973.0
now epsilon is 0.4119468831952935, the reward is 193.33333333333334 with loss [17.490534342825413, 15.571937680244446] in episode 564
Report: 
rewardSum:193.33333333333334
loss:[17.490534342825413, 15.571937680244446]
policies:[0, 3, 2]
qAverage:[0.0, 42.68882751464844]
ws:[6.803744316101074, 8.097273826599121]
memory len:7094
memory used:2985.0
now epsilon is 0.41122651660475884, the reward is 191.33333333333334 with loss [37.50102856755257, 24.514912962913513] in episode 565
Report: 
rewardSum:191.33333333333334
loss:[37.50102856755257, 24.514912962913513]
policies:[2, 1, 4]
qAverage:[0.0, 31.91847801208496]
ws:[2.421783208847046, 2.5137734413146973]
memory len:7108
memory used:2985.0
now epsilon is 0.4107127404113297, the reward is 193.33333333333334 with loss [10.1018176227808, 22.490272998809814] in episode 566
Report: 
rewardSum:193.33333333333334
loss:[10.1018176227808, 22.490272998809814]
policies:[1, 3, 1]
qAverage:[0.0, 51.25321706136068]
ws:[5.555979410807292, 7.11035950978597]
memory len:7118
memory used:2985.0
now epsilon is 0.4100970562155833, the reward is 994.0 with loss [28.917651653289795, 24.628557682037354] in episode 567
Report: 
rewardSum:994.0
loss:[28.917651653289795, 24.628557682037354]
policies:[1, 2, 3]
qAverage:[0.0, 34.61166000366211]
ws:[0.6811407208442688, 1.1664372682571411]
memory len:7130
memory used:2973.0
############# STATE ###############
0-		9*		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.4094822949691189, the reward is 44.33333333333334 with loss [26.10313320159912, 51.75795936584473] in episode 568
Report: 
rewardSum:44.33333333333334
loss:[26.10313320159912, 51.75795936584473]
policies:[0, 2, 4]
qAverage:[0.0, 49.04103088378906]
ws:[5.559418201446533, 6.964926719665527]
memory len:7142
memory used:2973.0
now epsilon is 0.40907296620441935, the reward is 194.33333333333334 with loss [19.418567895889282, 15.572429418563843] in episode 569
Report: 
rewardSum:194.33333333333334
loss:[19.418567895889282, 15.572429418563843]
policies:[1, 2, 1]
qAverage:[0.0, 50.51424916585287]
ws:[5.451213677724202, 6.738265832265218]
memory len:7150
memory used:2973.0
now epsilon is 0.4086640466150119, the reward is 46.33333333333334 with loss [35.43659448623657, 18.80771493911743] in episode 570
Report: 
rewardSum:46.33333333333334
loss:[35.43659448623657, 18.80771493911743]
policies:[1, 1, 2]
qAverage:[0.0, 31.23769760131836]
ws:[0.4274977445602417, 0.699022114276886]
memory len:7158
memory used:2973.0
now epsilon is 0.40825553579187446, the reward is 194.33333333333334 with loss [20.068753957748413, 24.883419036865234] in episode 571
Report: 
rewardSum:194.33333333333334
loss:[20.068753957748413, 24.883419036865234]
policies:[0, 4, 0]
qAverage:[0.0, 43.618534088134766]
ws:[2.426874041557312, 3.9091051618258157]
memory len:7166
memory used:2973.0
now epsilon is 0.40764353510019563, the reward is 44.33333333333334 with loss [31.59699857234955, 32.68680262565613] in episode 572
Report: 
rewardSum:44.33333333333334
loss:[31.59699857234955, 32.68680262565613]
policies:[0, 3, 3]
qAverage:[0.0, 51.93020534515381]
ws:[2.970565140247345, 4.936367139220238]
memory len:7178
memory used:2973.0
now epsilon is 0.407236044405945, the reward is 194.33333333333334 with loss [8.88107168674469, 11.511487513780594] in episode 573
Report: 
rewardSum:194.33333333333334
loss:[8.88107168674469, 11.511487513780594]
policies:[0, 1, 3]
qAverage:[0.0, 42.274776458740234]
ws:[5.321353435516357, 6.799237251281738]
memory len:7186
memory used:2973.0
now epsilon is 0.40662557199589044, the reward is 44.33333333333334 with loss [32.067084193229675, 21.66378879547119] in episode 574
Report: 
rewardSum:44.33333333333334
loss:[32.067084193229675, 21.66378879547119]
policies:[1, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7198
memory used:2973.0
now epsilon is 0.4062190988830716, the reward is 194.33333333333334 with loss [22.41747283935547, 9.99995744228363] in episode 575
Report: 
rewardSum:194.33333333333334
loss:[22.41747283935547, 9.99995744228363]
policies:[0, 2, 2]
qAverage:[0.0, 50.32581329345703]
ws:[3.7987006505330405, 5.494904200236003]
memory len:7206
memory used:2973.0
now epsilon is 0.40581303209096353, the reward is 194.33333333333334 with loss [22.879465579986572, 19.60468888282776] in episode 576
Report: 
rewardSum:194.33333333333334
loss:[22.879465579986572, 19.60468888282776]
policies:[0, 3, 1]
qAverage:[0.0, 51.349223136901855]
ws:[3.596015155315399, 5.5648932456970215]
memory len:7214
memory used:2973.0
now epsilon is 0.40540737121339787, the reward is -3.0 with loss [21.29140830039978, 27.597310543060303] in episode 577
Report: 
rewardSum:-3.0
loss:[21.29140830039978, 27.597310543060303]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7222
memory used:2973.0
now epsilon is 0.4050021158446123, the reward is 194.33333333333334 with loss [7.97966867685318, 11.95204246044159] in episode 578
Report: 
rewardSum:194.33333333333334
loss:[7.97966867685318, 11.95204246044159]
policies:[0, 3, 1]
qAverage:[0.0, 51.4461784362793]
ws:[3.1646288707852364, 5.529181003570557]
memory len:7230
memory used:2973.0
now epsilon is 0.40459726557925013, the reward is 194.33333333333334 with loss [31.76125478744507, 16.340312361717224] in episode 579
Report: 
rewardSum:194.33333333333334
loss:[31.76125478744507, 16.340312361717224]
policies:[0, 4, 0]
qAverage:[0.0, 46.903788248697914]
ws:[1.9893558025360107, 4.0936279296875]
memory len:7238
memory used:2984.0
now epsilon is 0.4041928200123598, the reward is 194.33333333333334 with loss [17.787222623825073, 16.468475818634033] in episode 580
Report: 
rewardSum:194.33333333333334
loss:[17.787222623825073, 16.468475818634033]
policies:[1, 2, 1]
qAverage:[0.0, 50.675402323404946]
ws:[6.564857800801595, 9.528549830118815]
memory len:7246
memory used:2980.0
now epsilon is 0.4033851413562119, the reward is 190.33333333333334 with loss [24.05938220024109, 33.35227358341217] in episode 581
Report: 
rewardSum:190.33333333333334
loss:[24.05938220024109, 33.35227358341217]
policies:[2, 3, 3]
qAverage:[0.0, 45.566296895345054]
ws:[4.058664719263713, 6.4109885692596436]
memory len:7262
memory used:2980.0
now epsilon is 0.40217664850999474, the reward is 186.33333333333334 with loss [40.41163778305054, 48.130253314971924] in episode 582
Report: 
rewardSum:186.33333333333334
loss:[40.41163778305054, 48.130253314971924]
policies:[1, 5, 6]
qAverage:[0.0, 55.66737365722656]
ws:[0.985333596666654, 3.0024948120117188]
memory len:7286
memory used:2979.0
now epsilon is 0.4017746226525935, the reward is 194.33333333333334 with loss [15.62433934211731, 18.084535360336304] in episode 583
Report: 
rewardSum:194.33333333333334
loss:[15.62433934211731, 18.084535360336304]
policies:[0, 4, 0]
qAverage:[0.0, 52.8169059753418]
ws:[1.507950983941555, 3.624365597963333]
memory len:7294
memory used:2979.0
now epsilon is 0.4015737604521811, the reward is -1.0 with loss [5.298323571681976, 6.6811792850494385] in episode 584
Report: 
rewardSum:-1.0
loss:[5.298323571681976, 6.6811792850494385]
policies:[0, 1, 1]
qAverage:[0.0, 31.01761245727539]
ws:[0.1987687349319458, 1.3111302852630615]
memory len:7298
memory used:2979.0
now epsilon is 0.40117233725679236, the reward is 46.33333333333334 with loss [17.00554084777832, 23.092215538024902] in episode 585
Report: 
rewardSum:46.33333333333334
loss:[17.00554084777832, 23.092215538024902]
policies:[0, 2, 2]
qAverage:[0.0, 47.60703786214193]
ws:[4.748138904571533, 7.197274367014567]
memory len:7306
memory used:2979.0
now epsilon is 0.40077131533409033, the reward is -3.0 with loss [29.05926449596882, 13.036906063556671] in episode 586
Report: 
rewardSum:-3.0
loss:[29.05926449596882, 13.036906063556671]
policies:[0, 1, 3]
qAverage:[0.0, 30.88587760925293]
ws:[0.905331552028656, 2.057485342025757]
memory len:7314
memory used:2979.0
now epsilon is 0.4003706942829529, the reward is -3.0 with loss [23.712190866470337, 23.664743185043335] in episode 587
Report: 
rewardSum:-3.0
loss:[23.712190866470337, 23.664743185043335]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7322
memory used:2979.0
now epsilon is 0.39997047370265876, the reward is 194.33333333333334 with loss [12.293976679444313, 34.13399910926819] in episode 588
Report: 
rewardSum:194.33333333333334
loss:[12.293976679444313, 34.13399910926819]
policies:[0, 4, 0]
qAverage:[0.0, 48.446800231933594]
ws:[1.9277522563934326, 3.5079272588094077]
memory len:7330
memory used:2979.0
now epsilon is 0.39917123235371776, the reward is 190.33333333333334 with loss [50.514904260635376, 57.857197999954224] in episode 589
Report: 
rewardSum:190.33333333333334
loss:[50.514904260635376, 57.857197999954224]
policies:[0, 3, 5]
qAverage:[0.0, 52.94268608093262]
ws:[3.4077336341142654, 5.392923593521118]
memory len:7346
memory used:2979.0
now epsilon is 0.3987722107856296, the reward is 46.33333333333334 with loss [21.916398286819458, 17.16851532459259] in episode 590
Report: 
rewardSum:46.33333333333334
loss:[21.916398286819458, 17.16851532459259]
policies:[0, 2, 2]
qAverage:[0.0, 42.870129903157554]
ws:[0.6237632135550181, 1.8310436805089314]
memory len:7354
memory used:2979.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20*		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.3983735880895013, the reward is 194.33333333333334 with loss [22.91524910926819, 17.450100004673004] in episode 591
Report: 
rewardSum:194.33333333333334
loss:[22.91524910926819, 17.450100004673004]
policies:[0, 4, 0]
qAverage:[0.0, 53.30114237467448]
ws:[4.109327634175618, 6.063282648722331]
memory len:7362
memory used:2979.0
now epsilon is 0.3979753638666106, the reward is 194.33333333333334 with loss [21.212483406066895, 20.073768854141235] in episode 592
Report: 
rewardSum:194.33333333333334
loss:[21.212483406066895, 20.073768854141235]
policies:[1, 3, 0]
qAverage:[0.0, 46.461246490478516]
ws:[3.415634791056315, 4.748535633087158]
memory len:7370
memory used:2979.0
now epsilon is 0.3973787737983704, the reward is 192.33333333333334 with loss [38.21188521385193, 13.85462498664856] in episode 593
Report: 
rewardSum:192.33333333333334
loss:[38.21188521385193, 13.85462498664856]
policies:[1, 3, 2]
qAverage:[0.0, 45.72941970825195]
ws:[2.7477354208628335, 3.9820712407430015]
memory len:7382
memory used:2979.0
now epsilon is 0.3965847113160301, the reward is 190.33333333333334 with loss [20.353148102760315, 24.045958876609802] in episode 594
Report: 
rewardSum:190.33333333333334
loss:[20.353148102760315, 24.045958876609802]
policies:[1, 5, 2]
qAverage:[0.0, 58.4408327738444]
ws:[6.27700940767924, 8.580394903818766]
memory len:7398
memory used:2979.0
now epsilon is 0.39618827529919587, the reward is 194.33333333333334 with loss [25.0073464512825, 18.387128114700317] in episode 595
Report: 
rewardSum:194.33333333333334
loss:[25.0073464512825, 18.387128114700317]
policies:[1, 2, 1]
qAverage:[0.0, 48.41444778442383]
ws:[4.417817910512288, 5.458932240804036]
memory len:7406
memory used:2979.0
now epsilon is 0.3953965917315229, the reward is 190.33333333333334 with loss [43.445653200149536, 45.81848907470703] in episode 596
Report: 
rewardSum:190.33333333333334
loss:[43.445653200149536, 45.81848907470703]
policies:[2, 4, 2]
qAverage:[0.0, 51.074462890625]
ws:[1.5339182615280151, 2.531028687953949]
memory len:7422
memory used:2980.0
now epsilon is 0.3945078385236951, the reward is 189.33333333333334 with loss [37.88960111141205, 59.04665172100067] in episode 597
Report: 
rewardSum:189.33333333333334
loss:[37.88960111141205, 59.04665172100067]
policies:[2, 3, 4]
qAverage:[0.0, 47.96441904703776]
ws:[3.6426241397857666, 4.942983865737915]
memory len:7440
memory used:2980.0
now epsilon is 0.3941134786009557, the reward is 46.33333333333334 with loss [31.477354764938354, 39.72448182106018] in episode 598
Report: 
rewardSum:46.33333333333334
loss:[31.477354764938354, 39.72448182106018]
policies:[0, 2, 2]
qAverage:[0.0, 34.33624267578125]
ws:[0.2554866075515747, 0.7371957302093506]
memory len:7448
memory used:2980.0
now epsilon is 0.3937195128902787, the reward is -3.0 with loss [17.392008006572723, 20.04407924413681] in episode 599
Report: 
rewardSum:-3.0
loss:[17.392008006572723, 20.04407924413681]
policies:[0, 1, 3]
qAverage:[0.0, 30.917091369628906]
ws:[-0.5111868977546692, -0.29543232917785645]
memory len:7456
memory used:2979.0
now epsilon is 0.39303102028431997, the reward is 993.0 with loss [25.836722373962402, 15.131372690200806] in episode 600
Report: 
rewardSum:993.0
loss:[25.836722373962402, 15.131372690200806]
policies:[1, 1, 5]
qAverage:[0.0, 30.812545776367188]
ws:[0.5148393511772156, 1.167689323425293]
memory len:7470
memory used:2979.0
now epsilon is 0.3922456457042423, the reward is 190.33333333333334 with loss [39.32465481758118, 35.020344227552414] in episode 601
Report: 
rewardSum:190.33333333333334
loss:[39.32465481758118, 35.020344227552414]
policies:[0, 4, 4]
qAverage:[0.0, 50.57966613769531]
ws:[7.830187797546387, 10.7958984375]
memory len:7486
memory used:2981.0
now epsilon is 0.39175558373935987, the reward is 193.33333333333334 with loss [48.59157037734985, 10.352116107940674] in episode 602
Report: 
rewardSum:193.33333333333334
loss:[48.59157037734985, 10.352116107940674]
policies:[0, 2, 3]
qAverage:[0.0, 37.14630126953125]
ws:[2.423380136489868, 3.8432562351226807]
memory len:7496
memory used:2981.0
now epsilon is 0.3913639750394812, the reward is 46.33333333333334 with loss [19.619556546211243, 21.752771854400635] in episode 603
Report: 
rewardSum:46.33333333333334
loss:[19.619556546211243, 21.752771854400635]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7504
memory used:2981.0
now epsilon is 0.3907772958583703, the reward is 44.33333333333334 with loss [22.214396953582764, 19.976301550865173] in episode 604
Report: 
rewardSum:44.33333333333334
loss:[22.214396953582764, 19.976301550865173]
policies:[2, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7516
memory used:2982.0
now epsilon is 0.3899964247850981, the reward is 42.33333333333334 with loss [53.205213129520416, 54.137126326560974] in episode 605
Report: 
rewardSum:42.33333333333334
loss:[53.205213129520416, 54.137126326560974]
policies:[0, 4, 4]
qAverage:[0.0, 54.56598663330078]
ws:[2.135709749162197, 4.590258181095123]
memory len:7532
memory used:2982.0
now epsilon is 0.38960657458459913, the reward is 194.33333333333334 with loss [22.877570152282715, 27.644901037216187] in episode 606
Report: 
rewardSum:194.33333333333334
loss:[22.877570152282715, 27.644901037216187]
policies:[1, 3, 0]
qAverage:[0.0, 43.272650400797524]
ws:[2.4300405184427896, 4.823943376541138]
memory len:7540
memory used:2982.0
now epsilon is 0.3889252742246925, the reward is 191.33333333333334 with loss [25.933430910110474, 27.493427336215973] in episode 607
Report: 
rewardSum:191.33333333333334
loss:[25.933430910110474, 27.493427336215973]
policies:[1, 1, 5]
qAverage:[0.0, 40.19504928588867]
ws:[3.0528132915496826, 4.966411590576172]
memory len:7554
memory used:2982.0
now epsilon is 0.38843936064944606, the reward is 193.33333333333334 with loss [21.847176790237427, 27.395749807357788] in episode 608
Report: 
rewardSum:193.33333333333334
loss:[21.847176790237427, 27.395749807357788]
policies:[0, 3, 2]
qAverage:[0.0, 38.003684997558594]
ws:[2.9798743724823, 4.237129211425781]
memory len:7564
memory used:2982.0
now epsilon is 0.38805106692928093, the reward is 194.33333333333334 with loss [11.601941466331482, 15.354194164276123] in episode 609
Report: 
rewardSum:194.33333333333334
loss:[11.601941466331482, 15.354194164276123]
policies:[0, 4, 0]
qAverage:[0.0, 54.949161529541016]
ws:[5.92310357093811, 8.305685877799988]
memory len:7572
memory used:2982.0
############# STATE ###############
0-		9-		18-		27-		36*		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.38727564354535093, the reward is 42.33333333333334 with loss [60.64288949966431, 21.389678955078125] in episode 610
Report: 
rewardSum:42.33333333333334
loss:[60.64288949966431, 21.389678955078125]
policies:[0, 3, 5]
qAverage:[0.0, 51.670902252197266]
ws:[-0.2216610684990883, 0.8772681504487991]
memory len:7588
memory used:2981.0
now epsilon is 0.3868885131059688, the reward is 194.33333333333334 with loss [16.415814995765686, 19.770211935043335] in episode 611
Report: 
rewardSum:194.33333333333334
loss:[16.415814995765686, 19.770211935043335]
policies:[0, 4, 0]
qAverage:[0.0, 54.770487785339355]
ws:[1.3664923012256622, 3.3824922516942024]
memory len:7596
memory used:2981.0
now epsilon is 0.3862119657876801, the reward is 191.33333333333334 with loss [24.57654595375061, 56.80125045776367] in episode 612
Report: 
rewardSum:191.33333333333334
loss:[24.57654595375061, 56.80125045776367]
policies:[0, 2, 5]
qAverage:[0.0, 31.66893196105957]
ws:[0.9139209985733032, 2.2549502849578857]
memory len:7610
memory used:2981.0
now epsilon is 0.38582589862724287, the reward is 194.33333333333334 with loss [13.253865242004395, 27.68650770187378] in episode 613
Report: 
rewardSum:194.33333333333334
loss:[13.253865242004395, 27.68650770187378]
policies:[1, 3, 0]
qAverage:[0.0, 52.275275230407715]
ws:[2.146610051393509, 4.249669909477234]
memory len:7618
memory used:2981.0
now epsilon is 0.3850549216878189, the reward is 190.33333333333334 with loss [37.03663682937622, 41.76967167854309] in episode 614
Report: 
rewardSum:190.33333333333334
loss:[37.03663682937622, 41.76967167854309]
policies:[0, 6, 2]
qAverage:[0.0, 57.66364987691244]
ws:[2.761530344684919, 4.976645330588023]
memory len:7634
memory used:2981.0
now epsilon is 0.3845738436348779, the reward is 193.33333333333334 with loss [17.73758840560913, 21.8848397731781] in episode 615
Report: 
rewardSum:193.33333333333334
loss:[17.73758840560913, 21.8848397731781]
policies:[0, 2, 3]
qAverage:[0.0, 36.611228942871094]
ws:[0.43850287795066833, 1.4627325534820557]
memory len:7644
memory used:2981.0
now epsilon is 0.38418941398240003, the reward is 194.33333333333334 with loss [15.805755615234375, 20.257373809814453] in episode 616
Report: 
rewardSum:194.33333333333334
loss:[15.805755615234375, 20.257373809814453]
policies:[0, 1, 3]
qAverage:[0.0, 36.10939407348633]
ws:[0.45944106578826904, 1.43167245388031]
memory len:7652
memory used:2981.0
now epsilon is 0.3834217071498491, the reward is 190.33333333333334 with loss [49.06013059616089, 44.371267318725586] in episode 617
Report: 
rewardSum:190.33333333333334
loss:[49.06013059616089, 44.371267318725586]
policies:[0, 4, 4]
qAverage:[0.0, 53.09102439880371]
ws:[5.176115810871124, 8.2666095495224]
memory len:7668
memory used:2981.0
now epsilon is 0.3830384292018771, the reward is 194.33333333333334 with loss [22.300426125526428, 19.76289464533329] in episode 618
Report: 
rewardSum:194.33333333333334
loss:[22.300426125526428, 19.76289464533329]
policies:[1, 2, 1]
qAverage:[0.0, 49.17583211263021]
ws:[5.4869387944539385, 8.203651746114096]
memory len:7676
memory used:2981.0
now epsilon is 0.38246423053692463, the reward is 192.33333333333334 with loss [24.666502952575684, 20.645068764686584] in episode 619
Report: 
rewardSum:192.33333333333334
loss:[24.666502952575684, 20.645068764686584]
policies:[0, 5, 1]
qAverage:[0.0, 57.16849009195963]
ws:[2.5478533109029136, 4.243905782699585]
memory len:7688
memory used:2981.0
now epsilon is 0.38189089263183784, the reward is 192.33333333333334 with loss [31.040562689304352, 34.40142285823822] in episode 620
Report: 
rewardSum:192.33333333333334
loss:[31.040562689304352, 34.40142285823822]
policies:[0, 3, 3]
qAverage:[0.0, 40.85391616821289]
ws:[2.3083934783935547, 3.9788994789123535]
memory len:7700
memory used:2981.0
now epsilon is 0.38150914492442406, the reward is 194.33333333333334 with loss [10.000394821166992, 23.25604486465454] in episode 621
Report: 
rewardSum:194.33333333333334
loss:[10.000394821166992, 23.25604486465454]
policies:[1, 2, 1]
qAverage:[0.0, 36.00925827026367]
ws:[1.606231689453125, 2.8412346839904785]
memory len:7708
memory used:2981.0
now epsilon is 0.38112777882158616, the reward is 194.33333333333334 with loss [8.70049101114273, 10.058242321014404] in episode 622
Report: 
rewardSum:194.33333333333334
loss:[8.70049101114273, 10.058242321014404]
policies:[0, 4, 0]
qAverage:[0.0, 56.35670394897461]
ws:[0.7143131010234356, 2.3941868245601654]
memory len:7716
memory used:2981.0
now epsilon is 0.38074679394186267, the reward is 194.33333333333334 with loss [19.774438619613647, 18.221524715423584] in episode 623
Report: 
rewardSum:194.33333333333334
loss:[19.774438619613647, 18.221524715423584]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7724
memory used:2981.0
now epsilon is 0.38036618990417337, the reward is 194.33333333333334 with loss [38.88738243281841, 18.726171493530273] in episode 624
Report: 
rewardSum:194.33333333333334
loss:[38.88738243281841, 18.726171493530273]
policies:[0, 2, 2]
qAverage:[0.0, 50.75329335530599]
ws:[2.6424084504445395, 5.315345764160156]
memory len:7732
memory used:2981.0
now epsilon is 0.37951122130177284, the reward is 991.0 with loss [41.28015422821045, 52.721234917640686] in episode 625
Report: 
rewardSum:991.0
loss:[41.28015422821045, 52.721234917640686]
policies:[0, 2, 7]
qAverage:[0.0, 29.751462936401367]
ws:[-0.7295072078704834, 0.35706084966659546]
memory len:7750
memory used:2981.0
now epsilon is 0.3787528626718381, the reward is 190.33333333333334 with loss [32.60842173546553, 72.24210619926453] in episode 626
Report: 
rewardSum:190.33333333333334
loss:[32.60842173546553, 72.24210619926453]
policies:[0, 4, 4]
qAverage:[0.0, 55.03862953186035]
ws:[2.716364026069641, 5.232399344444275]
memory len:7766
memory used:2981.0
now epsilon is 0.37837425181781925, the reward is 194.33333333333334 with loss [15.688066124916077, 37.56658411026001] in episode 627
Report: 
rewardSum:194.33333333333334
loss:[15.688066124916077, 37.56658411026001]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7774
memory used:2981.0
now epsilon is 0.3775237605968659, the reward is 189.33333333333334 with loss [50.93040359020233, 61.44784891605377] in episode 628
Report: 
rewardSum:189.33333333333334
loss:[50.93040359020233, 61.44784891605377]
policies:[2, 4, 3]
qAverage:[0.0, 47.87839317321777]
ws:[1.4277343600988388, 2.182966321706772]
memory len:7792
memory used:2981.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2*		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.37658101227340307, the reward is 40.33333333333334 with loss [56.02230739593506, 50.225234389305115] in episode 629
Report: 
rewardSum:40.33333333333334
loss:[56.02230739593506, 50.225234389305115]
policies:[0, 3, 7]
qAverage:[0.0, 51.13624572753906]
ws:[3.7093263417482376, 6.130986332893372]
memory len:7812
memory used:2981.0
now epsilon is 0.3762045724554745, the reward is 194.33333333333334 with loss [33.41486304998398, 21.644229412078857] in episode 630
Report: 
rewardSum:194.33333333333334
loss:[33.41486304998398, 21.644229412078857]
policies:[0, 1, 3]
qAverage:[0.0, 40.75817108154297]
ws:[2.72870135307312, 5.507894992828369]
memory len:7820
memory used:2981.0
now epsilon is 0.37526511839462096, the reward is 188.33333333333334 with loss [53.83939474821091, 50.86324739456177] in episode 631
Report: 
rewardSum:188.33333333333334
loss:[53.83939474821091, 50.86324739456177]
policies:[1, 2, 7]
qAverage:[0.0, 45.947608947753906]
ws:[4.580843448638916, 7.89601453145345]
memory len:7840
memory used:2981.0
now epsilon is 0.3748899939771932, the reward is 194.33333333333334 with loss [25.062885999679565, 21.65975832939148] in episode 632
Report: 
rewardSum:194.33333333333334
loss:[25.062885999679565, 21.65975832939148]
policies:[1, 2, 1]
qAverage:[0.0, 41.958961486816406]
ws:[2.3835201263427734, 4.902219295501709]
memory len:7848
memory used:2981.0
now epsilon is 0.37432801032846563, the reward is 192.33333333333334 with loss [39.606555342674255, 39.03902792930603] in episode 633
Report: 
rewardSum:192.33333333333334
loss:[39.606555342674255, 39.03902792930603]
policies:[0, 4, 2]
qAverage:[0.0, 51.24740982055664]
ws:[0.5027900785207748, 2.548667937517166]
memory len:7860
memory used:2987.0
now epsilon is 0.3739538226677471, the reward is 194.33333333333334 with loss [26.631484746932983, 16.760253310203552] in episode 634
Report: 
rewardSum:194.33333333333334
loss:[26.631484746932983, 16.760253310203552]
policies:[0, 3, 1]
qAverage:[0.0, 49.76738452911377]
ws:[2.582418628036976, 5.112340360879898]
memory len:7868
memory used:2987.0
now epsilon is 0.37329989408801606, the reward is 191.33333333333334 with loss [32.84991455078125, 32.867441177368164] in episode 635
Report: 
rewardSum:191.33333333333334
loss:[32.84991455078125, 32.867441177368164]
policies:[1, 3, 3]
qAverage:[0.0, 53.92103385925293]
ws:[0.7321810685098171, 3.2042925357818604]
memory len:7882
memory used:2987.0
now epsilon is 0.372833502474519, the reward is 193.33333333333334 with loss [43.410489559173584, 20.69840383529663] in episode 636
Report: 
rewardSum:193.33333333333334
loss:[43.410489559173584, 20.69840383529663]
policies:[1, 3, 1]
qAverage:[0.0, 44.647324879964195]
ws:[1.12296462059021, 3.1186911265055337]
memory len:7892
memory used:2987.0
now epsilon is 0.3724608087613073, the reward is 194.33333333333334 with loss [26.875316619873047, 26.067182302474976] in episode 637
Report: 
rewardSum:194.33333333333334
loss:[26.875316619873047, 26.067182302474976]
policies:[2, 2, 0]
qAverage:[0.0, 48.62744140625]
ws:[4.708097139994304, 7.926479021708171]
memory len:7900
memory used:2987.0
now epsilon is 0.372088487602072, the reward is 194.33333333333334 with loss [24.728655576705933, 8.975493609905243] in episode 638
Report: 
rewardSum:194.33333333333334
loss:[24.728655576705933, 8.975493609905243]
policies:[0, 4, 0]
qAverage:[0.0, 55.43506927490235]
ws:[2.189035153388977, 4.811539888381958]
memory len:7908
memory used:2987.0
now epsilon is 0.3716236094897427, the reward is 193.33333333333334 with loss [24.20911157131195, 26.828722953796387] in episode 639
Report: 
rewardSum:193.33333333333334
loss:[24.20911157131195, 26.828722953796387]
policies:[0, 4, 1]
qAverage:[0.0, 55.1443733215332]
ws:[2.9382928609848022, 5.849430751800537]
memory len:7918
memory used:2988.0
now epsilon is 0.3710665223565314, the reward is 192.33333333333334 with loss [26.04380151629448, 31.81453311443329] in episode 640
Report: 
rewardSum:192.33333333333334
loss:[26.04380151629448, 31.81453311443329]
policies:[0, 5, 1]
qAverage:[0.0, 56.01683807373047]
ws:[1.3330808877944946, 3.7187653680642447]
memory len:7930
memory used:2988.0
now epsilon is 0.3706955949609307, the reward is 194.33333333333334 with loss [18.821167945861816, 31.24299669265747] in episode 641
Report: 
rewardSum:194.33333333333334
loss:[18.821167945861816, 31.24299669265747]
policies:[0, 3, 1]
qAverage:[0.0, 54.01141548156738]
ws:[1.2473648637533188, 3.6420950889587402]
memory len:7938
memory used:2988.0
now epsilon is 0.3703250383536508, the reward is 194.33333333333334 with loss [28.645495891571045, 21.64316439628601] in episode 642
Report: 
rewardSum:194.33333333333334
loss:[28.645495891571045, 21.64316439628601]
policies:[0, 3, 1]
qAverage:[0.0, 43.983053843180336]
ws:[1.9343778491020203, 3.409726619720459]
memory len:7946
memory used:2988.0
now epsilon is 0.36976989786013903, the reward is 44.33333333333334 with loss [18.933977127075195, 31.55132031440735] in episode 643
Report: 
rewardSum:44.33333333333334
loss:[18.933977127075195, 31.55132031440735]
policies:[0, 3, 3]
qAverage:[0.0, 35.70781707763672]
ws:[1.6504096984863281, 3.3796849250793457]
memory len:7958
memory used:2988.0
now epsilon is 0.36940026660288144, the reward is 194.33333333333334 with loss [12.01237279176712, 22.70155620574951] in episode 644
Report: 
rewardSum:194.33333333333334
loss:[12.01237279176712, 22.70155620574951]
policies:[1, 3, 0]
qAverage:[0.0, 45.089717864990234]
ws:[2.4805020491282144, 4.7627518971761065]
memory len:7966
memory used:2988.0
now epsilon is 0.3690310048382925, the reward is 46.33333333333334 with loss [34.77737760543823, 39.43432092666626] in episode 645
Report: 
rewardSum:46.33333333333334
loss:[34.77737760543823, 39.43432092666626]
policies:[0, 2, 2]
qAverage:[0.0, 29.882610321044922]
ws:[0.15205428004264832, 1.2182049751281738]
memory len:7974
memory used:2988.0
now epsilon is 0.368662112197018, the reward is 194.33333333333334 with loss [23.416887998580933, 22.06234049797058] in episode 646
Report: 
rewardSum:194.33333333333334
loss:[23.416887998580933, 22.06234049797058]
policies:[1, 2, 1]
qAverage:[0.0, 48.24640655517578]
ws:[3.984756072362264, 7.6313737233479815]
memory len:7982
memory used:2988.0
now epsilon is 0.36820151491299563, the reward is 193.33333333333334 with loss [16.65855073928833, 27.394667387008667] in episode 647
Report: 
rewardSum:193.33333333333334
loss:[16.65855073928833, 27.394667387008667]
policies:[0, 3, 2]
qAverage:[0.0, 50.26238250732422]
ws:[3.303558349609375, 7.265197992324829]
memory len:7992
memory used:2988.0
now epsilon is 0.367649557714505, the reward is 192.33333333333334 with loss [35.755369424819946, 43.14104342460632] in episode 648
Report: 
rewardSum:192.33333333333334
loss:[35.755369424819946, 43.14104342460632]
policies:[2, 1, 3]
qAverage:[0.0, 33.19214630126953]
ws:[-0.2178865373134613, 1.1913907527923584]
memory len:8004
memory used:2988.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20*		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.36682317293879324, the reward is 189.33333333333334 with loss [55.63436007499695, 48.26592659950256] in episode 649
Report: 
rewardSum:189.33333333333334
loss:[55.63436007499695, 48.26592659950256]
policies:[1, 4, 4]
qAverage:[0.0, 50.55488395690918]
ws:[3.8710425794124603, 7.123208165168762]
memory len:8022
memory used:2993.0
now epsilon is 0.3664564873016193, the reward is 194.33333333333334 with loss [18.463977098464966, 19.660160541534424] in episode 650
Report: 
rewardSum:194.33333333333334
loss:[18.463977098464966, 19.660160541534424]
policies:[2, 2, 0]
qAverage:[0.0, 46.538012186686196]
ws:[6.083140055338542, 8.455273310343424]
memory len:8030
memory used:2994.0
now epsilon is 0.3660901682125984, the reward is 194.33333333333334 with loss [19.574445486068726, 22.002768516540527] in episode 651
Report: 
rewardSum:194.33333333333334
loss:[19.574445486068726, 22.002768516540527]
policies:[1, 2, 1]
qAverage:[0.0, 41.34320831298828]
ws:[3.4613970518112183, 5.133651415506999]
memory len:8038
memory used:2994.0
now epsilon is 0.36563278425149337, the reward is 193.33333333333334 with loss [49.25306987762451, 17.829413175582886] in episode 652
Report: 
rewardSum:193.33333333333334
loss:[49.25306987762451, 17.829413175582886]
policies:[0, 3, 2]
qAverage:[0.0, 51.21242014567057]
ws:[0.23445447285970053, 3.6294339497884116]
memory len:8048
memory used:2994.0
now epsilon is 0.36508467774161263, the reward is 192.33333333333334 with loss [43.09272038936615, 17.015350997447968] in episode 653
Report: 
rewardSum:192.33333333333334
loss:[43.09272038936615, 17.015350997447968]
policies:[1, 2, 3]
qAverage:[0.0, 29.129985809326172]
ws:[-1.0403233766555786, 0.6275573968887329]
memory len:8060
memory used:2994.0
now epsilon is 0.3649021582205342, the reward is -1.0 with loss [6.565617799758911, 9.979534149169922] in episode 654
Report: 
rewardSum:-1.0
loss:[6.565617799758911, 9.979534149169922]
policies:[0, 1, 1]
qAverage:[0.0, 29.108556747436523]
ws:[2.2626359462738037, 3.814807653427124]
memory len:8064
memory used:2994.0
now epsilon is 0.36435514696496624, the reward is 44.33333333333334 with loss [28.947648763656616, 25.326260685920715] in episode 655
Report: 
rewardSum:44.33333333333334
loss:[28.947648763656616, 25.326260685920715]
policies:[0, 2, 4]
qAverage:[0.0, 42.278271993001304]
ws:[5.285037835439046, 8.472858428955078]
memory len:8076
memory used:2994.0
now epsilon is 0.3638089557136294, the reward is 44.33333333333334 with loss [22.415127635002136, 16.52272403240204] in episode 656
Report: 
rewardSum:44.33333333333334
loss:[22.415127635002136, 16.52272403240204]
policies:[1, 2, 3]
qAverage:[0.0, 41.17237218221029]
ws:[4.377243359883626, 6.948807716369629]
memory len:8088
memory used:2994.0
now epsilon is 0.3632635832372861, the reward is 192.33333333333334 with loss [35.320475339889526, 14.368670642375946] in episode 657
Report: 
rewardSum:192.33333333333334
loss:[35.320475339889526, 14.368670642375946]
policies:[2, 2, 2]
qAverage:[0.0, 50.00645955403646]
ws:[2.074876348177592, 4.745589892069499]
memory len:8100
memory used:2994.0
now epsilon is 0.3628097307412262, the reward is 193.33333333333334 with loss [42.143568992614746, 22.893070816993713] in episode 658
Report: 
rewardSum:193.33333333333334
loss:[42.143568992614746, 22.893070816993713]
policies:[0, 3, 2]
qAverage:[0.0, 55.2520637512207]
ws:[2.169286072254181, 5.031518459320068]
memory len:8110
memory used:2994.0
now epsilon is 0.3624470570414599, the reward is 194.33333333333334 with loss [9.643242627382278, 18.10598397254944] in episode 659
Report: 
rewardSum:194.33333333333334
loss:[9.643242627382278, 18.10598397254944]
policies:[1, 3, 0]
qAverage:[0.0, 47.47081629435221]
ws:[1.5859438081582387, 3.8582793871561685]
memory len:8118
memory used:2994.0
now epsilon is 0.36190372613677024, the reward is 44.33333333333334 with loss [22.616328239440918, 24.934168577194214] in episode 660
Report: 
rewardSum:44.33333333333334
loss:[22.616328239440918, 24.934168577194214]
policies:[0, 2, 4]
qAverage:[0.0, 33.53406524658203]
ws:[0.01313767023384571, 1.4647271633148193]
memory len:8130
memory used:2994.0
now epsilon is 0.36154195810191325, the reward is 194.33333333333334 with loss [17.875064611434937, 19.165271401405334] in episode 661
Report: 
rewardSum:194.33333333333334
loss:[17.875064611434937, 19.165271401405334]
policies:[1, 3, 0]
qAverage:[0.0, 44.54153823852539]
ws:[1.220846896370252, 2.748274326324463]
memory len:8138
memory used:2994.0
now epsilon is 0.36118055169945074, the reward is 194.33333333333334 with loss [17.22436285018921, 21.039611220359802] in episode 662
Report: 
rewardSum:194.33333333333334
loss:[17.22436285018921, 21.039611220359802]
policies:[0, 4, 0]
qAverage:[0.0, 56.52599868774414]
ws:[2.082582378387451, 4.33059401512146]
memory len:8146
memory used:2994.0
now epsilon is 0.36081950656788586, the reward is 194.33333333333334 with loss [22.878076553344727, 15.51735782623291] in episode 663
Report: 
rewardSum:194.33333333333334
loss:[22.878076553344727, 15.51735782623291]
policies:[1, 3, 0]
qAverage:[0.0, 42.3245964050293]
ws:[2.7306020259857178, 4.988626480102539]
memory len:8154
memory used:2994.0
now epsilon is 0.3604588223460832, the reward is 194.33333333333334 with loss [30.951653957366943, 18.929982662200928] in episode 664
Report: 
rewardSum:194.33333333333334
loss:[30.951653957366943, 18.929982662200928]
policies:[1, 2, 1]
qAverage:[0.0, 36.30514907836914]
ws:[2.926823377609253, 4.384745121002197]
memory len:8162
memory used:2994.0
now epsilon is 0.3599184719300878, the reward is 192.33333333333334 with loss [41.663772225379944, 29.679967164993286] in episode 665
Report: 
rewardSum:192.33333333333334
loss:[41.663772225379944, 29.679967164993286]
policies:[1, 4, 1]
qAverage:[0.0, 56.27866439819336]
ws:[4.795592451095581, 7.746553707122803]
memory len:8174
memory used:2994.0
now epsilon is 0.3595586884050912, the reward is 194.33333333333334 with loss [15.239949464797974, 24.735502243041992] in episode 666
Report: 
rewardSum:194.33333333333334
loss:[15.239949464797974, 24.735502243041992]
policies:[0, 3, 1]
qAverage:[0.0, 42.32926940917969]
ws:[1.9582370519638062, 4.056573867797852]
memory len:8182
memory used:3000.0
now epsilon is 0.35910946471259114, the reward is 193.33333333333334 with loss [29.96646285057068, 14.727214932441711] in episode 667
Report: 
rewardSum:193.33333333333334
loss:[29.96646285057068, 14.727214932441711]
policies:[1, 3, 1]
qAverage:[0.0, 53.410216331481934]
ws:[1.040008906275034, 3.248243063688278]
memory len:8192
memory used:3000.0
now epsilon is 0.3587504898914849, the reward is 194.33333333333334 with loss [24.29878282546997, 19.198944568634033] in episode 668
Report: 
rewardSum:194.33333333333334
loss:[24.29878282546997, 19.198944568634033]
policies:[2, 2, 0]
qAverage:[0.0, 30.27818489074707]
ws:[-0.5716111063957214, 0.5056885480880737]
memory len:8200
memory used:3000.0
now epsilon is 0.3585711370684448, the reward is -1.0 with loss [20.361145973205566, 3.665665864944458] in episode 669
Report: 
rewardSum:-1.0
loss:[20.361145973205566, 3.665665864944458]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:8204
memory used:3000.0
now epsilon is 0.3580336164112507, the reward is 192.33333333333334 with loss [23.135135650634766, 25.71339988708496] in episode 670
Report: 
rewardSum:192.33333333333334
loss:[23.135135650634766, 25.71339988708496]
policies:[1, 4, 1]
qAverage:[0.0, 56.085182189941406]
ws:[3.35135680437088, 5.828461289405823]
memory len:8216
memory used:3001.0
now epsilon is 0.35767571703506995, the reward is 194.33333333333334 with loss [24.85344696044922, 12.290456056594849] in episode 671
Report: 
rewardSum:194.33333333333334
loss:[24.85344696044922, 12.290456056594849]
policies:[1, 3, 0]
qAverage:[0.0, 46.957715352376304]
ws:[2.667002518971761, 4.622352917989095]
memory len:8224
memory used:3001.0
now epsilon is 0.3571395386687495, the reward is 192.33333333333334 with loss [23.416213512420654, 20.79402232170105] in episode 672
Report: 
rewardSum:192.33333333333334
loss:[23.416213512420654, 20.79402232170105]
policies:[0, 3, 3]
qAverage:[0.0, 53.843393325805664]
ws:[1.8651033788919449, 4.314907252788544]
memory len:8236
memory used:3001.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2*		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.35678253303508795, the reward is 194.33333333333334 with loss [20.26509118080139, 29.198943257331848] in episode 673
Report: 
rewardSum:194.33333333333334
loss:[20.26509118080139, 29.198943257331848]
policies:[1, 2, 1]
qAverage:[0.0, 47.07769266764323]
ws:[4.557276884714763, 7.195313930511475]
memory len:8244
memory used:3001.0
now epsilon is 0.35624769360768643, the reward is 192.33333333333334 with loss [40.15917897224426, 41.535384118556976] in episode 674
Report: 
rewardSum:192.33333333333334
loss:[40.15917897224426, 41.535384118556976]
policies:[1, 4, 1]
qAverage:[0.0, 51.76005744934082]
ws:[2.9128544330596924, 4.973019003868103]
memory len:8256
memory used:3001.0
now epsilon is 0.3558915794846998, the reward is 194.33333333333334 with loss [10.54210078716278, 13.686023831367493] in episode 675
Report: 
rewardSum:194.33333333333334
loss:[10.54210078716278, 13.686023831367493]
policies:[0, 4, 0]
qAverage:[0.0, 56.57678985595703]
ws:[4.449342584609985, 6.726233673095703]
memory len:8264
memory used:3001.0
now epsilon is 0.3553580756526333, the reward is 192.33333333333334 with loss [23.29015338420868, 27.837576627731323] in episode 676
Report: 
rewardSum:192.33333333333334
loss:[23.29015338420868, 27.837576627731323]
policies:[0, 5, 1]
qAverage:[0.0, 56.85187606811523]
ws:[3.2777632027864456, 5.877150058746338]
memory len:8276
memory used:3000.0
now epsilon is 0.35438206157227403, the reward is 187.33333333333334 with loss [51.392417430877686, 67.82444655895233] in episode 677
Report: 
rewardSum:187.33333333333334
loss:[51.392417430877686, 67.82444655895233]
policies:[0, 5, 6]
qAverage:[0.0, 50.4398307800293]
ws:[3.37761752307415, 5.260211646556854]
memory len:8298
memory used:3001.0
now epsilon is 0.3540278123818274, the reward is 194.33333333333334 with loss [22.866162061691284, 26.45565629005432] in episode 678
Report: 
rewardSum:194.33333333333334
loss:[22.866162061691284, 26.45565629005432]
policies:[0, 4, 0]
qAverage:[0.0, 56.33895645141602]
ws:[2.129887044429779, 4.323498201370239]
memory len:8306
memory used:3001.0
now epsilon is 0.35349710245371596, the reward is 192.33333333333334 with loss [21.835388660430908, 27.081877946853638] in episode 679
Report: 
rewardSum:192.33333333333334
loss:[21.835388660430908, 27.081877946853638]
policies:[0, 5, 1]
qAverage:[0.0, 49.734846115112305]
ws:[4.305090844631195, 6.413348913192749]
memory len:8318
memory used:3001.0
now epsilon is 0.3531437378905835, the reward is 194.33333333333334 with loss [24.200523853302002, 34.1755268573761] in episode 680
Report: 
rewardSum:194.33333333333334
loss:[24.200523853302002, 34.1755268573761]
policies:[0, 2, 2]
qAverage:[0.0, 41.7889773050944]
ws:[2.121745745340983, 3.278040329615275]
memory len:8326
memory used:3001.0
now epsilon is 0.3527907265595246, the reward is 194.33333333333334 with loss [22.048957586288452, 16.113354921340942] in episode 681
Report: 
rewardSum:194.33333333333334
loss:[22.048957586288452, 16.113354921340942]
policies:[0, 2, 2]
qAverage:[0.0, 41.31181335449219]
ws:[1.0865339040756226, 3.0963263511657715]
memory len:8334
memory used:3000.0
now epsilon is 0.35243806810743955, the reward is 194.33333333333334 with loss [30.88810420036316, 32.399752736091614] in episode 682
Report: 
rewardSum:194.33333333333334
loss:[30.88810420036316, 32.399752736091614]
policies:[1, 3, 0]
qAverage:[0.0, 41.34732309977213]
ws:[-0.46614054466287297, 0.7339971860249838]
memory len:8342
memory used:3000.0
now epsilon is 0.35208576218158166, the reward is 194.33333333333334 with loss [16.24657368659973, 19.818536043167114] in episode 683
Report: 
rewardSum:194.33333333333334
loss:[16.24657368659973, 19.818536043167114]
policies:[0, 4, 0]
qAverage:[0.0, 55.9935791015625]
ws:[2.5924718379974365, 5.320979940891266]
memory len:8350
memory used:3000.0
now epsilon is 0.351733808429557, the reward is 46.33333333333334 with loss [17.252782344818115, 41.702765107154846] in episode 684
Report: 
rewardSum:46.33333333333334
loss:[17.252782344818115, 41.702765107154846]
policies:[1, 2, 1]
qAverage:[0.0, 29.272369384765625]
ws:[-0.27697792649269104, 0.6180426478385925]
memory len:8358
memory used:3000.0
now epsilon is 0.3513822064993236, the reward is 46.33333333333334 with loss [11.382041931152344, 12.88757836818695] in episode 685
Report: 
rewardSum:46.33333333333334
loss:[11.382041931152344, 12.88757836818695]
policies:[0, 2, 2]
qAverage:[0.0, 46.48653666178385]
ws:[1.7877679665883381, 3.4402225812276206]
memory len:8366
memory used:3000.0
now epsilon is 0.35103095603919177, the reward is 194.33333333333334 with loss [7.792005300521851, 25.24536395072937] in episode 686
Report: 
rewardSum:194.33333333333334
loss:[7.792005300521851, 25.24536395072937]
policies:[1, 3, 0]
qAverage:[0.0, 44.40692011515299]
ws:[0.8737513224283854, 2.000362736483415]
memory len:8374
memory used:3000.0
now epsilon is 0.3506800566978231, the reward is 194.33333333333334 with loss [38.979305267333984, 22.039467215538025] in episode 687
Report: 
rewardSum:194.33333333333334
loss:[38.979305267333984, 22.039467215538025]
policies:[1, 3, 0]
qAverage:[0.0, 40.113731384277344]
ws:[4.106983661651611, 5.864653309186299]
memory len:8382
memory used:3000.0
now epsilon is 0.35032950812423047, the reward is 194.33333333333334 with loss [24.40419864654541, 17.779189586639404] in episode 688
Report: 
rewardSum:194.33333333333334
loss:[24.40419864654541, 17.779189586639404]
policies:[0, 3, 1]
qAverage:[0.0, 53.77946186065674]
ws:[2.4236632138490677, 4.564421713352203]
memory len:8390
memory used:3007.0
now epsilon is 0.3498043421865006, the reward is 192.33333333333334 with loss [49.446643352508545, 21.993377447128296] in episode 689
Report: 
rewardSum:192.33333333333334
loss:[49.446643352508545, 21.993377447128296]
policies:[0, 4, 2]
qAverage:[0.0, 46.43317667643229]
ws:[1.7972637017567952, 3.7453207969665527]
memory len:8402
memory used:3007.0
now epsilon is 0.3492799635054983, the reward is 192.33333333333334 with loss [16.0598247051239, 26.704931020736694] in episode 690
Report: 
rewardSum:192.33333333333334
loss:[16.0598247051239, 26.704931020736694]
policies:[0, 4, 2]
qAverage:[0.0, 42.945135752360024]
ws:[3.9182294408480325, 6.278155485788981]
memory len:8414
memory used:3007.0
now epsilon is 0.3484077452920185, the reward is 188.33333333333334 with loss [48.14541886001825, 47.06869477033615] in episode 691
Report: 
rewardSum:188.33333333333334
loss:[48.14541886001825, 47.06869477033615]
policies:[1, 4, 5]
qAverage:[0.0, 56.173784255981445]
ws:[3.2240697145462036, 5.386759519577026]
memory len:8434
memory used:3007.0
now epsilon is 0.34797245331081245, the reward is 45.33333333333334 with loss [16.212547659873962, 20.99176275730133] in episode 692
Report: 
rewardSum:45.33333333333334
loss:[16.212547659873962, 20.99176275730133]
policies:[0, 2, 3]
qAverage:[0.0, 43.361473083496094]
ws:[3.9244990746180215, 5.672276735305786]
memory len:8444
memory used:3006.0
now epsilon is 0.3476246113254247, the reward is 194.33333333333334 with loss [22.428879261016846, 29.26720881462097] in episode 693
Report: 
rewardSum:194.33333333333334
loss:[22.428879261016846, 29.26720881462097]
policies:[1, 2, 1]
qAverage:[0.0, 46.43350728352865]
ws:[1.6945818265279133, 3.084843953450521]
memory len:8452
memory used:3006.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23*		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.3472771170516034, the reward is 194.33333333333334 with loss [13.89541220664978, 26.077702283859253] in episode 694
Report: 
rewardSum:194.33333333333334
loss:[13.89541220664978, 26.077702283859253]
policies:[0, 3, 1]
qAverage:[0.0, 33.672523498535156]
ws:[5.304311752319336, 7.381714344024658]
memory len:8460
memory used:3006.0
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.34692997014176724, the reward is 194.33333333333334 with loss [33.957213401794434, 7.173858284950256] in episode 695
Report: 
rewardSum:194.33333333333334
loss:[33.957213401794434, 7.173858284950256]
policies:[0, 4, 0]
qAverage:[0.0, 56.88015556335449]
ws:[2.8053997457027435, 4.956541538238525]
memory len:8468
memory used:3007.0
now epsilon is 0.3464099003250063, the reward is 192.33333333333334 with loss [37.30725538730621, 40.56365489959717] in episode 696
Report: 
rewardSum:192.33333333333334
loss:[37.30725538730621, 40.56365489959717]
policies:[3, 1, 2]
qAverage:[0.0, 30.018695831298828]
ws:[0.5505582690238953, 1.7191896438598633]
memory len:8480
memory used:3008.0
now epsilon is 0.34571768643866796, the reward is 190.33333333333334 with loss [38.05325174331665, 37.0808207988739] in episode 697
Report: 
rewardSum:190.33333333333334
loss:[38.05325174331665, 37.0808207988739]
policies:[0, 5, 3]
qAverage:[0.0, 54.462205505371095]
ws:[0.7117029272019864, 2.7829806804656982]
memory len:8496
memory used:3014.0
now epsilon is 0.3453720983747558, the reward is 194.33333333333334 with loss [24.271316528320312, 12.849409818649292] in episode 698
Report: 
rewardSum:194.33333333333334
loss:[24.271316528320312, 12.849409818649292]
policies:[0, 4, 0]
qAverage:[0.0, 58.12944412231445]
ws:[2.6468038231134416, 4.872313714027404]
memory len:8504
memory used:3026.0
now epsilon is 0.34502685576933356, the reward is 194.33333333333334 with loss [15.760808289051056, 19.15416669845581] in episode 699
Report: 
rewardSum:194.33333333333334
loss:[15.760808289051056, 19.15416669845581]
policies:[1, 3, 0]
qAverage:[0.0, 54.701517740885414]
ws:[3.819276491800944, 6.042576948801677]
memory len:8512
memory used:3031.0
now epsilon is 0.34450963884055624, the reward is 192.33333333333334 with loss [29.084031581878662, 26.794846534729004] in episode 700
Report: 
rewardSum:192.33333333333334
loss:[29.084031581878662, 26.794846534729004]
policies:[2, 2, 2]
qAverage:[0.0, 33.62021255493164]
ws:[6.313416481018066, 8.233549118041992]
memory len:8524
memory used:3031.0
now epsilon is 0.34407921705670697, the reward is 193.33333333333334 with loss [24.21785318851471, 27.706310033798218] in episode 701
Report: 
rewardSum:193.33333333333334
loss:[24.21785318851471, 27.706310033798218]
policies:[2, 1, 2]
qAverage:[0.0, 29.61359405517578]
ws:[0.5298178195953369, 0.6489582061767578]
memory len:8534
memory used:3031.0
now epsilon is 0.3437352668478531, the reward is 194.33333333333334 with loss [13.777073860168457, 26.5650475025177] in episode 702
Report: 
rewardSum:194.33333333333334
loss:[13.777073860168457, 26.5650475025177]
policies:[1, 3, 0]
qAverage:[0.0, 54.04325199127197]
ws:[4.164965584874153, 5.757887035608292]
memory len:8542
memory used:3031.0
now epsilon is 0.3433916604602482, the reward is 194.33333333333334 with loss [17.201669335365295, 37.63854217529297] in episode 703
Report: 
rewardSum:194.33333333333334
loss:[17.201669335365295, 37.63854217529297]
policies:[0, 4, 0]
qAverage:[0.0, 58.81658363342285]
ws:[5.102986931800842, 7.30207085609436]
memory len:8550
memory used:3031.0
now epsilon is 0.34304839755020006, the reward is 194.33333333333334 with loss [31.40118408203125, 11.004940032958984] in episode 704
Report: 
rewardSum:194.33333333333334
loss:[31.40118408203125, 11.004940032958984]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:8558
memory used:3032.0
now epsilon is 0.3428768947919498, the reward is -1.0 with loss [11.422008037567139, 13.410531997680664] in episode 705
Report: 
rewardSum:-1.0
loss:[11.422008037567139, 13.410531997680664]
policies:[0, 1, 1]
qAverage:[0.0, 30.30396842956543]
ws:[1.0487643480300903, 1.3967407941818237]
memory len:8562
memory used:3032.0
now epsilon is 0.34236290078972187, the reward is 192.33333333333334 with loss [26.234552145004272, 29.286901712417603] in episode 706
Report: 
rewardSum:192.33333333333334
loss:[26.234552145004272, 29.286901712417603]
policies:[2, 3, 1]
qAverage:[0.0, 52.620470682779946]
ws:[6.350996176401774, 8.768998463948568]
memory len:8574
memory used:3032.0
now epsilon is 0.34202066625362365, the reward is 194.33333333333334 with loss [30.32384943962097, 25.882340908050537] in episode 707
Report: 
rewardSum:194.33333333333334
loss:[30.32384943962097, 25.882340908050537]
policies:[1, 2, 1]
qAverage:[0.0, 44.065711975097656]
ws:[2.28139591217041, 4.219947338104248]
memory len:8582
memory used:3032.0
now epsilon is 0.34167877382374495, the reward is 194.33333333333334 with loss [13.616081774234772, 18.137543201446533] in episode 708
Report: 
rewardSum:194.33333333333334
loss:[13.616081774234772, 18.137543201446533]
policies:[1, 3, 0]
qAverage:[0.0, 50.969526290893555]
ws:[2.3365806341171265, 4.438439726829529]
memory len:8590
memory used:3032.0
now epsilon is 0.34116657588010524, the reward is 192.33333333333334 with loss [30.561597108840942, 21.949681282043457] in episode 709
Report: 
rewardSum:192.33333333333334
loss:[30.561597108840942, 21.949681282043457]
policies:[0, 4, 2]
qAverage:[0.0, 53.866162872314455]
ws:[3.058075523376465, 5.701734849810601]
memory len:8602
memory used:3033.0
now epsilon is 0.3408255372203695, the reward is 46.33333333333334 with loss [17.6164870262146, 28.837704181671143] in episode 710
Report: 
rewardSum:46.33333333333334
loss:[17.6164870262146, 28.837704181671143]
policies:[1, 2, 1]
qAverage:[0.0, 30.699298858642578]
ws:[1.1074106693267822, 2.2045443058013916]
memory len:8610
memory used:3033.0
now epsilon is 0.3404848394714254, the reward is 194.33333333333334 with loss [11.750932216644287, 43.34617519378662] in episode 711
Report: 
rewardSum:194.33333333333334
loss:[11.750932216644287, 43.34617519378662]
policies:[1, 3, 0]
qAverage:[0.0, 54.14641571044922]
ws:[3.9095798432826996, 6.532554805278778]
memory len:8618
memory used:3033.0
now epsilon is 0.33997443131037375, the reward is 192.33333333333334 with loss [25.09526515007019, 40.495656967163086] in episode 712
Report: 
rewardSum:192.33333333333334
loss:[25.09526515007019, 40.495656967163086]
policies:[1, 2, 3]
qAverage:[0.0, 51.90081024169922]
ws:[2.9018454949061074, 4.891092618306478]
memory len:8630
memory used:3033.0
now epsilon is 0.3396345843482281, the reward is 194.33333333333334 with loss [32.45301961898804, 18.304349422454834] in episode 713
Report: 
rewardSum:194.33333333333334
loss:[32.45301961898804, 18.304349422454834]
policies:[2, 2, 0]
qAverage:[0.0, 34.2695426940918]
ws:[5.6832499504089355, 7.672207832336426]
memory len:8638
memory used:3033.0
now epsilon is 0.33912545077301265, the reward is 192.33333333333334 with loss [52.40336453914642, 41.25200068950653] in episode 714
Report: 
rewardSum:192.33333333333334
loss:[52.40336453914642, 41.25200068950653]
policies:[0, 3, 3]
qAverage:[0.0, 53.51058451334635]
ws:[1.1634372075398762, 3.3762741883595786]
memory len:8650
memory used:3033.0
now epsilon is 0.3387864524730897, the reward is 194.33333333333334 with loss [17.14602518081665, 15.593413352966309] in episode 715
Report: 
rewardSum:194.33333333333334
loss:[17.14602518081665, 15.593413352966309]
policies:[1, 2, 1]
qAverage:[0.0, 44.79807662963867]
ws:[2.1108407974243164, 4.573009490966797]
memory len:8658
memory used:3033.0
now epsilon is 0.33827859030082835, the reward is 192.33333333333334 with loss [45.89575266838074, 34.1542809009552] in episode 716
Report: 
rewardSum:192.33333333333334
loss:[45.89575266838074, 34.1542809009552]
policies:[0, 5, 1]
qAverage:[0.0, 55.36463165283203]
ws:[2.444460964202881, 4.964031171798706]
memory len:8670
memory used:3034.0
now epsilon is 0.3378559534342219, the reward is 193.33333333333334 with loss [17.480189323425293, 10.161298155784607] in episode 717
Report: 
rewardSum:193.33333333333334
loss:[17.480189323425293, 10.161298155784607]
policies:[0, 3, 2]
qAverage:[0.0, 53.18467712402344]
ws:[5.5929005940755205, 8.978498776753744]
memory len:8680
memory used:3034.0
############# STATE ###############
0-		9*		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.33734948613846677, the reward is 192.33333333333334 with loss [23.772362232208252, 40.67338538169861] in episode 718
Report: 
rewardSum:192.33333333333334
loss:[23.772362232208252, 40.67338538169861]
policies:[1, 4, 1]
qAverage:[0.0, 54.90700435638428]
ws:[5.237708032131195, 8.315813183784485]
memory len:8692
memory used:3034.0
now epsilon is 0.3370122631373026, the reward is 194.33333333333334 with loss [24.797240555286407, 26.313657999038696] in episode 719
Report: 
rewardSum:194.33333333333334
loss:[24.797240555286407, 26.313657999038696]
policies:[0, 4, 0]
qAverage:[0.0, 59.27516555786133]
ws:[5.489538192749023, 7.883172631263733]
memory len:8700
memory used:3034.0
now epsilon is 0.3365070605862968, the reward is 192.33333333333334 with loss [58.025054693222046, 36.497968673706055] in episode 720
Report: 
rewardSum:192.33333333333334
loss:[58.025054693222046, 36.497968673706055]
policies:[0, 5, 1]
qAverage:[0.0, 55.07674102783203]
ws:[2.9159156918525695, 4.5280365467071535]
memory len:8712
memory used:3034.0
now epsilon is 0.33600261536564796, the reward is 44.33333333333334 with loss [44.2509491443634, 34.38843059539795] in episode 721
Report: 
rewardSum:44.33333333333334
loss:[44.2509491443634, 34.38843059539795]
policies:[0, 2, 4]
qAverage:[0.0, 33.63313674926758]
ws:[2.5004358291625977, 3.6239914894104004]
memory len:8724
memory used:3034.0
now epsilon is 0.33566673873026426, the reward is 194.33333333333334 with loss [21.37960147857666, 35.70057392120361] in episode 722
Report: 
rewardSum:194.33333333333334
loss:[21.37960147857666, 35.70057392120361]
policies:[1, 3, 0]
qAverage:[0.0, 60.380184173583984]
ws:[6.287100791931152, 8.379846096038818]
memory len:8732
memory used:3034.0
now epsilon is 0.33524736504612185, the reward is 45.33333333333334 with loss [34.05507016181946, 31.264784574508667] in episode 723
Report: 
rewardSum:45.33333333333334
loss:[34.05507016181946, 31.264784574508667]
policies:[1, 2, 2]
qAverage:[0.0, 49.94363149007162]
ws:[4.751774152119954, 6.72902234395345]
memory len:8742
memory used:3034.0
now epsilon is 0.33499599237598005, the reward is -2.0 with loss [14.684890031814575, 17.173582553863525] in episode 724
Report: 
rewardSum:-2.0
loss:[14.684890031814575, 17.173582553863525]
policies:[0, 1, 2]
qAverage:[0.0, 30.438735961914062]
ws:[-0.20500211417675018, 0.3624577522277832]
memory len:8748
memory used:3034.0
now epsilon is 0.3346611219861653, the reward is 194.33333333333334 with loss [26.32759666442871, 17.342198848724365] in episode 725
Report: 
rewardSum:194.33333333333334
loss:[26.32759666442871, 17.342198848724365]
policies:[0, 2, 2]
qAverage:[0.0, 57.94962819417318]
ws:[3.7708357175191245, 5.791470368703206]
memory len:8756
memory used:3034.0
now epsilon is 0.33415944394342595, the reward is 192.33333333333334 with loss [20.81944239139557, 32.542579889297485] in episode 726
Report: 
rewardSum:192.33333333333334
loss:[20.81944239139557, 32.542579889297485]
policies:[2, 1, 3]
qAverage:[0.0, 45.544166564941406]
ws:[2.7622406482696533, 5.103615760803223]
memory len:8768
memory used:3034.0
now epsilon is 0.33382540978839037, the reward is 194.33333333333334 with loss [22.534335136413574, 22.43360674381256] in episode 727
Report: 
rewardSum:194.33333333333334
loss:[22.534335136413574, 22.43360674381256]
policies:[1, 2, 1]
qAverage:[0.0, 53.72954813639323]
ws:[4.0773704051971436, 6.812323570251465]
memory len:8776
memory used:3034.0
now epsilon is 0.33332498453072856, the reward is 44.33333333333334 with loss [16.752377718687057, 36.84173655509949] in episode 728
Report: 
rewardSum:44.33333333333334
loss:[16.752377718687057, 36.84173655509949]
policies:[1, 3, 2]
qAverage:[0.0, 47.03820292154948]
ws:[3.352525552113851, 4.97141679128011]
memory len:8788
memory used:3034.0
now epsilon is 0.332908536576105, the reward is 193.33333333333334 with loss [22.222410321235657, 34.30813503265381] in episode 729
Report: 
rewardSum:193.33333333333334
loss:[22.222410321235657, 34.30813503265381]
policies:[0, 3, 2]
qAverage:[0.0, 46.62176767985026]
ws:[3.2672207355499268, 4.336377779642741]
memory len:8798
memory used:3034.0
now epsilon is 0.33240948576897955, the reward is 994.0 with loss [25.31561803817749, 47.9265296459198] in episode 730
Report: 
rewardSum:994.0
loss:[25.31561803817749, 47.9265296459198]
policies:[1, 2, 3]
qAverage:[15.83100700378418, 33.589813232421875]
ws:[-6.761142000555992, -7.332567483186722]
memory len:8810
memory used:3034.0
now epsilon is 0.3319111830703606, the reward is 192.33333333333334 with loss [20.37419891357422, 23.14191722869873] in episode 731
Report: 
rewardSum:192.33333333333334
loss:[20.37419891357422, 23.14191722869873]
policies:[2, 3, 1]
qAverage:[0.0, 42.7110710144043]
ws:[3.39074969291687, 4.180575370788574]
memory len:8822
memory used:3034.0
now epsilon is 0.33149650148415744, the reward is 193.33333333333334 with loss [16.704830765724182, 16.727836966514587] in episode 732
Report: 
rewardSum:193.33333333333334
loss:[16.704830765724182, 16.727836966514587]
policies:[0, 2, 3]
qAverage:[0.0, 31.272363662719727]
ws:[2.28344988822937, 2.547738790512085]
memory len:8832
memory used:3034.0
now epsilon is 0.3311651292731441, the reward is 46.33333333333334 with loss [25.84280776977539, 17.204177111387253] in episode 733
Report: 
rewardSum:46.33333333333334
loss:[25.84280776977539, 17.204177111387253]
policies:[1, 2, 1]
qAverage:[0.0, 52.620862325032554]
ws:[7.92089049021403, 9.607416152954102]
memory len:8840
memory used:3034.0
now epsilon is 0.330834088310098, the reward is 46.33333333333334 with loss [21.822080373764038, 19.855502605438232] in episode 734
Report: 
rewardSum:46.33333333333334
loss:[21.822080373764038, 19.855502605438232]
policies:[1, 2, 1]
qAverage:[15.500937461853027, 38.569332122802734]
ws:[6.060747861862183, 7.062934041023254]
memory len:8848
memory used:3034.0
now epsilon is 0.3304207524193292, the reward is 193.33333333333334 with loss [21.10140323638916, 21.40106427669525] in episode 735
Report: 
rewardSum:193.33333333333334
loss:[21.10140323638916, 21.40106427669525]
policies:[0, 2, 3]
qAverage:[0.0, 51.334869384765625]
ws:[4.7012864748636884, 6.839929421742757]
memory len:8858
memory used:3034.0
now epsilon is 0.32992543095691856, the reward is 192.33333333333334 with loss [35.20277214050293, 32.56768822669983] in episode 736
Report: 
rewardSum:192.33333333333334
loss:[35.20277214050293, 32.56768822669983]
policies:[0, 4, 2]
qAverage:[0.0, 57.22652053833008]
ws:[7.442433834075928, 10.155907154083252]
memory len:8870
memory used:3034.0
now epsilon is 0.32943085201249245, the reward is 192.33333333333334 with loss [52.107075452804565, 54.84906768798828] in episode 737
Report: 
rewardSum:192.33333333333334
loss:[52.107075452804565, 54.84906768798828]
policies:[0, 5, 1]
qAverage:[0.0, 58.537557983398436]
ws:[3.7729188799858093, 5.553772926330566]
memory len:8882
memory used:3034.0
now epsilon is 0.3289370144729697, the reward is 192.33333333333334 with loss [35.11287498474121, 45.88163924217224] in episode 738
Report: 
rewardSum:192.33333333333334
loss:[35.11287498474121, 45.88163924217224]
policies:[1, 3, 2]
qAverage:[0.0, 54.40044911702474]
ws:[2.9973298062880835, 5.3715871175130205]
memory len:8894
memory used:3034.0
now epsilon is 0.3286082007893199, the reward is 194.33333333333334 with loss [29.944218635559082, 33.68893480300903] in episode 739
Report: 
rewardSum:194.33333333333334
loss:[29.944218635559082, 33.68893480300903]
policies:[0, 4, 0]
qAverage:[0.0, 60.550381660461426]
ws:[3.3443974256515503, 5.638673201203346]
memory len:8902
memory used:3034.0
############# STATE ###############
0-		9-		18*		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.3282797157960692, the reward is 194.33333333333334 with loss [12.378676533699036, 16.305435180664062] in episode 740
Report: 
rewardSum:194.33333333333334
loss:[12.378676533699036, 16.305435180664062]
policies:[0, 1, 3]
qAverage:[0.0, 48.18282699584961]
ws:[6.600535869598389, 10.139659881591797]
memory len:8910
memory used:3034.0
now epsilon is 0.3279515591646504, the reward is 194.33333333333334 with loss [7.586260497570038, 18.791023015975952] in episode 741
Report: 
rewardSum:194.33333333333334
loss:[7.586260497570038, 18.791023015975952]
policies:[0, 3, 1]
qAverage:[0.0, 63.65984916687012]
ws:[8.20699429512024, 11.621073961257935]
memory len:8918
memory used:3040.0
now epsilon is 0.3276237305668248, the reward is 46.33333333333334 with loss [16.29789113998413, 9.454126238822937] in episode 742
Report: 
rewardSum:46.33333333333334
loss:[16.29789113998413, 9.454126238822937]
policies:[0, 3, 1]
qAverage:[0.0, 56.995361328125]
ws:[6.955875754356384, 9.103199362754822]
memory len:8926
memory used:3045.0
now epsilon is 0.32729622967468175, the reward is 194.33333333333334 with loss [21.13926410675049, 16.541651010513306] in episode 743
Report: 
rewardSum:194.33333333333334
loss:[21.13926410675049, 16.541651010513306]
policies:[1, 3, 0]
qAverage:[0.0, 62.044819831848145]
ws:[4.188508063554764, 7.1503273248672485]
memory len:8934
memory used:3045.0
now epsilon is 0.3268873138965983, the reward is 193.33333333333334 with loss [28.969866275787354, 20.06652009487152] in episode 744
Report: 
rewardSum:193.33333333333334
loss:[28.969866275787354, 20.06652009487152]
policies:[1, 3, 1]
qAverage:[0.0, 55.71468544006348]
ws:[5.051448330283165, 7.795141518115997]
memory len:8944
memory used:3058.0
now epsilon is 0.32656054914501526, the reward is 194.33333333333334 with loss [17.421855926513672, 12.955618977546692] in episode 745
Report: 
rewardSum:194.33333333333334
loss:[17.421855926513672, 12.955618977546692]
policies:[0, 4, 0]
qAverage:[0.0, 64.89012298583984]
ws:[3.578994607925415, 6.06663179397583]
memory len:8952
memory used:3058.0
now epsilon is 0.3260710143697816, the reward is 192.33333333333334 with loss [24.390197157859802, 13.053117036819458] in episode 746
Report: 
rewardSum:192.33333333333334
loss:[24.390197157859802, 13.053117036819458]
policies:[0, 3, 3]
qAverage:[0.0, 59.23171424865723]
ws:[6.604781866073608, 9.649535059928894]
memory len:8964
memory used:3070.0
now epsilon is 0.3257450656116641, the reward is 194.33333333333334 with loss [23.967732071876526, 31.038498163223267] in episode 747
Report: 
rewardSum:194.33333333333334
loss:[23.967732071876526, 31.038498163223267]
policies:[2, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:8972
memory used:3077.0
now epsilon is 0.3254194426800943, the reward is 194.33333333333334 with loss [16.065549731254578, 18.723624110221863] in episode 748
Report: 
rewardSum:194.33333333333334
loss:[16.065549731254578, 18.723624110221863]
policies:[0, 2, 2]
qAverage:[0.0, 54.95011901855469]
ws:[3.0198431412378945, 5.3193332354227705]
memory len:8980
memory used:3077.0
now epsilon is 0.3250941452493678, the reward is 194.33333333333334 with loss [16.81485629081726, 17.080304622650146] in episode 749
Report: 
rewardSum:194.33333333333334
loss:[16.81485629081726, 17.080304622650146]
policies:[0, 3, 1]
qAverage:[0.0, 61.43167495727539]
ws:[3.8476160764694214, 6.52593207359314]
memory len:8988
memory used:3078.0
now epsilon is 0.32460680870568215, the reward is 192.33333333333334 with loss [26.289959192276, 18.740300059318542] in episode 750
Report: 
rewardSum:192.33333333333334
loss:[26.289959192276, 18.740300059318542]
policies:[0, 5, 1]
qAverage:[0.0, 56.0640754699707]
ws:[5.202993988990784, 7.413666665554047]
memory len:9000
memory used:3078.0
now epsilon is 0.324201253023342, the reward is -4.0 with loss [23.626338720321655, 32.5929536819458] in episode 751
Report: 
rewardSum:-4.0
loss:[23.626338720321655, 32.5929536819458]
policies:[1, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:9010
memory used:3052.0
now epsilon is 0.3237152549811879, the reward is 192.33333333333334 with loss [31.94599425792694, 19.51816701889038] in episode 752
Report: 
rewardSum:192.33333333333334
loss:[31.94599425792694, 19.51816701889038]
policies:[0, 5, 1]
qAverage:[0.0, 61.260885620117186]
ws:[3.4301168441772463, 5.019762325286865]
memory len:9022
memory used:3065.0
now epsilon is 0.3233916610991965, the reward is 194.33333333333334 with loss [15.563277244567871, 24.705448500812054] in episode 753
Report: 
rewardSum:194.33333333333334
loss:[15.563277244567871, 24.705448500812054]
policies:[1, 2, 1]
qAverage:[0.0, 58.76500956217448]
ws:[4.782578786214192, 7.076738357543945]
memory len:9030
memory used:3064.0
now epsilon is 0.3230683906897595, the reward is 194.33333333333334 with loss [26.061810731887817, 20.406245708465576] in episode 754
Report: 
rewardSum:194.33333333333334
loss:[26.061810731887817, 20.406245708465576]
policies:[1, 3, 0]
qAverage:[0.0, 61.26452445983887]
ws:[4.104568809270859, 5.867003440856934]
memory len:9038
memory used:3064.0
now epsilon is 0.32218106233046134, the reward is 187.33333333333334 with loss [42.71561378240585, 85.54696476459503] in episode 755
Report: 
rewardSum:187.33333333333334
loss:[42.71561378240585, 85.54696476459503]
policies:[0, 4, 7]
qAverage:[0.0, 63.859619140625]
ws:[5.2152974009513855, 7.057182133197784]
memory len:9060
memory used:3071.0
now epsilon is 0.32185900206589424, the reward is 194.33333333333334 with loss [25.6902756690979, 23.786712646484375] in episode 756
Report: 
rewardSum:194.33333333333334
loss:[25.6902756690979, 23.786712646484375]
policies:[0, 4, 0]
qAverage:[0.0, 65.78759765625]
ws:[6.883071708679199, 9.400589179992675]
memory len:9068
memory used:3071.0
now epsilon is 0.32145687942490403, the reward is 193.33333333333334 with loss [23.624515056610107, 41.921770095825195] in episode 757
Report: 
rewardSum:193.33333333333334
loss:[23.624515056610107, 41.921770095825195]
policies:[1, 3, 1]
qAverage:[0.0, 61.087554931640625]
ws:[3.562282383441925, 5.550517678260803]
memory len:9078
memory used:3045.0
now epsilon is 0.32105525918595124, the reward is 193.33333333333334 with loss [14.590052366256714, 15.363807320594788] in episode 758
Report: 
rewardSum:193.33333333333334
loss:[14.590052366256714, 15.363807320594788]
policies:[0, 4, 1]
qAverage:[0.0, 64.64225311279297]
ws:[5.416517904400825, 8.06153085231781]
memory len:9088
memory used:3045.0
now epsilon is 0.3207343243024228, the reward is 46.33333333333334 with loss [17.913719654083252, 43.74532639980316] in episode 759
Report: 
rewardSum:46.33333333333334
loss:[17.913719654083252, 43.74532639980316]
policies:[0, 3, 1]
qAverage:[0.0, 58.56180763244629]
ws:[3.6989936232566833, 5.622956573963165]
memory len:9096
memory used:3051.0
now epsilon is 0.32041371023344745, the reward is 194.33333333333334 with loss [21.646075010299683, 16.239149808883667] in episode 760
Report: 
rewardSum:194.33333333333334
loss:[21.646075010299683, 16.239149808883667]
policies:[0, 3, 1]
qAverage:[0.0, 56.6213493347168]
ws:[5.84685206413269, 7.580750823020935]
memory len:9104
memory used:3064.0
now epsilon is 0.31993338995584014, the reward is 192.33333333333334 with loss [42.55070650577545, 22.32235300540924] in episode 761
Report: 
rewardSum:192.33333333333334
loss:[42.55070650577545, 22.32235300540924]
policies:[0, 3, 3]
qAverage:[0.0, 58.295841217041016]
ws:[5.868352055549622, 7.348312258720398]
memory len:9116
memory used:3065.0
now epsilon is 0.31977344325669915, the reward is -1.0 with loss [7.81195855140686, 12.22713851928711] in episode 762
Report: 
rewardSum:-1.0
loss:[7.81195855140686, 12.22713851928711]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:9120
memory used:3065.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21*		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.3191344556939972, the reward is 190.33333333333334 with loss [44.853155851364136, 42.851439237594604] in episode 763
Report: 
rewardSum:190.33333333333334
loss:[44.853155851364136, 42.851439237594604]
policies:[1, 4, 3]
qAverage:[0.0, 59.932498931884766]
ws:[3.6654901802539825, 5.063014209270477]
memory len:9136
memory used:3065.0
now epsilon is 0.3188154408937795, the reward is 194.33333333333334 with loss [15.478872537612915, 12.094930648803711] in episode 764
Report: 
rewardSum:194.33333333333334
loss:[15.478872537612915, 12.094930648803711]
policies:[1, 2, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:9144
memory used:3065.0
now epsilon is 0.31849674498875136, the reward is 46.33333333333334 with loss [26.84817147254944, 30.11241888999939] in episode 765
Report: 
rewardSum:46.33333333333334
loss:[26.84817147254944, 30.11241888999939]
policies:[0, 3, 1]
qAverage:[0.0, 57.34025764465332]
ws:[3.9183866400271654, 5.5997234135866165]
memory len:9152
memory used:3065.0
now epsilon is 0.3181783676601372, the reward is -3.0 with loss [25.68257164955139, 38.08014249801636] in episode 766
Report: 
rewardSum:-3.0
loss:[25.68257164955139, 38.08014249801636]
policies:[0, 1, 3]
qAverage:[0.0, 38.689605712890625]
ws:[1.5837421417236328, 2.5243420600891113]
memory len:9160
memory used:3065.0
now epsilon is 0.3177808435123327, the reward is 193.33333333333334 with loss [42.88031816482544, 21.764889657497406] in episode 767
Report: 
rewardSum:193.33333333333334
loss:[42.88031816482544, 21.764889657497406]
policies:[1, 3, 1]
qAverage:[0.0, 62.43173027038574]
ws:[4.557755768299103, 6.431848585605621]
memory len:9170
memory used:3066.0
now epsilon is 0.3174631818167767, the reward is 194.33333333333334 with loss [17.002923727035522, 22.36872887611389] in episode 768
Report: 
rewardSum:194.33333333333334
loss:[17.002923727035522, 22.36872887611389]
policies:[1, 3, 0]
qAverage:[0.0, 65.12358474731445]
ws:[5.613704085350037, 7.691774606704712]
memory len:9178
memory used:3066.0
now epsilon is 0.3171458376638129, the reward is 194.33333333333334 with loss [24.197393655776978, 14.917778015136719] in episode 769
Report: 
rewardSum:194.33333333333334
loss:[24.197393655776978, 14.917778015136719]
policies:[0, 4, 0]
qAverage:[0.0, 65.28205108642578]
ws:[4.642913866043091, 6.571432781219483]
memory len:9186
memory used:3066.0
now epsilon is 0.3166704161324505, the reward is 192.33333333333334 with loss [29.344367504119873, 33.62893605232239] in episode 770
Report: 
rewardSum:192.33333333333334
loss:[29.344367504119873, 33.62893605232239]
policies:[1, 3, 2]
qAverage:[0.0, 51.13408660888672]
ws:[2.5362583796183267, 3.2121939659118652]
memory len:9198
memory used:3066.0
now epsilon is 0.3163538644479335, the reward is 194.33333333333334 with loss [9.923742651939392, 12.066076755523682] in episode 771
Report: 
rewardSum:194.33333333333334
loss:[9.923742651939392, 12.066076755523682]
policies:[0, 3, 1]
qAverage:[0.0, 62.92489814758301]
ws:[5.253187894821167, 6.805835843086243]
memory len:9206
memory used:3078.0
now epsilon is 0.31587963013416753, the reward is 192.33333333333334 with loss [24.23032581806183, 28.358636498451233] in episode 772
Report: 
rewardSum:192.33333333333334
loss:[24.23032581806183, 28.358636498451233]
policies:[1, 3, 2]
qAverage:[0.0, 55.359265645345054]
ws:[4.970194657643636, 6.678437868754069]
memory len:9218
memory used:3078.0
now epsilon is 0.31556386893915345, the reward is 194.33333333333334 with loss [15.529956102371216, 13.62073540687561] in episode 773
Report: 
rewardSum:194.33333333333334
loss:[15.529956102371216, 13.62073540687561]
policies:[0, 3, 1]
qAverage:[0.0, 52.352508544921875]
ws:[3.10406764348348, 3.889183521270752]
memory len:9226
memory used:3058.0
now epsilon is 0.31524842338694364, the reward is 46.33333333333334 with loss [18.506094336509705, 23.2330002784729] in episode 774
Report: 
rewardSum:46.33333333333334
loss:[18.506094336509705, 23.2330002784729]
policies:[0, 3, 1]
qAverage:[0.0, 57.73013687133789]
ws:[3.140970528125763, 4.102678120136261]
memory len:9234
memory used:3070.0
now epsilon is 0.31493329316201374, the reward is 46.33333333333334 with loss [19.818472146987915, 19.75282382965088] in episode 775
Report: 
rewardSum:46.33333333333334
loss:[19.818472146987915, 19.75282382965088]
policies:[0, 3, 1]
qAverage:[0.0, 58.05327033996582]
ws:[2.89353084564209, 4.12470555305481]
memory len:9242
memory used:3076.0
now epsilon is 0.3146184779491546, the reward is 194.33333333333334 with loss [20.34804129600525, 12.811729192733765] in episode 776
Report: 
rewardSum:194.33333333333334
loss:[20.34804129600525, 12.811729192733765]
policies:[0, 4, 0]
qAverage:[0.0, 66.54403228759766]
ws:[5.350839853286743, 7.174522066116333]
memory len:9250
memory used:3078.0
now epsilon is 0.3143039774334722, the reward is 194.33333333333334 with loss [37.27096152305603, 35.335670948028564] in episode 777
Report: 
rewardSum:194.33333333333334
loss:[37.27096152305603, 35.335670948028564]
policies:[0, 3, 1]
qAverage:[0.0, 57.18214225769043]
ws:[3.3112645149230957, 4.536509692668915]
memory len:9258
memory used:3045.0
now epsilon is 0.31398979130038757, the reward is 46.33333333333334 with loss [16.877036333084106, 14.836702823638916] in episode 778
Report: 
rewardSum:46.33333333333334
loss:[16.877036333084106, 14.836702823638916]
policies:[1, 1, 2]
qAverage:[0.0, 35.15621566772461]
ws:[0.5811800360679626, 0.9392166137695312]
memory len:9266
memory used:3057.0
now epsilon is 0.31351910088076296, the reward is 192.33333333333334 with loss [36.89115047454834, 34.01924228668213] in episode 779
Report: 
rewardSum:192.33333333333334
loss:[36.89115047454834, 34.01924228668213]
policies:[0, 4, 2]
qAverage:[0.0, 60.27670478820801]
ws:[4.064499914646149, 6.534896016120911]
memory len:9278
memory used:3057.0
now epsilon is 0.3130491160556425, the reward is 192.33333333333334 with loss [38.12197160720825, 30.919520378112793] in episode 780
Report: 
rewardSum:192.33333333333334
loss:[38.12197160720825, 30.919520378112793]
policies:[0, 5, 1]
qAverage:[0.0, 58.4671573638916]
ws:[8.508800029754639, 12.183196067810059]
memory len:9290
memory used:3069.0
now epsilon is 0.3125798357672959, the reward is 192.33333333333334 with loss [27.81010127067566, 25.496506452560425] in episode 781
Report: 
rewardSum:192.33333333333334
loss:[27.81010127067566, 25.496506452560425]
policies:[0, 5, 1]
qAverage:[0.0, 65.74518432617188]
ws:[3.89166579246521, 6.64533576965332]
memory len:9302
memory used:3070.0
now epsilon is 0.3122673731294321, the reward is 194.33333333333334 with loss [17.83878993988037, 11.13566780090332] in episode 782
Report: 
rewardSum:194.33333333333334
loss:[17.83878993988037, 11.13566780090332]
policies:[0, 4, 0]
qAverage:[0.0, 64.53176498413086]
ws:[4.617981851100922, 7.313660502433777]
memory len:9310
memory used:3070.0
now epsilon is 0.3119552228370521, the reward is -3.0 with loss [21.59196162223816, 15.063764333724976] in episode 783
Report: 
rewardSum:-3.0
loss:[21.59196162223816, 15.063764333724976]
policies:[0, 2, 2]
qAverage:[0.0, 34.71112823486328]
ws:[-0.4646542966365814, 0.6504812836647034]
memory len:9318
memory used:3070.0
now epsilon is 0.31140971046775945, the reward is 191.33333333333334 with loss [33.562200635671616, 24.63411593437195] in episode 784
Report: 
rewardSum:191.33333333333334
loss:[33.562200635671616, 24.63411593437195]
policies:[1, 3, 3]
qAverage:[0.0, 53.36766815185547]
ws:[1.94041109085083, 4.19858185450236]
memory len:9332
memory used:3071.0
now epsilon is 0.31109841751647127, the reward is 194.33333333333334 with loss [15.22374701499939, 26.37266755104065] in episode 785
Report: 
rewardSum:194.33333333333334
loss:[15.22374701499939, 26.37266755104065]
policies:[0, 4, 0]
qAverage:[0.0, 64.191015625]
ws:[5.932424998283386, 9.199670219421387]
memory len:9340
memory used:3071.0
now epsilon is 0.310787435741419, the reward is 194.33333333333334 with loss [25.23616349697113, 26.413859605789185] in episode 786
Report: 
rewardSum:194.33333333333334
loss:[25.23616349697113, 26.413859605789185]
policies:[1, 3, 0]
qAverage:[0.0, 34.34623718261719]
ws:[0.8171867728233337, 1.9195141792297363]
memory len:9348
memory used:3071.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11*		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.3104767648315431, the reward is 194.33333333333334 with loss [33.74367332458496, 12.999003410339355] in episode 787
Report: 
rewardSum:194.33333333333334
loss:[33.74367332458496, 12.999003410339355]
policies:[0, 3, 1]
qAverage:[0.0, 51.06165313720703]
ws:[1.2878813942273457, 2.9451984564463296]
memory len:9356
memory used:3071.0
now epsilon is 0.31001134065925706, the reward is 192.33333333333334 with loss [21.57409906387329, 30.11026656627655] in episode 788
Report: 
rewardSum:192.33333333333334
loss:[21.57409906387329, 30.11026656627655]
policies:[2, 3, 1]
qAverage:[0.0, 49.79684702555338]
ws:[3.256215492884318, 5.483570575714111]
memory len:9368
memory used:3071.0
now epsilon is 0.3093918602266096, the reward is 190.33333333333334 with loss [35.950883984565735, 37.49242544174194] in episode 789
Report: 
rewardSum:190.33333333333334
loss:[35.950883984565735, 37.49242544174194]
policies:[1, 4, 3]
qAverage:[0.0, 61.30624847412109]
ws:[4.303538753837347, 7.240038204193115]
memory len:9384
memory used:3071.0
now epsilon is 0.30877361767127853, the reward is 190.33333333333334 with loss [36.44312560558319, 60.39186775684357] in episode 790
Report: 
rewardSum:190.33333333333334
loss:[36.44312560558319, 60.39186775684357]
policies:[1, 4, 3]
qAverage:[0.0, 62.20036506652832]
ws:[4.872536659240723, 7.481314182281494]
memory len:9400
memory used:3071.0
now epsilon is 0.3084649598244168, the reward is 194.33333333333334 with loss [17.83251929283142, 14.01577091217041] in episode 791
Report: 
rewardSum:194.33333333333334
loss:[17.83251929283142, 14.01577091217041]
policies:[1, 3, 0]
qAverage:[0.0, 52.127105712890625]
ws:[1.7274621725082397, 3.4514803091684976]
memory len:9408
memory used:3071.0
now epsilon is 0.3081566105196745, the reward is 194.33333333333334 with loss [16.810272693634033, 15.182191133499146] in episode 792
Report: 
rewardSum:194.33333333333334
loss:[16.810272693634033, 15.182191133499146]
policies:[0, 4, 0]
qAverage:[0.0, 64.43098602294921]
ws:[4.83808069229126, 8.29328203201294]
memory len:9416
memory used:3071.0
now epsilon is 0.30784856944862526, the reward is 194.33333333333334 with loss [17.54016375541687, 14.96274721622467] in episode 793
Report: 
rewardSum:194.33333333333334
loss:[17.54016375541687, 14.96274721622467]
policies:[2, 2, 0]
qAverage:[0.0, 34.26592254638672]
ws:[1.3817358016967773, 2.8900527954101562]
memory len:9424
memory used:3071.0
now epsilon is 0.30746395109407504, the reward is 45.33333333333334 with loss [22.238657474517822, 26.94934779405594] in episode 794
Report: 
rewardSum:45.33333333333334
loss:[22.238657474517822, 26.94934779405594]
policies:[0, 3, 2]
qAverage:[0.0, 57.116119384765625]
ws:[4.1130757331848145, 6.282759428024292]
memory len:9434
memory used:3071.0
now epsilon is 0.30715660242274734, the reward is 46.33333333333334 with loss [16.821691036224365, 12.825980693101883] in episode 795
Report: 
rewardSum:46.33333333333334
loss:[16.821691036224365, 12.825980693101883]
policies:[0, 3, 1]
qAverage:[0.0, 53.7387949625651]
ws:[5.180129051208496, 7.187477270762126]
memory len:9442
memory used:3039.0
now epsilon is 0.30669615538245965, the reward is 192.33333333333334 with loss [42.104899406433105, 22.27912449836731] in episode 796
Report: 
rewardSum:192.33333333333334
loss:[42.104899406433105, 22.27912449836731]
policies:[0, 4, 2]
qAverage:[0.0, 62.236795043945314]
ws:[1.7244593381881714, 3.294038105010986]
memory len:9454
memory used:3053.0
now epsilon is 0.30615983948156184, the reward is 191.33333333333334 with loss [41.91563034057617, 29.84187212586403] in episode 797
Report: 
rewardSum:191.33333333333334
loss:[41.91563034057617, 29.84187212586403]
policies:[1, 2, 4]
qAverage:[0.0, 39.44801712036133]
ws:[4.997162342071533, 7.08629846572876]
memory len:9468
memory used:3066.0
now epsilon is 0.30577733098427806, the reward is 193.33333333333334 with loss [25.978212356567383, 50.42710590362549] in episode 798
Report: 
rewardSum:193.33333333333334
loss:[25.978212356567383, 50.42710590362549]
policies:[0, 3, 2]
qAverage:[0.0, 58.93993377685547]
ws:[5.260707229375839, 8.538410037755966]
memory len:9478
memory used:3066.0
now epsilon is 0.305471668300683, the reward is 194.33333333333334 with loss [10.275127172470093, 28.50907278060913] in episode 799
Report: 
rewardSum:194.33333333333334
loss:[10.275127172470093, 28.50907278060913]
policies:[0, 2, 2]
qAverage:[0.0, 39.22160339355469]
ws:[1.1112089157104492, 2.696622610092163]
memory len:9486
memory used:3066.0
now epsilon is 0.3051663111651672, the reward is 194.33333333333334 with loss [32.16589283943176, 20.009884357452393] in episode 800
Report: 
rewardSum:194.33333333333334
loss:[32.16589283943176, 20.009884357452393]
policies:[0, 3, 1]
qAverage:[0.0, 64.96561050415039]
ws:[6.888812065124512, 10.803869605064392]
memory len:9494
memory used:3066.0
now epsilon is 0.30470884769648965, the reward is 192.33333333333334 with loss [19.27169382572174, 27.90821886062622] in episode 801
Report: 
rewardSum:192.33333333333334
loss:[19.27169382572174, 27.90821886062622]
policies:[1, 3, 2]
qAverage:[0.0, 57.019662857055664]
ws:[4.332640051841736, 7.568996429443359]
memory len:9506
memory used:3067.0
now epsilon is 0.30440425309556796, the reward is 194.33333333333334 with loss [26.121615648269653, 17.000426054000854] in episode 802
Report: 
rewardSum:194.33333333333334
loss:[26.121615648269653, 17.000426054000854]
policies:[2, 2, 0]
qAverage:[0.0, 60.20900217692057]
ws:[5.482812245686849, 9.180132548014322]
memory len:9514
memory used:3022.0
now epsilon is 0.30394793199980336, the reward is 192.33333333333334 with loss [31.188007712364197, 27.27929264307022] in episode 803
Report: 
rewardSum:192.33333333333334
loss:[31.188007712364197, 27.27929264307022]
policies:[1, 4, 1]
qAverage:[0.0, 65.18026428222656]
ws:[4.106471908092499, 7.947096681594848]
memory len:9526
memory used:3046.0
now epsilon is 0.3035681870047752, the reward is 193.33333333333334 with loss [12.832590408623219, 15.011548399925232] in episode 804
Report: 
rewardSum:193.33333333333334
loss:[12.832590408623219, 15.011548399925232]
policies:[0, 4, 1]
qAverage:[0.0, 65.00597839355468]
ws:[1.9054318904876708, 5.360564559698105]
memory len:9536
memory used:3052.0
now epsilon is 0.30326473263686876, the reward is 194.33333333333334 with loss [41.56803894042969, 26.886964976787567] in episode 805
Report: 
rewardSum:194.33333333333334
loss:[41.56803894042969, 26.886964976787567]
policies:[1, 3, 0]
qAverage:[0.0, 59.929395039876304]
ws:[4.003771543502808, 7.303922653198242]
memory len:9544
memory used:3051.0
now epsilon is 0.3029615816095538, the reward is 194.33333333333334 with loss [19.91645312309265, 18.726709365844727] in episode 806
Report: 
rewardSum:194.33333333333334
loss:[19.91645312309265, 18.726709365844727]
policies:[1, 3, 0]
qAverage:[0.0, 57.52073860168457]
ws:[1.5494968742132187, 3.819109559059143]
memory len:9552
memory used:3051.0
now epsilon is 0.30265873361960344, the reward is 194.33333333333334 with loss [10.614212036132812, 29.639765739440918] in episode 807
Report: 
rewardSum:194.33333333333334
loss:[10.614212036132812, 29.639765739440918]
policies:[1, 3, 0]
qAverage:[0.0, 62.34694862365723]
ws:[4.553433805704117, 7.8315190076828]
memory len:9560
memory used:3058.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22*		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.30197843205401936, the reward is 189.33333333333334 with loss [43.14222586154938, 50.05711317062378] in episode 808
Report: 
rewardSum:189.33333333333334
loss:[43.14222586154938, 50.05711317062378]
policies:[2, 3, 4]
qAverage:[0.0, 37.150386810302734]
ws:[5.425677299499512, 7.832357406616211]
memory len:9578
memory used:3058.0
now epsilon is 0.3016765668450049, the reward is 194.33333333333334 with loss [8.887277841567993, 16.415557980537415] in episode 809
Report: 
rewardSum:194.33333333333334
loss:[8.887277841567993, 16.415557980537415]
policies:[0, 3, 1]
qAverage:[0.0, 64.07019996643066]
ws:[5.345109343528748, 9.618245124816895]
memory len:9586
memory used:3058.0
now epsilon is 0.30137500338801887, the reward is 194.33333333333334 with loss [22.813467025756836, 17.207420349121094] in episode 810
Report: 
rewardSum:194.33333333333334
loss:[22.813467025756836, 17.207420349121094]
policies:[1, 2, 1]
qAverage:[0.0, 48.9574228922526]
ws:[0.6646949450174967, 3.120407978693644]
memory len:9594
memory used:3058.0
now epsilon is 0.30107374138142234, the reward is 194.33333333333334 with loss [16.361649990081787, 29.200976848602295] in episode 811
Report: 
rewardSum:194.33333333333334
loss:[16.361649990081787, 29.200976848602295]
policies:[0, 3, 1]
qAverage:[0.0, 55.53373718261719]
ws:[3.5294212102890015, 7.002144813537598]
memory len:9602
memory used:3058.0
now epsilon is 0.30077278052387807, the reward is 46.33333333333334 with loss [25.48555874824524, 19.783591270446777] in episode 812
Report: 
rewardSum:46.33333333333334
loss:[25.48555874824524, 19.783591270446777]
policies:[0, 3, 1]
qAverage:[0.0, 52.18802134195963]
ws:[1.6572217146555583, 3.818063656489054]
memory len:9610
memory used:3058.0
now epsilon is 0.3004721205143498, the reward is 194.33333333333334 with loss [15.029793381690979, 25.4214745759964] in episode 813
Report: 
rewardSum:194.33333333333334
loss:[15.029793381690979, 25.4214745759964]
policies:[1, 3, 0]
qAverage:[0.0, 57.7391357421875]
ws:[4.320246607065201, 7.230184018611908]
memory len:9618
memory used:3058.0
now epsilon is 0.30017176105210236, the reward is 194.33333333333334 with loss [31.872740030288696, 14.117803573608398] in episode 814
Report: 
rewardSum:194.33333333333334
loss:[31.872740030288696, 14.117803573608398]
policies:[1, 2, 1]
qAverage:[0.0, 51.89392598470052]
ws:[1.1977305014928181, 3.021280209223429]
memory len:9626
memory used:3058.0
now epsilon is 0.2998717018367012, the reward is 194.33333333333334 with loss [12.751820087432861, 13.851618230342865] in episode 815
Report: 
rewardSum:194.33333333333334
loss:[12.751820087432861, 13.851618230342865]
policies:[0, 3, 1]
qAverage:[0.0, 66.93110847473145]
ws:[4.358950138092041, 8.27324652671814]
memory len:9634
memory used:3064.0
now epsilon is 0.2994221753199743, the reward is 192.33333333333334 with loss [25.061115503311157, 33.7947074174881] in episode 816
Report: 
rewardSum:192.33333333333334
loss:[25.061115503311157, 33.7947074174881]
policies:[1, 3, 2]
qAverage:[0.0, 48.099194844563804]
ws:[3.1405216654141745, 5.794946511586507]
memory len:9646
memory used:3066.0
now epsilon is 0.29912286540925737, the reward is 194.33333333333334 with loss [34.07742643356323, 29.479045391082764] in episode 817
Report: 
rewardSum:194.33333333333334
loss:[34.07742643356323, 29.479045391082764]
policies:[2, 2, 0]
qAverage:[0.0, 56.244433085123696]
ws:[4.2540310223897295, 7.52350378036499]
memory len:9654
memory used:3067.0
now epsilon is 0.29882385469622863, the reward is 194.33333333333334 with loss [26.135774850845337, 18.634128093719482] in episode 818
Report: 
rewardSum:194.33333333333334
loss:[26.135774850845337, 18.634128093719482]
policies:[0, 4, 0]
qAverage:[0.0, 58.170576095581055]
ws:[5.338968455791473, 8.590136766433716]
memory len:9662
memory used:3067.0
now epsilon is 0.29852514288180265, the reward is 194.33333333333334 with loss [15.687062740325928, 23.34808111190796] in episode 819
Report: 
rewardSum:194.33333333333334
loss:[15.687062740325928, 23.34808111190796]
policies:[0, 4, 0]
qAverage:[0.0, 66.29681243896485]
ws:[4.335432147979736, 7.897507190704346]
memory len:9670
memory used:3067.0
now epsilon is 0.2982267296671928, the reward is 194.33333333333334 with loss [35.638495445251465, 15.31787657737732] in episode 820
Report: 
rewardSum:194.33333333333334
loss:[35.638495445251465, 15.31787657737732]
policies:[1, 3, 0]
qAverage:[0.0, 48.75885518391927]
ws:[1.4765461285909016, 3.5082691510518393]
memory len:9678
memory used:3066.0
now epsilon is 0.2977796690670727, the reward is 192.33333333333334 with loss [17.194477796554565, 28.207372903823853] in episode 821
Report: 
rewardSum:192.33333333333334
loss:[17.194477796554565, 28.207372903823853]
policies:[1, 4, 1]
qAverage:[0.0, 67.9763687133789]
ws:[4.118286514282227, 8.083237552642823]
memory len:9690
memory used:3066.0
now epsilon is 0.2973332786388732, the reward is 192.33333333333334 with loss [43.0547354221344, 36.78692716360092] in episode 822
Report: 
rewardSum:192.33333333333334
loss:[43.0547354221344, 36.78692716360092]
policies:[0, 5, 1]
qAverage:[0.0, 53.73695945739746]
ws:[0.20228485530242324, 2.289263606071472]
memory len:9702
memory used:3066.0
now epsilon is 0.2970360568416317, the reward is 194.33333333333334 with loss [20.55375862121582, 27.657207369804382] in episode 823
Report: 
rewardSum:194.33333333333334
loss:[20.55375862121582, 27.657207369804382]
policies:[0, 4, 0]
qAverage:[0.0, 59.60050964355469]
ws:[2.9492940306663513, 6.260523557662964]
memory len:9710
memory used:3066.0
now epsilon is 0.29673913215474784, the reward is 194.33333333333334 with loss [11.101941347122192, 23.40481734275818] in episode 824
Report: 
rewardSum:194.33333333333334
loss:[11.101941347122192, 23.40481734275818]
policies:[0, 4, 0]
qAverage:[0.0, 59.449066162109375]
ws:[2.7647995948791504, 5.990045517683029]
memory len:9718
memory used:3067.0
now epsilon is 0.29644250428122265, the reward is 194.33333333333334 with loss [17.87690818309784, 13.833122253417969] in episode 825
Report: 
rewardSum:194.33333333333334
loss:[17.87690818309784, 13.833122253417969]
policies:[1, 2, 1]
qAverage:[0.0, 53.79515838623047]
ws:[0.7518862883249918, 3.299512346585592]
memory len:9726
memory used:3067.0
now epsilon is 0.296146172924354, the reward is 194.33333333333334 with loss [31.190755605697632, 14.073126792907715] in episode 826
Report: 
rewardSum:194.33333333333334
loss:[31.190755605697632, 14.073126792907715]
policies:[0, 4, 0]
qAverage:[0.0, 68.54128646850586]
ws:[5.42574155330658, 9.666020154953003]
memory len:9734
memory used:3067.0
now epsilon is 0.2957022312094763, the reward is 192.33333333333334 with loss [59.93758702278137, 59.5417582988739] in episode 827
Report: 
rewardSum:192.33333333333334
loss:[59.93758702278137, 59.5417582988739]
policies:[0, 5, 1]
qAverage:[0.0, 64.24241180419922]
ws:[5.510555458068848, 10.004188537597656]
memory len:9746
memory used:3067.0
now epsilon is 0.29511134396730343, the reward is 190.33333333333334 with loss [48.51467728614807, 31.591229915618896] in episode 828
Report: 
rewardSum:190.33333333333334
loss:[48.51467728614807, 31.591229915618896]
policies:[1, 5, 2]
qAverage:[0.0, 57.26446278889974]
ws:[6.097633361816406, 10.687940915425619]
memory len:9762
memory used:3073.0
now epsilon is 0.29481634327164685, the reward is 194.33333333333334 with loss [21.413382530212402, 19.214576244354248] in episode 829
Report: 
rewardSum:194.33333333333334
loss:[21.413382530212402, 19.214576244354248]
policies:[0, 4, 0]
qAverage:[0.0, 65.76006317138672]
ws:[5.84131446480751, 10.547557353973389]
memory len:9770
memory used:3078.0
now epsilon is 0.2945216374660791, the reward is 46.33333333333334 with loss [23.45088243484497, 15.673304319381714] in episode 830
Report: 
rewardSum:46.33333333333334
loss:[23.45088243484497, 15.673304319381714]
policies:[2, 1, 1]
qAverage:[0.0, 35.04193115234375]
ws:[0.3600480556488037, 2.1895759105682373]
memory len:9778
memory used:3078.0
now epsilon is 0.2940801310318944, the reward is 192.33333333333334 with loss [26.98106861114502, 37.17966938018799] in episode 831
Report: 
rewardSum:192.33333333333334
loss:[26.98106861114502, 37.17966938018799]
policies:[0, 5, 1]
qAverage:[0.0, 68.85728759765625]
ws:[5.413837790489197, 9.535813522338866]
memory len:9790
memory used:3078.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22*		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.2937861611625328, the reward is 194.33333333333334 with loss [30.523223638534546, 25.469674348831177] in episode 832
Report: 
rewardSum:194.33333333333334
loss:[30.523223638534546, 25.469674348831177]
policies:[0, 2, 2]
qAverage:[0.0, 51.6016845703125]
ws:[7.144111156463623, 11.56316089630127]
memory len:9798
memory used:3078.0
now epsilon is 0.2933457572535242, the reward is 192.33333333333334 with loss [24.524977684020996, 34.87449049949646] in episode 833
Report: 
rewardSum:192.33333333333334
loss:[24.524977684020996, 34.87449049949646]
policies:[0, 4, 2]
qAverage:[0.0, 64.03083801269531]
ws:[2.7052740812301637, 6.104847812652588]
memory len:9810
memory used:3048.0
now epsilon is 0.29290601353763795, the reward is 192.33333333333334 with loss [26.24865484237671, 24.234973430633545] in episode 834
Report: 
rewardSum:192.33333333333334
loss:[26.24865484237671, 24.234973430633545]
policies:[0, 4, 2]
qAverage:[0.0, 66.15408325195312]
ws:[5.43851884206136, 9.577669143676758]
memory len:9822
memory used:3065.0
now epsilon is 0.2926132173455499, the reward is 194.33333333333334 with loss [24.440570831298828, 22.000255584716797] in episode 835
Report: 
rewardSum:194.33333333333334
loss:[24.440570831298828, 22.000255584716797]
policies:[0, 3, 1]
qAverage:[0.0, 69.20748138427734]
ws:[6.597349643707275, 11.323561191558838]
memory len:9830
memory used:3065.0
now epsilon is 0.2923207138398737, the reward is 194.33333333333334 with loss [37.635218381881714, 30.157689452171326] in episode 836
Report: 
rewardSum:194.33333333333334
loss:[37.635218381881714, 30.157689452171326]
policies:[0, 4, 0]
qAverage:[0.0, 46.01598358154297]
ws:[1.5045061111450195, 3.6103334426879883]
memory len:9838
memory used:3067.0
now epsilon is 0.29188250672845006, the reward is 192.33333333333334 with loss [29.070537328720093, 25.84504598379135] in episode 837
Report: 
rewardSum:192.33333333333334
loss:[29.070537328720093, 25.84504598379135]
policies:[0, 3, 3]
qAverage:[0.0, 62.00032043457031]
ws:[1.2076390385627747, 4.428550571203232]
memory len:9850
memory used:3067.0
now epsilon is 0.29151783597600534, the reward is 193.33333333333334 with loss [27.116498112678528, 21.828616619110107] in episode 838
Report: 
rewardSum:193.33333333333334
loss:[27.116498112678528, 21.828616619110107]
policies:[1, 3, 1]
qAverage:[0.0, 59.37866973876953]
ws:[3.171978851159414, 6.870561798413594]
memory len:9860
memory used:3067.0
now epsilon is 0.29122642744099914, the reward is 194.33333333333334 with loss [15.419745206832886, 22.17169952392578] in episode 839
Report: 
rewardSum:194.33333333333334
loss:[15.419745206832886, 22.17169952392578]
policies:[1, 3, 0]
qAverage:[0.0, 61.99080467224121]
ws:[4.188439190387726, 7.818800508975983]
memory len:9868
memory used:3067.0
now epsilon is 0.29093531020526797, the reward is 194.33333333333334 with loss [33.49114656448364, 19.265401124954224] in episode 840
Report: 
rewardSum:194.33333333333334
loss:[33.49114656448364, 19.265401124954224]
policies:[0, 4, 0]
qAverage:[0.0, 62.92168935139974]
ws:[5.458741664886475, 9.671209335327148]
memory len:9876
memory used:3067.0
now epsilon is 0.2906444839776217, the reward is 194.33333333333334 with loss [14.727865219116211, 41.67366313934326] in episode 841
Report: 
rewardSum:194.33333333333334
loss:[14.727865219116211, 41.67366313934326]
policies:[0, 3, 1]
qAverage:[0.0, 64.00912094116211]
ws:[6.8453434109687805, 11.433179020881653]
memory len:9884
memory used:3067.0
now epsilon is 0.29028135998004473, the reward is 193.33333333333334 with loss [20.960960865020752, 18.505937218666077] in episode 842
Report: 
rewardSum:193.33333333333334
loss:[20.960960865020752, 18.505937218666077]
policies:[1, 3, 1]
qAverage:[0.0, 54.96093241373698]
ws:[2.322768251101176, 4.410406827926636]
memory len:9894
memory used:3067.0
now epsilon is 0.2899911874574333, the reward is 194.33333333333334 with loss [14.434665322303772, 27.053874969482422] in episode 843
Report: 
rewardSum:194.33333333333334
loss:[14.434665322303772, 27.053874969482422]
policies:[0, 3, 1]
qAverage:[0.0, 67.46343612670898]
ws:[4.3448526263237, 7.453614115715027]
memory len:9902
memory used:3067.0
now epsilon is 0.2897013049985479, the reward is 46.33333333333334 with loss [22.538012504577637, 26.62410545349121] in episode 844
Report: 
rewardSum:46.33333333333334
loss:[22.538012504577637, 26.62410545349121]
policies:[0, 3, 1]
qAverage:[0.0, 62.536067962646484]
ws:[3.0858373641967773, 5.485846757888794]
memory len:9910
memory used:3084.0
now epsilon is 0.28941171231343354, the reward is 194.33333333333334 with loss [13.528999090194702, 26.94968342781067] in episode 845
Report: 
rewardSum:194.33333333333334
loss:[13.528999090194702, 26.94968342781067]
policies:[0, 4, 0]
qAverage:[0.0, 65.00455474853516]
ws:[8.456237077713013, 12.731583714485168]
memory len:9918
memory used:3084.0
now epsilon is 0.28912240911242515, the reward is 194.33333333333334 with loss [21.15378999710083, 20.295092582702637] in episode 846
Report: 
rewardSum:194.33333333333334
loss:[21.15378999710083, 20.295092582702637]
policies:[0, 4, 0]
qAverage:[0.0, 62.255197525024414]
ws:[6.091391205787659, 9.112354755401611]
memory len:9926
memory used:3084.0
now epsilon is 0.28832831560062633, the reward is 187.33333333333334 with loss [77.94533145427704, 64.35655945539474] in episode 847
Report: 
rewardSum:187.33333333333334
loss:[77.94533145427704, 64.35655945539474]
policies:[1, 5, 5]
qAverage:[0.0, 68.7921257019043]
ws:[4.343264778455098, 7.739746173222859]
memory len:9948
memory used:3084.0
now epsilon is 0.28789609334493554, the reward is 192.33333333333334 with loss [13.636292278766632, 31.31932520866394] in episode 848
Report: 
rewardSum:192.33333333333334
loss:[13.636292278766632, 31.31932520866394]
policies:[1, 4, 1]
qAverage:[0.0, 64.89569282531738]
ws:[0.5159816965460777, 3.0087960958480835]
memory len:9960
memory used:3097.0
now epsilon is 0.2876083051946332, the reward is 46.33333333333334 with loss [14.215094089508057, 20.260335564613342] in episode 849
Report: 
rewardSum:46.33333333333334
loss:[14.215094089508057, 20.260335564613342]
policies:[0, 3, 1]
qAverage:[0.0, 62.94464111328125]
ws:[2.561251312494278, 4.957827568054199]
memory len:9968
memory used:3097.0
now epsilon is 0.2871053679891968, the reward is 191.33333333333334 with loss [33.58301067352295, 49.32255160808563] in episode 850
Report: 
rewardSum:191.33333333333334
loss:[33.58301067352295, 49.32255160808563]
policies:[0, 4, 3]
qAverage:[0.0, 70.51719512939454]
ws:[6.037390613555909, 10.11371808052063]
memory len:9982
memory used:3097.0
now epsilon is 0.2866749790087919, the reward is 192.33333333333334 with loss [24.496410131454468, 28.978251218795776] in episode 851
Report: 
rewardSum:192.33333333333334
loss:[24.496410131454468, 28.978251218795776]
policies:[1, 4, 1]
qAverage:[0.0, 65.30474395751953]
ws:[4.169189977645874, 6.961643695831299]
memory len:9994
memory used:3097.0
now epsilon is 0.28638841151498423, the reward is 194.33333333333334 with loss [16.691540479660034, 35.09812068939209] in episode 852
Report: 
rewardSum:194.33333333333334
loss:[16.691540479660034, 35.09812068939209]
policies:[0, 4, 0]
qAverage:[0.0, 70.53090858459473]
ws:[6.318065404891968, 10.516027212142944]
memory len:10000
memory used:3097.0
now epsilon is 0.285959097297368, the reward is 192.33333333333334 with loss [18.207009315490723, 44.55086421966553] in episode 853
Report: 
rewardSum:192.33333333333334
loss:[18.207009315490723, 44.55086421966553]
policies:[2, 3, 1]
qAverage:[0.0, 63.633949279785156]
ws:[4.9161049127578735, 7.8439435958862305]
memory len:10000
memory used:3097.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20*		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.2855304266487303, the reward is 192.33333333333334 with loss [51.169567584991455, 30.499588012695312] in episode 854
Report: 
rewardSum:192.33333333333334
loss:[51.169567584991455, 30.499588012695312]
policies:[0, 4, 2]
qAverage:[0.0, 63.719696044921875]
ws:[4.1263488829135895, 7.218731462955475]
memory len:10000
memory used:3097.0
now epsilon is 0.28524500327814706, the reward is 194.33333333333334 with loss [20.057191848754883, 16.34477710723877] in episode 855
Report: 
rewardSum:194.33333333333334
loss:[20.057191848754883, 16.34477710723877]
policies:[4, 0, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:3116.0
now epsilon is 0.2851023986043207, the reward is -1.0 with loss [14.586105823516846, 14.283401489257812] in episode 856
Report: 
rewardSum:-1.0
loss:[14.586105823516846, 14.283401489257812]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:3118.0
now epsilon is 0.28481740310129805, the reward is 194.33333333333334 with loss [19.394636154174805, 20.936599016189575] in episode 857
Report: 
rewardSum:194.33333333333334
loss:[19.394636154174805, 20.936599016189575]
policies:[0, 4, 0]
qAverage:[0.0, 69.75261688232422]
ws:[3.237958312034607, 6.774203681945801]
memory len:10000
memory used:3119.0
now epsilon is 0.28424826647641355, the reward is 42.33333333333334 with loss [49.6475476026535, 51.115062952041626] in episode 858
Report: 
rewardSum:42.33333333333334
loss:[49.6475476026535, 51.115062952041626]
policies:[0, 5, 3]
qAverage:[0.0, 68.97559204101563]
ws:[5.973606014251709, 9.915434169769288]
memory len:10000
memory used:3119.0
now epsilon is 0.28382216047063785, the reward is 192.33333333333334 with loss [35.35708689689636, 23.196017026901245] in episode 859
Report: 
rewardSum:192.33333333333334
loss:[35.35708689689636, 23.196017026901245]
policies:[1, 4, 1]
qAverage:[0.0, 67.84980773925781]
ws:[4.3550891280174255, 8.019928932189941]
memory len:10000
memory used:3099.0
now epsilon is 0.28353844472573964, the reward is 194.33333333333334 with loss [32.04045557975769, 17.92796301841736] in episode 860
Report: 
rewardSum:194.33333333333334
loss:[32.04045557975769, 17.92796301841736]
policies:[0, 4, 0]
qAverage:[0.0, 62.977203369140625]
ws:[5.348338186740875, 8.91394829750061]
memory len:10000
memory used:3105.0
now epsilon is 0.28297186378054784, the reward is 190.33333333333334 with loss [36.158217787742615, 51.83965802192688] in episode 861
Report: 
rewardSum:190.33333333333334
loss:[36.158217787742615, 51.83965802192688]
policies:[1, 5, 2]
qAverage:[0.0, 73.4020601908366]
ws:[3.7579023043314614, 7.074584503968556]
memory len:10000
memory used:3105.0
now epsilon is 0.2826889980135316, the reward is 194.33333333333334 with loss [27.522622108459473, 20.246139764785767] in episode 862
Report: 
rewardSum:194.33333333333334
loss:[27.522622108459473, 20.246139764785767]
policies:[0, 2, 2]
qAverage:[0.0, 41.4583854675293]
ws:[3.6179776191711426, 6.150402545928955]
memory len:10000
memory used:3118.0
now epsilon is 0.28226522944912324, the reward is 192.33333333333334 with loss [24.467822313308716, 27.55834372341633] in episode 863
Report: 
rewardSum:192.33333333333334
loss:[24.467822313308716, 27.55834372341633]
policies:[0, 4, 2]
qAverage:[0.0, 63.80808448791504]
ws:[0.6852540671825409, 1.998973160982132]
memory len:10000
memory used:3125.0
now epsilon is 0.2819830700514947, the reward is 46.33333333333334 with loss [10.193594694137573, 25.26146960258484] in episode 864
Report: 
rewardSum:46.33333333333334
loss:[10.193594694137573, 25.26146960258484]
policies:[0, 3, 1]
qAverage:[0.0, 53.45542653401693]
ws:[2.450334151585897, 3.3737831115722656]
memory len:10000
memory used:3125.0
now epsilon is 0.28184209614041084, the reward is -1.0 with loss [5.11775815486908, 13.879672050476074] in episode 865
Report: 
rewardSum:-1.0
loss:[5.11775815486908, 13.879672050476074]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:3131.0
now epsilon is 0.2814195971351063, the reward is 192.33333333333334 with loss [27.02528476715088, 21.7473703622818] in episode 866
Report: 
rewardSum:192.33333333333334
loss:[27.02528476715088, 21.7473703622818]
policies:[1, 4, 1]
qAverage:[0.0, 66.45991668701171]
ws:[6.076766586303711, 8.921535873413086]
memory len:10000
memory used:3131.0
now epsilon is 0.28106799848196934, the reward is 193.33333333333334 with loss [21.88884127140045, 26.789105772972107] in episode 867
Report: 
rewardSum:193.33333333333334
loss:[21.88884127140045, 26.789105772972107]
policies:[1, 2, 2]
qAverage:[0.0, 55.20589701334635]
ws:[1.8276770909627278, 3.3702968756357827]
memory len:10000
memory used:3104.0
now epsilon is 0.2807870358664212, the reward is 194.33333333333334 with loss [32.56667947769165, 16.25858759880066] in episode 868
Report: 
rewardSum:194.33333333333334
loss:[32.56667947769165, 16.25858759880066]
policies:[1, 3, 0]
qAverage:[0.0, 62.422882080078125]
ws:[4.08387291431427, 6.855723321437836]
memory len:10000
memory used:3129.0
now epsilon is 0.2802259529263893, the reward is 190.33333333333334 with loss [45.891308307647705, 57.77371025085449] in episode 869
Report: 
rewardSum:190.33333333333334
loss:[45.891308307647705, 57.77371025085449]
policies:[1, 4, 3]
qAverage:[0.0, 37.451263427734375]
ws:[1.113174557685852, 1.7527436017990112]
memory len:10000
memory used:3129.0
now epsilon is 0.27994583204068224, the reward is 194.33333333333334 with loss [16.41822361946106, 21.245198726654053] in episode 870
Report: 
rewardSum:194.33333333333334
loss:[16.41822361946106, 21.245198726654053]
policies:[0, 4, 0]
qAverage:[0.0, 69.86772155761719]
ws:[6.712520956993103, 10.960466623306274]
memory len:10000
memory used:3129.0
now epsilon is 0.2795261756543721, the reward is 192.33333333333334 with loss [35.56634569168091, 36.31178855895996] in episode 871
Report: 
rewardSum:192.33333333333334
loss:[35.56634569168091, 36.31178855895996]
policies:[0, 4, 2]
qAverage:[0.0, 68.88212890625]
ws:[5.102390286326409, 8.566476655006408]
memory len:10000
memory used:3129.0
now epsilon is 0.2792467542835643, the reward is 46.33333333333334 with loss [18.51902973651886, 21.090418815612793] in episode 872
Report: 
rewardSum:46.33333333333334
loss:[18.51902973651886, 21.090418815612793]
policies:[0, 2, 2]
qAverage:[0.0, 41.16943359375]
ws:[5.604196071624756, 8.523200988769531]
memory len:10000
memory used:3130.0
now epsilon is 0.27896761222936184, the reward is 194.33333333333334 with loss [24.750545501708984, 17.404722690582275] in episode 873
Report: 
rewardSum:194.33333333333334
loss:[24.750545501708984, 17.404722690582275]
policies:[1, 2, 1]
qAverage:[0.0, 58.95785776774088]
ws:[2.7950026194254556, 4.669078350067139]
memory len:10000
memory used:3130.0
now epsilon is 0.27882814585872295, the reward is -1.0 with loss [12.505982160568237, 8.045051097869873] in episode 874
Report: 
rewardSum:-1.0
loss:[12.505982160568237, 8.045051097869873]
policies:[0, 1, 1]
qAverage:[0.0, 36.971527099609375]
ws:[-0.20911861956119537, 0.34887054562568665]
memory len:10000
memory used:3130.0
now epsilon is 0.2784101649542042, the reward is 192.33333333333334 with loss [37.02863812446594, 19.549391984939575] in episode 875
Report: 
rewardSum:192.33333333333334
loss:[37.02863812446594, 19.549391984939575]
policies:[1, 4, 1]
qAverage:[0.0, 57.09176445007324]
ws:[2.533711701631546, 4.905831336975098]
memory len:10000
memory used:3130.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40*		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.2778538315985517, the reward is 190.33333333333334 with loss [37.56031954288483, 40.601996183395386] in episode 876
Report: 
rewardSum:190.33333333333334
loss:[37.56031954288483, 40.601996183395386]
policies:[1, 4, 3]
qAverage:[0.0, 64.03351020812988]
ws:[2.6314080506563187, 5.68583944439888]
memory len:10000
memory used:3131.0
now epsilon is 0.2775760819447753, the reward is 46.33333333333334 with loss [16.862358570098877, 20.38327693939209] in episode 877
Report: 
rewardSum:46.33333333333334
loss:[16.862358570098877, 20.38327693939209]
policies:[0, 3, 1]
qAverage:[0.0, 57.563697814941406]
ws:[2.3937137126922607, 4.263245741526286]
memory len:10000
memory used:3137.0
now epsilon is 0.27715997796270875, the reward is 44.33333333333334 with loss [35.19926309585571, 26.88587737083435] in episode 878
Report: 
rewardSum:44.33333333333334
loss:[35.19926309585571, 26.88587737083435]
policies:[1, 3, 2]
qAverage:[0.0, 62.53848648071289]
ws:[1.2712805196642876, 2.8460550010204315]
memory len:10000
memory used:3137.0
now epsilon is 0.27688292190241637, the reward is 46.33333333333334 with loss [18.91304174810648, 24.055362701416016] in episode 879
Report: 
rewardSum:46.33333333333334
loss:[18.91304174810648, 24.055362701416016]
policies:[0, 3, 1]
qAverage:[0.0, 37.042991638183594]
ws:[0.03689424693584442, 0.35379481315612793]
memory len:10000
memory used:3137.0
now epsilon is 0.27660614279430557, the reward is 194.33333333333334 with loss [13.05151104927063, 33.661513805389404] in episode 880
Report: 
rewardSum:194.33333333333334
loss:[13.05151104927063, 33.661513805389404]
policies:[0, 4, 0]
qAverage:[0.0, 69.87203216552734]
ws:[4.097929620742798, 6.790125226974487]
memory len:10000
memory used:3137.0
now epsilon is 0.276329640361528, the reward is 194.33333333333334 with loss [22.553726196289062, 29.4795184135437] in episode 881
Report: 
rewardSum:194.33333333333334
loss:[22.553726196289062, 29.4795184135437]
policies:[0, 4, 0]
qAverage:[0.0, 70.87772216796876]
ws:[4.099559181928635, 6.757091224193573]
memory len:10000
memory used:3137.0
now epsilon is 0.2760534143275121, the reward is 194.33333333333334 with loss [42.38741195201874, 11.893180131912231] in episode 882
Report: 
rewardSum:194.33333333333334
loss:[42.38741195201874, 11.893180131912231]
policies:[0, 3, 1]
qAverage:[0.0, 69.82370948791504]
ws:[4.600914537906647, 7.9800028800964355]
memory len:10000
memory used:3137.0
now epsilon is 0.2756395929198463, the reward is 192.33333333333334 with loss [29.188063144683838, 34.08209204673767] in episode 883
Report: 
rewardSum:192.33333333333334
loss:[29.188063144683838, 34.08209204673767]
policies:[1, 4, 1]
qAverage:[0.0, 69.88998870849609]
ws:[3.7208901405334474, 7.774161589145661]
memory len:10000
memory used:3137.0
now epsilon is 0.2750200236632194, the reward is 189.33333333333334 with loss [35.56256306171417, 48.556965827941895] in episode 884
Report: 
rewardSum:189.33333333333334
loss:[35.56256306171417, 48.556965827941895]
policies:[0, 6, 3]
qAverage:[0.0, 66.0018325805664]
ws:[2.843425679206848, 6.1476305484771725]
memory len:10000
memory used:3105.0
now epsilon is 0.2746077513730692, the reward is 192.33333333333334 with loss [18.099292397499084, 28.550673246383667] in episode 885
Report: 
rewardSum:192.33333333333334
loss:[18.099292397499084, 28.550673246383667]
policies:[0, 3, 3]
qAverage:[0.0, 57.89768600463867]
ws:[-0.027613647282123566, 1.1568510383367538]
memory len:10000
memory used:3105.0
now epsilon is 0.27419609710497767, the reward is 192.33333333333334 with loss [39.189340710639954, 31.024500489234924] in episode 886
Report: 
rewardSum:192.33333333333334
loss:[39.189340710639954, 31.024500489234924]
policies:[0, 3, 3]
qAverage:[0.0, 67.60276794433594]
ws:[2.950647220015526, 6.240282250568271]
memory len:10000
memory used:3111.0
now epsilon is 0.2739905014396329, the reward is -2.0 with loss [7.678925275802612, 18.821369647979736] in episode 887
Report: 
rewardSum:-2.0
loss:[7.678925275802612, 18.821369647979736]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:3111.0
now epsilon is 0.2736481845140912, the reward is 193.33333333333334 with loss [37.83047556877136, 27.54976201057434] in episode 888
Report: 
rewardSum:193.33333333333334
loss:[37.83047556877136, 27.54976201057434]
policies:[0, 3, 2]
qAverage:[0.0, 45.910152435302734]
ws:[2.15649676322937, 3.592109441757202]
memory len:10000
memory used:3111.0
now epsilon is 0.27337463893054437, the reward is 46.33333333333334 with loss [24.379760026931763, 20.97966194152832] in episode 889
Report: 
rewardSum:46.33333333333334
loss:[24.379760026931763, 20.97966194152832]
policies:[0, 2, 2]
qAverage:[0.0, 58.415626525878906]
ws:[2.0446504751841226, 3.3484179178873696]
memory len:10000
memory used:3111.0
now epsilon is 0.27310136679001856, the reward is 46.33333333333334 with loss [19.545687437057495, 9.050297737121582] in episode 890
Report: 
rewardSum:46.33333333333334
loss:[19.545687437057495, 9.050297737121582]
policies:[0, 3, 1]
qAverage:[0.0, 61.33552360534668]
ws:[1.006405919790268, 2.4407844841480255]
memory len:10000
memory used:3111.0
now epsilon is 0.2726919706870368, the reward is 44.33333333333334 with loss [21.086761593818665, 41.77459669113159] in episode 891
Report: 
rewardSum:44.33333333333334
loss:[21.086761593818665, 41.77459669113159]
policies:[0, 4, 2]
qAverage:[0.0, 63.921994018554685]
ws:[1.356879425048828, 3.7795112133026123]
memory len:10000
memory used:3111.0
now epsilon is 0.2722831882945286, the reward is 192.33333333333334 with loss [16.814188838005066, 33.422003507614136] in episode 892
Report: 
rewardSum:192.33333333333334
loss:[16.814188838005066, 33.422003507614136]
policies:[1, 4, 1]
qAverage:[0.0, 68.47954864501953]
ws:[4.109560418128967, 7.8540925025939945]
memory len:10000
memory used:3111.0
now epsilon is 0.27201100719541305, the reward is 194.33333333333334 with loss [11.49256420135498, 15.024454355239868] in episode 893
Report: 
rewardSum:194.33333333333334
loss:[11.49256420135498, 15.024454355239868]
policies:[1, 3, 0]
qAverage:[0.0, 64.54965718587239]
ws:[5.682027816772461, 9.754087289174398]
memory len:10000
memory used:3112.0
now epsilon is 0.2717390981753457, the reward is 194.33333333333334 with loss [19.068703055381775, 23.030491828918457] in episode 894
Report: 
rewardSum:194.33333333333334
loss:[19.068703055381775, 23.030491828918457]
policies:[1, 3, 0]
qAverage:[0.0, 65.85537528991699]
ws:[3.904683768749237, 7.077850759029388]
memory len:10000
memory used:3117.0
now epsilon is 0.2714674609623496, the reward is 194.33333333333334 with loss [15.214473962783813, 32.503353118896484] in episode 895
Report: 
rewardSum:194.33333333333334
loss:[15.214473962783813, 32.503353118896484]
policies:[0, 4, 0]
qAverage:[0.0, 67.05715942382812]
ws:[2.474456161260605, 5.168691497296095]
memory len:10000
memory used:3117.0
now epsilon is 0.2711960952847195, the reward is 194.33333333333334 with loss [17.45555305480957, 20.80213975906372] in episode 896
Report: 
rewardSum:194.33333333333334
loss:[17.45555305480957, 20.80213975906372]
policies:[0, 2, 2]
qAverage:[0.0, 64.14315287272136]
ws:[4.163731336593628, 7.65548833211263]
memory len:10000
memory used:3118.0
now epsilon is 0.27092500087102184, the reward is 194.33333333333334 with loss [13.165641784667969, 30.51601219177246] in episode 897
Report: 
rewardSum:194.33333333333334
loss:[13.165641784667969, 30.51601219177246]
policies:[1, 2, 1]
qAverage:[0.0, 52.20298512776693]
ws:[0.8043489853541056, 1.9121164480845134]
memory len:10000
memory used:3118.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40*		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.2701133425032564, the reward is 186.33333333333334 with loss [58.74680852890015, 48.55344122648239] in episode 898
Report: 
rewardSum:186.33333333333334
loss:[58.74680852890015, 48.55344122648239]
policies:[1, 5, 6]
qAverage:[0.0, 66.14344787597656]
ws:[3.079282427827517, 5.302815318107605]
memory len:10000
memory used:3125.0
now epsilon is 0.26977586960376654, the reward is 193.33333333333334 with loss [36.651824951171875, 30.77840495109558] in episode 899
Report: 
rewardSum:193.33333333333334
loss:[36.651824951171875, 30.77840495109558]
policies:[2, 2, 1]
qAverage:[0.0, 58.18078104654948]
ws:[1.9701542059580486, 5.945075154304504]
memory len:10000
memory used:3125.0
now epsilon is 0.269506194883254, the reward is 194.33333333333334 with loss [29.290196418762207, 24.57229745388031] in episode 900
Report: 
rewardSum:194.33333333333334
loss:[29.290196418762207, 24.57229745388031]
policies:[0, 4, 0]
qAverage:[0.0, 70.27267150878906]
ws:[2.9339085102081297, 7.895211935043335]
memory len:10000
memory used:3125.0
now epsilon is 0.2692367897363508, the reward is 194.33333333333334 with loss [14.999109387397766, 26.821852684020996] in episode 901
Report: 
rewardSum:194.33333333333334
loss:[14.999109387397766, 26.821852684020996]
policies:[0, 2, 2]
qAverage:[0.0, 40.45204162597656]
ws:[1.2301498651504517, 3.455256223678589]
memory len:10000
memory used:3125.0
now epsilon is 0.268833186877116, the reward is 994.0 with loss [29.827040433883667, 48.72348642349243] in episode 902
Report: 
rewardSum:994.0
loss:[29.827040433883667, 48.72348642349243]
policies:[1, 2, 3]
qAverage:[18.003602981567383, 41.348976135253906]
ws:[-11.78864599764347, -11.666439175605774]
memory len:10000
memory used:3125.0
now epsilon is 0.2682959907262834, the reward is 190.33333333333334 with loss [48.09582328796387, 51.371153473854065] in episode 903
Report: 
rewardSum:190.33333333333334
loss:[48.09582328796387, 51.371153473854065]
policies:[2, 2, 4]
qAverage:[0.0, 52.30216979980469]
ws:[3.0907687544822693, 5.655157029628754]
memory len:10000
memory used:3125.0
now epsilon is 0.2679607883809538, the reward is 193.33333333333334 with loss [36.5411639213562, 17.872798204421997] in episode 904
Report: 
rewardSum:193.33333333333334
loss:[36.5411639213562, 17.872798204421997]
policies:[0, 4, 1]
qAverage:[0.0, 69.4278450012207]
ws:[4.854221701622009, 9.32912278175354]
memory len:10000
memory used:3125.0
now epsilon is 0.2676260048291067, the reward is 193.33333333333334 with loss [34.18631875514984, 23.42879867553711] in episode 905
Report: 
rewardSum:193.33333333333334
loss:[34.18631875514984, 23.42879867553711]
policies:[0, 4, 1]
qAverage:[0.0, 62.001853942871094]
ws:[4.604690372943878, 7.4448100328445435]
memory len:10000
memory used:3125.0
now epsilon is 0.2673584791673039, the reward is 194.33333333333334 with loss [20.38865613937378, 24.172070026397705] in episode 906
Report: 
rewardSum:194.33333333333334
loss:[20.38865613937378, 24.172070026397705]
policies:[1, 3, 0]
qAverage:[0.0, 51.699867248535156]
ws:[1.6478901704152424, 3.4707882404327393]
memory len:10000
memory used:3125.0
now epsilon is 0.26709122093085746, the reward is 194.33333333333334 with loss [24.378645300865173, 20.079702019691467] in episode 907
Report: 
rewardSum:194.33333333333334
loss:[24.378645300865173, 20.079702019691467]
policies:[0, 4, 0]
qAverage:[0.0, 70.1400650024414]
ws:[5.877397966384888, 11.222879695892335]
memory len:10000
memory used:3125.0
now epsilon is 0.26682422985244236, the reward is 194.33333333333334 with loss [15.811034679412842, 27.79850435256958] in episode 908
Report: 
rewardSum:194.33333333333334
loss:[15.811034679412842, 27.79850435256958]
policies:[0, 4, 0]
qAverage:[0.0, 71.03277435302735]
ws:[4.791318893432617, 10.007290935516357]
memory len:10000
memory used:3125.0
now epsilon is 0.2664242435720123, the reward is 192.33333333333334 with loss [19.48317527770996, 39.48716187477112] in episode 909
Report: 
rewardSum:192.33333333333334
loss:[19.48317527770996, 39.48716187477112]
policies:[0, 3, 3]
qAverage:[0.0, 65.46025848388672]
ws:[3.2994871735572815, 7.761946797370911]
memory len:10000
memory used:3125.0
now epsilon is 0.26609137974107594, the reward is 193.33333333333334 with loss [20.3496972322464, 26.353667497634888] in episode 910
Report: 
rewardSum:193.33333333333334
loss:[20.3496972322464, 26.353667497634888]
policies:[0, 3, 2]
qAverage:[0.0, 70.70660018920898]
ws:[3.3424885869026184, 8.808374166488647]
memory len:10000
memory used:3125.0
now epsilon is 0.2658253881289726, the reward is 194.33333333333334 with loss [32.67354071140289, 24.187229990959167] in episode 911
Report: 
rewardSum:194.33333333333334
loss:[32.67354071140289, 24.187229990959167]
policies:[0, 4, 0]
qAverage:[0.0, 61.13210868835449]
ws:[1.6460336297750473, 5.0322349071502686]
memory len:10000
memory used:3125.0
now epsilon is 0.2655596624087512, the reward is 194.33333333333334 with loss [23.187756061553955, 20.76359713077545] in episode 912
Report: 
rewardSum:194.33333333333334
loss:[23.187756061553955, 20.76359713077545]
policies:[0, 4, 0]
qAverage:[0.0, 73.06867370605468]
ws:[5.360248327255249, 11.34195680618286]
memory len:10000
memory used:3126.0
now epsilon is 0.2652278787640407, the reward is 193.33333333333334 with loss [33.914979219436646, 30.004101037979126] in episode 913
Report: 
rewardSum:193.33333333333334
loss:[33.914979219436646, 30.004101037979126]
policies:[0, 3, 2]
qAverage:[0.0, 52.257181803385414]
ws:[4.833902676900228, 8.655208587646484]
memory len:10000
memory used:3126.0
now epsilon is 0.2649627503291555, the reward is 194.33333333333334 with loss [11.990166902542114, 12.031074166297913] in episode 914
Report: 
rewardSum:194.33333333333334
loss:[11.990166902542114, 12.031074166297913]
policies:[0, 3, 1]
qAverage:[0.0, 72.47707939147949]
ws:[3.0214802026748657, 8.55624783039093]
memory len:10000
memory used:3126.0
now epsilon is 0.26469788692329865, the reward is 194.33333333333334 with loss [14.954292297363281, 38.061659812927246] in episode 915
Report: 
rewardSum:194.33333333333334
loss:[14.954292297363281, 38.061659812927246]
policies:[0, 3, 1]
qAverage:[0.0, 63.922106424967446]
ws:[6.80877685546875, 13.63563092549642]
memory len:10000
memory used:3126.0
now epsilon is 0.26430108816448017, the reward is 192.33333333333334 with loss [22.97783660888672, 51.338648319244385] in episode 916
Report: 
rewardSum:192.33333333333334
loss:[22.97783660888672, 51.338648319244385]
policies:[1, 4, 1]
qAverage:[0.0, 62.739505767822266]
ws:[3.076741397380829, 6.089548587799072]
memory len:10000
memory used:3126.0
now epsilon is 0.2639048842319251, the reward is 192.33333333333334 with loss [28.447431206703186, 36.91933298110962] in episode 917
Report: 
rewardSum:192.33333333333334
loss:[28.447431206703186, 36.91933298110962]
policies:[2, 3, 1]
qAverage:[0.0, 48.301856994628906]
ws:[3.089844226837158, 6.651889801025391]
memory len:10000
memory used:3126.0
now epsilon is 0.26377294828386444, the reward is -1.0 with loss [15.172466278076172, 17.311386585235596] in episode 918
Report: 
rewardSum:-1.0
loss:[15.172466278076172, 17.311386585235596]
policies:[0, 1, 1]
qAverage:[0.0, 36.930015563964844]
ws:[-0.018897145986557007, 0.8512243628501892]
memory len:10000
memory used:3126.0
now epsilon is 0.26350927423395143, the reward is -3.0 with loss [10.158841073513031, 26.63138222694397] in episode 919
Report: 
rewardSum:-3.0
loss:[10.158841073513031, 26.63138222694397]
policies:[2, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:3126.0
now epsilon is 0.26324586375922704, the reward is 194.33333333333334 with loss [12.00672197341919, 23.04594850540161] in episode 920
Report: 
rewardSum:194.33333333333334
loss:[12.00672197341919, 23.04594850540161]
policies:[1, 3, 0]
qAverage:[0.0, 66.13139152526855]
ws:[2.8770657926797867, 7.979424059391022]
memory len:10000
memory used:3126.0
now epsilon is 0.2629827165962149, the reward is 194.33333333333334 with loss [13.656073093414307, 14.057417154312134] in episode 921
Report: 
rewardSum:194.33333333333334
loss:[13.656073093414307, 14.057417154312134]
policies:[0, 4, 0]
qAverage:[0.0, 56.609230041503906]
ws:[2.136450489362081, 5.709338982899983]
memory len:10000
memory used:3126.0
############# STATE ###############
0-		9*		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.2627198324817021, the reward is 194.33333333333334 with loss [11.597779989242554, 24.703123807907104] in episode 922
Report: 
rewardSum:194.33333333333334
loss:[11.597779989242554, 24.703123807907104]
policies:[0, 3, 1]
qAverage:[0.0, 63.90708541870117]
ws:[2.4745753407478333, 6.7626073360443115]
memory len:10000
memory used:3126.0
now epsilon is 0.26219485234663764, the reward is 190.33333333333334 with loss [45.84404373168945, 60.73519492149353] in episode 923
Report: 
rewardSum:190.33333333333334
loss:[45.84404373168945, 60.73519492149353]
policies:[0, 5, 3]
qAverage:[0.0, 77.63204193115234]
ws:[3.083803584178289, 8.280264536539713]
memory len:10000
memory used:3126.0
now epsilon is 0.26193275580097447, the reward is 194.33333333333334 with loss [13.338966012001038, 34.32873582839966] in episode 924
Report: 
rewardSum:194.33333333333334
loss:[13.338966012001038, 34.32873582839966]
policies:[0, 4, 0]
qAverage:[0.0, 66.65903854370117]
ws:[3.8789762258529663, 8.824600875377655]
memory len:10000
memory used:3126.0
now epsilon is 0.2616709212535872, the reward is 194.33333333333334 with loss [24.213616371154785, 16.06985878944397] in episode 925
Report: 
rewardSum:194.33333333333334
loss:[24.213616371154785, 16.06985878944397]
policies:[0, 3, 1]
qAverage:[0.0, 38.691898345947266]
ws:[0.147761732339859, 1.2139335870742798]
memory len:10000
memory used:3126.0
now epsilon is 0.26140934844257574, the reward is 194.33333333333334 with loss [22.48296570777893, 17.41937017440796] in episode 926
Report: 
rewardSum:194.33333333333334
loss:[22.48296570777893, 17.41937017440796]
policies:[0, 4, 0]
qAverage:[0.0, 62.95830535888672]
ws:[4.864917695522308, 9.871116042137146]
memory len:10000
memory used:3126.0
now epsilon is 0.2609522250396486, the reward is 993.0 with loss [27.17185616493225, 28.392696619033813] in episode 927
Report: 
rewardSum:993.0
loss:[27.17185616493225, 28.392696619033813]
policies:[1, 1, 5]
qAverage:[23.767425537109375, 29.178810119628906]
ws:[-10.831371784210205, -11.689445813496908]
memory len:10000
memory used:3126.0
now epsilon is 0.26069137065538484, the reward is 194.33333333333334 with loss [14.475348711013794, 16.942097544670105] in episode 928
Report: 
rewardSum:194.33333333333334
loss:[14.475348711013794, 16.942097544670105]
policies:[0, 4, 0]
qAverage:[0.0, 75.47530364990234]
ws:[4.27090585231781, 9.121996760368347]
memory len:10000
memory used:3126.0
now epsilon is 0.26043077702770134, the reward is 194.33333333333334 with loss [24.773998022079468, 28.108004093170166] in episode 929
Report: 
rewardSum:194.33333333333334
loss:[24.773998022079468, 28.108004093170166]
policies:[0, 4, 0]
qAverage:[0.0, 63.26997756958008]
ws:[2.758539780974388, 6.097012579441071]
memory len:10000
memory used:3126.0
now epsilon is 0.26004037493464394, the reward is 192.33333333333334 with loss [36.50875282287598, 27.260891556739807] in episode 930
Report: 
rewardSum:192.33333333333334
loss:[36.50875282287598, 27.260891556739807]
policies:[0, 5, 1]
qAverage:[0.0, 68.58424949645996]
ws:[4.792917191982269, 8.941460609436035]
memory len:10000
memory used:3125.0
now epsilon is 0.2597804320585984, the reward is 194.33333333333334 with loss [33.92797565460205, 25.587575912475586] in episode 931
Report: 
rewardSum:194.33333333333334
loss:[33.92797565460205, 25.587575912475586]
policies:[0, 3, 1]
qAverage:[0.0, 67.00321451822917]
ws:[4.063596487045288, 7.965454578399658]
memory len:10000
memory used:3125.0
now epsilon is 0.2595207490279666, the reward is 194.33333333333334 with loss [19.934821486473083, 12.827515602111816] in episode 932
Report: 
rewardSum:194.33333333333334
loss:[19.934821486473083, 12.827515602111816]
policies:[0, 4, 0]
qAverage:[0.0, 73.53494415283203]
ws:[5.699585628509522, 10.12823190689087]
memory len:10000
memory used:3125.0
now epsilon is 0.2592613255830006, the reward is 194.33333333333334 with loss [18.008774280548096, 14.975278735160828] in episode 933
Report: 
rewardSum:194.33333333333334
loss:[18.008774280548096, 14.975278735160828]
policies:[0, 4, 0]
qAverage:[0.0, 75.32537536621093]
ws:[6.057966899871826, 10.398160076141357]
memory len:10000
memory used:3125.0
now epsilon is 0.25900216146421184, the reward is 194.33333333333334 with loss [22.360379934310913, 20.35097646713257] in episode 934
Report: 
rewardSum:194.33333333333334
loss:[22.360379934310913, 20.35097646713257]
policies:[0, 4, 0]
qAverage:[0.0, 68.95708084106445]
ws:[4.593861222267151, 7.985000848770142]
memory len:10000
memory used:3125.0
now epsilon is 0.25874325641237156, the reward is 194.33333333333334 with loss [20.378355741500854, 25.388396739959717] in episode 935
Report: 
rewardSum:194.33333333333334
loss:[20.378355741500854, 25.388396739959717]
policies:[1, 3, 0]
qAverage:[0.0, 60.94375864664713]
ws:[1.9973529974619548, 4.167800744374593]
memory len:10000
memory used:3125.0
now epsilon is 0.25848461016851, the reward is 194.33333333333334 with loss [15.17784070968628, 13.526107668876648] in episode 936
Report: 
rewardSum:194.33333333333334
loss:[15.17784070968628, 13.526107668876648]
policies:[0, 4, 0]
qAverage:[0.0, 75.28732299804688]
ws:[6.476740789413452, 11.46659984588623]
memory len:10000
memory used:3126.0
now epsilon is 0.258226222473916, the reward is 46.33333333333334 with loss [17.763559579849243, 23.2670955657959] in episode 937
Report: 
rewardSum:46.33333333333334
loss:[17.763559579849243, 23.2670955657959]
policies:[2, 1, 1]
qAverage:[0.0, 44.358558654785156]
ws:[2.3709309101104736, 4.637034893035889]
memory len:10000
memory used:3137.0
now epsilon is 0.2579680930701374, the reward is 194.33333333333334 with loss [25.11712098121643, 19.08252263069153] in episode 938
Report: 
rewardSum:194.33333333333334
loss:[25.11712098121643, 19.08252263069153]
policies:[0, 4, 0]
qAverage:[0.0, 75.84639434814453]
ws:[7.265628433227539, 12.790454387664795]
memory len:10000
memory used:3137.0
now epsilon is 0.25771022169898017, the reward is 46.33333333333334 with loss [14.051874160766602, 18.99692964553833] in episode 939
Report: 
rewardSum:46.33333333333334
loss:[14.051874160766602, 18.99692964553833]
policies:[1, 1, 2]
qAverage:[0.0, 43.60845947265625]
ws:[2.8102102279663086, 5.453019618988037]
memory len:10000
memory used:3137.0
now epsilon is 0.2574526081025085, the reward is 194.33333333333334 with loss [20.806878209114075, 17.48073720932007] in episode 940
Report: 
rewardSum:194.33333333333334
loss:[20.806878209114075, 17.48073720932007]
policies:[0, 3, 1]
qAverage:[0.0, 76.44873428344727]
ws:[7.957698583602905, 14.37696886062622]
memory len:10000
memory used:3125.0
now epsilon is 0.25732389788924526, the reward is -1.0 with loss [7.5169219970703125, 16.421079635620117] in episode 941
Report: 
rewardSum:-1.0
loss:[7.5169219970703125, 16.421079635620117]
policies:[0, 1, 1]
qAverage:[0.0, 38.42881393432617]
ws:[1.2246662378311157, 3.270733594894409]
memory len:10000
memory used:3125.0
now epsilon is 0.25706667047173604, the reward is 194.33333333333334 with loss [11.098872542381287, 16.158066749572754] in episode 942
Report: 
rewardSum:194.33333333333334
loss:[11.098872542381287, 16.158066749572754]
policies:[0, 4, 0]
qAverage:[0.0, 65.78545570373535]
ws:[4.790797293186188, 9.397633790969849]
memory len:10000
memory used:3125.0
now epsilon is 0.25668131138571376, the reward is 192.33333333333334 with loss [20.749147593975067, 37.969682931900024] in episode 943
Report: 
rewardSum:192.33333333333334
loss:[20.749147593975067, 37.969682931900024]
policies:[0, 5, 1]
qAverage:[0.0, 72.2730941772461]
ws:[5.507113993167877, 11.11715018749237]
memory len:10000
memory used:3125.0
now epsilon is 0.2564247263137782, the reward is 194.33333333333334 with loss [30.677247047424316, 17.21235990524292] in episode 944
Report: 
rewardSum:194.33333333333334
loss:[30.677247047424316, 17.21235990524292]
policies:[0, 1, 3]
qAverage:[0.0, 50.48831558227539]
ws:[2.161379337310791, 4.978241920471191]
memory len:10000
memory used:3125.0
now epsilon is 0.25610435563127865, the reward is 193.33333333333334 with loss [32.07937145233154, 22.882941246032715] in episode 945
Report: 
rewardSum:193.33333333333334
loss:[32.07937145233154, 22.882941246032715]
policies:[0, 3, 2]
qAverage:[0.0, 64.41658020019531]
ws:[6.905821641286214, 11.890836079915365]
memory len:10000
memory used:3125.0
now epsilon is 0.2557204391156475, the reward is 192.33333333333334 with loss [27.17190647125244, 25.182811617851257] in episode 946
Report: 
rewardSum:192.33333333333334
loss:[27.17190647125244, 25.182811617851257]
policies:[1, 3, 2]
qAverage:[0.0, 66.55751419067383]
ws:[7.227747797966003, 12.33052110671997]
memory len:10000
memory used:3125.0
############# STATE ###############
0-		9*		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.255464814555715, the reward is 194.33333333333334 with loss [25.146847248077393, 23.787211179733276] in episode 947
Report: 
rewardSum:194.33333333333334
loss:[25.146847248077393, 23.787211179733276]
policies:[2, 1, 1]
qAverage:[0.0, 44.148250579833984]
ws:[1.778315782546997, 3.6780524253845215]
memory len:10000
memory used:3125.0
now epsilon is 0.2550180862881394, the reward is 191.33333333333334 with loss [40.47794795036316, 45.86447477340698] in episode 948
Report: 
rewardSum:191.33333333333334
loss:[40.47794795036316, 45.86447477340698]
policies:[0, 5, 2]
qAverage:[0.0, 76.73940912882487]
ws:[5.7639371156692505, 10.131798187891642]
memory len:10000
memory used:3125.0
now epsilon is 0.254763163817696, the reward is 194.33333333333334 with loss [26.42138385772705, 21.94976019859314] in episode 949
Report: 
rewardSum:194.33333333333334
loss:[26.42138385772705, 21.94976019859314]
policies:[1, 2, 1]
qAverage:[0.0, 54.36048126220703]
ws:[4.9528530438741045, 8.549330393473307]
memory len:10000
memory used:3125.0
now epsilon is 0.254508496174143, the reward is 194.33333333333334 with loss [15.600321292877197, 23.70902967453003] in episode 950
Report: 
rewardSum:194.33333333333334
loss:[15.600321292877197, 23.70902967453003]
policies:[0, 2, 2]
qAverage:[0.0, 71.14141082763672]
ws:[5.954674402872722, 10.092266082763672]
memory len:10000
memory used:3125.0
now epsilon is 0.2542540831027492, the reward is 46.33333333333334 with loss [28.98779606819153, 20.18208336830139] in episode 951
Report: 
rewardSum:46.33333333333334
loss:[28.98779606819153, 20.18208336830139]
policies:[0, 3, 1]
qAverage:[0.0, 63.13096364339193]
ws:[3.3462326526641846, 7.506734053293864]
memory len:10000
memory used:3125.0
now epsilon is 0.2539999243490378, the reward is 194.33333333333334 with loss [15.0692138671875, 25.4615535736084] in episode 952
Report: 
rewardSum:194.33333333333334
loss:[15.0692138671875, 25.4615535736084]
policies:[1, 3, 0]
qAverage:[0.0, 54.72436269124349]
ws:[0.4364716211954753, 2.901041030883789]
memory len:10000
memory used:3125.0
now epsilon is 0.2537460196587864, the reward is 194.33333333333334 with loss [36.26477360725403, 19.3225359916687] in episode 953
Report: 
rewardSum:194.33333333333334
loss:[36.26477360725403, 19.3225359916687]
policies:[0, 3, 1]
qAverage:[0.0, 72.12549336751302]
ws:[7.945269425710042, 13.475257873535156]
memory len:10000
memory used:3125.0
now epsilon is 0.25336563843691096, the reward is 44.33333333333334 with loss [26.338576078414917, 25.77435851097107] in episode 954
Report: 
rewardSum:44.33333333333334
loss:[26.338576078414917, 25.77435851097107]
policies:[1, 2, 3]
qAverage:[0.0, 55.00236511230469]
ws:[5.080958207448323, 9.119584401448568]
memory len:10000
memory used:3126.0
now epsilon is 0.2532389714530449, the reward is -1.0 with loss [8.985151767730713, 8.977328777313232] in episode 955
Report: 
rewardSum:-1.0
loss:[8.985151767730713, 8.977328777313232]
policies:[0, 1, 1]
qAverage:[0.0, 37.73146438598633]
ws:[1.1311702728271484, 2.8631153106689453]
memory len:10000
memory used:3126.0
now epsilon is 0.25285935032827883, the reward is 192.33333333333334 with loss [22.784609079360962, 25.942008018493652] in episode 956
Report: 
rewardSum:192.33333333333334
loss:[22.784609079360962, 25.942008018493652]
policies:[1, 4, 1]
qAverage:[0.0, 72.5527400970459]
ws:[6.103406846523285, 11.155340552330017]
memory len:10000
memory used:3125.0
now epsilon is 0.2526065857844043, the reward is 194.33333333333334 with loss [18.52398443222046, 30.970247507095337] in episode 957
Report: 
rewardSum:194.33333333333334
loss:[18.52398443222046, 30.970247507095337]
policies:[1, 3, 0]
qAverage:[0.0, 52.704917907714844]
ws:[1.2247039079666138, 4.0158233642578125]
memory len:10000
memory used:3125.0
now epsilon is 0.25235407391030273, the reward is 194.33333333333334 with loss [18.400832414627075, 17.321757793426514] in episode 958
Report: 
rewardSum:194.33333333333334
loss:[18.400832414627075, 17.321757793426514]
policies:[1, 3, 0]
qAverage:[0.0, 74.93916511535645]
ws:[5.723558843135834, 11.159074544906616]
memory len:10000
memory used:3125.0
now epsilon is 0.2519757793025358, the reward is 44.33333333333334 with loss [25.867976188659668, 18.464123964309692] in episode 959
Report: 
rewardSum:44.33333333333334
loss:[25.867976188659668, 18.464123964309692]
policies:[0, 3, 3]
qAverage:[0.0, 65.72574996948242]
ws:[3.7225798964500427, 8.244382858276367]
memory len:10000
memory used:3125.0
now epsilon is 0.25159805178214745, the reward is 192.33333333333334 with loss [41.5802264213562, 26.333484649658203] in episode 960
Report: 
rewardSum:192.33333333333334
loss:[41.5802264213562, 26.333484649658203]
policies:[0, 4, 2]
qAverage:[0.0, 55.92867533365885]
ws:[2.222846349080404, 5.013870398203532]
memory len:10000
memory used:3125.0
now epsilon is 0.2513465480639108, the reward is 194.33333333333334 with loss [9.96700519323349, 18.915743589401245] in episode 961
Report: 
rewardSum:194.33333333333334
loss:[9.96700519323349, 18.915743589401245]
policies:[0, 3, 1]
qAverage:[0.0, 67.86180877685547]
ws:[1.9692657887935638, 4.506993651390076]
memory len:10000
memory used:3125.0
now epsilon is 0.25103252193115555, the reward is 193.33333333333334 with loss [17.559420108795166, 23.227901697158813] in episode 962
Report: 
rewardSum:193.33333333333334
loss:[17.559420108795166, 23.227901697158813]
policies:[0, 4, 1]
qAverage:[0.0, 71.43615531921387]
ws:[6.678439423441887, 10.200446324422956]
memory len:10000
memory used:3125.0
now epsilon is 0.2506562084128152, the reward is 192.33333333333334 with loss [29.232583045959473, 24.55702018737793] in episode 963
Report: 
rewardSum:192.33333333333334
loss:[29.232583045959473, 24.55702018737793]
policies:[0, 4, 2]
qAverage:[0.0, 75.33509674072266]
ws:[3.051198233664036, 5.1577085733413695]
memory len:10000
memory used:3125.0
now epsilon is 0.25040564618481553, the reward is 46.33333333333334 with loss [27.52310061454773, 23.445221662521362] in episode 964
Report: 
rewardSum:46.33333333333334
loss:[27.52310061454773, 23.445221662521362]
policies:[0, 2, 2]
qAverage:[0.0, 63.4466552734375]
ws:[3.4554025332132974, 6.214937845865886]
memory len:10000
memory used:3125.0
now epsilon is 0.2501553344250987, the reward is 194.33333333333334 with loss [36.119765520095825, 17.64962911605835] in episode 965
Report: 
rewardSum:194.33333333333334
loss:[36.119765520095825, 17.64962911605835]
policies:[0, 4, 0]
qAverage:[0.0, 78.12500610351563]
ws:[7.408512687683105, 11.978764820098878]
memory len:10000
memory used:3125.0
now epsilon is 0.2499052728832903, the reward is 46.33333333333334 with loss [19.167691946029663, 20.17969048023224] in episode 966
Report: 
rewardSum:46.33333333333334
loss:[19.167691946029663, 20.17969048023224]
policies:[0, 3, 1]
qAverage:[0.0, 68.14055061340332]
ws:[4.14899754524231, 6.959908723831177]
memory len:10000
memory used:3125.0
now epsilon is 0.24946826651978246, the reward is 191.33333333333334 with loss [46.266149044036865, 27.758044958114624] in episode 967
Report: 
rewardSum:191.33333333333334
loss:[46.266149044036865, 27.758044958114624]
policies:[0, 5, 2]
qAverage:[0.0, 80.01441446940105]
ws:[6.833630263805389, 11.91190799077352]
memory len:10000
memory used:3125.0
now epsilon is 0.24915658706532484, the reward is 193.33333333333334 with loss [23.788448095321655, 20.41715806722641] in episode 968
Report: 
rewardSum:193.33333333333334
loss:[23.788448095321655, 20.41715806722641]
policies:[0, 4, 1]
qAverage:[0.0, 70.45567893981934]
ws:[6.163276791572571, 11.030388861894608]
memory len:10000
memory used:3124.0
now epsilon is 0.24878308569118046, the reward is 44.33333333333334 with loss [20.377358362078667, 31.442323923110962] in episode 969
Report: 
rewardSum:44.33333333333334
loss:[20.377358362078667, 31.442323923110962]
policies:[1, 2, 3]
qAverage:[0.0, 62.900105794270836]
ws:[1.8269383907318115, 4.907993157704671]
memory len:10000
memory used:3125.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20*		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.24853439588359844, the reward is 194.33333333333334 with loss [10.07216501235962, 25.73712694644928] in episode 970
Report: 
rewardSum:194.33333333333334
loss:[10.07216501235962, 25.73712694644928]
policies:[1, 3, 0]
qAverage:[0.0, 67.20382436116536]
ws:[11.31909211476644, 17.894404729207356]
memory len:10000
memory used:3125.0
now epsilon is 0.24828595467258088, the reward is 194.33333333333334 with loss [19.380838632583618, 15.512414932250977] in episode 971
Report: 
rewardSum:194.33333333333334
loss:[19.380838632583618, 15.512414932250977]
policies:[0, 4, 0]
qAverage:[0.0, 57.26155344645182]
ws:[2.970151742299398, 5.469997406005859]
memory len:10000
memory used:3125.0
now epsilon is 0.24803776180962442, the reward is 194.33333333333334 with loss [16.560638263821602, 20.68324899673462] in episode 972
Report: 
rewardSum:194.33333333333334
loss:[16.560638263821602, 20.68324899673462]
policies:[0, 2, 2]
qAverage:[0.0, 72.37184397379558]
ws:[7.77388858795166, 12.052267710367838]
memory len:10000
memory used:3125.0
now epsilon is 0.24778981704647413, the reward is 194.33333333333334 with loss [14.829108953475952, 13.153619766235352] in episode 973
Report: 
rewardSum:194.33333333333334
loss:[14.829108953475952, 13.153619766235352]
policies:[0, 4, 0]
qAverage:[0.0, 76.73714904785156]
ws:[4.5453409314155575, 7.847300577163696]
memory len:10000
memory used:3125.0
now epsilon is 0.2474802346050894, the reward is 193.33333333333334 with loss [26.41448473930359, 19.483030080795288] in episode 974
Report: 
rewardSum:193.33333333333334
loss:[26.41448473930359, 19.483030080795288]
policies:[0, 2, 3]
qAverage:[0.0, 68.01765950520833]
ws:[8.149745305379232, 13.26456356048584]
memory len:10000
memory used:3125.0
now epsilon is 0.24717103894831574, the reward is 193.33333333333334 with loss [33.18069362640381, 19.10305619239807] in episode 975
Report: 
rewardSum:193.33333333333334
loss:[33.18069362640381, 19.10305619239807]
policies:[1, 3, 1]
qAverage:[0.0, 65.81681060791016]
ws:[4.605314056078593, 8.002678121129671]
memory len:10000
memory used:3125.0
now epsilon is 0.24692396058305982, the reward is 46.33333333333334 with loss [28.957534790039062, 22.988354682922363] in episode 976
Report: 
rewardSum:46.33333333333334
loss:[28.957534790039062, 22.988354682922363]
policies:[1, 2, 1]
qAverage:[0.0, 62.925079345703125]
ws:[1.551059405008952, 3.974792003631592]
memory len:10000
memory used:3125.0
now epsilon is 0.24667712920353022, the reward is 194.33333333333334 with loss [15.678291320800781, 24.471784591674805] in episode 977
Report: 
rewardSum:194.33333333333334
loss:[15.678291320800781, 24.471784591674805]
policies:[0, 3, 1]
qAverage:[0.0, 74.57842254638672]
ws:[5.306803688406944, 9.70661723613739]
memory len:10000
memory used:3125.0
now epsilon is 0.2464305445628338, the reward is 194.33333333333334 with loss [14.853074312210083, 13.499159812927246] in episode 978
Report: 
rewardSum:194.33333333333334
loss:[14.853074312210083, 13.499159812927246]
policies:[1, 3, 0]
qAverage:[0.0, 72.0168399810791]
ws:[6.202176362276077, 10.915005207061768]
memory len:10000
memory used:3125.0
now epsilon is 0.24618420641432426, the reward is 194.33333333333334 with loss [20.961458921432495, 30.440452098846436] in episode 979
Report: 
rewardSum:194.33333333333334
loss:[20.961458921432495, 30.440452098846436]
policies:[1, 2, 1]
qAverage:[0.0, 41.2940673828125]
ws:[1.326250672340393, 3.0765252113342285]
memory len:10000
memory used:3127.0
now epsilon is 0.2459381145116018, the reward is 46.33333333333334 with loss [14.015314936637878, 32.89301562309265] in episode 980
Report: 
rewardSum:46.33333333333334
loss:[14.015314936637878, 32.89301562309265]
policies:[1, 2, 1]
qAverage:[0.0, 56.49640655517578]
ws:[4.517843961715698, 8.657198905944824]
memory len:10000
memory used:3126.0
now epsilon is 0.24569226860851298, the reward is 46.33333333333334 with loss [18.999125719070435, 21.395626068115234] in episode 981
Report: 
rewardSum:46.33333333333334
loss:[18.999125719070435, 21.395626068115234]
policies:[0, 3, 1]
qAverage:[0.0, 68.6635856628418]
ws:[1.9863414466381073, 5.245824813842773]
memory len:10000
memory used:3126.0
now epsilon is 0.24532396046533764, the reward is 192.33333333333334 with loss [31.64347219467163, 32.716931104660034] in episode 982
Report: 
rewardSum:192.33333333333334
loss:[31.64347219467163, 32.716931104660034]
policies:[0, 5, 1]
qAverage:[0.0, 78.5625503540039]
ws:[5.070290493965149, 10.72201566696167]
memory len:10000
memory used:3126.0
now epsilon is 0.24507872848602572, the reward is 194.33333333333334 with loss [26.36321997642517, 19.046570539474487] in episode 983
Report: 
rewardSum:194.33333333333334
loss:[26.36321997642517, 19.046570539474487]
policies:[1, 3, 0]
qAverage:[0.0, 72.32782554626465]
ws:[4.468471437692642, 9.583961427211761]
memory len:10000
memory used:3126.0
now epsilon is 0.2447113400780319, the reward is 192.33333333333334 with loss [32.50250720977783, 24.710418701171875] in episode 984
Report: 
rewardSum:192.33333333333334
loss:[32.50250720977783, 24.710418701171875]
policies:[2, 2, 2]
qAverage:[0.0, 66.15254465738933]
ws:[4.812279224395752, 10.591721534729004]
memory len:10000
memory used:3126.0
now epsilon is 0.24446672048941293, the reward is 194.33333333333334 with loss [16.32271933555603, 12.30722975730896] in episode 985
Report: 
rewardSum:194.33333333333334
loss:[16.32271933555603, 12.30722975730896]
policies:[0, 2, 2]
qAverage:[0.0, 74.17100524902344]
ws:[7.43667459487915, 14.351812680562338]
memory len:10000
memory used:3126.0
now epsilon is 0.2441002495198478, the reward is 192.33333333333334 with loss [41.71943807601929, 19.576963663101196] in episode 986
Report: 
rewardSum:192.33333333333334
loss:[41.71943807601929, 19.576963663101196]
policies:[4, 1, 1]
qAverage:[0.0, 43.84577560424805]
ws:[1.539226770401001, 4.245262622833252]
memory len:10000
memory used:3126.0
now epsilon is 0.24373432791328498, the reward is 192.33333333333334 with loss [25.230825662612915, 35.4649019241333] in episode 987
Report: 
rewardSum:192.33333333333334
loss:[25.230825662612915, 35.4649019241333]
policies:[2, 1, 3]
qAverage:[0.0, 44.53218460083008]
ws:[0.5382602214813232, 2.291055679321289]
memory len:10000
memory used:3126.0
now epsilon is 0.2434298122992696, the reward is 193.33333333333334 with loss [27.407444715499878, 15.830473899841309] in episode 988
Report: 
rewardSum:193.33333333333334
loss:[27.407444715499878, 15.830473899841309]
policies:[0, 2, 3]
qAverage:[0.0, 56.92596689860026]
ws:[3.920671065648397, 8.27847162882487]
memory len:10000
memory used:3126.0
now epsilon is 0.2431864737579366, the reward is 46.33333333333334 with loss [35.209062576293945, 21.134228706359863] in episode 989
Report: 
rewardSum:46.33333333333334
loss:[35.209062576293945, 21.134228706359863]
policies:[0, 3, 1]
qAverage:[0.0, 68.52505683898926]
ws:[2.1675972640514374, 5.912725925445557]
memory len:10000
memory used:3126.0
now epsilon is 0.24288264261929216, the reward is 193.33333333333334 with loss [25.06183958053589, 16.449381351470947] in episode 990
Report: 
rewardSum:193.33333333333334
loss:[25.06183958053589, 16.449381351470947]
policies:[1, 2, 2]
qAverage:[0.0, 64.94154866536458]
ws:[4.290140484770139, 9.808551549911499]
memory len:10000
memory used:3126.0
now epsilon is 0.24263985104248464, the reward is 194.33333333333334 with loss [23.950700402259827, 16.77482169866562] in episode 991
Report: 
rewardSum:194.33333333333334
loss:[23.950700402259827, 16.77482169866562]
policies:[0, 3, 1]
qAverage:[0.0, 74.4144401550293]
ws:[3.4781227111816406, 8.404400289058685]
memory len:10000
memory used:3126.0
now epsilon is 0.2423973021662223, the reward is 194.33333333333334 with loss [19.620667695999146, 13.78038501739502] in episode 992
Report: 
rewardSum:194.33333333333334
loss:[19.620667695999146, 13.78038501739502]
policies:[0, 4, 0]
qAverage:[0.0, 77.69307250976563]
ws:[4.168699479103088, 9.456447768211365]
memory len:10000
memory used:3126.0
now epsilon is 0.2421549957478955, the reward is 194.33333333333334 with loss [15.119441866874695, 29.57430934906006] in episode 993
Report: 
rewardSum:194.33333333333334
loss:[15.119441866874695, 29.57430934906006]
policies:[0, 4, 0]
qAverage:[0.0, 77.13998107910156]
ws:[3.836668112874031, 8.213324689865113]
memory len:10000
memory used:3126.0
now epsilon is 0.2419129315451373, the reward is 194.33333333333334 with loss [19.597025454044342, 16.252988159656525] in episode 994
Report: 
rewardSum:194.33333333333334
loss:[19.597025454044342, 16.252988159656525]
policies:[1, 3, 0]
qAverage:[0.0, 51.410987854003906]
ws:[2.1584534645080566, 4.716559886932373]
memory len:10000
memory used:3126.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21*		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.24148990129339293, the reward is 191.33333333333334 with loss [27.112081289291382, 23.433620929718018] in episode 995
Report: 
rewardSum:191.33333333333334
loss:[27.112081289291382, 23.433620929718018]
policies:[0, 5, 2]
qAverage:[0.0, 72.10314178466797]
ws:[3.927957034111023, 8.245538806915283]
memory len:10000
memory used:3126.0
now epsilon is 0.24118818981023646, the reward is 193.33333333333334 with loss [28.225302934646606, 23.879226207733154] in episode 996
Report: 
rewardSum:193.33333333333334
loss:[28.225302934646606, 23.879226207733154]
policies:[0, 3, 2]
qAverage:[0.0, 71.0304069519043]
ws:[7.01692396402359, 13.19244110584259]
memory len:10000
memory used:3126.0
now epsilon is 0.2409470920509241, the reward is 194.33333333333334 with loss [45.33625602722168, 14.745640754699707] in episode 997
Report: 
rewardSum:194.33333333333334
loss:[45.33625602722168, 14.745640754699707]
policies:[0, 4, 0]
qAverage:[0.0, 78.95977783203125]
ws:[4.557784247398376, 9.461595630645752]
memory len:10000
memory used:3126.0
now epsilon is 0.24070623529897447, the reward is 194.33333333333334 with loss [24.826438426971436, 18.565244913101196] in episode 998
Report: 
rewardSum:194.33333333333334
loss:[24.826438426971436, 18.565244913101196]
policies:[1, 2, 1]
qAverage:[0.0, 64.94800313313802]
ws:[4.921251714229584, 10.43351697921753]
memory len:10000
memory used:3126.0
now epsilon is 0.24046561931347057, the reward is 194.33333333333334 with loss [14.319887042045593, 23.90681552886963] in episode 999
Report: 
rewardSum:194.33333333333334
loss:[14.319887042045593, 23.90681552886963]
policies:[0, 4, 0]
qAverage:[0.0, 78.90700225830078]
ws:[5.310307529568672, 11.471621227264404]
memory len:10000
memory used:3126.0
now epsilon is 0.24022524385373623, the reward is 194.33333333333334 with loss [15.811739444732666, 18.90092706680298] in episode 1000
Report: 
rewardSum:194.33333333333334
loss:[15.811739444732666, 18.90092706680298]
policies:[1, 3, 0]
qAverage:[0.0, 40.273441314697266]
ws:[-0.298503577709198, 0.9427580833435059]
memory len:10000
memory used:3126.0
now epsilon is 0.239925112402166, the reward is 193.33333333333334 with loss [34.07318830490112, 23.03902840614319] in episode 1001
Report: 
rewardSum:193.33333333333334
loss:[34.07318830490112, 23.03902840614319]
policies:[0, 3, 2]
qAverage:[0.0, 77.28257179260254]
ws:[3.622988611459732, 8.765679657459259]
memory len:10000
memory used:3126.0
now epsilon is 0.23968527724668662, the reward is 46.33333333333334 with loss [17.89962649345398, 31.308080434799194] in episode 1002
Report: 
rewardSum:46.33333333333334
loss:[17.89962649345398, 31.308080434799194]
policies:[1, 1, 2]
qAverage:[0.0, 48.256465911865234]
ws:[0.7331703305244446, 3.142613172531128]
memory len:10000
memory used:3126.0
now epsilon is 0.23932597396087643, the reward is 192.33333333333334 with loss [28.275623083114624, 35.570534229278564] in episode 1003
Report: 
rewardSum:192.33333333333334
loss:[28.275623083114624, 35.570534229278564]
policies:[0, 4, 2]
qAverage:[0.0, 81.08476257324219]
ws:[5.353843611478806, 11.247852516174316]
memory len:10000
memory used:3126.0
now epsilon is 0.23908673771919886, the reward is 194.33333333333334 with loss [14.24869966506958, 14.205337762832642] in episode 1004
Report: 
rewardSum:194.33333333333334
loss:[14.24869966506958, 14.205337762832642]
policies:[0, 3, 1]
qAverage:[0.0, 80.42074012756348]
ws:[5.939189970493317, 11.718523621559143]
memory len:10000
memory used:3126.0
now epsilon is 0.23884774062406433, the reward is 194.33333333333334 with loss [14.884350538253784, 22.48012375831604] in episode 1005
Report: 
rewardSum:194.33333333333334
loss:[14.884350538253784, 22.48012375831604]
policies:[0, 4, 0]
qAverage:[0.0, 81.79141044616699]
ws:[8.455999970436096, 16.11666750907898]
memory len:10000
memory used:3144.0
now epsilon is 0.23860898243641598, the reward is 194.33333333333334 with loss [10.553035140037537, 30.785996675491333] in episode 1006
Report: 
rewardSum:194.33333333333334
loss:[10.553035140037537, 30.785996675491333]
policies:[1, 3, 0]
qAverage:[0.0, 80.56964492797852]
ws:[5.818227648735046, 10.970108270645142]
memory len:10000
memory used:3151.0
now epsilon is 0.23831087030170653, the reward is 193.33333333333334 with loss [31.120587587356567, 20.324496269226074] in episode 1007
Report: 
rewardSum:193.33333333333334
loss:[31.120587587356567, 20.324496269226074]
policies:[0, 4, 1]
qAverage:[0.0, 81.76748504638672]
ws:[5.0628090620040895, 10.0976016163826]
memory len:10000
memory used:3151.0
now epsilon is 0.2380726487830877, the reward is 194.33333333333334 with loss [21.967918157577515, 31.825520038604736] in episode 1008
Report: 
rewardSum:194.33333333333334
loss:[21.967918157577515, 31.825520038604736]
policies:[1, 3, 0]
qAverage:[0.0, 69.16330973307292]
ws:[4.573360621929169, 9.885444601376852]
memory len:10000
memory used:3151.0
now epsilon is 0.23777520673032015, the reward is 193.33333333333334 with loss [24.120359897613525, 16.803855776786804] in episode 1009
Report: 
rewardSum:193.33333333333334
loss:[24.120359897613525, 16.803855776786804]
policies:[0, 1, 4]
qAverage:[0.0, 46.31718826293945]
ws:[5.682047367095947, 11.651979446411133]
memory len:10000
memory used:3151.0
now epsilon is 0.23753752067443235, the reward is 194.33333333333334 with loss [27.394141912460327, 20.602052807807922] in episode 1010
Report: 
rewardSum:194.33333333333334
loss:[27.394141912460327, 20.602052807807922]
policies:[0, 4, 0]
qAverage:[0.0, 75.5880069732666]
ws:[4.798404224216938, 10.07267439365387]
memory len:10000
memory used:3151.0
now epsilon is 0.237300072215483, the reward is 194.33333333333334 with loss [26.451249599456787, 21.794788002967834] in episode 1011
Report: 
rewardSum:194.33333333333334
loss:[26.451249599456787, 21.794788002967834]
policies:[1, 3, 0]
qAverage:[0.0, 76.23540878295898]
ws:[5.122404724359512, 10.667349219322205]
memory len:10000
memory used:3112.0
now epsilon is 0.23706286111596428, the reward is 194.33333333333334 with loss [23.549055337905884, 18.526066780090332] in episode 1012
Report: 
rewardSum:194.33333333333334
loss:[23.549055337905884, 18.526066780090332]
policies:[0, 4, 0]
qAverage:[0.0, 85.72262420654297]
ws:[6.7656977534294125, 14.142636346817017]
memory len:10000
memory used:3118.0
now epsilon is 0.23682588713860575, the reward is 194.33333333333334 with loss [18.505652904510498, 16.336617469787598] in episode 1013
Report: 
rewardSum:194.33333333333334
loss:[18.505652904510498, 16.336617469787598]
policies:[0, 2, 2]
qAverage:[0.0, 72.98487599690755]
ws:[11.572120984395346, 21.150487263997395]
memory len:10000
memory used:3125.0
now epsilon is 0.23658915004637418, the reward is 194.33333333333334 with loss [22.000539302825928, 17.44698929786682] in episode 1014
Report: 
rewardSum:194.33333333333334
loss:[22.000539302825928, 17.44698929786682]
policies:[1, 2, 1]
qAverage:[0.0, 71.25985209147136]
ws:[7.912678241729736, 15.888851801554361]
memory len:10000
memory used:3125.0
now epsilon is 0.2362344880497126, the reward is 192.33333333333334 with loss [25.20328640937805, 24.08435559272766] in episode 1015
Report: 
rewardSum:192.33333333333334
loss:[25.20328640937805, 24.08435559272766]
policies:[0, 5, 1]
qAverage:[0.0, 83.58835474650066]
ws:[5.3342662652333575, 12.233959515889486]
memory len:10000
memory used:3144.0
now epsilon is 0.23588035771366117, the reward is 192.33333333333334 with loss [33.447086811065674, 24.128518104553223] in episode 1016
Report: 
rewardSum:192.33333333333334
loss:[33.447086811065674, 24.128518104553223]
policies:[0, 5, 1]
qAverage:[0.0, 81.7875762939453]
ws:[4.252934771776199, 10.082564640045167]
memory len:10000
memory used:3144.0
now epsilon is 0.23564456579634008, the reward is 194.33333333333334 with loss [28.855995178222656, 18.483729541301727] in episode 1017
Report: 
rewardSum:194.33333333333334
loss:[28.855995178222656, 18.483729541301727]
policies:[0, 3, 1]
qAverage:[0.0, 74.629150390625]
ws:[8.310959815979004, 16.443973541259766]
memory len:10000
memory used:3143.0
now epsilon is 0.23529131979080092, the reward is 192.33333333333334 with loss [32.87647747993469, 28.279218912124634] in episode 1018
Report: 
rewardSum:192.33333333333334
loss:[32.87647747993469, 28.279218912124634]
policies:[1, 3, 2]
qAverage:[0.0, 77.80907821655273]
ws:[5.192536473274231, 10.637977987527847]
memory len:10000
memory used:3143.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23*		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.2350561166905503, the reward is 194.33333333333334 with loss [41.08255195617676, 15.765584468841553] in episode 1019
Report: 
rewardSum:194.33333333333334
loss:[41.08255195617676, 15.765584468841553]
policies:[0, 4, 0]
qAverage:[0.0, 80.26829528808594]
ws:[7.833535134792328, 14.21263301372528]
memory len:10000
memory used:3143.0
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.23458641559976362, the reward is 190.33333333333334 with loss [45.81610679626465, 45.61956214904785] in episode 1020
Report: 
rewardSum:190.33333333333334
loss:[45.81610679626465, 45.61956214904785]
policies:[1, 5, 2]
qAverage:[0.0, 82.72330627441406]
ws:[6.631393909454346, 12.281702041625977]
memory len:10000
memory used:3143.0
now epsilon is 0.234351917139409, the reward is 194.33333333333334 with loss [17.763814449310303, 19.433056235313416] in episode 1021
Report: 
rewardSum:194.33333333333334
loss:[17.763814449310303, 19.433056235313416]
policies:[1, 3, 0]
qAverage:[0.0, 86.25675964355469]
ws:[7.039296209812164, 13.173916101455688]
memory len:10000
memory used:3145.0
now epsilon is 0.23411765308959245, the reward is 194.33333333333334 with loss [14.007175087928772, 13.051528692245483] in episode 1022
Report: 
rewardSum:194.33333333333334
loss:[14.007175087928772, 13.051528692245483]
policies:[1, 3, 0]
qAverage:[0.0, 86.40546417236328]
ws:[7.0437469482421875, 13.056000709533691]
memory len:10000
memory used:3145.0
now epsilon is 0.2336498272845173, the reward is 190.33333333333334 with loss [34.07016837596893, 34.83323073387146] in episode 1023
Report: 
rewardSum:190.33333333333334
loss:[34.07016837596893, 34.83323073387146]
policies:[1, 4, 3]
qAverage:[0.0, 84.50315399169922]
ws:[5.676255178451538, 11.398498439788819]
memory len:10000
memory used:3145.0
now epsilon is 0.23341626506131585, the reward is 46.33333333333334 with loss [22.906378746032715, 21.741073846817017] in episode 1024
Report: 
rewardSum:46.33333333333334
loss:[22.906378746032715, 21.741073846817017]
policies:[0, 3, 1]
qAverage:[0.0, 74.65931510925293]
ws:[5.904010355472565, 11.646416425704956]
memory len:10000
memory used:3145.0
now epsilon is 0.23318293631276638, the reward is 194.33333333333334 with loss [22.78327226638794, 22.111467123031616] in episode 1025
Report: 
rewardSum:194.33333333333334
loss:[22.78327226638794, 22.111467123031616]
policies:[0, 4, 0]
qAverage:[0.0, 79.52318954467773]
ws:[9.275247991085052, 16.883386254310608]
memory len:10000
memory used:3145.0
now epsilon is 0.23294984080548173, the reward is 46.33333333333334 with loss [21.40745186805725, 30.10927963256836] in episode 1026
Report: 
rewardSum:46.33333333333334
loss:[21.40745186805725, 30.10927963256836]
policies:[0, 3, 1]
qAverage:[0.0, 76.2719554901123]
ws:[4.143766283988953, 8.313329577445984]
memory len:10000
memory used:3145.0
now epsilon is 0.2327169783063081, the reward is 194.33333333333334 with loss [20.158085346221924, 14.824869871139526] in episode 1027
Report: 
rewardSum:194.33333333333334
loss:[20.158085346221924, 14.824869871139526]
policies:[0, 4, 0]
qAverage:[0.0, 85.68238830566406]
ws:[8.489514172077179, 16.439433097839355]
memory len:10000
memory used:3106.0
now epsilon is 0.23242622749517922, the reward is 193.33333333333334 with loss [26.046167612075806, 19.908292412757874] in episode 1028
Report: 
rewardSum:193.33333333333334
loss:[26.046167612075806, 19.908292412757874]
policies:[0, 4, 1]
qAverage:[0.0, 86.90013275146484]
ws:[8.754983770847321, 17.02946834564209]
memory len:10000
memory used:3125.0
now epsilon is 0.23207780598090516, the reward is 192.33333333333334 with loss [26.614398956298828, 38.725805044174194] in episode 1029
Report: 
rewardSum:192.33333333333334
loss:[26.614398956298828, 38.725805044174194]
policies:[0, 5, 1]
qAverage:[0.0, 79.94648551940918]
ws:[8.250713467597961, 14.495070219039917]
memory len:10000
memory used:3131.0
now epsilon is 0.23184581518959754, the reward is 194.33333333333334 with loss [21.819470405578613, 16.83728015422821] in episode 1030
Report: 
rewardSum:194.33333333333334
loss:[21.819470405578613, 16.83728015422821]
policies:[0, 3, 1]
qAverage:[0.0, 76.3815180460612]
ws:[10.676619529724121, 19.345019658406574]
memory len:10000
memory used:3131.0
now epsilon is 0.23161405630209922, the reward is 194.33333333333334 with loss [23.149868965148926, 32.94145655632019] in episode 1031
Report: 
rewardSum:194.33333333333334
loss:[23.149868965148926, 32.94145655632019]
policies:[0, 4, 0]
qAverage:[0.0, 88.49534797668457]
ws:[8.517006278038025, 16.963810920715332]
memory len:10000
memory used:3132.0
now epsilon is 0.23138252908659326, the reward is 194.33333333333334 with loss [31.724663734436035, 13.744009971618652] in episode 1032
Report: 
rewardSum:194.33333333333334
loss:[31.724663734436035, 13.744009971618652]
policies:[0, 4, 0]
qAverage:[0.0, 77.55657196044922]
ws:[4.01036773622036, 10.021425008773804]
memory len:10000
memory used:3132.0
now epsilon is 0.23115123331149456, the reward is 194.33333333333334 with loss [15.099339127540588, 13.223431825637817] in episode 1033
Report: 
rewardSum:194.33333333333334
loss:[15.099339127540588, 13.223431825637817]
policies:[0, 4, 0]
qAverage:[0.0, 80.15420913696289]
ws:[4.5823615193367, 11.606861591339111]
memory len:10000
memory used:3132.0
now epsilon is 0.23092016874544954, the reward is 194.33333333333334 with loss [23.657516479492188, 16.68035912513733] in episode 1034
Report: 
rewardSum:194.33333333333334
loss:[23.657516479492188, 16.68035912513733]
policies:[0, 4, 0]
qAverage:[0.0, 90.8150390625]
ws:[8.467360544204713, 17.20486183166504]
memory len:10000
memory used:3132.0
now epsilon is 0.2306893351573358, the reward is 46.33333333333334 with loss [31.32791566848755, 15.360886454582214] in episode 1035
Report: 
rewardSum:46.33333333333334
loss:[31.32791566848755, 15.360886454582214]
policies:[0, 3, 1]
qAverage:[0.0, 79.53839111328125]
ws:[3.9553928077220917, 9.944428205490112]
memory len:10000
memory used:3132.0
now epsilon is 0.230458732316262, the reward is 194.33333333333334 with loss [15.56191897392273, 18.498655796051025] in episode 1036
Report: 
rewardSum:194.33333333333334
loss:[15.56191897392273, 18.498655796051025]
policies:[0, 3, 1]
qAverage:[0.0, 83.79806327819824]
ws:[8.532824706286192, 17.753143787384033]
memory len:10000
memory used:3132.0
now epsilon is 0.2302283599915676, the reward is 194.33333333333334 with loss [16.95476198196411, 30.062833309173584] in episode 1037
Report: 
rewardSum:194.33333333333334
loss:[16.95476198196411, 30.062833309173584]
policies:[0, 3, 1]
qAverage:[0.0, 77.33542442321777]
ws:[2.6733580827713013, 8.32617437839508]
memory len:10000
memory used:3132.0
now epsilon is 0.2299982179528227, the reward is 194.33333333333334 with loss [20.895756244659424, 21.193838834762573] in episode 1038
Report: 
rewardSum:194.33333333333334
loss:[20.895756244659424, 21.193838834762573]
policies:[0, 3, 1]
qAverage:[0.0, 89.47858047485352]
ws:[8.75046506524086, 17.161039113998413]
memory len:10000
memory used:3092.0
now epsilon is 0.22965343617736186, the reward is 192.33333333333334 with loss [41.740753173828125, 36.2573606967926] in episode 1039
Report: 
rewardSum:192.33333333333334
loss:[41.740753173828125, 36.2573606967926]
policies:[0, 4, 2]
qAverage:[0.0, 81.9434871673584]
ws:[5.645286381244659, 12.239473581314087]
memory len:10000
memory used:3098.0
now epsilon is 0.22942386884687066, the reward is 46.33333333333334 with loss [18.640000343322754, 15.606464147567749] in episode 1040
Report: 
rewardSum:46.33333333333334
loss:[18.640000343322754, 15.606464147567749]
policies:[1, 2, 1]
qAverage:[0.0, 73.68303934733073]
ws:[6.607048352559407, 13.996084531148275]
memory len:10000
memory used:3098.0
now epsilon is 0.22907994805679593, the reward is 192.33333333333334 with loss [29.52671718597412, 25.58867597579956] in episode 1041
Report: 
rewardSum:192.33333333333334
loss:[29.52671718597412, 25.58867597579956]
policies:[0, 5, 1]
qAverage:[0.0, 89.92986551920573]
ws:[5.505928022166093, 11.442002614339193]
memory len:10000
memory used:3100.0
now epsilon is 0.22885095399940308, the reward is 194.33333333333334 with loss [17.618473052978516, 33.81372308731079] in episode 1042
Report: 
rewardSum:194.33333333333334
loss:[17.618473052978516, 33.81372308731079]
policies:[0, 4, 0]
qAverage:[0.0, 83.15221405029297]
ws:[7.302498251199722, 12.84854593873024]
memory len:10000
memory used:3100.0
now epsilon is 0.22862218885020918, the reward is 194.33333333333334 with loss [17.583404064178467, 27.62242031097412] in episode 1043
Report: 
rewardSum:194.33333333333334
loss:[17.583404064178467, 27.62242031097412]
policies:[0, 4, 0]
qAverage:[0.0, 86.81929524739583]
ws:[11.768036047617594, 19.264569918314617]
memory len:10000
memory used:3106.0
############# STATE ###############
0-		9-		18*		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.22839365238039183, the reward is 194.33333333333334 with loss [19.79590082168579, 22.65100336074829] in episode 1044
Report: 
rewardSum:194.33333333333334
loss:[19.79590082168579, 22.65100336074829]
policies:[0, 4, 0]
qAverage:[0.0, 94.16189575195312]
ws:[6.640811705589295, 11.74388097524643]
memory len:10000
memory used:3106.0
now epsilon is 0.22793726456474078, the reward is 190.33333333333334 with loss [45.85150706768036, 29.710486888885498] in episode 1045
Report: 
rewardSum:190.33333333333334
loss:[45.85150706768036, 29.710486888885498]
policies:[1, 4, 3]
qAverage:[0.0, 87.6188461303711]
ws:[9.350696802139282, 16.211893248558045]
memory len:10000
memory used:3106.0
now epsilon is 0.22753867339479023, the reward is 191.33333333333334 with loss [36.86474418640137, 40.08381223678589] in episode 1046
Report: 
rewardSum:191.33333333333334
loss:[36.86474418640137, 40.08381223678589]
policies:[2, 3, 2]
qAverage:[0.0, 72.46113840738933]
ws:[1.3185103336970012, 2.5342655976613364]
memory len:10000
memory used:3106.0
now epsilon is 0.2273112200341777, the reward is 194.33333333333334 with loss [17.404497861862183, 22.392260313034058] in episode 1047
Report: 
rewardSum:194.33333333333334
loss:[17.404497861862183, 22.392260313034058]
policies:[1, 3, 0]
qAverage:[18.07749786376953, 73.23061981201172]
ws:[7.680959177017212, 11.496393871307372]
memory len:10000
memory used:3106.0
now epsilon is 0.22708399404164498, the reward is 194.33333333333334 with loss [20.894670248031616, 37.995781898498535] in episode 1048
Report: 
rewardSum:194.33333333333334
loss:[20.894670248031616, 37.995781898498535]
policies:[0, 4, 0]
qAverage:[0.0, 93.85189971923828]
ws:[8.295812003314495, 12.476051524281502]
memory len:10000
memory used:3105.0
now epsilon is 0.2268002809411118, the reward is 193.33333333333334 with loss [11.812741756439209, 29.636972427368164] in episode 1049
Report: 
rewardSum:193.33333333333334
loss:[11.812741756439209, 29.636972427368164]
policies:[1, 3, 1]
qAverage:[0.0, 70.43890380859375]
ws:[15.921745300292969, 21.745948791503906]
memory len:10000
memory used:3105.0
now epsilon is 0.22657356569610193, the reward is 194.33333333333334 with loss [19.349097728729248, 18.406546592712402] in episode 1050
Report: 
rewardSum:194.33333333333334
loss:[19.349097728729248, 18.406546592712402]
policies:[0, 4, 0]
qAverage:[0.0, 91.88298988342285]
ws:[10.14474105834961, 14.873693108558655]
memory len:10000
memory used:3105.0
now epsilon is 0.22634707708133303, the reward is 194.33333333333334 with loss [14.937213659286499, 29.28181505203247] in episode 1051
Report: 
rewardSum:194.33333333333334
loss:[14.937213659286499, 29.28181505203247]
policies:[0, 4, 0]
qAverage:[0.0, 93.47630615234375]
ws:[8.650377404689788, 14.84063550978899]
memory len:10000
memory used:3105.0
now epsilon is 0.22612081487025978, the reward is 194.33333333333334 with loss [29.513214349746704, 25.781575679779053] in episode 1052
Report: 
rewardSum:194.33333333333334
loss:[29.513214349746704, 25.781575679779053]
policies:[0, 4, 0]
qAverage:[0.0, 91.74954528808594]
ws:[9.68946385383606, 16.542105770111085]
memory len:10000
memory used:3105.0
now epsilon is 0.22589477883656342, the reward is 194.33333333333334 with loss [18.248770713806152, 18.854355096817017] in episode 1053
Report: 
rewardSum:194.33333333333334
loss:[18.248770713806152, 18.854355096817017]
policies:[0, 4, 0]
qAverage:[0.0, 93.02190780639648]
ws:[11.401766657829285, 18.805928230285645]
memory len:10000
memory used:3105.0
now epsilon is 0.22572540010417746, the reward is -2.0 with loss [9.182289361953735, 14.2553551197052] in episode 1054
Report: 
rewardSum:-2.0
loss:[9.182289361953735, 14.2553551197052]
policies:[0, 1, 2]
qAverage:[0.0, 45.97315216064453]
ws:[1.154537558555603, 2.625033140182495]
memory len:10000
memory used:3106.0
now epsilon is 0.2254997593369914, the reward is 194.33333333333334 with loss [18.899767875671387, 12.611126899719238] in episode 1055
Report: 
rewardSum:194.33333333333334
loss:[18.899767875671387, 12.611126899719238]
policies:[0, 3, 1]
qAverage:[0.0, 78.93502044677734]
ws:[4.382560729980469, 9.96255874633789]
memory len:10000
memory used:3106.0
now epsilon is 0.22527434412597133, the reward is 194.33333333333334 with loss [31.39125680923462, 20.452025890350342] in episode 1056
Report: 
rewardSum:194.33333333333334
loss:[31.39125680923462, 20.452025890350342]
policies:[0, 3, 1]
qAverage:[0.0, 78.09103139241536]
ws:[2.585724194844564, 7.245587348937988]
memory len:10000
memory used:3106.0
now epsilon is 0.2250491542456457, the reward is 194.33333333333334 with loss [20.33456027507782, 21.622231006622314] in episode 1057
Report: 
rewardSum:194.33333333333334
loss:[20.33456027507782, 21.622231006622314]
policies:[1, 2, 1]
qAverage:[0.0, 87.01236724853516]
ws:[12.401492754618326, 21.052165349324543]
memory len:10000
memory used:3106.0
now epsilon is 0.22482418947076824, the reward is -3.0 with loss [14.573062300682068, 16.304166078567505] in episode 1058
Report: 
rewardSum:-3.0
loss:[14.573062300682068, 16.304166078567505]
policies:[0, 2, 2]
qAverage:[0.0, 68.14461008707683]
ws:[3.214931090672811, 6.57498820622762]
memory len:10000
memory used:3106.0
now epsilon is 0.22443104209802311, the reward is 191.33333333333334 with loss [39.693840742111206, 53.78965538740158] in episode 1059
Report: 
rewardSum:191.33333333333334
loss:[39.693840742111206, 53.78965538740158]
policies:[1, 4, 2]
qAverage:[0.0, 91.31831207275391]
ws:[8.732700085639953, 15.184266757965087]
memory len:10000
memory used:3106.0
now epsilon is 0.22431884060391424, the reward is -1.0 with loss [5.673102140426636, 16.457077980041504] in episode 1060
Report: 
rewardSum:-1.0
loss:[5.673102140426636, 16.457077980041504]
policies:[0, 1, 1]
qAverage:[0.0, 48.919273376464844]
ws:[-0.12088041007518768, 0.8614441156387329]
memory len:10000
memory used:3106.0
now epsilon is 0.223982572571835, the reward is 192.33333333333334 with loss [38.349504232406616, 25.917054653167725] in episode 1061
Report: 
rewardSum:192.33333333333334
loss:[38.349504232406616, 25.917054653167725]
policies:[2, 2, 2]
qAverage:[0.0, 74.46781921386719]
ws:[5.872807135184606, 10.54517674446106]
memory len:10000
memory used:3106.0
now epsilon is 0.22375867397872984, the reward is 194.33333333333334 with loss [15.28292179107666, 21.31335461139679] in episode 1062
Report: 
rewardSum:194.33333333333334
loss:[15.28292179107666, 21.31335461139679]
policies:[1, 2, 1]
qAverage:[0.0, 82.97506968180339]
ws:[14.223475456237793, 23.2805913289388]
memory len:10000
memory used:3106.0
now epsilon is 0.22353499920026984, the reward is 194.33333333333334 with loss [15.929895401000977, 20.130948543548584] in episode 1063
Report: 
rewardSum:194.33333333333334
loss:[15.929895401000977, 20.130948543548584]
policies:[0, 3, 1]
qAverage:[0.0, 85.22356033325195]
ws:[9.751749157905579, 16.296536087989807]
memory len:10000
memory used:3107.0
now epsilon is 0.22331154801272424, the reward is 46.33333333333334 with loss [13.362170100212097, 11.552526354789734] in episode 1064
Report: 
rewardSum:46.33333333333334
loss:[13.362170100212097, 11.552526354789734]
policies:[0, 3, 1]
qAverage:[0.0, 83.3272647857666]
ws:[3.037588208913803, 7.879481196403503]
memory len:10000
memory used:3107.0
now epsilon is 0.22308832019258593, the reward is 194.33333333333334 with loss [18.032768726348877, 32.080737590789795] in episode 1065
Report: 
rewardSum:194.33333333333334
loss:[18.032768726348877, 32.080737590789795]
policies:[1, 3, 0]
qAverage:[0.0, 93.95930862426758]
ws:[11.616325616836548, 19.375307083129883]
memory len:10000
memory used:3107.0
now epsilon is 0.22286531551657127, the reward is 46.33333333333334 with loss [36.34732484817505, 26.87897753715515] in episode 1066
Report: 
rewardSum:46.33333333333334
loss:[36.34732484817505, 26.87897753715515]
policies:[0, 2, 2]
qAverage:[0.0, 71.14182027180989]
ws:[1.6547480821609497, 4.508158047993978]
memory len:10000
memory used:3107.0
now epsilon is 0.22258687312817946, the reward is 193.33333333333334 with loss [40.7247109413147, 26.671844482421875] in episode 1067
Report: 
rewardSum:193.33333333333334
loss:[40.7247109413147, 26.671844482421875]
policies:[0, 4, 1]
qAverage:[0.0, 88.49233754475911]
ws:[9.922114292780558, 16.79154109954834]
memory len:10000
memory used:3107.0
now epsilon is 0.22236436971121792, the reward is 194.33333333333334 with loss [28.911205053329468, 12.775296568870544] in episode 1068
Report: 
rewardSum:194.33333333333334
loss:[28.911205053329468, 12.775296568870544]
policies:[0, 4, 0]
qAverage:[0.0, 96.05926513671875]
ws:[4.852566438913345, 9.789876055717468]
memory len:10000
memory used:3107.0
now epsilon is 0.22225320142413543, the reward is -1.0 with loss [7.500772714614868, 7.286556720733643] in episode 1069
Report: 
rewardSum:-1.0
loss:[7.500772714614868, 7.286556720733643]
policies:[0, 1, 1]
qAverage:[0.0, 49.441585540771484]
ws:[-1.3659658432006836, -0.49153146147727966]
memory len:10000
memory used:3107.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2*		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.2219755237958835, the reward is 193.33333333333334 with loss [16.34518551826477, 26.619619131088257] in episode 1070
Report: 
rewardSum:193.33333333333334
loss:[16.34518551826477, 26.619619131088257]
policies:[1, 3, 1]
qAverage:[0.0, 57.447303771972656]
ws:[6.935220241546631, 12.842377662658691]
memory len:10000
memory used:3107.0
now epsilon is 0.2216427685428889, the reward is 192.33333333333334 with loss [27.296025156974792, 25.76952624320984] in episode 1071
Report: 
rewardSum:192.33333333333334
loss:[27.296025156974792, 25.76952624320984]
policies:[0, 5, 1]
qAverage:[0.0, 95.86552581787109]
ws:[9.072434234619141, 14.607429218292236]
memory len:10000
memory used:3107.0
now epsilon is 0.22142120887653244, the reward is 194.33333333333334 with loss [18.989322662353516, 16.91236388683319] in episode 1072
Report: 
rewardSum:194.33333333333334
loss:[18.989322662353516, 16.91236388683319]
policies:[1, 3, 0]
qAverage:[0.0, 84.10162925720215]
ws:[2.514266163110733, 5.028706431388855]
memory len:10000
memory used:3107.0
now epsilon is 0.22108928457641985, the reward is 192.33333333333334 with loss [31.62309217453003, 34.683056354522705] in episode 1073
Report: 
rewardSum:192.33333333333334
loss:[31.62309217453003, 34.683056354522705]
policies:[1, 3, 2]
qAverage:[0.0, 90.2291488647461]
ws:[3.2137821316719055, 6.645730018615723]
memory len:10000
memory used:3107.0
now epsilon is 0.22086827818650795, the reward is 194.33333333333334 with loss [16.19502067565918, 21.617122650146484] in episode 1074
Report: 
rewardSum:194.33333333333334
loss:[16.19502067565918, 21.617122650146484]
policies:[1, 3, 0]
qAverage:[0.0, 93.8912353515625]
ws:[11.727269291877747, 18.600443124771118]
memory len:10000
memory used:3107.0
now epsilon is 0.22048204846853958, the reward is 191.33333333333334 with loss [28.17740297317505, 27.302488923072815] in episode 1075
Report: 
rewardSum:191.33333333333334
loss:[28.17740297317505, 27.302488923072815]
policies:[1, 4, 2]
qAverage:[0.0, 97.09811248779297]
ws:[11.2258695602417, 17.8116943359375]
memory len:10000
memory used:3107.0
now epsilon is 0.22026164908706, the reward is 194.33333333333334 with loss [25.87717628479004, 23.338914394378662] in episode 1076
Report: 
rewardSum:194.33333333333334
loss:[25.87717628479004, 23.338914394378662]
policies:[1, 2, 1]
qAverage:[0.0, 78.46665445963542]
ws:[10.903659900029501, 15.741208553314209]
memory len:10000
memory used:3107.0
now epsilon is 0.21993146303990663, the reward is 192.33333333333334 with loss [26.12170386314392, 35.2020537853241] in episode 1077
Report: 
rewardSum:192.33333333333334
loss:[26.12170386314392, 35.2020537853241]
policies:[0, 5, 1]
qAverage:[0.0, 86.97091102600098]
ws:[9.923223555088043, 14.785163283348083]
memory len:10000
memory used:3107.0
now epsilon is 0.21965668613391118, the reward is 193.33333333333334 with loss [40.719480752944946, 20.265330910682678] in episode 1078
Report: 
rewardSum:193.33333333333334
loss:[40.719480752944946, 20.265330910682678]
policies:[0, 3, 2]
qAverage:[0.0, 90.8598861694336]
ws:[8.484989047050476, 12.807550311088562]
memory len:10000
memory used:3107.0
now epsilon is 0.2194371118053069, the reward is 194.33333333333334 with loss [16.530739545822144, 27.57487988471985] in episode 1079
Report: 
rewardSum:194.33333333333334
loss:[16.530739545822144, 27.57487988471985]
policies:[0, 4, 0]
qAverage:[0.0, 93.79823608398438]
ws:[6.297694754600525, 10.522126412391662]
memory len:10000
memory used:3107.0
now epsilon is 0.21921775696870457, the reward is 194.33333333333334 with loss [22.96208930015564, 17.29342031478882] in episode 1080
Report: 
rewardSum:194.33333333333334
loss:[22.96208930015564, 17.29342031478882]
policies:[1, 2, 1]
qAverage:[0.0, 85.75002034505208]
ws:[12.040152390797934, 18.354594230651855]
memory len:10000
memory used:3107.0
now epsilon is 0.21899862140469453, the reward is 194.33333333333334 with loss [17.30878782272339, 22.88110065460205] in episode 1081
Report: 
rewardSum:194.33333333333334
loss:[17.30878782272339, 22.88110065460205]
policies:[1, 2, 1]
qAverage:[0.0, 73.78501383463542]
ws:[2.6003130276997886, 5.232664267222087]
memory len:10000
memory used:3112.0
now epsilon is 0.21877970489408632, the reward is 194.33333333333334 with loss [22.909603536128998, 26.045331239700317] in episode 1082
Report: 
rewardSum:194.33333333333334
loss:[22.909603536128998, 26.045331239700317]
policies:[0, 3, 1]
qAverage:[0.0, 82.12521107991536]
ws:[10.976128896077475, 16.07456684112549]
memory len:10000
memory used:3112.0
now epsilon is 0.21856100721790872, the reward is 194.33333333333334 with loss [16.233713388442993, 30.531028509140015] in episode 1083
Report: 
rewardSum:194.33333333333334
loss:[16.233713388442993, 30.531028509140015]
policies:[0, 4, 0]
qAverage:[0.0, 83.08495903015137]
ws:[2.3326070308685303, 5.042466759681702]
memory len:10000
memory used:3119.0
now epsilon is 0.21834252815740934, the reward is 194.33333333333334 with loss [19.24658465385437, 24.637943983078003] in episode 1084
Report: 
rewardSum:194.33333333333334
loss:[19.24658465385437, 24.637943983078003]
policies:[0, 4, 0]
qAverage:[0.0, 89.20406532287598]
ws:[9.236306428909302, 14.145334541797638]
memory len:10000
memory used:3120.0
now epsilon is 0.21812426749405447, the reward is 194.33333333333334 with loss [23.6673002243042, 21.270440816879272] in episode 1085
Report: 
rewardSum:194.33333333333334
loss:[23.6673002243042, 21.270440816879272]
policies:[1, 3, 0]
qAverage:[0.0, 90.01277542114258]
ws:[10.023433327674866, 15.892857789993286]
memory len:10000
memory used:3120.0
now epsilon is 0.2180152189930742, the reward is -1.0 with loss [10.622925758361816, 8.7531418800354] in episode 1086
Report: 
rewardSum:-1.0
loss:[10.622925758361816, 8.7531418800354]
policies:[0, 1, 1]
qAverage:[0.0, 50.179100036621094]
ws:[0.4256281852722168, 1.5941334962844849]
memory len:10000
memory used:3120.0
now epsilon is 0.21779728551616317, the reward is 194.33333333333334 with loss [24.998095989227295, 25.971049785614014] in episode 1087
Report: 
rewardSum:194.33333333333334
loss:[24.998095989227295, 25.971049785614014]
policies:[0, 4, 0]
qAverage:[0.0, 71.13873291015625]
ws:[0.8777948220570883, 2.759251912434896]
memory len:10000
memory used:3126.0
now epsilon is 0.2175795698910176, the reward is 194.33333333333334 with loss [13.602226257324219, 24.982601642608643] in episode 1088
Report: 
rewardSum:194.33333333333334
loss:[13.602226257324219, 24.982601642608643]
policies:[0, 3, 1]
qAverage:[0.0, 93.29057121276855]
ws:[5.727402750402689, 10.319048345088959]
memory len:10000
memory used:3126.0
now epsilon is 0.21736207189986742, the reward is 194.33333333333334 with loss [29.185612678527832, 17.371593713760376] in episode 1089
Report: 
rewardSum:194.33333333333334
loss:[29.185612678527832, 17.371593713760376]
policies:[0, 3, 1]
qAverage:[0.0, 86.45771598815918]
ws:[5.9433284252882, 10.333994491025805]
memory len:10000
memory used:3126.0
now epsilon is 0.21714479132516026, the reward is -3.0 with loss [18.72783151268959, 24.056556940078735] in episode 1090
Report: 
rewardSum:-3.0
loss:[18.72783151268959, 24.056556940078735]
policies:[1, 1, 2]
qAverage:[0.0, 50.962215423583984]
ws:[1.4662771224975586, 2.8229291439056396]
memory len:10000
memory used:3126.0
now epsilon is 0.21692772794956117, the reward is 194.33333333333334 with loss [15.606461763381958, 30.017400979995728] in episode 1091
Report: 
rewardSum:194.33333333333334
loss:[15.606461763381958, 30.017400979995728]
policies:[0, 4, 0]
qAverage:[0.0, 77.93888600667317]
ws:[8.429911772410074, 13.622501055399576]
memory len:10000
memory used:3126.0
now epsilon is 0.21660253965960463, the reward is 994.0 with loss [27.37121820449829, 27.866485118865967] in episode 1092
Report: 
rewardSum:994.0
loss:[27.37121820449829, 27.866485118865967]
policies:[1, 3, 2]
qAverage:[24.486371994018555, 53.71662521362305]
ws:[-7.28726589679718, -7.746622741222382]
memory len:10000
memory used:3127.0
now epsilon is 0.2161697134452619, the reward is 190.33333333333334 with loss [55.723767042160034, 47.935739517211914] in episode 1093
Report: 
rewardSum:190.33333333333334
loss:[55.723767042160034, 47.935739517211914]
policies:[1, 5, 2]
qAverage:[0.0, 80.2381820678711]
ws:[6.712365508079529, 12.995561301708221]
memory len:10000
memory used:3127.0
############# STATE ###############
0-		9*		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.21589963637575396, the reward is 193.33333333333334 with loss [28.884714126586914, 33.90653872489929] in episode 1094
Report: 
rewardSum:193.33333333333334
loss:[28.884714126586914, 33.90653872489929]
policies:[0, 4, 1]
qAverage:[0.0, 81.92716407775879]
ws:[2.5556690394878387, 5.808236360549927]
memory len:10000
memory used:3127.0
now epsilon is 0.21562989673382696, the reward is 193.33333333333334 with loss [31.79676914215088, 34.620373010635376] in episode 1095
Report: 
rewardSum:193.33333333333334
loss:[31.79676914215088, 34.620373010635376]
policies:[0, 3, 2]
qAverage:[0.0, 91.00885009765625]
ws:[7.626127332448959, 12.612580418586731]
memory len:10000
memory used:3127.0
now epsilon is 0.21530665397438273, the reward is 192.33333333333334 with loss [29.05870223045349, 30.066500663757324] in episode 1096
Report: 
rewardSum:192.33333333333334
loss:[29.05870223045349, 30.066500663757324]
policies:[0, 5, 1]
qAverage:[0.0, 94.32223968505859]
ws:[7.605278170108795, 13.652302932739257]
memory len:10000
memory used:3127.0
now epsilon is 0.21498389577613858, the reward is 192.33333333333334 with loss [25.963154554367065, 34.16709865629673] in episode 1097
Report: 
rewardSum:192.33333333333334
loss:[25.963154554367065, 34.16709865629673]
policies:[1, 4, 1]
qAverage:[0.0, 90.17073974609374]
ws:[7.064873671531677, 12.400704395771026]
memory len:10000
memory used:3127.0
now epsilon is 0.21476899248588774, the reward is 194.33333333333334 with loss [23.529359340667725, 23.654645442962646] in episode 1098
Report: 
rewardSum:194.33333333333334
loss:[23.529359340667725, 23.654645442962646]
policies:[1, 1, 2]
qAverage:[0.0, 61.98930358886719]
ws:[13.870596885681152, 21.251493453979492]
memory len:10000
memory used:3127.0
now epsilon is 0.21455430401835182, the reward is 194.33333333333334 with loss [19.911844491958618, 22.506163597106934] in episode 1099
Report: 
rewardSum:194.33333333333334
loss:[19.911844491958618, 22.506163597106934]
policies:[1, 3, 0]
qAverage:[0.0, 71.06920115152995]
ws:[2.8462160428365073, 5.78418763478597]
memory len:10000
memory used:3127.0
now epsilon is 0.21433983015878869, the reward is 194.33333333333334 with loss [35.506083965301514, 19.4132661819458] in episode 1100
Report: 
rewardSum:194.33333333333334
loss:[35.506083965301514, 19.4132661819458]
policies:[0, 4, 0]
qAverage:[0.0, 89.84031524658204]
ws:[7.51545398235321, 11.770894002914428]
memory len:10000
memory used:3127.0
now epsilon is 0.21412557069267082, the reward is 194.33333333333334 with loss [38.47651481628418, 17.190419912338257] in episode 1101
Report: 
rewardSum:194.33333333333334
loss:[38.47651481628418, 17.190419912338257]
policies:[1, 3, 0]
qAverage:[23.517580032348633, 60.77188491821289]
ws:[6.197412043809891, 9.939874783158302]
memory len:10000
memory used:3127.0
now epsilon is 0.21391152540568517, the reward is 194.33333333333334 with loss [23.753546237945557, 16.28018021583557] in episode 1102
Report: 
rewardSum:194.33333333333334
loss:[23.753546237945557, 16.28018021583557]
policies:[0, 4, 0]
qAverage:[0.0, 89.46619720458985]
ws:[4.712509298324585, 8.504639530181885]
memory len:10000
memory used:3127.0
now epsilon is 0.2136976940837329, the reward is 194.33333333333334 with loss [12.887088537216187, 25.61705107986927] in episode 1103
Report: 
rewardSum:194.33333333333334
loss:[12.887088537216187, 25.61705107986927]
policies:[1, 3, 0]
qAverage:[0.0, 79.83090209960938]
ws:[2.128393679857254, 5.266302235424519]
memory len:10000
memory used:3127.0
now epsilon is 0.2134840765129292, the reward is 194.33333333333334 with loss [19.287893414497375, 27.412420749664307] in episode 1104
Report: 
rewardSum:194.33333333333334
loss:[19.287893414497375, 27.412420749664307]
policies:[0, 3, 1]
qAverage:[0.0, 87.25254821777344]
ws:[7.234936952590942, 13.168885827064514]
memory len:10000
memory used:3126.0
now epsilon is 0.21327067247960307, the reward is 194.33333333333334 with loss [16.22658348083496, 20.248773097991943] in episode 1105
Report: 
rewardSum:194.33333333333334
loss:[16.22658348083496, 20.248773097991943]
policies:[1, 3, 0]
qAverage:[0.0, 86.74902534484863]
ws:[7.480127155780792, 13.384097695350647]
memory len:10000
memory used:3127.0
now epsilon is 0.21305748177029707, the reward is 194.33333333333334 with loss [20.74415397644043, 23.98862886428833] in episode 1106
Report: 
rewardSum:194.33333333333334
loss:[20.74415397644043, 23.98862886428833]
policies:[0, 4, 0]
qAverage:[0.0, 83.29578590393066]
ws:[5.269574373960495, 9.067935526371002]
memory len:10000
memory used:3127.0
now epsilon is 0.21284450417176723, the reward is 194.33333333333334 with loss [20.2360897064209, 15.85185170173645] in episode 1107
Report: 
rewardSum:194.33333333333334
loss:[20.2360897064209, 15.85185170173645]
policies:[1, 3, 0]
qAverage:[0.0, 82.79963302612305]
ws:[6.612134903669357, 12.017155557870865]
memory len:10000
memory used:3127.0
now epsilon is 0.21247230553150817, the reward is 191.33333333333334 with loss [41.05527937412262, 38.49721074104309] in episode 1108
Report: 
rewardSum:191.33333333333334
loss:[41.05527937412262, 38.49721074104309]
policies:[1, 4, 2]
qAverage:[0.0, 91.7243667602539]
ws:[9.737035942077636, 16.376198387145998]
memory len:10000
memory used:3127.0
now epsilon is 0.21215379619961225, the reward is 192.33333333333334 with loss [32.746524810791016, 48.768537521362305] in episode 1109
Report: 
rewardSum:192.33333333333334
loss:[32.746524810791016, 48.768537521362305]
policies:[0, 4, 2]
qAverage:[0.0, 77.99258995056152]
ws:[3.9094494581222534, 7.311849892139435]
memory len:10000
memory used:3127.0
now epsilon is 0.21183576433321116, the reward is 192.33333333333334 with loss [28.801455974578857, 36.28098726272583] in episode 1110
Report: 
rewardSum:192.33333333333334
loss:[28.801455974578857, 36.28098726272583]
policies:[0, 4, 2]
qAverage:[0.0, 87.1837158203125]
ws:[8.611614894866943, 15.210244178771973]
memory len:10000
memory used:3127.0
now epsilon is 0.2115182092165542, the reward is 192.33333333333334 with loss [30.89809536933899, 29.356778383255005] in episode 1111
Report: 
rewardSum:192.33333333333334
loss:[30.89809536933899, 29.356778383255005]
policies:[1, 4, 1]
qAverage:[0.0, 83.52829170227051]
ws:[8.302219033241272, 14.710020542144775]
memory len:10000
memory used:3127.0
now epsilon is 0.21130677031344708, the reward is 194.33333333333334 with loss [18.07598352432251, 13.902650833129883] in episode 1112
Report: 
rewardSum:194.33333333333334
loss:[18.07598352432251, 13.902650833129883]
policies:[1, 3, 0]
qAverage:[0.0, 80.87211608886719]
ws:[6.039535459131002, 9.738107580691576]
memory len:10000
memory used:3126.0
now epsilon is 0.2109900081920531, the reward is 192.33333333333334 with loss [28.65538740158081, 25.934088706970215] in episode 1113
Report: 
rewardSum:192.33333333333334
loss:[28.65538740158081, 25.934088706970215]
policies:[1, 4, 1]
qAverage:[0.0, 82.84072303771973]
ws:[5.357877425849438, 9.340940475463867]
memory len:10000
memory used:3126.0
now epsilon is 0.2107790972919281, the reward is 194.33333333333334 with loss [14.128755569458008, 17.412431240081787] in episode 1114
Report: 
rewardSum:194.33333333333334
loss:[14.128755569458008, 17.412431240081787]
policies:[0, 3, 1]
qAverage:[0.0, 82.3655776977539]
ws:[5.808756969869137, 10.588057935237885]
memory len:10000
memory used:3126.0
now epsilon is 0.21056839722362478, the reward is 46.33333333333334 with loss [19.153523445129395, 19.347798585891724] in episode 1115
Report: 
rewardSum:46.33333333333334
loss:[19.153523445129395, 19.347798585891724]
policies:[0, 3, 1]
qAverage:[0.0, 78.44062614440918]
ws:[3.143872857093811, 7.375498950481415]
memory len:10000
memory used:3126.0
now epsilon is 0.21035790777639043, the reward is 194.33333333333334 with loss [29.041940450668335, 12.262459635734558] in episode 1116
Report: 
rewardSum:194.33333333333334
loss:[29.041940450668335, 12.262459635734558]
policies:[0, 3, 1]
qAverage:[0.0, 64.84954579671223]
ws:[1.5926909844080608, 3.290682792663574]
memory len:10000
memory used:3126.0
now epsilon is 0.21014762873968293, the reward is 194.33333333333334 with loss [25.85365104675293, 11.2696293592453] in episode 1117
Report: 
rewardSum:194.33333333333334
loss:[25.85365104675293, 11.2696293592453]
policies:[0, 4, 0]
qAverage:[0.0, 86.75045776367188]
ws:[6.416972100734711, 12.139201545715332]
memory len:10000
memory used:3126.0
############# STATE ###############
0-		9-		18-		27-		36-		
1*		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.20993755990317065, the reward is 194.33333333333334 with loss [15.04021668434143, 26.959280490875244] in episode 1118
Report: 
rewardSum:194.33333333333334
loss:[15.04021668434143, 26.959280490875244]
policies:[0, 3, 1]
qAverage:[0.0, 85.37424850463867]
ws:[7.706635653972626, 14.324860692024231]
memory len:10000
memory used:3127.0
now epsilon is 0.2097277010567322, the reward is 194.33333333333334 with loss [13.606638550758362, 23.122188091278076] in episode 1119
Report: 
rewardSum:194.33333333333334
loss:[13.606638550758362, 23.122188091278076]
policies:[0, 4, 0]
qAverage:[0.0, 85.87693786621094]
ws:[4.8927671074867245, 9.08099398612976]
memory len:10000
memory used:3127.0
now epsilon is 0.20951805199045623, the reward is 194.33333333333334 with loss [21.29902744293213, 19.442707300186157] in episode 1120
Report: 
rewardSum:194.33333333333334
loss:[21.29902744293213, 19.442707300186157]
policies:[0, 4, 0]
qAverage:[0.0, 80.98442840576172]
ws:[4.814852438867092, 8.900178253650665]
memory len:10000
memory used:3127.0
now epsilon is 0.20930861249464122, the reward is 194.33333333333334 with loss [32.12776279449463, 19.229857206344604] in episode 1121
Report: 
rewardSum:194.33333333333334
loss:[32.12776279449463, 19.229857206344604]
policies:[0, 4, 0]
qAverage:[0.0, 82.26376914978027]
ws:[5.451519399881363, 10.465288162231445]
memory len:10000
memory used:3127.0
now epsilon is 0.20899484573732682, the reward is 44.33333333333334 with loss [21.86369252204895, 20.361506581306458] in episode 1122
Report: 
rewardSum:44.33333333333334
loss:[21.86369252204895, 20.361506581306458]
policies:[0, 4, 2]
qAverage:[0.0, 75.41608619689941]
ws:[0.46424955874681473, 1.9910330325365067]
memory len:10000
memory used:3127.0
now epsilon is 0.2085772216040189, the reward is 190.33333333333334 with loss [41.047308921813965, 34.18644833564758] in episode 1123
Report: 
rewardSum:190.33333333333334
loss:[41.047308921813965, 34.18644833564758]
policies:[0, 6, 2]
qAverage:[0.0, 88.56056867327008]
ws:[2.7097976548331126, 5.042082003184727]
memory len:10000
memory used:3127.0
now epsilon is 0.20836872258583775, the reward is 194.33333333333334 with loss [15.154218673706055, 18.584026098251343] in episode 1124
Report: 
rewardSum:194.33333333333334
loss:[15.154218673706055, 18.584026098251343]
policies:[0, 3, 1]
qAverage:[0.0, 83.11173629760742]
ws:[4.848396509885788, 8.458490461111069]
memory len:10000
memory used:3127.0
now epsilon is 0.20816043198850068, the reward is 194.33333333333334 with loss [16.706852674484253, 30.111871242523193] in episode 1125
Report: 
rewardSum:194.33333333333334
loss:[16.706852674484253, 30.111871242523193]
policies:[1, 3, 0]
qAverage:[0.0, 66.01416015625]
ws:[2.452368294199308, 5.357684820890427]
memory len:10000
memory used:3127.0
now epsilon is 0.207952349603665, the reward is 194.33333333333334 with loss [27.839097023010254, 18.24377179145813] in episode 1126
Report: 
rewardSum:194.33333333333334
loss:[27.839097023010254, 18.24377179145813]
policies:[0, 3, 1]
qAverage:[0.0, 84.56142997741699]
ws:[7.9658695459365845, 14.505278468132019]
memory len:10000
memory used:3127.0
now epsilon is 0.20769253910439045, the reward is -4.0 with loss [17.124927163124084, 24.457542896270752] in episode 1127
Report: 
rewardSum:-4.0
loss:[17.124927163124084, 24.457542896270752]
policies:[1, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:3127.0
now epsilon is 0.20743305320589903, the reward is 193.33333333333334 with loss [31.61204504966736, 27.331034183502197] in episode 1128
Report: 
rewardSum:193.33333333333334
loss:[31.61204504966736, 27.331034183502197]
policies:[1, 2, 2]
qAverage:[0.0, 70.69730885823567]
ws:[8.953399976094564, 14.0885059038798]
memory len:10000
memory used:3127.0
now epsilon is 0.20717389150264257, the reward is 193.33333333333334 with loss [28.23052740097046, 25.75929832458496] in episode 1129
Report: 
rewardSum:193.33333333333334
loss:[28.23052740097046, 25.75929832458496]
policies:[0, 4, 1]
qAverage:[0.0, 85.83865509033203]
ws:[7.335030424594879, 12.572662568092346]
memory len:10000
memory used:3127.0
now epsilon is 0.2069667952884017, the reward is 194.33333333333334 with loss [14.372879266738892, 20.129305362701416] in episode 1130
Report: 
rewardSum:194.33333333333334
loss:[14.372879266738892, 20.129305362701416]
policies:[0, 4, 0]
qAverage:[0.0, 80.66989707946777]
ws:[7.156833797693253, 11.055412754416466]
memory len:10000
memory used:3127.0
now epsilon is 0.20675990609272693, the reward is 194.33333333333334 with loss [20.68145990371704, 21.128761768341064] in episode 1131
Report: 
rewardSum:194.33333333333334
loss:[20.68145990371704, 21.128761768341064]
policies:[0, 3, 1]
qAverage:[0.0, 68.38705190022786]
ws:[4.267744938532512, 8.371270020802816]
memory len:10000
memory used:3127.0
now epsilon is 0.2065532237086773, the reward is 194.33333333333334 with loss [16.681396007537842, 18.252613067626953] in episode 1132
Report: 
rewardSum:194.33333333333334
loss:[16.681396007537842, 18.252613067626953]
policies:[0, 2, 2]
qAverage:[0.0, 53.18828201293945]
ws:[1.3867403268814087, 2.199934720993042]
memory len:10000
memory used:3128.0
now epsilon is 0.20634674792951876, the reward is 194.33333333333334 with loss [19.37862467765808, 30.908710479736328] in episode 1133
Report: 
rewardSum:194.33333333333334
loss:[19.37862467765808, 30.908710479736328]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:3128.0
now epsilon is 0.20614047854872386, the reward is 194.33333333333334 with loss [22.73846125602722, 19.975616931915283] in episode 1134
Report: 
rewardSum:194.33333333333334
loss:[22.73846125602722, 19.975616931915283]
policies:[1, 3, 0]
qAverage:[17.465904235839844, 63.7462158203125]
ws:[6.050028443336487, 9.848529624938966]
memory len:10000
memory used:3128.0
now epsilon is 0.20593441535997162, the reward is 194.33333333333334 with loss [17.41577982902527, 33.615962982177734] in episode 1135
Report: 
rewardSum:194.33333333333334
loss:[17.41577982902527, 33.615962982177734]
policies:[1, 3, 0]
qAverage:[22.22736167907715, 51.03924369812012]
ws:[1.9569417834281921, 4.5319178104400635]
memory len:10000
memory used:3128.0
now epsilon is 0.2057285581571473, the reward is 194.33333333333334 with loss [21.325989246368408, 23.719658851623535] in episode 1136
Report: 
rewardSum:194.33333333333334
loss:[21.325989246368408, 23.719658851623535]
policies:[1, 3, 0]
qAverage:[0.0, 79.86987113952637]
ws:[8.231848895549774, 13.34313189983368]
memory len:10000
memory used:3128.0
now epsilon is 0.20552290673434223, the reward is 194.33333333333334 with loss [15.06955087184906, 19.61626148223877] in episode 1137
Report: 
rewardSum:194.33333333333334
loss:[15.06955087184906, 19.61626148223877]
policies:[0, 4, 0]
qAverage:[0.0, 82.39434204101562]
ws:[5.5608946740627285, 8.551759696006775]
memory len:10000
memory used:3128.0
now epsilon is 0.20531746088585356, the reward is 194.33333333333334 with loss [20.624876141548157, 25.688339710235596] in episode 1138
Report: 
rewardSum:194.33333333333334
loss:[20.624876141548157, 25.688339710235596]
policies:[0, 4, 0]
qAverage:[0.0, 81.93245086669921]
ws:[6.598629355430603, 10.8975421667099]
memory len:10000
memory used:3128.0
now epsilon is 0.20500967711549473, the reward is 44.33333333333334 with loss [33.06112241744995, 36.57118892669678] in episode 1139
Report: 
rewardSum:44.33333333333334
loss:[33.06112241744995, 36.57118892669678]
policies:[1, 3, 2]
qAverage:[17.848219299316405, 60.46666412353515]
ws:[1.8376176953315735, 4.069273829460144]
memory len:10000
memory used:3128.0
now epsilon is 0.20475354311811983, the reward is 193.33333333333334 with loss [25.758450984954834, 17.47950303554535] in episode 1140
Report: 
rewardSum:193.33333333333334
loss:[25.758450984954834, 17.47950303554535]
policies:[2, 2, 1]
qAverage:[30.11676788330078, 34.64391072591146]
ws:[0.6500378052393595, 1.462570071220398]
memory len:10000
memory used:3128.0
now epsilon is 0.2044977291281979, the reward is 193.33333333333334 with loss [26.991063594818115, 32.860817074775696] in episode 1141
Report: 
rewardSum:193.33333333333334
loss:[26.991063594818115, 32.860817074775696]
policies:[1, 3, 1]
qAverage:[17.699119567871094, 63.520989990234376]
ws:[6.027852331846953, 9.73456841558218]
memory len:10000
memory used:3128.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11*		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.20419117418723312, the reward is 192.33333333333334 with loss [19.60361611843109, 35.29584884643555] in episode 1142
Report: 
rewardSum:192.33333333333334
loss:[19.60361611843109, 35.29584884643555]
policies:[1, 2, 3]
qAverage:[0.0, 68.43822224934895]
ws:[9.940987904866537, 15.612262725830078]
memory len:10000
memory used:3128.0
now epsilon is 0.20368127015675147, the reward is 188.33333333333334 with loss [42.284828305244446, 53.93038582801819] in episode 1143
Report: 
rewardSum:188.33333333333334
loss:[42.284828305244446, 53.93038582801819]
policies:[0, 6, 4]
qAverage:[0.0, 77.26581115722657]
ws:[7.700498867034912, 12.750044012069703]
memory len:10000
memory used:3128.0
now epsilon is 0.20337593913906873, the reward is 192.33333333333334 with loss [25.88934290409088, 20.9122257232666] in episode 1144
Report: 
rewardSum:192.33333333333334
loss:[25.88934290409088, 20.9122257232666]
policies:[0, 5, 1]
qAverage:[0.0, 82.00743357340495]
ws:[4.713383456071218, 8.327335794766745]
memory len:10000
memory used:3128.0
now epsilon is 0.20317263945319666, the reward is 46.33333333333334 with loss [16.542516708374023, 23.12584662437439] in episode 1145
Report: 
rewardSum:46.33333333333334
loss:[16.542516708374023, 23.12584662437439]
policies:[1, 2, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:3128.0
now epsilon is 0.20286807090488684, the reward is 44.33333333333334 with loss [34.69593000411987, 37.93698596954346] in episode 1146
Report: 
rewardSum:44.33333333333334
loss:[34.69593000411987, 37.93698596954346]
policies:[0, 2, 4]
qAverage:[0.0, 64.02171325683594]
ws:[2.9021227757136026, 6.274996360143025]
memory len:10000
memory used:3128.0
now epsilon is 0.20261461257710592, the reward is 193.33333333333334 with loss [17.07882833480835, 16.5010644197464] in episode 1147
Report: 
rewardSum:193.33333333333334
loss:[17.07882833480835, 16.5010644197464]
policies:[0, 4, 1]
qAverage:[0.0, 75.5164680480957]
ws:[6.75945246219635, 12.076568245887756]
memory len:10000
memory used:3128.0
now epsilon is 0.20251331793423066, the reward is -1.0 with loss [8.874202728271484, 13.504437685012817] in episode 1148
Report: 
rewardSum:-1.0
loss:[8.874202728271484, 13.504437685012817]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:3128.0
now epsilon is 0.2023108805461344, the reward is 194.33333333333334 with loss [23.7423677444458, 16.05529475212097] in episode 1149
Report: 
rewardSum:194.33333333333334
loss:[23.7423677444458, 16.05529475212097]
policies:[0, 4, 0]
qAverage:[0.0, 78.55363006591797]
ws:[4.6515597939491276, 8.957422542572022]
memory len:10000
memory used:3128.0
now epsilon is 0.2020076038285554, the reward is 192.33333333333334 with loss [32.67787003517151, 48.56238126754761] in episode 1150
Report: 
rewardSum:192.33333333333334
loss:[32.67787003517151, 48.56238126754761]
policies:[0, 4, 2]
qAverage:[0.0, 78.80524291992188]
ws:[4.622136831283569, 8.70922510623932]
memory len:10000
memory used:3128.0
now epsilon is 0.20180567196495364, the reward is 194.33333333333334 with loss [20.96664333343506, 22.648452877998352] in episode 1151
Report: 
rewardSum:194.33333333333334
loss:[20.96664333343506, 22.648452877998352]
policies:[0, 3, 1]
qAverage:[0.0, 65.45716603597005]
ws:[2.3202621142069497, 5.601408799489339]
memory len:10000
memory used:3128.0
now epsilon is 0.2016039419575036, the reward is 194.33333333333334 with loss [16.762879014015198, 18.061588287353516] in episode 1152
Report: 
rewardSum:194.33333333333334
loss:[16.762879014015198, 18.061588287353516]
policies:[1, 3, 0]
qAverage:[17.196923828125, 57.921250915527345]
ws:[2.8228548884391786, 5.849736022949219]
memory len:10000
memory used:3128.0
now epsilon is 0.20140241360442493, the reward is 194.33333333333334 with loss [20.890723705291748, 20.974515438079834] in episode 1153
Report: 
rewardSum:194.33333333333334
loss:[20.890723705291748, 20.974515438079834]
policies:[0, 4, 0]
qAverage:[0.0, 76.73933563232421]
ws:[2.8876198053359987, 6.251405370235443]
memory len:10000
memory used:3128.0
now epsilon is 0.20120108670413878, the reward is 194.33333333333334 with loss [17.550655364990234, 27.258925914764404] in episode 1154
Report: 
rewardSum:194.33333333333334
loss:[17.550655364990234, 27.258925914764404]
policies:[0, 3, 1]
qAverage:[0.0, 69.22018623352051]
ws:[1.6011886149644852, 4.49643125385046]
memory len:10000
memory used:3128.0
now epsilon is 0.2009999610552679, the reward is 194.33333333333334 with loss [16.10730218887329, 23.209475994110107] in episode 1155
Report: 
rewardSum:194.33333333333334
loss:[16.10730218887329, 23.209475994110107]
policies:[1, 3, 0]
qAverage:[0.0, 74.8006534576416]
ws:[3.2958549559116364, 7.159136056900024]
memory len:10000
memory used:3128.0
now epsilon is 0.20079903645663633, the reward is 194.33333333333334 with loss [14.584390640258789, 22.63894534111023] in episode 1156
Report: 
rewardSum:194.33333333333334
loss:[14.584390640258789, 22.63894534111023]
policies:[0, 4, 0]
qAverage:[0.0, 76.1971435546875]
ws:[3.3135992169380186, 7.491516107320786]
memory len:10000
memory used:3128.0
now epsilon is 0.20059831270726922, the reward is 194.33333333333334 with loss [18.759270668029785, 14.535565853118896] in episode 1157
Report: 
rewardSum:194.33333333333334
loss:[18.759270668029785, 14.535565853118896]
policies:[0, 4, 0]
qAverage:[0.0, 69.87752342224121]
ws:[0.7590963020920753, 2.7185593098402023]
memory len:10000
memory used:3128.0
now epsilon is 0.20039778960639262, the reward is 194.33333333333334 with loss [18.78664255142212, 22.882954359054565] in episode 1158
Report: 
rewardSum:194.33333333333334
loss:[18.78664255142212, 22.882954359054565]
policies:[0, 3, 1]
qAverage:[0.0, 70.72515296936035]
ws:[3.9564524786546826, 8.478322461247444]
memory len:10000
memory used:3128.0
now epsilon is 0.20019746695343327, the reward is 194.33333333333334 with loss [27.23548412322998, 13.272612452507019] in episode 1159
Report: 
rewardSum:194.33333333333334
loss:[27.23548412322998, 13.272612452507019]
policies:[0, 3, 1]
qAverage:[0.0, 72.16725158691406]
ws:[3.103625831194222, 6.174676865339279]
memory len:10000
memory used:3128.0
now epsilon is 0.19989735837557843, the reward is 192.33333333333334 with loss [28.841645002365112, 36.078437089920044] in episode 1160
Report: 
rewardSum:192.33333333333334
loss:[28.841645002365112, 36.078437089920044]
policies:[2, 3, 1]
qAverage:[0.0, 66.5927505493164]
ws:[4.854594151178996, 8.779524803161621]
memory len:10000
memory used:3128.0
now epsilon is 0.19969753596621945, the reward is 46.33333333333334 with loss [31.06593084335327, 24.758377075195312] in episode 1161
Report: 
rewardSum:46.33333333333334
loss:[31.06593084335327, 24.758377075195312]
policies:[0, 2, 2]
qAverage:[0.0, 60.04981994628906]
ws:[1.2528067032496135, 2.7791542212168374]
memory len:10000
memory used:3128.0
now epsilon is 0.1994979133043489, the reward is 194.33333333333334 with loss [14.3295396566391, 15.1712486743927] in episode 1162
Report: 
rewardSum:194.33333333333334
loss:[14.3295396566391, 15.1712486743927]
policies:[1, 3, 0]
qAverage:[0.0, 50.102760314941406]
ws:[2.0166094303131104, 4.238307952880859]
memory len:10000
memory used:3128.0
now epsilon is 0.1990992664245824, the reward is 190.33333333333334 with loss [47.9621297121048, 41.61332559585571] in episode 1163
Report: 
rewardSum:190.33333333333334
loss:[47.9621297121048, 41.61332559585571]
policies:[1, 4, 3]
qAverage:[0.0, 70.34944763183594]
ws:[3.3327637672424317, 5.787941789627075]
memory len:10000
memory used:3128.0
now epsilon is 0.1989002418079398, the reward is 194.33333333333334 with loss [20.74039649963379, 21.487594604492188] in episode 1164
Report: 
rewardSum:194.33333333333334
loss:[20.74039649963379, 21.487594604492188]
policies:[0, 4, 0]
qAverage:[0.0, 74.14404296875]
ws:[3.1960088133811952, 6.2273879766464235]
memory len:10000
memory used:3128.0
now epsilon is 0.19870141614129208, the reward is 194.33333333333334 with loss [25.03452491760254, 13.108251571655273] in episode 1165
Report: 
rewardSum:194.33333333333334
loss:[25.03452491760254, 13.108251571655273]
policies:[0, 3, 1]
qAverage:[0.0, 68.99599075317383]
ws:[5.089679807424545, 10.465075612068176]
memory len:10000
memory used:3128.0
############# STATE ###############
0-		9-		18-		27-		36-		
1*		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.19850278922576378, the reward is 194.33333333333334 with loss [19.48868417739868, 20.367687702178955] in episode 1166
Report: 
rewardSum:194.33333333333334
loss:[19.48868417739868, 20.367687702178955]
policies:[0, 4, 0]
qAverage:[0.0, 56.54068501790365]
ws:[2.084642549355825, 5.958461920420329]
memory len:10000
memory used:3128.0
now epsilon is 0.19830436086267836, the reward is 194.33333333333334 with loss [14.236291885375977, 25.77571201324463] in episode 1167
Report: 
rewardSum:194.33333333333334
loss:[14.236291885375977, 25.77571201324463]
policies:[0, 4, 0]
qAverage:[0.0, 71.12208709716796]
ws:[5.458665347099304, 11.018606281280517]
memory len:10000
memory used:3128.0
now epsilon is 0.19810613085355777, the reward is 194.33333333333334 with loss [22.935486316680908, 22.30126428604126] in episode 1168
Report: 
rewardSum:194.33333333333334
loss:[22.935486316680908, 22.30126428604126]
policies:[1, 3, 0]
qAverage:[0.0, 68.97306251525879]
ws:[4.307485938072205, 8.106049716472626]
memory len:10000
memory used:3128.0
now epsilon is 0.19775970503054863, the reward is 191.33333333333334 with loss [28.493995428085327, 29.37987107038498] in episode 1169
Report: 
rewardSum:191.33333333333334
loss:[28.493995428085327, 29.37987107038498]
policies:[1, 3, 3]
qAverage:[0.0, 65.22033882141113]
ws:[2.5607677698135376, 6.377974689006805]
memory len:10000
memory used:3128.0
now epsilon is 0.19756201947304824, the reward is 194.33333333333334 with loss [10.376969575881958, 25.183288097381592] in episode 1170
Report: 
rewardSum:194.33333333333334
loss:[10.376969575881958, 25.183288097381592]
policies:[1, 3, 0]
qAverage:[0.0, 48.023319244384766]
ws:[5.398471355438232, 10.112532615661621]
memory len:10000
memory used:3128.0
now epsilon is 0.1972658615965054, the reward is 192.33333333333334 with loss [40.577771067619324, 31.810725212097168] in episode 1171
Report: 
rewardSum:192.33333333333334
loss:[40.577771067619324, 31.810725212097168]
policies:[0, 5, 1]
qAverage:[0.0, 74.2792854309082]
ws:[2.320821426808834, 6.4087493022282915]
memory len:10000
memory used:3128.0
now epsilon is 0.19706866969727865, the reward is 194.33333333333334 with loss [20.25137162208557, 24.295210599899292] in episode 1172
Report: 
rewardSum:194.33333333333334
loss:[20.25137162208557, 24.295210599899292]
policies:[0, 3, 1]
qAverage:[0.0, 67.67418479919434]
ws:[2.406148999929428, 6.258280545473099]
memory len:10000
memory used:3128.0
now epsilon is 0.19677325138303822, the reward is 192.33333333333334 with loss [32.66480112075806, 28.378350257873535] in episode 1173
Report: 
rewardSum:192.33333333333334
loss:[32.66480112075806, 28.378350257873535]
policies:[0, 5, 1]
qAverage:[0.0, 68.47213134765624]
ws:[0.3937362343072891, 3.6545155167579653]
memory len:10000
memory used:3128.0
now epsilon is 0.19647827591940678, the reward is 192.33333333333334 with loss [24.760140657424927, 27.49691903591156] in episode 1174
Report: 
rewardSum:192.33333333333334
loss:[24.760140657424927, 27.49691903591156]
policies:[2, 1, 3]
qAverage:[0.0, 42.52803421020508]
ws:[-0.7120015621185303, 0.4732745885848999]
memory len:10000
memory used:3128.0
now epsilon is 0.19628187131056174, the reward is 194.33333333333334 with loss [18.22127366065979, 23.820215225219727] in episode 1175
Report: 
rewardSum:194.33333333333334
loss:[18.22127366065979, 23.820215225219727]
policies:[0, 2, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:3128.0
now epsilon is 0.1960856630326861, the reward is 194.33333333333334 with loss [16.64600157737732, 17.661562204360962] in episode 1176
Report: 
rewardSum:194.33333333333334
loss:[16.64600157737732, 17.661562204360962]
policies:[0, 3, 1]
qAverage:[0.0, 63.72585678100586]
ws:[2.250413551926613, 6.679069250822067]
memory len:10000
memory used:3128.0
now epsilon is 0.19588965088952248, the reward is 194.33333333333334 with loss [20.08140468597412, 17.83087730407715] in episode 1177
Report: 
rewardSum:194.33333333333334
loss:[20.08140468597412, 17.83087730407715]
policies:[1, 3, 0]
qAverage:[0.0, 63.77341842651367]
ws:[0.4800032749772072, 3.583286225795746]
memory len:10000
memory used:3128.0
now epsilon is 0.19569383468500973, the reward is 194.33333333333334 with loss [16.00790786743164, 14.305715560913086] in episode 1178
Report: 
rewardSum:194.33333333333334
loss:[16.00790786743164, 14.305715560913086]
policies:[0, 4, 0]
qAverage:[0.0, 68.87787322998047]
ws:[1.6486548781394958, 6.0169168472290036]
memory len:10000
memory used:3128.0
now epsilon is 0.19549821422328262, the reward is 194.33333333333334 with loss [19.130779266357422, 25.721284866333008] in episode 1179
Report: 
rewardSum:194.33333333333334
loss:[19.130779266357422, 25.721284866333008]
policies:[0, 4, 0]
qAverage:[0.0, 67.32581520080566]
ws:[2.049652848392725, 7.057361125946045]
memory len:10000
memory used:3128.0
now epsilon is 0.19530278930867181, the reward is 194.33333333333334 with loss [29.7231388092041, 19.903762340545654] in episode 1180
Report: 
rewardSum:194.33333333333334
loss:[29.7231388092041, 19.903762340545654]
policies:[1, 3, 0]
qAverage:[0.0, 62.745370864868164]
ws:[-0.0007012635469436646, 2.7350724041461945]
memory len:10000
memory used:3128.0
now epsilon is 0.19510755974570348, the reward is 194.33333333333334 with loss [33.0176043510437, 12.808136463165283] in episode 1181
Report: 
rewardSum:194.33333333333334
loss:[33.0176043510437, 12.808136463165283]
policies:[1, 3, 0]
qAverage:[0.0, 64.37750816345215]
ws:[1.787580706179142, 5.723577201366425]
memory len:10000
memory used:3128.0
now epsilon is 0.1945716841358792, the reward is 187.33333333333334 with loss [47.29395604133606, 60.79747724533081] in episode 1182
Report: 
rewardSum:187.33333333333334
loss:[47.29395604133606, 60.79747724533081]
policies:[0, 7, 4]
qAverage:[0.0, 73.88746929168701]
ws:[4.276089262217283, 10.881032317876816]
memory len:10000
memory used:3128.0
now epsilon is 0.19437718540396492, the reward is 194.33333333333334 with loss [12.524653196334839, 24.954139947891235] in episode 1183
Report: 
rewardSum:194.33333333333334
loss:[12.524653196334839, 24.954139947891235]
policies:[0, 4, 0]
qAverage:[0.0, 68.70223999023438]
ws:[1.865381109714508, 6.612478351593017]
memory len:10000
memory used:3128.0
now epsilon is 0.1941828810978577, the reward is 194.33333333333334 with loss [26.994359493255615, 13.439619541168213] in episode 1184
Report: 
rewardSum:194.33333333333334
loss:[26.994359493255615, 13.439619541168213]
policies:[1, 3, 0]
qAverage:[0.0, 61.14980697631836]
ws:[1.666776642203331, 6.363331198692322]
memory len:10000
memory used:3128.0
now epsilon is 0.19379485498584703, the reward is 190.33333333333334 with loss [46.79443359375, 39.61052465438843] in episode 1185
Report: 
rewardSum:190.33333333333334
loss:[46.79443359375, 39.61052465438843]
policies:[1, 4, 3]
qAverage:[0.0, 64.71630249023437]
ws:[3.2307143211364746, 9.850618982315064]
memory len:10000
memory used:3128.0
now epsilon is 0.1936011327918204, the reward is 194.33333333333334 with loss [19.468773126602173, 29.46435570716858] in episode 1186
Report: 
rewardSum:194.33333333333334
loss:[19.468773126602173, 29.46435570716858]
policies:[1, 3, 0]
qAverage:[0.0, 54.89208475748698]
ws:[2.0036403437455497, 6.514360745747884]
memory len:10000
memory used:3129.0
now epsilon is 0.19340760424735406, the reward is 194.33333333333334 with loss [27.26258635520935, 22.21186089515686] in episode 1187
Report: 
rewardSum:194.33333333333334
loss:[27.26258635520935, 22.21186089515686]
policies:[0, 3, 1]
qAverage:[0.0, 64.99100112915039]
ws:[1.6970058009028435, 6.837107300758362]
memory len:10000
memory used:3128.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23*		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.1932142691588711, the reward is 194.33333333333334 with loss [12.309721231460571, 24.553997039794922] in episode 1188
Report: 
rewardSum:194.33333333333334
loss:[12.309721231460571, 24.553997039794922]
policies:[0, 4, 0]
qAverage:[0.0, 65.08052520751953]
ws:[2.9746885403990744, 8.413362121582031]
memory len:10000
memory used:3128.0
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.19302112733298804, the reward is 194.33333333333334 with loss [11.549901485443115, 16.48686385154724] in episode 1189
Report: 
rewardSum:194.33333333333334
loss:[11.549901485443115, 16.48686385154724]
policies:[1, 2, 1]
qAverage:[0.0, 54.953051249186196]
ws:[1.136382261912028, 4.448726495107015]
memory len:10000
memory used:3128.0
now epsilon is 0.19282817857651474, the reward is 194.33333333333334 with loss [19.188435792922974, 26.673274040222168] in episode 1190
Report: 
rewardSum:194.33333333333334
loss:[19.188435792922974, 26.673274040222168]
policies:[0, 3, 1]
qAverage:[0.0, 63.32481002807617]
ws:[4.17774111032486, 10.764909625053406]
memory len:10000
memory used:3128.0
now epsilon is 0.1926354226964542, the reward is 194.33333333333334 with loss [25.79011344909668, 22.30950355529785] in episode 1191
Report: 
rewardSum:194.33333333333334
loss:[25.79011344909668, 22.30950355529785]
policies:[0, 4, 0]
qAverage:[0.0, 60.8219051361084]
ws:[3.3728101532906294, 9.080095022916794]
memory len:10000
memory used:3129.0
now epsilon is 0.19239474878512736, the reward is 45.33333333333334 with loss [24.804439067840576, 36.293981075286865] in episode 1192
Report: 
rewardSum:45.33333333333334
loss:[24.804439067840576, 36.293981075286865]
policies:[0, 3, 2]
qAverage:[0.0, 59.89622688293457]
ws:[-0.18388537876307964, 2.212329089641571]
memory len:10000
memory used:3129.0
now epsilon is 0.19220242617234912, the reward is 194.33333333333334 with loss [25.30546283721924, 11.181090354919434] in episode 1193
Report: 
rewardSum:194.33333333333334
loss:[25.30546283721924, 11.181090354919434]
policies:[0, 4, 0]
qAverage:[0.0, 65.18981170654297]
ws:[0.9439692586660385, 4.840676242113114]
memory len:10000
memory used:3129.0
now epsilon is 0.19201029581007473, the reward is 194.33333333333334 with loss [26.547796726226807, 28.21141290664673] in episode 1194
Report: 
rewardSum:194.33333333333334
loss:[26.547796726226807, 28.21141290664673]
policies:[1, 3, 0]
qAverage:[0.0, 53.16783142089844]
ws:[-0.4459797541300456, 1.5045411959290504]
memory len:10000
memory used:3129.0
now epsilon is 0.19191430266281317, the reward is -1.0 with loss [8.069957733154297, 10.05439305305481] in episode 1195
Report: 
rewardSum:-1.0
loss:[8.069957733154297, 10.05439305305481]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:3129.0
now epsilon is 0.1915308097396447, the reward is 190.33333333333334 with loss [46.316617012023926, 49.30723977088928] in episode 1196
Report: 
rewardSum:190.33333333333334
loss:[46.316617012023926, 49.30723977088928]
policies:[1, 5, 2]
qAverage:[0.0, 63.74470774332682]
ws:[1.04847085972627, 4.470941722393036]
memory len:10000
memory used:3129.0
now epsilon is 0.1913393507419888, the reward is 194.33333333333334 with loss [22.181070804595947, 16.93053913116455] in episode 1197
Report: 
rewardSum:194.33333333333334
loss:[22.181070804595947, 16.93053913116455]
policies:[0, 4, 0]
qAverage:[0.0, 59.23666572570801]
ws:[1.8722896836698055, 6.047356903553009]
memory len:10000
memory used:3129.0
now epsilon is 0.1911480831315454, the reward is 46.33333333333334 with loss [21.933157920837402, 22.253629684448242] in episode 1198
Report: 
rewardSum:46.33333333333334
loss:[21.933157920837402, 22.253629684448242]
policies:[0, 2, 2]
qAverage:[0.0, 51.9314219156901]
ws:[0.684508204460144, 3.3774867057800293]
memory len:10000
memory used:3128.0
now epsilon is 0.19081382476341638, the reward is 191.33333333333334 with loss [41.26616835594177, 36.63044059276581] in episode 1199
Report: 
rewardSum:191.33333333333334
loss:[41.26616835594177, 36.63044059276581]
policies:[1, 4, 2]
qAverage:[0.0, 58.33571243286133]
ws:[1.4156492091715336, 4.38693842291832]
memory len:10000
memory used:3128.0
now epsilon is 0.19062308248191215, the reward is 194.33333333333334 with loss [21.85736083984375, 26.333829164505005] in episode 1200
Report: 
rewardSum:194.33333333333334
loss:[21.85736083984375, 26.333829164505005]
policies:[0, 4, 0]
qAverage:[0.0, 60.81595420837402]
ws:[1.405475940555334, 4.3059892281889915]
memory len:10000
memory used:3128.0
now epsilon is 0.190432530871173, the reward is 194.33333333333334 with loss [14.99998688697815, 14.883702278137207] in episode 1201
Report: 
rewardSum:194.33333333333334
loss:[14.99998688697815, 14.883702278137207]
policies:[1, 2, 1]
qAverage:[0.0, 50.17925516764323]
ws:[-0.16792426506678262, 0.82084192832311]
memory len:10000
memory used:3128.0
now epsilon is 0.19019460919816447, the reward is 193.33333333333334 with loss [18.085164308547974, 29.187917947769165] in episode 1202
Report: 
rewardSum:193.33333333333334
loss:[18.085164308547974, 29.187917947769165]
policies:[0, 3, 2]
qAverage:[0.0, 61.14309310913086]
ws:[1.577535666525364, 5.277791917324066]
memory len:10000
memory used:3128.0
now epsilon is 0.19000448590005836, the reward is 194.33333333333334 with loss [25.13007879257202, 22.915229320526123] in episode 1203
Report: 
rewardSum:194.33333333333334
loss:[25.13007879257202, 22.915229320526123]
policies:[0, 4, 0]
qAverage:[0.0, 62.67919006347656]
ws:[2.0628321379423142, 6.133449220657349]
memory len:10000
memory used:3128.0
now epsilon is 0.18981455265396602, the reward is 194.33333333333334 with loss [30.85796308517456, 12.61050009727478] in episode 1204
Report: 
rewardSum:194.33333333333334
loss:[30.85796308517456, 12.61050009727478]
policies:[1, 2, 1]
qAverage:[0.0, 52.25844065348307]
ws:[0.28657351930936176, 2.4634817441304526]
memory len:10000
memory used:3128.0
now epsilon is 0.18962480926990666, the reward is 194.33333333333334 with loss [29.540037155151367, 22.90954303741455] in episode 1205
Report: 
rewardSum:194.33333333333334
loss:[29.540037155151367, 22.90954303741455]
policies:[0, 4, 0]
qAverage:[0.0, 61.49326515197754]
ws:[1.1130372881889343, 5.255466163158417]
memory len:10000
memory used:3128.0
now epsilon is 0.18943525555808943, the reward is 194.33333333333334 with loss [13.956926107406616, 16.01689648628235] in episode 1206
Report: 
rewardSum:194.33333333333334
loss:[13.956926107406616, 16.01689648628235]
policies:[0, 4, 0]
qAverage:[0.0, 62.110670471191405]
ws:[0.6750901162624359, 5.579567623138428]
memory len:10000
memory used:3128.0
now epsilon is 0.189151280211117, the reward is 192.33333333333334 with loss [24.001825094223022, 30.517789840698242] in episode 1207
Report: 
rewardSum:192.33333333333334
loss:[24.001825094223022, 30.517789840698242]
policies:[0, 5, 1]
qAverage:[0.0, 59.95120544433594]
ws:[1.522314453125, 7.713277971744537]
memory len:10000
memory used:3128.0
now epsilon is 0.18896219985081475, the reward is 194.33333333333334 with loss [33.33901023864746, 21.518868446350098] in episode 1208
Report: 
rewardSum:194.33333333333334
loss:[33.33901023864746, 21.518868446350098]
policies:[0, 4, 0]
qAverage:[0.0, 61.76414337158203]
ws:[0.9061259448528289, 6.062706881575286]
memory len:10000
memory used:3128.0
now epsilon is 0.1887733084999795, the reward is 194.33333333333334 with loss [14.896612882614136, 24.269617319107056] in episode 1209
Report: 
rewardSum:194.33333333333334
loss:[14.896612882614136, 24.269617319107056]
policies:[0, 4, 0]
qAverage:[0.0, 55.15008735656738]
ws:[-0.5130951516330242, 2.392628028988838]
memory len:10000
memory used:3128.0
now epsilon is 0.1884903254532257, the reward is 192.33333333333334 with loss [29.559434175491333, 27.890949249267578] in episode 1210
Report: 
rewardSum:192.33333333333334
loss:[29.559434175491333, 27.890949249267578]
policies:[1, 3, 2]
qAverage:[0.0, 58.118974685668945]
ws:[0.07242974638938904, 4.855341583490372]
memory len:10000
memory used:3128.0
now epsilon is 0.18820776661583383, the reward is 192.33333333333334 with loss [25.331148147583008, 25.531098127365112] in episode 1211
Report: 
rewardSum:192.33333333333334
loss:[25.331148147583008, 25.531098127365112]
policies:[0, 5, 1]
qAverage:[0.0, 62.208578491210936]
ws:[1.186394202709198, 5.621590805053711]
memory len:10000
memory used:3128.0
now epsilon is 0.18792563135188742, the reward is 192.33333333333334 with loss [26.653953671455383, 23.863741397857666] in episode 1212
Report: 
rewardSum:192.33333333333334
loss:[26.653953671455383, 23.863741397857666]
policies:[1, 4, 1]
qAverage:[0.0, 61.6492416381836]
ws:[1.2820175766479225, 5.329103422164917]
memory len:10000
memory used:3128.0
############# STATE ###############
0-		9-		18*		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.18773777618090268, the reward is 194.33333333333334 with loss [19.725659370422363, 27.186546087265015] in episode 1213
Report: 
rewardSum:194.33333333333334
loss:[19.725659370422363, 27.186546087265015]
policies:[0, 4, 0]
qAverage:[0.0, 54.66970443725586]
ws:[0.6949368603527546, 3.6809165477752686]
memory len:10000
memory used:3128.0
now epsilon is 0.18750322126745633, the reward is 193.33333333333334 with loss [18.594738245010376, 20.26054811477661] in episode 1214
Report: 
rewardSum:193.33333333333334
loss:[18.594738245010376, 20.26054811477661]
policies:[0, 4, 1]
qAverage:[0.0, 62.35887908935547]
ws:[0.5963680744171143, 4.463812375068665]
memory len:10000
memory used:3128.0
now epsilon is 0.1872689594010916, the reward is 193.33333333333334 with loss [37.515053272247314, 26.228846669197083] in episode 1215
Report: 
rewardSum:193.33333333333334
loss:[37.515053272247314, 26.228846669197083]
policies:[1, 2, 2]
qAverage:[0.0, 40.19324493408203]
ws:[0.17451535165309906, 2.43560528755188]
memory len:10000
memory used:3128.0
now epsilon is 0.1870817606558467, the reward is 194.33333333333334 with loss [16.80115818977356, 18.00156569480896] in episode 1216
Report: 
rewardSum:194.33333333333334
loss:[16.80115818977356, 18.00156569480896]
policies:[1, 3, 0]
qAverage:[0.0, 53.60084533691406]
ws:[0.00693628191947937, 5.610651969909668]
memory len:10000
memory used:3128.0
now epsilon is 0.18689474903915923, the reward is 46.33333333333334 with loss [14.335205793380737, 14.702634572982788] in episode 1217
Report: 
rewardSum:46.33333333333334
loss:[14.335205793380737, 14.702634572982788]
policies:[0, 3, 1]
qAverage:[0.0, 53.40258979797363]
ws:[-0.8193711116909981, 2.143551915884018]
memory len:10000
memory used:3128.0
now epsilon is 0.18652128644340996, the reward is 190.33333333333334 with loss [35.95124959945679, 56.21321678161621] in episode 1218
Report: 
rewardSum:190.33333333333334
loss:[35.95124959945679, 56.21321678161621]
policies:[0, 4, 4]
qAverage:[0.0, 55.20701599121094]
ws:[-0.2984534353017807, 4.3081808388233185]
memory len:10000
memory used:3128.0
now epsilon is 0.18642803745776867, the reward is -1.0 with loss [13.566315650939941, 14.545536518096924] in episode 1219
Report: 
rewardSum:-1.0
loss:[13.566315650939941, 14.545536518096924]
policies:[0, 1, 1]
qAverage:[0.0, 33.83280944824219]
ws:[-1.0216922760009766, 0.11884523928165436]
memory len:10000
memory used:3129.0
now epsilon is 0.18624167931917396, the reward is 194.33333333333334 with loss [15.939908504486084, 24.73072052001953] in episode 1220
Report: 
rewardSum:194.33333333333334
loss:[15.939908504486084, 24.73072052001953]
policies:[0, 3, 1]
qAverage:[0.0, 58.21131896972656]
ws:[1.042254202067852, 7.924725532531738]
memory len:10000
memory used:3129.0
now epsilon is 0.1860555074688452, the reward is 194.33333333333334 with loss [26.927430629730225, 19.555505752563477] in episode 1221
Report: 
rewardSum:194.33333333333334
loss:[26.927430629730225, 19.555505752563477]
policies:[0, 3, 1]
qAverage:[0.0, 56.44525337219238]
ws:[0.05217183008790016, 5.496453404426575]
memory len:10000
memory used:3129.0
now epsilon is 0.18573015442690463, the reward is 191.33333333333334 with loss [40.52968776226044, 32.120250940322876] in episode 1222
Report: 
rewardSum:191.33333333333334
loss:[40.52968776226044, 32.120250940322876]
policies:[0, 5, 2]
qAverage:[0.0, 59.36082000732422]
ws:[2.769687020778656, 9.010295915603638]
memory len:10000
memory used:3129.0
now epsilon is 0.18554449390967823, the reward is 194.33333333333334 with loss [15.107372999191284, 13.496694326400757] in episode 1223
Report: 
rewardSum:194.33333333333334
loss:[15.107372999191284, 13.496694326400757]
policies:[0, 4, 0]
qAverage:[0.0, 60.450852966308595]
ws:[2.4770026445388793, 9.339642906188965]
memory len:10000
memory used:3129.0
now epsilon is 0.18531267922861214, the reward is 193.33333333333334 with loss [34.4144561290741, 24.013609886169434] in episode 1224
Report: 
rewardSum:193.33333333333334
loss:[34.4144561290741, 24.013609886169434]
policies:[0, 3, 2]
qAverage:[0.0, 56.447357177734375]
ws:[1.8461084249429405, 8.46765947341919]
memory len:10000
memory used:3129.0
now epsilon is 0.1852200344710403, the reward is -1.0 with loss [7.1384453773498535, 13.039795398712158] in episode 1225
Report: 
rewardSum:-1.0
loss:[7.1384453773498535, 13.039795398712158]
policies:[0, 1, 1]
qAverage:[0.0, 33.48868179321289]
ws:[-0.43253806233406067, 0.9886128902435303]
memory len:10000
memory used:3129.0
now epsilon is 0.18484991837514173, the reward is 190.33333333333334 with loss [41.618207693099976, 36.56481575965881] in episode 1226
Report: 
rewardSum:190.33333333333334
loss:[41.618207693099976, 36.56481575965881]
policies:[0, 5, 3]
qAverage:[0.0, 60.00241724650065]
ws:[-0.8160926525791486, 3.02521280447642]
memory len:10000
memory used:3128.0
now epsilon is 0.1846651377639336, the reward is 194.33333333333334 with loss [24.095914840698242, 27.467514514923096] in episode 1227
Report: 
rewardSum:194.33333333333334
loss:[24.095914840698242, 27.467514514923096]
policies:[0, 4, 0]
qAverage:[0.0, 51.971665382385254]
ws:[-0.5319019816815853, 2.850905805826187]
memory len:10000
memory used:3128.0
now epsilon is 0.1844805418640555, the reward is 194.33333333333334 with loss [15.688529014587402, 13.729531049728394] in episode 1228
Report: 
rewardSum:194.33333333333334
loss:[15.688529014587402, 13.729531049728394]
policies:[0, 4, 0]
qAverage:[0.0, 60.255072021484374]
ws:[-1.1247766375541688, 3.868692231178284]
memory len:10000
memory used:3129.0
now epsilon is 0.18429613049086532, the reward is 194.33333333333334 with loss [18.918697357177734, 16.820363759994507] in episode 1229
Report: 
rewardSum:194.33333333333334
loss:[18.918697357177734, 16.820363759994507]
policies:[0, 2, 2]
qAverage:[0.0, 34.71641540527344]
ws:[-0.3547891080379486, 2.2686431407928467]
memory len:10000
memory used:3129.0
now epsilon is 0.18411190345990563, the reward is 194.33333333333334 with loss [18.22697162628174, 23.307253122329712] in episode 1230
Report: 
rewardSum:194.33333333333334
loss:[18.22697162628174, 23.307253122329712]
policies:[0, 4, 0]
qAverage:[0.0, 55.11248016357422]
ws:[1.9676577858626842, 8.546878814697266]
memory len:10000
memory used:3129.0
now epsilon is 0.18392786058690325, the reward is 194.33333333333334 with loss [14.971003651618958, 27.5147647857666] in episode 1231
Report: 
rewardSum:194.33333333333334
loss:[14.971003651618958, 27.5147647857666]
policies:[0, 3, 1]
qAverage:[0.0, 56.111772537231445]
ws:[1.2553663104772568, 5.858683109283447]
memory len:10000
memory used:3129.0
now epsilon is 0.18374400168776933, the reward is 194.33333333333334 with loss [15.791884660720825, 19.097137928009033] in episode 1232
Report: 
rewardSum:194.33333333333334
loss:[15.791884660720825, 19.097137928009033]
policies:[0, 4, 0]
qAverage:[0.0, 51.655128479003906]
ws:[1.6320215861002605, 6.4298350016276045]
memory len:10000
memory used:3129.0
now epsilon is 0.18356032657859894, the reward is 194.33333333333334 with loss [22.330482006072998, 13.120653629302979] in episode 1233
Report: 
rewardSum:194.33333333333334
loss:[22.330482006072998, 13.120653629302979]
policies:[0, 4, 0]
qAverage:[0.0, 54.64543151855469]
ws:[1.3882825374603271, 5.21355003118515]
memory len:10000
memory used:3129.0
now epsilon is 0.18328515811918542, the reward is 192.33333333333334 with loss [36.31585144996643, 54.15822792053223] in episode 1234
Report: 
rewardSum:192.33333333333334
loss:[36.31585144996643, 54.15822792053223]
policies:[0, 5, 1]
qAverage:[0.0, 61.26617940266927]
ws:[1.3474223837256432, 5.8666472633679705]
memory len:10000
memory used:3129.0
now epsilon is 0.18301040215457653, the reward is 192.33333333333334 with loss [38.41442823410034, 26.3175311088562] in episode 1235
Report: 
rewardSum:192.33333333333334
loss:[38.41442823410034, 26.3175311088562]
policies:[0, 4, 2]
qAverage:[0.0, 59.356301879882814]
ws:[1.0611716032028198, 7.05445122718811]
memory len:10000
memory used:3129.0
now epsilon is 0.18282746036988537, the reward is 194.33333333333334 with loss [22.30545973777771, 14.203272581100464] in episode 1236
Report: 
rewardSum:194.33333333333334
loss:[22.30545973777771, 14.203272581100464]
policies:[0, 4, 0]
qAverage:[0.0, 59.244137573242185]
ws:[-0.11494920998811722, 5.160661721229554]
memory len:10000
memory used:3129.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22*		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.18264470145838715, the reward is 194.33333333333334 with loss [29.607380390167236, 30.34682822227478] in episode 1237
Report: 
rewardSum:194.33333333333334
loss:[29.607380390167236, 30.34682822227478]
policies:[1, 3, 0]
qAverage:[0.0, 53.51114463806152]
ws:[-0.4256778880953789, 2.947676509618759]
memory len:10000
memory used:3129.0
now epsilon is 0.18246212523727726, the reward is 194.33333333333334 with loss [21.31157112121582, 26.26898431777954] in episode 1238
Report: 
rewardSum:194.33333333333334
loss:[21.31157112121582, 26.26898431777954]
policies:[0, 4, 0]
qAverage:[0.0, 52.422767639160156]
ws:[1.0418790131807327, 5.491213798522949]
memory len:10000
memory used:3129.0
now epsilon is 0.1821430558998924, the reward is 191.33333333333334 with loss [41.348416805267334, 50.72397541999817] in episode 1239
Report: 
rewardSum:191.33333333333334
loss:[41.348416805267334, 50.72397541999817]
policies:[0, 4, 3]
qAverage:[0.0, 55.61478042602539]
ws:[-0.6702425807714463, 2.8874169826507567]
memory len:10000
memory used:3129.0
now epsilon is 0.18196098113625528, the reward is 194.33333333333334 with loss [23.148645401000977, 16.327264547348022] in episode 1240
Report: 
rewardSum:194.33333333333334
loss:[23.148645401000977, 16.327264547348022]
policies:[0, 4, 0]
qAverage:[0.0, 57.78749465942383]
ws:[-0.48658229112625123, 5.534766668081284]
memory len:10000
memory used:3129.0
now epsilon is 0.1816882101961186, the reward is 192.33333333333334 with loss [31.359882354736328, 22.528850317001343] in episode 1241
Report: 
rewardSum:192.33333333333334
loss:[31.359882354736328, 22.528850317001343]
policies:[1, 4, 1]
qAverage:[0.0, 54.4473991394043]
ws:[-1.9100933074951172, 3.0239766128361225]
memory len:10000
memory used:3129.0
now epsilon is 0.18150659010764653, the reward is 194.33333333333334 with loss [23.64135432243347, 33.158536195755005] in episode 1242
Report: 
rewardSum:194.33333333333334
loss:[23.64135432243347, 33.158536195755005]
policies:[0, 3, 1]
qAverage:[0.0, 52.214033126831055]
ws:[0.9291825592517853, 5.521459579467773]
memory len:10000
memory used:3129.0
now epsilon is 0.18132515157116674, the reward is 194.33333333333334 with loss [13.249458193778992, 17.0720055103302] in episode 1243
Report: 
rewardSum:194.33333333333334
loss:[13.249458193778992, 17.0720055103302]
policies:[0, 3, 1]
qAverage:[0.0, 54.560157775878906]
ws:[-0.3087169826030731, 5.895092949271202]
memory len:10000
memory used:3129.0
now epsilon is 0.18114389440519535, the reward is 194.33333333333334 with loss [21.890009880065918, 34.016037464141846] in episode 1244
Report: 
rewardSum:194.33333333333334
loss:[21.890009880065918, 34.016037464141846]
policies:[0, 4, 0]
qAverage:[0.0, 54.68381881713867]
ws:[-0.9922284781932831, 4.481579512357712]
memory len:10000
memory used:3129.0
now epsilon is 0.18096281842842976, the reward is 194.33333333333334 with loss [18.421021938323975, 25.03481960296631] in episode 1245
Report: 
rewardSum:194.33333333333334
loss:[18.421021938323975, 25.03481960296631]
policies:[0, 3, 1]
qAverage:[0.0, 51.16178639729818]
ws:[0.2894350488980611, 6.647462050120036]
memory len:10000
memory used:3129.0
now epsilon is 0.1807819234597488, the reward is 194.33333333333334 with loss [20.50064516067505, 26.46495294570923] in episode 1246
Report: 
rewardSum:194.33333333333334
loss:[20.50064516067505, 26.46495294570923]
policies:[0, 4, 0]
qAverage:[0.0, 52.4384126663208]
ws:[1.0994979292154312, 4.87162709236145]
memory len:10000
memory used:3129.0
now epsilon is 0.1806012093182122, the reward is 194.33333333333334 with loss [15.96665632724762, 17.191118359565735] in episode 1247
Report: 
rewardSum:194.33333333333334
loss:[15.96665632724762, 17.191118359565735]
policies:[0, 4, 0]
qAverage:[0.0, 58.454708862304685]
ws:[0.26047985553741454, 5.075752210617066]
memory len:10000
memory used:3129.0
now epsilon is 0.1804206758230606, the reward is 194.33333333333334 with loss [14.23274540901184, 19.96711540222168] in episode 1248
Report: 
rewardSum:194.33333333333334
loss:[14.23274540901184, 19.96711540222168]
policies:[0, 4, 0]
qAverage:[0.0, 59.21424179077148]
ws:[-0.11062738597393036, 5.284692692756653]
memory len:10000
memory used:3129.0
now epsilon is 0.18024032279371538, the reward is 194.33333333333334 with loss [29.616028308868408, 22.62623643875122] in episode 1249
Report: 
rewardSum:194.33333333333334
loss:[29.616028308868408, 22.62623643875122]
policies:[0, 4, 0]
qAverage:[0.0, 57.772483825683594]
ws:[-1.351770007610321, 3.5294466733932497]
memory len:10000
memory used:3129.0
now epsilon is 0.17997013122851294, the reward is 192.33333333333334 with loss [37.46742820739746, 34.244261264801025] in episode 1250
Report: 
rewardSum:192.33333333333334
loss:[37.46742820739746, 34.244261264801025]
policies:[1, 3, 2]
qAverage:[0.0, 53.20334053039551]
ws:[0.10490707959979773, 5.606445133686066]
memory len:10000
memory used:3129.0
now epsilon is 0.1797003446974381, the reward is 192.33333333333334 with loss [23.67096734046936, 35.02431058883667] in episode 1251
Report: 
rewardSum:192.33333333333334
loss:[23.67096734046936, 35.02431058883667]
policies:[0, 4, 2]
qAverage:[0.0, 54.582723236083986]
ws:[-0.4685202121734619, 3.6628823280334473]
memory len:10000
memory used:3129.0
now epsilon is 0.17952071172913936, the reward is 194.33333333333334 with loss [13.062560319900513, 24.948450565338135] in episode 1252
Report: 
rewardSum:194.33333333333334
loss:[13.062560319900513, 24.948450565338135]
policies:[0, 3, 1]
qAverage:[0.0, 53.395071029663086]
ws:[1.6911930553615093, 9.377211153507233]
memory len:10000
memory used:3129.0
now epsilon is 0.17934125832645778, the reward is 194.33333333333334 with loss [13.47741985321045, 13.689942598342896] in episode 1253
Report: 
rewardSum:194.33333333333334
loss:[13.47741985321045, 13.689942598342896]
policies:[0, 3, 1]
qAverage:[0.0, 56.783220291137695]
ws:[3.6453992128372192, 11.535214185714722]
memory len:10000
memory used:3129.0
now epsilon is 0.1791619843098951, the reward is 194.33333333333334 with loss [19.747826099395752, 22.979381322860718] in episode 1254
Report: 
rewardSum:194.33333333333334
loss:[19.747826099395752, 22.979381322860718]
policies:[0, 3, 1]
qAverage:[0.0, 52.44586181640625]
ws:[1.700152079264323, 7.712474505106608]
memory len:10000
memory used:3129.0
now epsilon is 0.1789828895001324, the reward is 194.33333333333334 with loss [19.993886470794678, 15.65580129623413] in episode 1255
Report: 
rewardSum:194.33333333333334
loss:[19.993886470794678, 15.65580129623413]
policies:[1, 3, 0]
qAverage:[0.0, 48.67939376831055]
ws:[-0.385882705450058, 4.489293615023295]
memory len:10000
memory used:3129.0
now epsilon is 0.1788039737180301, the reward is 194.33333333333334 with loss [20.929221630096436, 26.12970781326294] in episode 1256
Report: 
rewardSum:194.33333333333334
loss:[20.929221630096436, 26.12970781326294]
policies:[1, 3, 0]
qAverage:[0.0, 33.7813606262207]
ws:[1.927103877067566, 6.59319543838501]
memory len:10000
memory used:3129.0
now epsilon is 0.1786252367846277, the reward is 194.33333333333334 with loss [14.119139313697815, 18.9335880279541] in episode 1257
Report: 
rewardSum:194.33333333333334
loss:[14.119139313697815, 18.9335880279541]
policies:[0, 4, 0]
qAverage:[0.0, 53.67641957600912]
ws:[0.7437862356503805, 7.231046676635742]
memory len:10000
memory used:3129.0
now epsilon is 0.1784466785211435, the reward is 194.33333333333334 with loss [27.937845706939697, 32.24691820144653] in episode 1258
Report: 
rewardSum:194.33333333333334
loss:[27.937845706939697, 32.24691820144653]
policies:[1, 2, 1]
qAverage:[0.0, 49.70918273925781]
ws:[2.265173057715098, 10.537503242492676]
memory len:10000
memory used:3129.0
now epsilon is 0.17817917574136877, the reward is 192.33333333333334 with loss [29.865745067596436, 31.896571159362793] in episode 1259
Report: 
rewardSum:192.33333333333334
loss:[29.865745067596436, 31.896571159362793]
policies:[0, 4, 2]
qAverage:[0.0, 59.39780120849609]
ws:[1.473611456155777, 8.765045738220214]
memory len:10000
memory used:3129.0
now epsilon is 0.17800106337168284, the reward is 194.33333333333334 with loss [12.997754216194153, 15.09792685508728] in episode 1260
Report: 
rewardSum:194.33333333333334
loss:[12.997754216194153, 15.09792685508728]
policies:[0, 4, 0]
qAverage:[0.0, 56.290869140625]
ws:[2.3161006569862366, 9.427190685272217]
memory len:10000
memory used:3129.0
now epsilon is 0.17773422859700738, the reward is 192.33333333333334 with loss [28.657588481903076, 39.750232219696045] in episode 1261
Report: 
rewardSum:192.33333333333334
loss:[28.657588481903076, 39.750232219696045]
policies:[0, 4, 2]
qAverage:[0.0, 53.55212249755859]
ws:[1.6792792782187462, 6.320875787734986]
memory len:10000
memory used:3129.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21*		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.1775565610076384, the reward is 194.33333333333334 with loss [16.041856050491333, 28.205063343048096] in episode 1262
Report: 
rewardSum:194.33333333333334
loss:[16.041856050491333, 28.205063343048096]
policies:[0, 3, 1]
qAverage:[0.0, 53.701459884643555]
ws:[1.3055011630058289, 6.859455168247223]
memory len:10000
memory used:3129.0
now epsilon is 0.17729039256992687, the reward is 192.33333333333334 with loss [42.259340047836304, 38.47004461288452] in episode 1263
Report: 
rewardSum:192.33333333333334
loss:[42.259340047836304, 38.47004461288452]
policies:[3, 2, 1]
qAverage:[0.0, 49.222460428873696]
ws:[3.056204080581665, 11.346444447835287]
memory len:10000
memory used:3129.0
now epsilon is 0.17684766488545717, the reward is 188.33333333333334 with loss [52.56775152683258, 47.688866913318634] in episode 1264
Report: 
rewardSum:188.33333333333334
loss:[52.56775152683258, 47.688866913318634]
policies:[0, 6, 4]
qAverage:[0.0, 58.39518247331892]
ws:[-0.9488474300929478, 3.299913321222578]
memory len:10000
memory used:3129.0
now epsilon is 0.17667088352739374, the reward is 194.33333333333334 with loss [16.65124487876892, 14.431355714797974] in episode 1265
Report: 
rewardSum:194.33333333333334
loss:[16.65124487876892, 14.431355714797974]
policies:[0, 4, 0]
qAverage:[0.0, 52.23278617858887]
ws:[-0.7216520458459854, 4.141360551118851]
memory len:10000
memory used:3129.0
now epsilon is 0.17649427888440644, the reward is 194.33333333333334 with loss [16.113003730773926, 21.6064190864563] in episode 1266
Report: 
rewardSum:194.33333333333334
loss:[16.113003730773926, 21.6064190864563]
policies:[0, 4, 0]
qAverage:[0.0, 55.920156478881836]
ws:[0.8512836992740631, 6.814306855201721]
memory len:10000
memory used:3129.0
now epsilon is 0.17631785077984646, the reward is 194.33333333333334 with loss [14.912293434143066, 23.43347692489624] in episode 1267
Report: 
rewardSum:194.33333333333334
loss:[14.912293434143066, 23.43347692489624]
policies:[0, 4, 0]
qAverage:[0.0, 56.333045959472656]
ws:[-0.8847420990467072, 3.437342756986618]
memory len:10000
memory used:3129.0
now epsilon is 0.17609756363748216, the reward is 193.33333333333334 with loss [17.898239374160767, 26.95185899734497] in episode 1268
Report: 
rewardSum:193.33333333333334
loss:[17.898239374160767, 26.95185899734497]
policies:[0, 4, 1]
qAverage:[0.0, 56.99748001098633]
ws:[-0.07936832904815674, 5.034029975533485]
memory len:10000
memory used:3129.0
now epsilon is 0.1758335823284717, the reward is 192.33333333333334 with loss [25.107407212257385, 40.60828232765198] in episode 1269
Report: 
rewardSum:192.33333333333334
loss:[25.107407212257385, 40.60828232765198]
policies:[0, 5, 1]
qAverage:[0.0, 53.4670524597168]
ws:[0.285802948474884, 3.9714115142822264]
memory len:10000
memory used:3129.0
now epsilon is 0.17570174010777467, the reward is -2.0 with loss [18.22342300415039, 21.009398460388184] in episode 1270
Report: 
rewardSum:-2.0
loss:[18.22342300415039, 21.009398460388184]
policies:[0, 1, 2]
qAverage:[0.0, 29.59107780456543]
ws:[0.9519907832145691, 2.1787846088409424]
memory len:10000
memory used:3129.0
now epsilon is 0.17561390021907955, the reward is -1.0 with loss [9.264744758605957, 14.886224746704102] in episode 1271
Report: 
rewardSum:-1.0
loss:[9.264744758605957, 14.886224746704102]
policies:[0, 1, 1]
qAverage:[0.0, 29.476280212402344]
ws:[0.27986034750938416, 1.4916067123413086]
memory len:10000
memory used:3129.0
now epsilon is 0.1754383521630979, the reward is 194.33333333333334 with loss [17.034929513931274, 16.155214309692383] in episode 1272
Report: 
rewardSum:194.33333333333334
loss:[17.034929513931274, 16.155214309692383]
policies:[0, 3, 1]
qAverage:[0.0, 53.13467216491699]
ws:[0.7182994484901428, 4.778465390205383]
memory len:10000
memory used:3129.0
now epsilon is 0.1751753590534942, the reward is 192.33333333333334 with loss [34.69746541976929, 33.04160833358765] in episode 1273
Report: 
rewardSum:192.33333333333334
loss:[34.69746541976929, 33.04160833358765]
policies:[0, 5, 1]
qAverage:[0.0, 54.70607223510742]
ws:[0.3789084732532501, 3.564858865737915]
memory len:10000
memory used:3129.0
now epsilon is 0.1750002493742526, the reward is 194.33333333333334 with loss [15.724659442901611, 32.004342555999756] in episode 1274
Report: 
rewardSum:194.33333333333334
loss:[15.724659442901611, 32.004342555999756]
policies:[0, 4, 0]
qAverage:[0.0, 55.208970642089845]
ws:[0.400006203353405, 4.469252741336822]
memory len:10000
memory used:3129.0
now epsilon is 0.17482531473903504, the reward is 194.33333333333334 with loss [17.233670234680176, 20.451493978500366] in episode 1275
Report: 
rewardSum:194.33333333333334
loss:[17.233670234680176, 20.451493978500366]
policies:[0, 4, 0]
qAverage:[0.0, 55.83236999511719]
ws:[2.133997426182032, 7.761871290206909]
memory len:10000
memory used:3129.0
now epsilon is 0.17465055497286314, the reward is 194.33333333333334 with loss [20.73207426071167, 27.16196322441101] in episode 1276
Report: 
rewardSum:194.33333333333334
loss:[20.73207426071167, 27.16196322441101]
policies:[1, 3, 0]
qAverage:[0.0, 51.42773723602295]
ws:[1.9728814512491226, 8.228049159049988]
memory len:10000
memory used:3129.0
now epsilon is 0.17447596990093342, the reward is 194.33333333333334 with loss [25.702935218811035, 10.747257113456726] in episode 1277
Report: 
rewardSum:194.33333333333334
loss:[25.702935218811035, 10.747257113456726]
policies:[0, 4, 0]
qAverage:[0.0, 55.941131591796875]
ws:[1.1193710684776306, 7.0486180782318115]
memory len:10000
memory used:3129.0
now epsilon is 0.17425798395878, the reward is 193.33333333333334 with loss [24.495580673217773, 25.58291506767273] in episode 1278
Report: 
rewardSum:193.33333333333334
loss:[24.495580673217773, 25.58291506767273]
policies:[0, 4, 1]
qAverage:[0.0, 49.61548328399658]
ws:[0.6114888489246368, 5.107997447252274]
memory len:10000
memory used:3129.0
now epsilon is 0.1739967602952564, the reward is 192.33333333333334 with loss [31.037335872650146, 40.8033332824707] in episode 1279
Report: 
rewardSum:192.33333333333334
loss:[31.037335872650146, 40.8033332824707]
policies:[0, 4, 2]
qAverage:[0.0, 57.28323059082031]
ws:[-1.5837584614753724, 4.5128645420074465]
memory len:10000
memory used:3129.0
now epsilon is 0.17382282877287217, the reward is 194.33333333333334 with loss [16.662036418914795, 13.338515996932983] in episode 1280
Report: 
rewardSum:194.33333333333334
loss:[16.662036418914795, 13.338515996932983]
policies:[0, 4, 0]
qAverage:[0.0, 56.19751052856445]
ws:[0.1303333282470703, 7.1076713562011715]
memory len:10000
memory used:3129.0
now epsilon is 0.17364907111679687, the reward is 194.33333333333334 with loss [33.71582555770874, 21.499779224395752] in episode 1281
Report: 
rewardSum:194.33333333333334
loss:[33.71582555770874, 21.499779224395752]
policies:[0, 4, 0]
qAverage:[0.0, 43.76693344116211]
ws:[-0.09847899278004964, 3.3978824615478516]
memory len:10000
memory used:3129.0
now epsilon is 0.17347548715322936, the reward is 194.33333333333334 with loss [18.92954385280609, 21.59311008453369] in episode 1282
Report: 
rewardSum:194.33333333333334
loss:[18.92954385280609, 21.59311008453369]
policies:[0, 4, 0]
qAverage:[0.0, 50.84592628479004]
ws:[1.2330524511635303, 9.019888937473297]
memory len:10000
memory used:3129.0
now epsilon is 0.17330207670854228, the reward is 194.33333333333334 with loss [17.294462203979492, 13.329598665237427] in episode 1283
Report: 
rewardSum:194.33333333333334
loss:[17.294462203979492, 13.329598665237427]
policies:[1, 3, 0]
qAverage:[0.0, 51.2614107131958]
ws:[1.8828055188059807, 9.424246191978455]
memory len:10000
memory used:3129.0
now epsilon is 0.17312883960928183, the reward is 194.33333333333334 with loss [23.038701057434082, 22.82467555999756] in episode 1284
Report: 
rewardSum:194.33333333333334
loss:[23.038701057434082, 22.82467555999756]
policies:[0, 4, 0]
qAverage:[0.0, 55.1609245300293]
ws:[1.4123965799808502, 7.763917303085327]
memory len:10000
memory used:3129.0
now epsilon is 0.17295577568216752, the reward is 194.33333333333334 with loss [24.29604721069336, 28.975308656692505] in episode 1285
Report: 
rewardSum:194.33333333333334
loss:[24.29604721069336, 28.975308656692505]
policies:[0, 4, 0]
qAverage:[0.0, 51.0912504196167]
ws:[1.004356823861599, 7.373613595962524]
memory len:10000
memory used:3129.0
now epsilon is 0.1727828847540922, the reward is 194.33333333333334 with loss [16.484498977661133, 16.368063926696777] in episode 1286
Report: 
rewardSum:194.33333333333334
loss:[16.484498977661133, 16.368063926696777]
policies:[0, 3, 1]
qAverage:[0.0, 48.63859748840332]
ws:[0.5692454800009727, 3.221055895090103]
memory len:10000
memory used:3129.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20*		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.17261016665212164, the reward is 194.33333333333334 with loss [20.696505069732666, 21.274675607681274] in episode 1287
Report: 
rewardSum:194.33333333333334
loss:[20.696505069732666, 21.274675607681274]
policies:[1, 3, 0]
qAverage:[0.0, 54.19332313537598]
ws:[2.1704543083906174, 8.973171830177307]
memory len:10000
memory used:3129.0
now epsilon is 0.17235141317024416, the reward is 192.33333333333334 with loss [27.802932024002075, 35.37674069404602] in episode 1288
Report: 
rewardSum:192.33333333333334
loss:[27.802932024002075, 35.37674069404602]
policies:[0, 4, 2]
qAverage:[0.0, 50.76094627380371]
ws:[1.050958663225174, 7.236083269119263]
memory len:10000
memory used:3129.0
now epsilon is 0.1721791263780826, the reward is 194.33333333333334 with loss [24.556119441986084, 19.01536512374878] in episode 1289
Report: 
rewardSum:194.33333333333334
loss:[24.556119441986084, 19.01536512374878]
policies:[0, 4, 0]
qAverage:[0.0, 43.639696756998696]
ws:[0.9382225672403971, 2.9698665936787925]
memory len:10000
memory used:3129.0
now epsilon is 0.17200701180811642, the reward is 194.33333333333334 with loss [25.192792892456055, 20.659064531326294] in episode 1290
Report: 
rewardSum:194.33333333333334
loss:[25.192792892456055, 20.659064531326294]
policies:[0, 4, 0]
qAverage:[0.0, 55.31576080322266]
ws:[1.3673417031764985, 7.5576647281646725]
memory len:10000
memory used:3129.0
now epsilon is 0.17170622520261242, the reward is 191.33333333333334 with loss [42.217233419418335, 27.562030792236328] in episode 1291
Report: 
rewardSum:191.33333333333334
loss:[42.217233419418335, 27.562030792236328]
policies:[1, 3, 3]
qAverage:[0.0, 50.863094329833984]
ws:[0.6909197121858597, 7.268392443656921]
memory len:10000
memory used:3129.0
now epsilon is 0.17153458335651334, the reward is 194.33333333333334 with loss [25.00738263130188, 17.39245891571045] in episode 1292
Report: 
rewardSum:194.33333333333334
loss:[25.00738263130188, 17.39245891571045]
policies:[0, 4, 0]
qAverage:[0.0, 55.79199295043945]
ws:[1.0291966438293456, 7.294342136383056]
memory len:10000
memory used:3129.0
now epsilon is 0.17136311308790536, the reward is 194.33333333333334 with loss [19.169525861740112, 26.53997564315796] in episode 1293
Report: 
rewardSum:194.33333333333334
loss:[19.169525861740112, 26.53997564315796]
policies:[0, 4, 0]
qAverage:[0.0, 56.35974273681641]
ws:[0.6316045880317688, 7.149472284317016]
memory len:10000
memory used:3129.0
now epsilon is 0.17119181422527538, the reward is 194.33333333333334 with loss [17.696547746658325, 25.247708082199097] in episode 1294
Report: 
rewardSum:194.33333333333334
loss:[17.696547746658325, 25.247708082199097]
policies:[1, 3, 0]
qAverage:[0.0, 51.4464168548584]
ws:[-0.9888749644160271, 4.961301803588867]
memory len:10000
memory used:3129.0
now epsilon is 0.17102068659728165, the reward is 194.33333333333334 with loss [13.402962923049927, 33.520033836364746] in episode 1295
Report: 
rewardSum:194.33333333333334
loss:[13.402962923049927, 33.520033836364746]
policies:[0, 3, 1]
qAverage:[0.0, 49.51876735687256]
ws:[0.00814568717032671, 4.5941552221775055]
memory len:10000
memory used:3136.0
now epsilon is 0.17084973003275372, the reward is 194.33333333333334 with loss [16.437727689743042, 16.196762919425964] in episode 1296
Report: 
rewardSum:194.33333333333334
loss:[16.437727689743042, 16.196762919425964]
policies:[0, 4, 0]
qAverage:[0.0, 55.950048828125]
ws:[-1.1879111155867577, 4.57709002494812]
memory len:10000
memory used:3136.0
now epsilon is 0.1706789443606923, the reward is 194.33333333333334 with loss [17.54074764251709, 19.34971857070923] in episode 1297
Report: 
rewardSum:194.33333333333334
loss:[17.54074764251709, 19.34971857070923]
policies:[1, 3, 0]
qAverage:[0.0, 53.71610641479492]
ws:[-0.39881905913352966, 4.634767472743988]
memory len:10000
memory used:3136.0
now epsilon is 0.17059361555594602, the reward is -1.0 with loss [9.486722707748413, 10.601783275604248] in episode 1298
Report: 
rewardSum:-1.0
loss:[9.486722707748413, 10.601783275604248]
policies:[0, 1, 1]
qAverage:[0.0, 28.88439178466797]
ws:[-0.3472899794578552, 1.0373810529708862]
memory len:10000
memory used:3136.0
now epsilon is 0.17050832941026903, the reward is -1.0 with loss [10.514699459075928, 8.874606609344482] in episode 1299
Report: 
rewardSum:-1.0
loss:[10.514699459075928, 8.874606609344482]
policies:[0, 1, 1]
qAverage:[0.0, 28.916893005371094]
ws:[-0.38132575154304504, 1.0400424003601074]
memory len:10000
memory used:3136.0
now epsilon is 0.1703378850108262, the reward is 194.33333333333334 with loss [20.369483470916748, 14.466958284378052] in episode 1300
Report: 
rewardSum:194.33333333333334
loss:[20.369483470916748, 14.466958284378052]
policies:[0, 3, 1]
qAverage:[0.0, 55.16518974304199]
ws:[0.13856162130832672, 5.391488552093506]
memory len:10000
memory used:3136.0
now epsilon is 0.17012506908912883, the reward is 193.33333333333334 with loss [21.330578207969666, 10.93779194355011] in episode 1301
Report: 
rewardSum:193.33333333333334
loss:[21.330578207969666, 10.93779194355011]
policies:[0, 3, 2]
qAverage:[0.0, 55.5871639251709]
ws:[1.7538900077342987, 8.43524718284607]
memory len:10000
memory used:3136.0
now epsilon is 0.1699550078063085, the reward is 194.33333333333334 with loss [25.780487060546875, 22.43196201324463] in episode 1302
Report: 
rewardSum:194.33333333333334
loss:[25.780487060546875, 22.43196201324463]
policies:[1, 3, 0]
qAverage:[0.0, 49.05919551849365]
ws:[0.8202308043837547, 3.9051772356033325]
memory len:10000
memory used:3136.0
now epsilon is 0.16978511652100864, the reward is 194.33333333333334 with loss [14.184452772140503, 27.72895383834839] in episode 1303
Report: 
rewardSum:194.33333333333334
loss:[14.184452772140503, 27.72895383834839]
policies:[0, 3, 1]
qAverage:[0.0, 49.248799641927086]
ws:[0.7123138507207235, 7.031689643859863]
memory len:10000
memory used:3136.0
now epsilon is 0.16961539506329545, the reward is 46.33333333333334 with loss [20.412532567977905, 26.308446884155273] in episode 1304
Report: 
rewardSum:46.33333333333334
loss:[20.412532567977905, 26.308446884155273]
policies:[2, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:3136.0
now epsilon is 0.16944584326340498, the reward is 194.33333333333334 with loss [20.334694385528564, 21.63491153717041] in episode 1305
Report: 
rewardSum:194.33333333333334
loss:[20.334694385528564, 21.63491153717041]
policies:[0, 4, 0]
qAverage:[0.0, 55.551072692871095]
ws:[1.3978452920913695, 5.77999267578125]
memory len:10000
memory used:3136.0
now epsilon is 0.1692764609517431, the reward is 194.33333333333334 with loss [27.424936771392822, 15.083777666091919] in episode 1306
Report: 
rewardSum:194.33333333333334
loss:[27.424936771392822, 15.083777666091919]
policies:[0, 3, 1]
qAverage:[0.0, 51.09215068817139]
ws:[1.9565047211945057, 7.719409942626953]
memory len:10000
memory used:3136.0
now epsilon is 0.1691072479588851, the reward is 194.33333333333334 with loss [15.072974920272827, 25.15326189994812] in episode 1307
Report: 
rewardSum:194.33333333333334
loss:[15.072974920272827, 25.15326189994812]
policies:[0, 4, 0]
qAverage:[0.0, 55.02652359008789]
ws:[0.903047239780426, 5.553319358825684]
memory len:10000
memory used:3136.0
now epsilon is 0.1689382041155757, the reward is 194.33333333333334 with loss [18.99050784111023, 17.46259367465973] in episode 1308
Report: 
rewardSum:194.33333333333334
loss:[18.99050784111023, 17.46259367465973]
policies:[1, 3, 0]
qAverage:[0.0, 55.36900329589844]
ws:[0.32117655873298645, 6.242484927177429]
memory len:10000
memory used:3136.0
now epsilon is 0.1687693292527287, the reward is 194.33333333333334 with loss [28.167742252349854, 17.303230047225952] in episode 1309
Report: 
rewardSum:194.33333333333334
loss:[28.167742252349854, 17.303230047225952]
policies:[0, 3, 1]
qAverage:[0.0, 50.42035675048828]
ws:[-0.07664785782496135, 6.671568711598714]
memory len:10000
memory used:3136.0
now epsilon is 0.16860062320142705, the reward is 194.33333333333334 with loss [22.348820686340332, 31.830070972442627] in episode 1310
Report: 
rewardSum:194.33333333333334
loss:[22.348820686340332, 31.830070972442627]
policies:[1, 3, 0]
qAverage:[0.0, 54.83432960510254]
ws:[0.23915790021419525, 6.520505428314209]
memory len:10000
memory used:3137.0
now epsilon is 0.16834788027703138, the reward is 192.33333333333334 with loss [25.248663306236267, 33.98319387435913] in episode 1311
Report: 
rewardSum:192.33333333333334
loss:[25.248663306236267, 33.98319387435913]
policies:[0, 3, 3]
qAverage:[0.0, 52.76102828979492]
ws:[-0.8778314962983131, 3.9960037916898727]
memory len:10000
memory used:3137.0
now epsilon is 0.16817959551668837, the reward is 194.33333333333334 with loss [22.639188289642334, 27.926515102386475] in episode 1312
Report: 
rewardSum:194.33333333333334
loss:[22.639188289642334, 27.926515102386475]
policies:[0, 4, 0]
qAverage:[0.0, 55.747711944580075]
ws:[0.03980688452720642, 5.7313642501831055]
memory len:10000
memory used:3137.0
now epsilon is 0.16801147897800947, the reward is 194.33333333333334 with loss [18.40240216255188, 25.1091046333313] in episode 1313
Report: 
rewardSum:194.33333333333334
loss:[18.40240216255188, 25.1091046333313]
policies:[0, 4, 0]
qAverage:[0.0, 55.8739730834961]
ws:[-0.21360033750534058, 5.700076985359192]
memory len:10000
memory used:3134.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11*		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.16767574989317763, the reward is 190.33333333333334 with loss [51.0624942779541, 55.194740533828735] in episode 1314
Report: 
rewardSum:190.33333333333334
loss:[51.0624942779541, 55.194740533828735]
policies:[0, 6, 2]
qAverage:[0.0, 56.75656182425363]
ws:[0.1607668740408761, 5.047684720584324]
memory len:10000
memory used:3133.0
now epsilon is 0.16742439341196458, the reward is 192.33333333333334 with loss [35.94240641593933, 41.9156596660614] in episode 1315
Report: 
rewardSum:192.33333333333334
loss:[35.94240641593933, 41.9156596660614]
policies:[0, 5, 1]
qAverage:[0.0, 57.34616216023763]
ws:[0.1026303619146347, 5.417238752047221]
memory len:10000
memory used:3132.0
now epsilon is 0.16725703179223678, the reward is 194.33333333333334 with loss [20.693960189819336, 29.97488009929657] in episode 1316
Report: 
rewardSum:194.33333333333334
loss:[20.693960189819336, 29.97488009929657]
policies:[0, 4, 0]
qAverage:[0.0, 55.281412506103514]
ws:[0.3768878042697906, 6.916587018966675]
memory len:10000
memory used:3133.0
now epsilon is 0.1670898374713786, the reward is 194.33333333333334 with loss [19.082775592803955, 14.396581172943115] in episode 1317
Report: 
rewardSum:194.33333333333334
loss:[19.082775592803955, 14.396581172943115]
policies:[0, 4, 0]
qAverage:[0.0, 55.43972930908203]
ws:[-0.11854844093322754, 6.777772676944733]
memory len:10000
memory used:3133.0
now epsilon is 0.16692281028215383, the reward is 194.33333333333334 with loss [11.790005505084991, 27.504262447357178] in episode 1318
Report: 
rewardSum:194.33333333333334
loss:[11.790005505084991, 27.504262447357178]
policies:[0, 3, 1]
qAverage:[0.0, 46.22562789916992]
ws:[-1.3748711744944255, 4.991373340288798]
memory len:10000
memory used:3133.0
now epsilon is 0.16675595005749352, the reward is 194.33333333333334 with loss [20.938050031661987, 20.118359804153442] in episode 1319
Report: 
rewardSum:194.33333333333334
loss:[20.938050031661987, 20.118359804153442]
policies:[1, 3, 0]
qAverage:[0.0, 49.24234485626221]
ws:[0.8921964466571808, 5.5230183601379395]
memory len:10000
memory used:3133.0
now epsilon is 0.16658925663049573, the reward is 194.33333333333334 with loss [17.49354887008667, 20.726013660430908] in episode 1320
Report: 
rewardSum:194.33333333333334
loss:[17.49354887008667, 20.726013660430908]
policies:[0, 3, 1]
qAverage:[0.0, 53.51583957672119]
ws:[0.7726050913333893, 8.164922833442688]
memory len:10000
memory used:3132.0
now epsilon is 0.1664227298344253, the reward is 194.33333333333334 with loss [24.51478362083435, 16.199064254760742] in episode 1321
Report: 
rewardSum:194.33333333333334
loss:[24.51478362083435, 16.199064254760742]
policies:[0, 4, 0]
qAverage:[0.0, 55.55868759155273]
ws:[0.8646630048751831, 6.731311941146851]
memory len:10000
memory used:3132.0
now epsilon is 0.16621480541033815, the reward is 193.33333333333334 with loss [22.098149061203003, 27.17487931251526] in episode 1322
Report: 
rewardSum:193.33333333333334
loss:[22.098149061203003, 27.17487931251526]
policies:[0, 4, 1]
qAverage:[0.0, 52.74576663970947]
ws:[0.00044143572449684143, 4.650597989559174]
memory len:10000
memory used:3132.0
now epsilon is 0.16604865292509205, the reward is 194.33333333333334 with loss [32.53499221801758, 30.787110328674316] in episode 1323
Report: 
rewardSum:194.33333333333334
loss:[32.53499221801758, 30.787110328674316]
policies:[1, 3, 0]
qAverage:[0.0, 54.038536071777344]
ws:[0.6462229490280151, 7.392849087715149]
memory len:10000
memory used:3132.0
now epsilon is 0.16588266653003444, the reward is 194.33333333333334 with loss [23.57207727432251, 20.020167112350464] in episode 1324
Report: 
rewardSum:194.33333333333334
loss:[23.57207727432251, 20.020167112350464]
policies:[0, 4, 0]
qAverage:[0.0, 55.314368438720706]
ws:[-0.7720495760440826, 5.5902263879776]
memory len:10000
memory used:3132.0
now epsilon is 0.1655925894939123, the reward is 191.33333333333334 with loss [47.20669770240784, 41.263070821762085] in episode 1325
Report: 
rewardSum:191.33333333333334
loss:[47.20669770240784, 41.263070821762085]
policies:[0, 5, 2]
qAverage:[0.0, 56.95727348327637]
ws:[0.667611320813497, 8.334975779056549]
memory len:10000
memory used:3133.0
now epsilon is 0.1654270589912906, the reward is 194.33333333333334 with loss [21.760628700256348, 24.438425540924072] in episode 1326
Report: 
rewardSum:194.33333333333334
loss:[21.760628700256348, 24.438425540924072]
policies:[0, 4, 0]
qAverage:[0.0, 55.21690979003906]
ws:[0.14530019760131835, 6.76334662437439]
memory len:10000
memory used:3132.0
now epsilon is 0.16534435580098614, the reward is -1.0 with loss [16.0574209690094, 7.725775480270386] in episode 1327
Report: 
rewardSum:-1.0
loss:[16.0574209690094, 7.725775480270386]
policies:[0, 1, 1]
qAverage:[0.0, 29.55736541748047]
ws:[-0.7085038423538208, 0.42134132981300354]
memory len:10000
memory used:3133.0
now epsilon is 0.1651790734389852, the reward is 194.33333333333334 with loss [20.40257740020752, 27.931701183319092] in episode 1328
Report: 
rewardSum:194.33333333333334
loss:[20.40257740020752, 27.931701183319092]
policies:[1, 3, 0]
qAverage:[0.0, 53.205013275146484]
ws:[-0.9730616509914398, 4.364673748612404]
memory len:10000
memory used:3133.0
now epsilon is 0.16501395629737575, the reward is 194.33333333333334 with loss [21.50578260421753, 21.441543102264404] in episode 1329
Report: 
rewardSum:194.33333333333334
loss:[21.50578260421753, 21.441543102264404]
policies:[0, 4, 0]
qAverage:[0.0, 54.821510314941406]
ws:[0.5964125990867615, 7.720519304275513]
memory len:10000
memory used:3133.0
now epsilon is 0.16464304596060875, the reward is 189.33333333333334 with loss [40.808486104011536, 36.34604299068451] in episode 1330
Report: 
rewardSum:189.33333333333334
loss:[40.808486104011536, 36.34604299068451]
policies:[1, 5, 3]
qAverage:[0.0, 52.44149208068848]
ws:[-0.5136645641177893, 5.2367435693740845]
memory len:10000
memory used:3133.0
now epsilon is 0.16447846464550084, the reward is 194.33333333333334 with loss [17.188536167144775, 28.834210634231567] in episode 1331
Report: 
rewardSum:194.33333333333334
loss:[17.188536167144775, 28.834210634231567]
policies:[1, 3, 0]
qAverage:[0.0, 50.70600986480713]
ws:[0.49243488907814026, 5.65841281414032]
memory len:10000
memory used:3133.0
now epsilon is 0.1643140478500003, the reward is 194.33333333333334 with loss [15.639992594718933, 15.518957138061523] in episode 1332
Report: 
rewardSum:194.33333333333334
loss:[15.639992594718933, 15.518957138061523]
policies:[0, 3, 1]
qAverage:[0.0, 53.03929042816162]
ws:[-0.34464268386363983, 4.01063072681427]
memory len:10000
memory used:3138.0
now epsilon is 0.16414979540964927, the reward is 194.33333333333334 with loss [29.530508518218994, 27.164179801940918] in episode 1333
Report: 
rewardSum:194.33333333333334
loss:[29.530508518218994, 27.164179801940918]
policies:[1, 3, 0]
qAverage:[0.0, 54.815202713012695]
ws:[-1.1839824840426445, 4.0631877183914185]
memory len:10000
memory used:3145.0
now epsilon is 0.16394471073336414, the reward is 193.33333333333334 with loss [24.888806208968163, 16.62469756603241] in episode 1334
Report: 
rewardSum:193.33333333333334
loss:[24.888806208968163, 16.62469756603241]
policies:[0, 4, 1]
qAverage:[0.0, 54.791404724121094]
ws:[0.16217024326324464, 6.985620212554932]
memory len:10000
memory used:3145.0
now epsilon is 0.16369894731420728, the reward is 192.33333333333334 with loss [34.34582257270813, 25.160725235939026] in episode 1335
Report: 
rewardSum:192.33333333333334
loss:[34.34582257270813, 25.160725235939026]
policies:[1, 4, 1]
qAverage:[0.0, 53.363705444335935]
ws:[1.369891381263733, 8.76063938140869]
memory len:10000
memory used:3132.0
now epsilon is 0.16345355230985278, the reward is 192.33333333333334 with loss [28.894038677215576, 37.26651382446289] in episode 1336
Report: 
rewardSum:192.33333333333334
loss:[28.894038677215576, 37.26651382446289]
policies:[0, 5, 1]
qAverage:[0.0, 53.49552841186524]
ws:[0.8181515127420426, 5.833369541168213]
memory len:10000
memory used:3132.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22*		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.16320852516802367, the reward is 192.33333333333334 with loss [32.06474828720093, 39.66208779811859] in episode 1337
Report: 
rewardSum:192.33333333333334
loss:[32.06474828720093, 39.66208779811859]
policies:[0, 3, 3]
qAverage:[0.0, 54.41594696044922]
ws:[-0.41661617159843445, 6.194246172904968]
memory len:10000
memory used:3133.0
now epsilon is 0.16304537783585268, the reward is 194.33333333333334 with loss [17.58854389190674, 15.339248180389404] in episode 1338
Report: 
rewardSum:194.33333333333334
loss:[17.58854389190674, 15.339248180389404]
policies:[0, 4, 0]
qAverage:[0.0, 54.41975021362305]
ws:[-0.2654471222311258, 6.006542921066284]
memory len:10000
memory used:3132.0
now epsilon is 0.16288239358984385, the reward is 194.33333333333334 with loss [21.613149642944336, 17.402195692062378] in episode 1339
Report: 
rewardSum:194.33333333333334
loss:[21.613149642944336, 17.402195692062378]
policies:[0, 4, 0]
qAverage:[0.0, 54.42662048339844]
ws:[-0.13472737967967988, 6.5273538589477536]
memory len:10000
memory used:3132.0
now epsilon is 0.16271957226697212, the reward is 46.33333333333334 with loss [13.05629014968872, 25.166951894760132] in episode 1340
Report: 
rewardSum:46.33333333333334
loss:[13.05629014968872, 25.166951894760132]
policies:[0, 3, 1]
qAverage:[0.0, 48.406630516052246]
ws:[0.8508302979171276, 4.894708633422852]
memory len:10000
memory used:3132.0
now epsilon is 0.16247564540733034, the reward is 192.33333333333334 with loss [30.71323013305664, 37.34797382354736] in episode 1341
Report: 
rewardSum:192.33333333333334
loss:[30.71323013305664, 37.34797382354736]
policies:[0, 5, 1]
qAverage:[0.0, 56.23497327168783]
ws:[1.8396879732608795, 9.025709788004557]
memory len:10000
memory used:3133.0
now epsilon is 0.16227265237246594, the reward is 193.33333333333334 with loss [25.635002613067627, 24.12145209312439] in episode 1342
Report: 
rewardSum:193.33333333333334
loss:[25.635002613067627, 24.12145209312439]
policies:[1, 2, 2]
qAverage:[0.0, 43.38084284464518]
ws:[0.8531362016995748, 2.898678779602051]
memory len:10000
memory used:3133.0
now epsilon is 0.16202939547381814, the reward is 192.33333333333334 with loss [29.776792526245117, 19.794803857803345] in episode 1343
Report: 
rewardSum:192.33333333333334
loss:[29.776792526245117, 19.794803857803345]
policies:[0, 3, 3]
qAverage:[0.0, 52.87057018280029]
ws:[-1.222019724547863, 4.49204258620739]
memory len:10000
memory used:3133.0
now epsilon is 0.161786503232541, the reward is 192.33333333333334 with loss [24.328149557113647, 40.86444115638733] in episode 1344
Report: 
rewardSum:192.33333333333334
loss:[24.328149557113647, 40.86444115638733]
policies:[0, 4, 2]
qAverage:[0.0, 52.259817123413086]
ws:[-1.0164566189050674, 4.536495804786682]
memory len:10000
memory used:3133.0
now epsilon is 0.1616247773891362, the reward is 194.33333333333334 with loss [27.16803503036499, 21.838473320007324] in episode 1345
Report: 
rewardSum:194.33333333333334
loss:[27.16803503036499, 21.838473320007324]
policies:[1, 3, 0]
qAverage:[0.0, 52.64740467071533]
ws:[-1.0269254446029663, 4.919881612062454]
memory len:10000
memory used:3133.0
now epsilon is 0.1614632132109377, the reward is 194.33333333333334 with loss [19.028290033340454, 23.653481483459473] in episode 1346
Report: 
rewardSum:194.33333333333334
loss:[19.028290033340454, 23.653481483459473]
policies:[0, 4, 0]
qAverage:[0.0, 54.94992599487305]
ws:[-0.5587251394987106, 5.402982258796692]
memory len:10000
memory used:3133.0
now epsilon is 0.16118086442000784, the reward is 191.33333333333334 with loss [37.86993598937988, 29.533815622329712] in episode 1347
Report: 
rewardSum:191.33333333333334
loss:[37.86993598937988, 29.533815622329712]
policies:[1, 4, 2]
qAverage:[0.0, 49.725321769714355]
ws:[2.525595545768738, 9.981600999832153]
memory len:10000
memory used:3133.0
now epsilon is 0.16093924418007868, the reward is 192.33333333333334 with loss [32.877270460128784, 30.422529220581055] in episode 1348
Report: 
rewardSum:192.33333333333334
loss:[32.877270460128784, 30.422529220581055]
policies:[1, 4, 1]
qAverage:[0.0, 53.610680389404294]
ws:[-0.3540386140346527, 6.744426250457764]
memory len:10000
memory used:3133.0
now epsilon is 0.1607783652780571, the reward is 194.33333333333334 with loss [25.16751194000244, 19.78294849395752] in episode 1349
Report: 
rewardSum:194.33333333333334
loss:[25.16751194000244, 19.78294849395752]
policies:[2, 1, 1]
qAverage:[0.0, 28.752981185913086]
ws:[-0.15361960232257843, 1.9481173753738403]
memory len:10000
memory used:3133.0
now epsilon is 0.16061764719461802, the reward is 194.33333333333334 with loss [27.398292541503906, 28.38818120956421] in episode 1350
Report: 
rewardSum:194.33333333333334
loss:[27.398292541503906, 28.38818120956421]
policies:[1, 3, 0]
qAverage:[0.0, 51.941741943359375]
ws:[0.6511530801653862, 8.9251269698143]
memory len:10000
memory used:3132.0
now epsilon is 0.16045708976900316, the reward is 194.33333333333334 with loss [15.952877044677734, 20.640456557273865] in episode 1351
Report: 
rewardSum:194.33333333333334
loss:[15.952877044677734, 20.640456557273865]
policies:[0, 3, 1]
qAverage:[0.0, 44.7626584370931]
ws:[0.3757684802015622, 3.0439629554748535]
memory len:10000
memory used:3168.0
now epsilon is 0.1602966928406149, the reward is 194.33333333333334 with loss [36.65561485290527, 24.803611993789673] in episode 1352
Report: 
rewardSum:194.33333333333334
loss:[36.65561485290527, 24.803611993789673]
policies:[0, 3, 1]
qAverage:[0.0, 55.842780113220215]
ws:[-0.6518087387084961, 5.29189658164978]
memory len:10000
memory used:3134.0
now epsilon is 0.1601364562490162, the reward is 194.33333333333334 with loss [24.392003297805786, 21.113722324371338] in episode 1353
Report: 
rewardSum:194.33333333333334
loss:[24.392003297805786, 21.113722324371338]
policies:[1, 3, 0]
qAverage:[0.0, 47.32384490966797]
ws:[-0.028140008449554443, 2.925191561381022]
memory len:10000
memory used:3133.0
now epsilon is 0.15989640164253718, the reward is 192.33333333333334 with loss [28.500029802322388, 33.59251260757446] in episode 1354
Report: 
rewardSum:192.33333333333334
loss:[28.500029802322388, 33.59251260757446]
policies:[0, 5, 1]
qAverage:[0.0, 58.34870147705078]
ws:[-1.2381693720817566, 3.4575236588716507]
memory len:10000
memory used:3133.0
now epsilon is 0.1597365651920524, the reward is 194.33333333333334 with loss [19.44443941116333, 24.216805458068848] in episode 1355
Report: 
rewardSum:194.33333333333334
loss:[19.44443941116333, 24.216805458068848]
policies:[0, 4, 0]
qAverage:[0.0, 52.06083393096924]
ws:[-0.4109639897942543, 4.872998952865601]
memory len:10000
memory used:3134.0
now epsilon is 0.15957688851808938, the reward is 194.33333333333334 with loss [23.821410179138184, 22.110419034957886] in episode 1356
Report: 
rewardSum:194.33333333333334
loss:[23.821410179138184, 22.110419034957886]
policies:[2, 2, 0]
qAverage:[0.0, 46.58591969807943]
ws:[1.9909608364105225, 5.444088617960612]
memory len:10000
memory used:3133.0
now epsilon is 0.15937751711806633, the reward is 193.33333333333334 with loss [27.728266954421997, 19.389119863510132] in episode 1357
Report: 
rewardSum:193.33333333333334
loss:[27.728266954421997, 19.389119863510132]
policies:[0, 4, 1]
qAverage:[0.0, 49.76804542541504]
ws:[0.7669136002659798, 3.802522301673889]
memory len:10000
memory used:3138.0
now epsilon is 0.15921819935755674, the reward is 46.33333333333334 with loss [20.68741464614868, 21.358523845672607] in episode 1358
Report: 
rewardSum:46.33333333333334
loss:[20.68741464614868, 21.358523845672607]
policies:[0, 2, 2]
qAverage:[0.0, 44.35488510131836]
ws:[0.3997657299041748, 2.8513495127360025]
memory len:10000
memory used:3137.0
now epsilon is 0.15897952127583598, the reward is 192.33333333333334 with loss [38.53046417236328, 28.961143970489502] in episode 1359
Report: 
rewardSum:192.33333333333334
loss:[38.53046417236328, 28.961143970489502]
policies:[1, 4, 1]
qAverage:[0.0, 53.11869430541992]
ws:[0.4031998157501221, 4.22777361869812]
memory len:10000
memory used:3137.0
now epsilon is 0.15882060136194504, the reward is 194.33333333333334 with loss [16.890825033187866, 15.835042476654053] in episode 1360
Report: 
rewardSum:194.33333333333334
loss:[16.890825033187866, 15.835042476654053]
policies:[0, 4, 0]
qAverage:[0.0, 57.81639022827149]
ws:[0.9404791355133056, 6.268127012252807]
memory len:10000
memory used:3137.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20*		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.15866184030838293, the reward is 194.33333333333334 with loss [23.532952070236206, 21.62275505065918] in episode 1361
Report: 
rewardSum:194.33333333333334
loss:[23.532952070236206, 21.62275505065918]
policies:[0, 4, 0]
qAverage:[0.0, 54.55261993408203]
ws:[1.0374675989151, 6.39834725856781]
memory len:10000
memory used:3137.0
now epsilon is 0.15850323795634894, the reward is 194.33333333333334 with loss [32.092230796813965, 19.800525426864624] in episode 1362
Report: 
rewardSum:194.33333333333334
loss:[32.092230796813965, 19.800525426864624]
policies:[0, 4, 0]
qAverage:[0.0, 58.64256210327149]
ws:[1.9807556867599487, 7.850315952301026]
memory len:10000
memory used:3137.0
now epsilon is 0.158344794147201, the reward is 194.33333333333334 with loss [25.54509449005127, 23.993284225463867] in episode 1363
Report: 
rewardSum:194.33333333333334
loss:[25.54509449005127, 23.993284225463867]
policies:[0, 4, 0]
qAverage:[0.0, 59.77117156982422]
ws:[0.8235667228698731, 6.752190446853637]
memory len:10000
memory used:3137.0
now epsilon is 0.15810742535475125, the reward is 192.33333333333334 with loss [34.10321116447449, 29.432504177093506] in episode 1364
Report: 
rewardSum:192.33333333333334
loss:[34.10321116447449, 29.432504177093506]
policies:[0, 5, 1]
qAverage:[0.0, 60.412619272867836]
ws:[-0.2323606014251709, 4.959898889064789]
memory len:10000
memory used:3137.0
now epsilon is 0.15794937720979993, the reward is 194.33333333333334 with loss [26.294055461883545, 20.568681955337524] in episode 1365
Report: 
rewardSum:194.33333333333334
loss:[26.294055461883545, 20.568681955337524]
policies:[0, 4, 0]
qAverage:[0.0, 58.817230987548825]
ws:[2.2118304789066316, 9.217845344543457]
memory len:10000
memory used:3137.0
now epsilon is 0.15779148705373536, the reward is 194.33333333333334 with loss [22.1374249458313, 27.80264377593994] in episode 1366
Report: 
rewardSum:194.33333333333334
loss:[22.1374249458313, 27.80264377593994]
policies:[0, 4, 0]
qAverage:[0.0, 59.53869476318359]
ws:[2.76503144800663, 10.146982860565185]
memory len:10000
memory used:3137.0
now epsilon is 0.15763375472862792, the reward is 194.33333333333334 with loss [15.915533304214478, 22.84062147140503] in episode 1367
Report: 
rewardSum:194.33333333333334
loss:[15.915533304214478, 22.84062147140503]
policies:[0, 3, 1]
qAverage:[0.0, 58.320526123046875]
ws:[2.109074756503105, 9.220150470733643]
memory len:10000
memory used:3137.0
now epsilon is 0.15743681103168666, the reward is 193.33333333333334 with loss [30.61263680458069, 20.02924609184265] in episode 1368
Report: 
rewardSum:193.33333333333334
loss:[30.61263680458069, 20.02924609184265]
policies:[0, 4, 1]
qAverage:[0.0, 58.996543884277344]
ws:[1.8527994930744172, 8.868504667282105]
memory len:10000
memory used:3137.0
now epsilon is 0.15727943324961993, the reward is 194.33333333333334 with loss [26.853402614593506, 16.273860931396484] in episode 1369
Report: 
rewardSum:194.33333333333334
loss:[26.853402614593506, 16.273860931396484]
policies:[0, 4, 0]
qAverage:[0.0, 57.291640281677246]
ws:[0.6634860411286354, 7.46257358789444]
memory len:10000
memory used:3137.0
now epsilon is 0.15712221278632846, the reward is 46.33333333333334 with loss [21.676871299743652, 16.919352114200592] in episode 1370
Report: 
rewardSum:46.33333333333334
loss:[21.676871299743652, 16.919352114200592]
policies:[0, 3, 1]
qAverage:[0.0, 50.98811721801758]
ws:[1.8833904042840004, 6.656303763389587]
memory len:10000
memory used:3137.0
now epsilon is 0.15696514948455242, the reward is 194.33333333333334 with loss [23.79331398010254, 15.349673748016357] in episode 1371
Report: 
rewardSum:194.33333333333334
loss:[23.79331398010254, 15.349673748016357]
policies:[0, 4, 0]
qAverage:[0.0, 61.015709686279294]
ws:[0.47755869626998904, 7.677459001541138]
memory len:10000
memory used:3137.0
now epsilon is 0.15672984886611085, the reward is 192.33333333333334 with loss [25.938339233398438, 36.93352222442627] in episode 1372
Report: 
rewardSum:192.33333333333334
loss:[25.938339233398438, 36.93352222442627]
policies:[1, 3, 2]
qAverage:[0.0, 46.800130208333336]
ws:[-0.5968855420748392, 2.4315808614095054]
memory len:10000
memory used:3137.0
now epsilon is 0.1565731777811431, the reward is 46.33333333333334 with loss [24.538206815719604, 20.578664183616638] in episode 1373
Report: 
rewardSum:46.33333333333334
loss:[24.538206815719604, 20.578664183616638]
policies:[0, 2, 2]
qAverage:[0.0, 49.51750183105469]
ws:[-0.22083663443724313, 3.2992199261983237]
memory len:10000
memory used:3137.0
now epsilon is 0.1561821849053065, the reward is 40.33333333333334 with loss [51.16233539581299, 54.25387382507324] in episode 1374
Report: 
rewardSum:40.33333333333334
loss:[51.16233539581299, 54.25387382507324]
policies:[1, 4, 5]
qAverage:[0.0, 55.677099609375]
ws:[2.0192715035751463, 8.743471050262452]
memory len:10000
memory used:3137.0
now epsilon is 0.15590907098544915, the reward is 191.33333333333334 with loss [28.730040550231934, 34.363221168518066] in episode 1375
Report: 
rewardSum:191.33333333333334
loss:[28.730040550231934, 34.363221168518066]
policies:[1, 4, 2]
qAverage:[0.0, 58.7747802734375]
ws:[2.2333272337913512, 9.255639553070068]
memory len:10000
memory used:3137.0
now epsilon is 0.15575322037062167, the reward is 194.33333333333334 with loss [12.636949896812439, 25.31662154197693] in episode 1376
Report: 
rewardSum:194.33333333333334
loss:[12.636949896812439, 25.31662154197693]
policies:[0, 2, 2]
qAverage:[0.0, 54.003639221191406]
ws:[1.8406499028205872, 12.299745559692383]
memory len:10000
memory used:3137.0
now epsilon is 0.1555975255479747, the reward is 46.33333333333334 with loss [17.186781644821167, 18.00230598449707] in episode 1377
Report: 
rewardSum:46.33333333333334
loss:[17.186781644821167, 18.00230598449707]
policies:[0, 3, 1]
qAverage:[0.0, 42.9911855061849]
ws:[1.052929202715556, 4.828622817993164]
memory len:10000
memory used:3137.0
now epsilon is 0.1554419863617746, the reward is 194.33333333333334 with loss [28.28470516204834, 27.10018253326416] in episode 1378
Report: 
rewardSum:194.33333333333334
loss:[28.28470516204834, 27.10018253326416]
policies:[1, 2, 1]
qAverage:[0.0, 34.57547378540039]
ws:[0.6098224520683289, 4.035641670227051]
memory len:10000
memory used:3137.0
now epsilon is 0.1552477810057791, the reward is 193.33333333333334 with loss [23.207873582839966, 26.995378732681274] in episode 1379
Report: 
rewardSum:193.33333333333334
loss:[23.207873582839966, 26.995378732681274]
policies:[0, 4, 1]
qAverage:[0.0, 60.3466178894043]
ws:[0.5721360921859742, 8.795391273498534]
memory len:10000
memory used:3137.0
now epsilon is 0.15509259143298884, the reward is 194.33333333333334 with loss [19.129364252090454, 19.272722959518433] in episode 1380
Report: 
rewardSum:194.33333333333334
loss:[19.129364252090454, 19.272722959518433]
policies:[0, 3, 1]
qAverage:[0.0, 52.35402170817057]
ws:[1.9320273001988728, 7.825836976369222]
memory len:10000
memory used:3137.0
now epsilon is 0.15493755699158498, the reward is 194.33333333333334 with loss [16.14600443840027, 25.50356388092041] in episode 1381
Report: 
rewardSum:194.33333333333334
loss:[16.14600443840027, 25.50356388092041]
policies:[0, 3, 1]
qAverage:[0.0, 55.33897908528646]
ws:[1.6055531899134319, 12.438407897949219]
memory len:10000
memory used:3137.0
now epsilon is 0.1547052958616484, the reward is 192.33333333333334 with loss [27.144863843917847, 27.20317244529724] in episode 1382
Report: 
rewardSum:192.33333333333334
loss:[27.144863843917847, 27.20317244529724]
policies:[1, 4, 1]
qAverage:[0.0, 60.42525939941406]
ws:[2.376064068078995, 10.242499446868896]
memory len:10000
memory used:3137.0
now epsilon is 0.15439615586886807, the reward is 190.33333333333334 with loss [44.06939911842346, 42.99138140678406] in episode 1383
Report: 
rewardSum:190.33333333333334
loss:[44.06939911842346, 42.99138140678406]
policies:[0, 6, 2]
qAverage:[0.0, 62.60241971697126]
ws:[2.5775866508483887, 8.533285140991211]
memory len:10000
memory used:3137.0
############# STATE ###############
0-		9*		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.15416470633322113, the reward is 192.33333333333334 with loss [32.259305477142334, 31.406062364578247] in episode 1384
Report: 
rewardSum:192.33333333333334
loss:[32.259305477142334, 31.406062364578247]
policies:[0, 4, 2]
qAverage:[0.0, 62.56960830688477]
ws:[1.1602027907967567, 5.291346621513367]
memory len:10000
memory used:3138.0
now epsilon is 0.1540105994290181, the reward is 194.33333333333334 with loss [19.840519905090332, 29.8295316696167] in episode 1385
Report: 
rewardSum:194.33333333333334
loss:[19.840519905090332, 29.8295316696167]
policies:[1, 3, 0]
qAverage:[11.96146240234375, 49.98255615234375]
ws:[0.7045080311596393, 4.750885915756226]
memory len:10000
memory used:3138.0
now epsilon is 0.15385664657393883, the reward is 194.33333333333334 with loss [32.48162078857422, 14.28431487083435] in episode 1386
Report: 
rewardSum:194.33333333333334
loss:[32.48162078857422, 14.28431487083435]
policies:[1, 3, 0]
qAverage:[0.0, 56.754886627197266]
ws:[2.5635240375995636, 9.462727770209312]
memory len:10000
memory used:3138.0
now epsilon is 0.15370284761399194, the reward is 194.33333333333334 with loss [22.214478969573975, 14.255234599113464] in episode 1387
Report: 
rewardSum:194.33333333333334
loss:[22.214478969573975, 14.255234599113464]
policies:[0, 4, 0]
qAverage:[0.0, 62.73846206665039]
ws:[2.2529258489608766, 7.942124736309052]
memory len:10000
memory used:3138.0
now epsilon is 0.15354920239533998, the reward is 194.33333333333334 with loss [15.519878625869751, 27.861454486846924] in episode 1388
Report: 
rewardSum:194.33333333333334
loss:[15.519878625869751, 27.861454486846924]
policies:[2, 2, 0]
qAverage:[0.0, 55.25110880533854]
ws:[3.1843980153401694, 11.412550290425619]
memory len:10000
memory used:3138.0
now epsilon is 0.15339571076429934, the reward is 194.33333333333334 with loss [14.26818299293518, 15.631766736507416] in episode 1389
Report: 
rewardSum:194.33333333333334
loss:[14.26818299293518, 15.631766736507416]
policies:[1, 3, 0]
qAverage:[0.0, 56.80347442626953]
ws:[2.8280976563692093, 10.623428225517273]
memory len:10000
memory used:3137.0
now epsilon is 0.15324237256733994, the reward is 194.33333333333334 with loss [26.554542541503906, 15.592712879180908] in episode 1390
Report: 
rewardSum:194.33333333333334
loss:[26.554542541503906, 15.592712879180908]
policies:[0, 4, 0]
qAverage:[0.0, 62.937112426757814]
ws:[0.5309929132461548, 6.262218761444092]
memory len:10000
memory used:3138.0
now epsilon is 0.15308918765108528, the reward is 194.33333333333334 with loss [27.026660919189453, 15.889415979385376] in episode 1391
Report: 
rewardSum:194.33333333333334
loss:[27.026660919189453, 15.889415979385376]
policies:[0, 4, 0]
qAverage:[0.0, 64.04409484863281]
ws:[1.5411155223846436, 8.106444582343102]
memory len:10000
memory used:3137.0
now epsilon is 0.1529361558623121, the reward is 194.33333333333334 with loss [19.905195713043213, 20.380210399627686] in episode 1392
Report: 
rewardSum:194.33333333333334
loss:[19.905195713043213, 20.380210399627686]
policies:[0, 4, 0]
qAverage:[0.0, 57.08470916748047]
ws:[1.0481215785257518, 7.419414728879929]
memory len:10000
memory used:3137.0
now epsilon is 0.1527832770479503, the reward is 194.33333333333334 with loss [18.01323962211609, 30.11117172241211] in episode 1393
Report: 
rewardSum:194.33333333333334
loss:[18.01323962211609, 30.11117172241211]
policies:[0, 4, 0]
qAverage:[0.0, 64.82018737792968]
ws:[2.3203359305858613, 9.487795972824097]
memory len:10000
memory used:3137.0
now epsilon is 0.1526305510550829, the reward is 194.33333333333334 with loss [14.996623516082764, 12.72220766544342] in episode 1394
Report: 
rewardSum:194.33333333333334
loss:[14.996623516082764, 12.72220766544342]
policies:[1, 3, 0]
qAverage:[0.0, 58.43397521972656]
ws:[3.010878562927246, 11.380874633789062]
memory len:10000
memory used:3137.0
now epsilon is 0.15240174827195385, the reward is 192.33333333333334 with loss [35.67483401298523, 30.569705486297607] in episode 1395
Report: 
rewardSum:192.33333333333334
loss:[35.67483401298523, 30.569705486297607]
policies:[1, 3, 2]
qAverage:[0.0, 37.69380569458008]
ws:[1.4242850542068481, 4.32330322265625]
memory len:10000
memory used:3137.0
now epsilon is 0.15221134131389683, the reward is 193.33333333333334 with loss [32.6589035987854, 25.47486925125122] in episode 1396
Report: 
rewardSum:193.33333333333334
loss:[32.6589035987854, 25.47486925125122]
policies:[0, 4, 1]
qAverage:[0.0, 64.63593139648438]
ws:[1.953054916858673, 8.84947624206543]
memory len:10000
memory used:3137.0
now epsilon is 0.15205918704232332, the reward is 194.33333333333334 with loss [19.255226850509644, 19.96855926513672] in episode 1397
Report: 
rewardSum:194.33333333333334
loss:[19.255226850509644, 19.96855926513672]
policies:[1, 3, 0]
qAverage:[0.0, 63.78515625]
ws:[1.2921030949801207, 9.467079520225525]
memory len:10000
memory used:3137.0
now epsilon is 0.15190718486797303, the reward is 194.33333333333334 with loss [18.493908405303955, 23.94864273071289] in episode 1398
Report: 
rewardSum:194.33333333333334
loss:[18.493908405303955, 23.94864273071289]
policies:[1, 3, 0]
qAverage:[0.0, 59.74234390258789]
ws:[0.9850983172655106, 8.460227966308594]
memory len:10000
memory used:3137.0
now epsilon is 0.15167946645619484, the reward is 192.33333333333334 with loss [32.88728094100952, 34.375516414642334] in episode 1399
Report: 
rewardSum:192.33333333333334
loss:[32.88728094100952, 34.375516414642334]
policies:[0, 3, 3]
qAverage:[0.0, 63.00784492492676]
ws:[0.270681980997324, 6.663843631744385]
memory len:10000
memory used:3137.0
now epsilon is 0.1515278438600592, the reward is 194.33333333333334 with loss [23.730103969573975, 24.970906972885132] in episode 1400
Report: 
rewardSum:194.33333333333334
loss:[23.730103969573975, 24.970906972885132]
policies:[0, 3, 1]
qAverage:[0.0, 59.185245513916016]
ws:[1.5838925577700138, 8.907277822494507]
memory len:10000
memory used:3137.0
now epsilon is 0.15137637282967073, the reward is 194.33333333333334 with loss [23.711302995681763, 24.915842056274414] in episode 1401
Report: 
rewardSum:194.33333333333334
loss:[23.711302995681763, 24.915842056274414]
policies:[0, 4, 0]
qAverage:[0.0, 65.7259017944336]
ws:[1.8693526089191437, 7.684056520462036]
memory len:10000
memory used:3137.0
now epsilon is 0.15122505321352042, the reward is 194.33333333333334 with loss [25.469326496124268, 21.170202255249023] in episode 1402
Report: 
rewardSum:194.33333333333334
loss:[25.469326496124268, 21.170202255249023]
policies:[0, 3, 1]
qAverage:[0.0, 69.55831909179688]
ws:[2.09711292386055, 8.370232582092285]
memory len:10000
memory used:3137.0
now epsilon is 0.1510738848602509, the reward is 194.33333333333334 with loss [27.61693811416626, 13.832691311836243] in episode 1403
Report: 
rewardSum:194.33333333333334
loss:[27.61693811416626, 13.832691311836243]
policies:[1, 3, 0]
qAverage:[0.0, 68.01904678344727]
ws:[4.45317542552948, 12.06331491470337]
memory len:10000
memory used:3137.0
now epsilon is 0.15092286761865598, the reward is 194.33333333333334 with loss [19.765400648117065, 22.38982915878296] in episode 1404
Report: 
rewardSum:194.33333333333334
loss:[19.765400648117065, 22.38982915878296]
policies:[0, 3, 1]
qAverage:[0.0, 69.1709976196289]
ws:[4.929951786994934, 12.511752605438232]
memory len:10000
memory used:3137.0
now epsilon is 0.1507720013376806, the reward is 194.33333333333334 with loss [17.523720502853394, 23.11876678466797] in episode 1405
Report: 
rewardSum:194.33333333333334
loss:[17.523720502853394, 23.11876678466797]
policies:[0, 4, 0]
qAverage:[0.0, 67.73971557617188]
ws:[3.70239577293396, 9.533678722381591]
memory len:10000
memory used:3137.0
now epsilon is 0.1506212858664208, the reward is 194.33333333333334 with loss [37.96028161048889, 34.16458988189697] in episode 1406
Report: 
rewardSum:194.33333333333334
loss:[37.96028161048889, 34.16458988189697]
policies:[0, 3, 1]
qAverage:[0.0, 48.12822723388672]
ws:[3.2921422322591147, 7.1721320152282715]
memory len:10000
memory used:3137.0
now epsilon is 0.15047072105412335, the reward is 194.33333333333334 with loss [25.45395827293396, 23.437061309814453] in episode 1407
Report: 
rewardSum:194.33333333333334
loss:[25.45395827293396, 23.437061309814453]
policies:[0, 2, 2]
qAverage:[0.0, 45.277164459228516]
ws:[0.49319547414779663, 2.0241074562072754]
memory len:10000
memory used:3137.0
now epsilon is 0.15032030675018582, the reward is 194.33333333333334 with loss [17.113781571388245, 22.711796283721924] in episode 1408
Report: 
rewardSum:194.33333333333334
loss:[17.113781571388245, 22.711796283721924]
policies:[1, 3, 0]
qAverage:[13.013522338867187, 54.532789611816405]
ws:[2.071581894159317, 8.17476019859314]
memory len:10000
memory used:3137.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20*		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.1500199290657331, the reward is 190.33333333333334 with loss [44.547046422958374, 52.20473575592041] in episode 1409
Report: 
rewardSum:190.33333333333334
loss:[44.547046422958374, 52.20473575592041]
policies:[0, 6, 2]
qAverage:[0.0, 69.13925606863839]
ws:[1.6894012178693498, 7.677732953003475]
memory len:10000
memory used:3137.0
now epsilon is 0.14986996538476513, the reward is 194.33333333333334 with loss [30.223270416259766, 17.89055633544922] in episode 1410
Report: 
rewardSum:194.33333333333334
loss:[30.223270416259766, 17.89055633544922]
policies:[0, 4, 0]
qAverage:[0.0, 59.980064392089844]
ws:[1.1945624202489853, 3.9004258513450623]
memory len:10000
memory used:3137.0
now epsilon is 0.1497201516112511, the reward is 194.33333333333334 with loss [19.361344814300537, 18.00477623939514] in episode 1411
Report: 
rewardSum:194.33333333333334
loss:[19.361344814300537, 18.00477623939514]
policies:[0, 3, 1]
qAverage:[0.0, 62.01881980895996]
ws:[2.9713230000343174, 9.812747776508331]
memory len:10000
memory used:3137.0
now epsilon is 0.1495704875953398, the reward is 194.33333333333334 with loss [21.9982168674469, 22.381264209747314] in episode 1412
Report: 
rewardSum:194.33333333333334
loss:[21.9982168674469, 22.381264209747314]
policies:[0, 3, 1]
qAverage:[0.0, 70.00072479248047]
ws:[4.47097635269165, 11.465190172195435]
memory len:10000
memory used:3137.0
now epsilon is 0.14942097318732975, the reward is 194.33333333333334 with loss [25.27449059486389, 19.83329439163208] in episode 1413
Report: 
rewardSum:194.33333333333334
loss:[25.27449059486389, 19.83329439163208]
policies:[0, 4, 0]
qAverage:[0.0, 69.28780059814453]
ws:[4.239033651351929, 10.288465976715088]
memory len:10000
memory used:3137.0
now epsilon is 0.14927160823766916, the reward is 194.33333333333334 with loss [25.03290605545044, 25.32413148880005] in episode 1414
Report: 
rewardSum:194.33333333333334
loss:[25.03290605545044, 25.32413148880005]
policies:[1, 3, 0]
qAverage:[0.0, 70.38913536071777]
ws:[2.9914727210998535, 8.857383728027344]
memory len:10000
memory used:3137.0
now epsilon is 0.1491223925969557, the reward is 194.33333333333334 with loss [25.09428310394287, 28.34398889541626] in episode 1415
Report: 
rewardSum:194.33333333333334
loss:[25.09428310394287, 28.34398889541626]
policies:[0, 3, 1]
qAverage:[0.0, 63.758155822753906]
ws:[2.933617800474167, 10.053265571594238]
memory len:10000
memory used:3137.0
now epsilon is 0.14897332611593644, the reward is 194.33333333333334 with loss [24.172256469726562, 26.039958953857422] in episode 1416
Report: 
rewardSum:194.33333333333334
loss:[24.172256469726562, 26.039958953857422]
policies:[0, 4, 0]
qAverage:[0.0, 69.32415771484375]
ws:[3.931335359811783, 11.860801458358765]
memory len:10000
memory used:3138.0
now epsilon is 0.14882440864550756, the reward is 194.33333333333334 with loss [23.361552715301514, 18.55475401878357] in episode 1417
Report: 
rewardSum:194.33333333333334
loss:[23.361552715301514, 18.55475401878357]
policies:[0, 4, 0]
qAverage:[0.0, 69.81034088134766]
ws:[4.651286268234253, 13.714120197296143]
memory len:10000
memory used:3137.0
now epsilon is 0.1486384711267052, the reward is 193.33333333333334 with loss [27.99632740020752, 29.09497046470642] in episode 1418
Report: 
rewardSum:193.33333333333334
loss:[27.99632740020752, 29.09497046470642]
policies:[0, 4, 1]
qAverage:[0.0, 71.48009948730468]
ws:[4.366291558742523, 13.011215305328369]
memory len:10000
memory used:3138.0
now epsilon is 0.14848988838571583, the reward is 194.33333333333334 with loss [30.516052722930908, 38.322495460510254] in episode 1419
Report: 
rewardSum:194.33333333333334
loss:[30.516052722930908, 38.322495460510254]
policies:[0, 3, 1]
qAverage:[0.0, 70.89077568054199]
ws:[4.415715128183365, 13.919273853302002]
memory len:10000
memory used:3138.0
now epsilon is 0.14841565272214102, the reward is -1.0 with loss [9.498970746994019, 11.306893348693848] in episode 1420
Report: 
rewardSum:-1.0
loss:[9.498970746994019, 11.306893348693848]
policies:[1, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:3137.0
now epsilon is 0.14826729271601327, the reward is 194.33333333333334 with loss [22.928495168685913, 24.262094020843506] in episode 1421
Report: 
rewardSum:194.33333333333334
loss:[22.928495168685913, 24.262094020843506]
policies:[1, 3, 0]
qAverage:[0.0, 64.84879493713379]
ws:[4.4010999500751495, 14.745038866996765]
memory len:10000
memory used:3137.0
now epsilon is 0.1481190810142659, the reward is 194.33333333333334 with loss [20.504657745361328, 19.597655773162842] in episode 1422
Report: 
rewardSum:194.33333333333334
loss:[20.504657745361328, 19.597655773162842]
policies:[1, 3, 0]
qAverage:[0.0, 62.20226860046387]
ws:[3.0260216891765594, 10.016990184783936]
memory len:10000
memory used:3138.0
now epsilon is 0.1479710174686502, the reward is 194.33333333333334 with loss [24.963003873825073, 20.707897663116455] in episode 1423
Report: 
rewardSum:194.33333333333334
loss:[24.963003873825073, 20.707897663116455]
policies:[0, 4, 0]
qAverage:[0.0, 70.39732055664062]
ws:[4.0501961529254915, 13.044516277313232]
memory len:10000
memory used:3138.0
now epsilon is 0.14782310193106551, the reward is 194.33333333333334 with loss [17.45061707496643, 20.346866130828857] in episode 1424
Report: 
rewardSum:194.33333333333334
loss:[17.45061707496643, 20.346866130828857]
policies:[0, 4, 0]
qAverage:[0.0, 71.10195770263672]
ws:[2.339198491722345, 10.49388301372528]
memory len:10000
memory used:3138.0
now epsilon is 0.14767533425355933, the reward is 194.33333333333334 with loss [19.25536870956421, 26.192277431488037] in episode 1425
Report: 
rewardSum:194.33333333333334
loss:[19.25536870956421, 26.192277431488037]
policies:[0, 4, 0]
qAverage:[0.0, 71.0815673828125]
ws:[2.5359201192855836, 11.643159818649291]
memory len:10000
memory used:3138.0
now epsilon is 0.147527714288327, the reward is 194.33333333333334 with loss [23.93256425857544, 24.21134090423584] in episode 1426
Report: 
rewardSum:194.33333333333334
loss:[23.93256425857544, 24.21134090423584]
policies:[0, 3, 1]
qAverage:[0.0, 76.5985221862793]
ws:[3.901155173778534, 14.183079481124878]
memory len:10000
memory used:3137.0
now epsilon is 0.14734339682723974, the reward is 193.33333333333334 with loss [33.86879920959473, 20.156381845474243] in episode 1427
Report: 
rewardSum:193.33333333333334
loss:[33.86879920959473, 20.156381845474243]
policies:[2, 2, 1]
qAverage:[0.0, 61.66974385579427]
ws:[3.34477835893631, 10.87231175104777]
memory len:10000
memory used:3137.0
now epsilon is 0.14712251982039723, the reward is 192.33333333333334 with loss [29.28337001800537, 27.85848903656006] in episode 1428
Report: 
rewardSum:192.33333333333334
loss:[29.28337001800537, 27.85848903656006]
policies:[0, 5, 1]
qAverage:[0.0, 75.00634511311848]
ws:[3.3418939113616943, 9.360935052235922]
memory len:10000
memory used:3137.0
now epsilon is 0.1469019739220618, the reward is 192.33333333333334 with loss [33.899288177490234, 35.922207832336426] in episode 1429
Report: 
rewardSum:192.33333333333334
loss:[33.899288177490234, 35.922207832336426]
policies:[0, 5, 1]
qAverage:[0.0, 76.61396026611328]
ws:[4.459636847178142, 11.653200387954712]
memory len:10000
memory used:3137.0
now epsilon is 0.14675512702719917, the reward is 194.33333333333334 with loss [15.671573638916016, 20.81498646736145] in episode 1430
Report: 
rewardSum:194.33333333333334
loss:[15.671573638916016, 20.81498646736145]
policies:[0, 3, 1]
qAverage:[0.0, 75.11529350280762]
ws:[4.789949297904968, 12.190128326416016]
memory len:10000
memory used:3137.0
now epsilon is 0.1465351318737376, the reward is 192.33333333333334 with loss [34.68711996078491, 37.34569716453552] in episode 1431
Report: 
rewardSum:192.33333333333334
loss:[34.68711996078491, 37.34569716453552]
policies:[1, 4, 1]
qAverage:[0.0, 76.54164123535156]
ws:[4.640921115875244, 11.755788421630859]
memory len:10000
memory used:3137.0
now epsilon is 0.1463520545204596, the reward is 193.33333333333334 with loss [23.462361335754395, 24.481067657470703] in episode 1432
Report: 
rewardSum:193.33333333333334
loss:[23.462361335754395, 24.481067657470703]
policies:[0, 3, 2]
qAverage:[0.0, 57.91479237874349]
ws:[1.2002866864204407, 3.239006598790487]
memory len:10000
memory used:3137.0
now epsilon is 0.1461692058994785, the reward is 193.33333333333334 with loss [29.507496118545532, 19.887157678604126] in episode 1433
Report: 
rewardSum:193.33333333333334
loss:[29.507496118545532, 19.887157678604126]
policies:[0, 4, 1]
qAverage:[0.0, 65.07555770874023]
ws:[2.908986121416092, 7.234665513038635]
memory len:10000
memory used:3137.0
now epsilon is 0.14602309149789625, the reward is 194.33333333333334 with loss [28.12626576423645, 29.05374526977539] in episode 1434
Report: 
rewardSum:194.33333333333334
loss:[28.12626576423645, 29.05374526977539]
policies:[0, 4, 0]
qAverage:[0.0, 64.1129379272461]
ws:[2.5293782353401184, 6.215334892272949]
memory len:10000
memory used:3137.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22*		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.1458771231559318, the reward is 194.33333333333334 with loss [22.292513370513916, 13.1844642162323] in episode 1435
Report: 
rewardSum:194.33333333333334
loss:[22.292513370513916, 13.1844642162323]
policies:[0, 3, 1]
qAverage:[0.0, 74.5743236541748]
ws:[3.9370044469833374, 11.154399156570435]
memory len:10000
memory used:3137.0
now epsilon is 0.14573130072758028, the reward is 194.33333333333334 with loss [15.752600193023682, 26.93288254737854] in episode 1436
Report: 
rewardSum:194.33333333333334
loss:[15.752600193023682, 26.93288254737854]
policies:[0, 3, 1]
qAverage:[0.0, 80.01454162597656]
ws:[3.2113601863384247, 9.045084357261658]
memory len:10000
memory used:3137.0
now epsilon is 0.14558562406698283, the reward is 194.33333333333334 with loss [23.856977462768555, 24.7431697845459] in episode 1437
Report: 
rewardSum:194.33333333333334
loss:[23.856977462768555, 24.7431697845459]
policies:[1, 3, 0]
qAverage:[0.0, 73.1224479675293]
ws:[3.01608669757843, 9.348390221595764]
memory len:10000
memory used:3137.0
now epsilon is 0.14544009302842636, the reward is 194.33333333333334 with loss [16.49640941619873, 18.902057647705078] in episode 1438
Report: 
rewardSum:194.33333333333334
loss:[16.49640941619873, 18.902057647705078]
policies:[0, 3, 1]
qAverage:[0.0, 64.73205757141113]
ws:[2.0716248899698257, 5.408645451068878]
memory len:10000
memory used:3137.0
now epsilon is 0.14529470746634338, the reward is 194.33333333333334 with loss [24.380690097808838, 20.527687430381775] in episode 1439
Report: 
rewardSum:194.33333333333334
loss:[24.380690097808838, 20.527687430381775]
policies:[0, 3, 1]
qAverage:[0.0, 69.24874496459961]
ws:[3.7157567143440247, 9.055847644805908]
memory len:10000
memory used:3137.0
now epsilon is 0.145149467235312, the reward is 194.33333333333334 with loss [18.367273092269897, 26.83312702178955] in episode 1440
Report: 
rewardSum:194.33333333333334
loss:[18.367273092269897, 26.83312702178955]
policies:[1, 3, 0]
qAverage:[0.0, 73.71735572814941]
ws:[5.8214059472084045, 13.483781337738037]
memory len:10000
memory used:3137.0
now epsilon is 0.14500437219005563, the reward is 194.33333333333334 with loss [28.921509742736816, 19.88337516784668] in episode 1441
Report: 
rewardSum:194.33333333333334
loss:[28.921509742736816, 19.88337516784668]
policies:[1, 3, 0]
qAverage:[0.0, 72.36521530151367]
ws:[5.380120277404785, 12.663291215896606]
memory len:10000
memory used:3137.0
now epsilon is 0.14485942218544295, the reward is 194.33333333333334 with loss [16.238523960113525, 21.523213863372803] in episode 1442
Report: 
rewardSum:194.33333333333334
loss:[16.238523960113525, 21.523213863372803]
policies:[0, 4, 0]
qAverage:[0.0, 77.24932403564453]
ws:[6.1633302688598635, 15.103877925872803]
memory len:10000
memory used:3137.0
now epsilon is 0.14471461707648767, the reward is 194.33333333333334 with loss [23.349802017211914, 19.482922315597534] in episode 1443
Report: 
rewardSum:194.33333333333334
loss:[23.349802017211914, 19.482922315597534]
policies:[1, 3, 0]
qAverage:[0.0, 78.97981071472168]
ws:[7.432918429374695, 17.600242614746094]
memory len:10000
memory used:3137.0
now epsilon is 0.1445699567183485, the reward is 194.33333333333334 with loss [20.149061918258667, 29.575613021850586] in episode 1444
Report: 
rewardSum:194.33333333333334
loss:[20.149061918258667, 29.575613021850586]
policies:[0, 4, 0]
qAverage:[0.0, 77.4461669921875]
ws:[7.180503273010254, 17.160010528564452]
memory len:10000
memory used:3138.0
now epsilon is 0.14442544096632887, the reward is 194.33333333333334 with loss [22.214537143707275, 25.10210657119751] in episode 1445
Report: 
rewardSum:194.33333333333334
loss:[22.214537143707275, 25.10210657119751]
policies:[0, 4, 0]
qAverage:[0.0, 77.53452911376954]
ws:[5.54397976398468, 12.873136425018311]
memory len:10000
memory used:3138.0
now epsilon is 0.1442810696758769, the reward is 194.33333333333334 with loss [15.393027782440186, 18.230919241905212] in episode 1446
Report: 
rewardSum:194.33333333333334
loss:[15.393027782440186, 18.230919241905212]
policies:[0, 3, 1]
qAverage:[0.0, 67.71667861938477]
ws:[0.7919527888298035, 4.403209865093231]
memory len:10000
memory used:3138.0
now epsilon is 0.14406478328978653, the reward is 44.33333333333334 with loss [37.16062545776367, 29.772674560546875] in episode 1447
Report: 
rewardSum:44.33333333333334
loss:[37.16062545776367, 29.772674560546875]
policies:[0, 4, 2]
qAverage:[0.0, 67.20471954345703]
ws:[1.2087972462177277, 5.948981329798698]
memory len:10000
memory used:3138.0
now epsilon is 0.143920772521787, the reward is 194.33333333333334 with loss [30.404627799987793, 18.14982795715332] in episode 1448
Report: 
rewardSum:194.33333333333334
loss:[30.404627799987793, 18.14982795715332]
policies:[0, 4, 0]
qAverage:[0.0, 79.04932861328125]
ws:[5.683182883262634, 13.271026134490967]
memory len:10000
memory used:3137.0
now epsilon is 0.14370502624376177, the reward is 192.33333333333334 with loss [31.809038400650024, 35.21640181541443] in episode 1449
Report: 
rewardSum:192.33333333333334
loss:[31.809038400650024, 35.21640181541443]
policies:[0, 5, 1]
qAverage:[0.0, 81.03152720133464]
ws:[3.9682154677187405, 9.808300693829855]
memory len:10000
memory used:3137.0
now epsilon is 0.14356137509792188, the reward is 194.33333333333334 with loss [15.572613716125488, 18.82517147064209] in episode 1450
Report: 
rewardSum:194.33333333333334
loss:[15.572613716125488, 18.82517147064209]
policies:[0, 4, 0]
qAverage:[0.0, 80.49559020996094]
ws:[4.912427872419357, 11.302664494514465]
memory len:10000
memory used:3137.0
now epsilon is 0.1434178675493676, the reward is 194.33333333333334 with loss [30.564541816711426, 16.205647706985474] in episode 1451
Report: 
rewardSum:194.33333333333334
loss:[30.564541816711426, 16.205647706985474]
policies:[0, 4, 0]
qAverage:[0.0, 71.32354545593262]
ws:[0.42322131991386414, 3.147902274504304]
memory len:10000
memory used:3137.0
now epsilon is 0.14327450345455553, the reward is 194.33333333333334 with loss [19.024850845336914, 18.622715950012207] in episode 1452
Report: 
rewardSum:194.33333333333334
loss:[19.024850845336914, 18.622715950012207]
policies:[0, 4, 0]
qAverage:[0.0, 71.70970153808594]
ws:[7.898944695790608, 16.72235679626465]
memory len:10000
memory used:3138.0
now epsilon is 0.14305972597445582, the reward is 192.33333333333334 with loss [29.439698457717896, 39.90495443344116] in episode 1453
Report: 
rewardSum:192.33333333333334
loss:[29.439698457717896, 39.90495443344116]
policies:[1, 4, 1]
qAverage:[0.0, 80.96331787109375]
ws:[5.965452283620834, 12.860352635383606]
memory len:10000
memory used:3137.0
now epsilon is 0.14291671988693797, the reward is 194.33333333333334 with loss [19.733062744140625, 21.650583505630493] in episode 1454
Report: 
rewardSum:194.33333333333334
loss:[19.733062744140625, 21.650583505630493]
policies:[0, 4, 0]
qAverage:[0.0, 81.78481597900391]
ws:[4.85952320098877, 11.449833917617799]
memory len:10000
memory used:3138.0
now epsilon is 0.14277385675188925, the reward is 194.33333333333334 with loss [18.936577320098877, 17.752328753471375] in episode 1455
Report: 
rewardSum:194.33333333333334
loss:[18.936577320098877, 17.752328753471375]
policies:[0, 3, 1]
qAverage:[0.0, 77.52342224121094]
ws:[5.875817090272903, 13.856505870819092]
memory len:10000
memory used:3138.0
now epsilon is 0.14263113642641087, the reward is 194.33333333333334 with loss [25.1958270072937, 21.60016620159149] in episode 1456
Report: 
rewardSum:194.33333333333334
loss:[25.1958270072937, 21.60016620159149]
policies:[0, 4, 0]
qAverage:[0.0, 81.2911148071289]
ws:[7.556168556213379, 16.025953102111817]
memory len:10000
memory used:3137.0
now epsilon is 0.1424173233938978, the reward is 192.33333333333334 with loss [36.93316829204559, 39.66514015197754] in episode 1457
Report: 
rewardSum:192.33333333333334
loss:[36.93316829204559, 39.66514015197754]
policies:[0, 5, 1]
qAverage:[0.0, 80.79647064208984]
ws:[5.742673397064209, 11.482284704844156]
memory len:10000
memory used:3138.0
now epsilon is 0.14220383088055058, the reward is 192.33333333333334 with loss [49.61491584777832, 28.34490466117859] in episode 1458
Report: 
rewardSum:192.33333333333334
loss:[49.61491584777832, 28.34490466117859]
policies:[0, 4, 2]
qAverage:[0.0, 67.03091684977214]
ws:[6.42160435517629, 12.020566860834757]
memory len:10000
memory used:3138.0
now epsilon is 0.14199065840589087, the reward is 192.33333333333334 with loss [29.806349754333496, 33.38047957420349] in episode 1459
Report: 
rewardSum:192.33333333333334
loss:[29.806349754333496, 33.38047957420349]
policies:[1, 4, 1]
qAverage:[0.0, 73.41140174865723]
ws:[2.6523441383615136, 7.58868283033371]
memory len:10000
memory used:3138.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23*		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.14184872098510803, the reward is 194.33333333333334 with loss [20.73289704322815, 18.92318844795227] in episode 1460
Report: 
rewardSum:194.33333333333334
loss:[20.73289704322815, 18.92318844795227]
policies:[0, 3, 1]
qAverage:[0.0, 83.55432319641113]
ws:[4.93694132566452, 13.18401575088501]
memory len:10000
memory used:3137.0
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.1417069254485283, the reward is 194.33333333333334 with loss [18.78433394432068, 20.134509563446045] in episode 1461
Report: 
rewardSum:194.33333333333334
loss:[18.78433394432068, 20.134509563446045]
policies:[0, 2, 2]
qAverage:[0.0, 62.4693603515625]
ws:[7.256383419036865, 16.070476531982422]
memory len:10000
memory used:3138.0
now epsilon is 0.14156527165432073, the reward is 194.33333333333334 with loss [27.995529651641846, 16.003016471862793] in episode 1462
Report: 
rewardSum:194.33333333333334
loss:[27.995529651641846, 16.003016471862793]
policies:[0, 3, 1]
qAverage:[0.0, 80.60661697387695]
ws:[4.29492312669754, 11.785845160484314]
memory len:10000
memory used:3138.0
now epsilon is 0.14142375946079602, the reward is 194.33333333333334 with loss [20.421584606170654, 23.044216871261597] in episode 1463
Report: 
rewardSum:194.33333333333334
loss:[20.421584606170654, 23.044216871261597]
policies:[0, 4, 0]
qAverage:[0.0, 82.3762264251709]
ws:[5.134378254413605, 13.161699533462524]
memory len:10000
memory used:3137.0
now epsilon is 0.14128238872640658, the reward is 194.33333333333334 with loss [20.938950061798096, 20.433964252471924] in episode 1464
Report: 
rewardSum:194.33333333333334
loss:[20.938950061798096, 20.433964252471924]
policies:[1, 3, 0]
qAverage:[0.0, 80.04818916320801]
ws:[5.310053437948227, 12.764206647872925]
memory len:10000
memory used:3138.0
now epsilon is 0.14114115930974638, the reward is 194.33333333333334 with loss [16.879827976226807, 18.003377079963684] in episode 1465
Report: 
rewardSum:194.33333333333334
loss:[16.879827976226807, 18.003377079963684]
policies:[0, 4, 0]
qAverage:[0.0, 83.48938751220703]
ws:[5.163639783859253, 11.992168807983399]
memory len:10000
memory used:3138.0
now epsilon is 0.14092957984652027, the reward is 192.33333333333334 with loss [18.24747920036316, 39.49029755592346] in episode 1466
Report: 
rewardSum:192.33333333333334
loss:[18.24747920036316, 39.49029755592346]
policies:[0, 4, 2]
qAverage:[0.0, 83.11228942871094]
ws:[5.136392915248871, 13.294008827209472]
memory len:10000
memory used:3138.0
now epsilon is 0.14078870310645866, the reward is 46.33333333333334 with loss [22.644379377365112, 25.17115306854248] in episode 1467
Report: 
rewardSum:46.33333333333334
loss:[22.644379377365112, 25.17115306854248]
policies:[0, 3, 1]
qAverage:[0.0, 72.95272636413574]
ws:[1.7266047596931458, 6.812526702880859]
memory len:10000
memory used:3138.0
now epsilon is 0.14064796719031716, the reward is 194.33333333333334 with loss [20.168970584869385, 18.204250812530518] in episode 1468
Report: 
rewardSum:194.33333333333334
loss:[20.168970584869385, 18.204250812530518]
policies:[0, 3, 1]
qAverage:[0.0, 79.59394264221191]
ws:[4.598322108387947, 12.5387704372406]
memory len:10000
memory used:3137.0
now epsilon is 0.1404020177712935, the reward is 191.33333333333334 with loss [30.998143196105957, 33.7650671005249] in episode 1469
Report: 
rewardSum:191.33333333333334
loss:[30.998143196105957, 33.7650671005249]
policies:[1, 4, 2]
qAverage:[0.0, 67.57973861694336]
ws:[2.5533230751752853, 8.21006166934967]
memory len:10000
memory used:3137.0
now epsilon is 0.1402616683955043, the reward is 194.33333333333334 with loss [18.084596633911133, 16.21605372428894] in episode 1470
Report: 
rewardSum:194.33333333333334
loss:[18.084596633911133, 16.21605372428894]
policies:[0, 4, 0]
qAverage:[0.0, 83.32940483093262]
ws:[5.448502898216248, 13.786528825759888]
memory len:10000
memory used:3137.0
now epsilon is 0.14012145931646866, the reward is 194.33333333333334 with loss [24.75676918029785, 18.587374210357666] in episode 1471
Report: 
rewardSum:194.33333333333334
loss:[24.75676918029785, 18.587374210357666]
policies:[1, 3, 0]
qAverage:[0.0, 73.2814826965332]
ws:[1.9496130421757698, 8.256858587265015]
memory len:10000
memory used:3137.0
now epsilon is 0.13991140844758235, the reward is 192.33333333333334 with loss [36.16768264770508, 38.36199378967285] in episode 1472
Report: 
rewardSum:192.33333333333334
loss:[36.16768264770508, 38.36199378967285]
policies:[0, 5, 1]
qAverage:[0.0, 84.47199885050456]
ws:[4.003191575407982, 11.404621601104736]
memory len:10000
memory used:3137.0
now epsilon is 0.13977154949716902, the reward is 194.33333333333334 with loss [26.736365795135498, 18.02650761604309] in episode 1473
Report: 
rewardSum:194.33333333333334
loss:[26.736365795135498, 18.02650761604309]
policies:[0, 4, 0]
qAverage:[0.0, 82.94766082763672]
ws:[5.447203254699707, 12.457932090759277]
memory len:10000
memory used:3137.0
now epsilon is 0.13963183035326773, the reward is 194.33333333333334 with loss [18.67158031463623, 19.41497230529785] in episode 1474
Report: 
rewardSum:194.33333333333334
loss:[18.67158031463623, 19.41497230529785]
policies:[0, 4, 0]
qAverage:[0.0, 85.41005859375]
ws:[5.45758193731308, 13.081133365631104]
memory len:10000
memory used:3137.0
now epsilon is 0.1394922508761244, the reward is 194.33333333333334 with loss [25.042696475982666, 31.01741075515747] in episode 1475
Report: 
rewardSum:194.33333333333334
loss:[25.042696475982666, 31.01741075515747]
policies:[1, 3, 0]
qAverage:[0.0, 72.85661125183105]
ws:[2.060257524251938, 7.112456560134888]
memory len:10000
memory used:3137.0
now epsilon is 0.13935281092612464, the reward is 194.33333333333334 with loss [19.774308443069458, 27.28198480606079] in episode 1476
Report: 
rewardSum:194.33333333333334
loss:[19.774308443069458, 27.28198480606079]
policies:[0, 4, 0]
qAverage:[0.0, 84.14323272705079]
ws:[3.7156349062919616, 10.197891902923583]
memory len:10000
memory used:3137.0
now epsilon is 0.13921351036379362, the reward is 194.33333333333334 with loss [29.390782594680786, 29.14158010482788] in episode 1477
Report: 
rewardSum:194.33333333333334
loss:[29.390782594680786, 29.14158010482788]
policies:[0, 4, 0]
qAverage:[0.0, 85.11731262207032]
ws:[2.0694708943367006, 7.800364327430725]
memory len:10000
memory used:3137.0
now epsilon is 0.1390743490497959, the reward is 194.33333333333334 with loss [24.155319213867188, 22.427902698516846] in episode 1478
Report: 
rewardSum:194.33333333333334
loss:[24.155319213867188, 22.427902698516846]
policies:[0, 4, 0]
qAverage:[0.0, 83.83965301513672]
ws:[1.9284953236579896, 8.705092763900756]
memory len:10000
memory used:3137.0
now epsilon is 0.13893532684493543, the reward is 194.33333333333334 with loss [26.96022391319275, 17.744720220565796] in episode 1479
Report: 
rewardSum:194.33333333333334
loss:[26.96022391319275, 17.744720220565796]
policies:[1, 3, 0]
qAverage:[0.0, 74.10637664794922]
ws:[-0.3733690604567528, 4.511216104030609]
memory len:10000
memory used:3137.0
now epsilon is 0.13879644361015514, the reward is 194.33333333333334 with loss [17.99941349029541, 26.72054672241211] in episode 1480
Report: 
rewardSum:194.33333333333334
loss:[17.99941349029541, 26.72054672241211]
policies:[0, 4, 0]
qAverage:[0.0, 84.11012725830078]
ws:[4.081163717806339, 13.25880298614502]
memory len:10000
memory used:3137.0
now epsilon is 0.13858837902304005, the reward is 192.33333333333334 with loss [24.9637451171875, 39.38463115692139] in episode 1481
Report: 
rewardSum:192.33333333333334
loss:[24.9637451171875, 39.38463115692139]
policies:[0, 4, 2]
qAverage:[0.0, 85.23308563232422]
ws:[3.543492007255554, 12.833162879943847]
memory len:10000
memory used:3137.0
now epsilon is 0.13844984260599794, the reward is 46.33333333333334 with loss [28.94368827342987, 27.974967002868652] in episode 1482
Report: 
rewardSum:46.33333333333334
loss:[28.94368827342987, 27.974967002868652]
policies:[1, 2, 1]
qAverage:[0.0, 69.19919077555339]
ws:[1.0919903914133708, 6.638916492462158]
memory len:10000
memory used:3138.0
now epsilon is 0.13820773702116004, the reward is 43.33333333333334 with loss [41.23608756065369, 35.52059489488602] in episode 1483
Report: 
rewardSum:43.33333333333334
loss:[41.23608756065369, 35.52059489488602]
policies:[0, 4, 3]
qAverage:[0.0, 73.76218223571777]
ws:[1.6518625020980835, 8.439650774002075]
memory len:10000
memory used:3138.0
now epsilon is 0.13800055494219995, the reward is 192.33333333333334 with loss [32.11436092853546, 26.60715079307556] in episode 1484
Report: 
rewardSum:192.33333333333334
loss:[32.11436092853546, 26.60715079307556]
policies:[1, 4, 1]
qAverage:[0.0, 86.67523956298828]
ws:[3.7052589058876038, 11.182779884338379]
memory len:10000
memory used:3138.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20*		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.13786260612884138, the reward is 194.33333333333334 with loss [25.297799825668335, 18.565809845924377] in episode 1485
Report: 
rewardSum:194.33333333333334
loss:[25.297799825668335, 18.565809845924377]
policies:[0, 4, 0]
qAverage:[0.0, 89.08004760742188]
ws:[3.6553867042064665, 10.407352781295776]
memory len:10000
memory used:3138.0
now epsilon is 0.137724795212574, the reward is 194.33333333333334 with loss [36.661818504333496, 20.126914024353027] in episode 1486
Report: 
rewardSum:194.33333333333334
loss:[36.661818504333496, 20.126914024353027]
policies:[0, 4, 0]
qAverage:[0.0, 88.50013427734375]
ws:[2.530057668685913, 8.991348242759704]
memory len:10000
memory used:3138.0
now epsilon is 0.13758712205555237, the reward is 194.33333333333334 with loss [24.947866439819336, 19.58599090576172] in episode 1487
Report: 
rewardSum:194.33333333333334
loss:[24.947866439819336, 19.58599090576172]
policies:[1, 2, 1]
qAverage:[0.0, 75.38672129313152]
ws:[0.23287626852591833, 4.63992961247762]
memory len:10000
memory used:3138.0
now epsilon is 0.13744958652006892, the reward is 194.33333333333334 with loss [19.311319828033447, 22.300167560577393] in episode 1488
Report: 
rewardSum:194.33333333333334
loss:[19.311319828033447, 22.300167560577393]
policies:[0, 3, 1]
qAverage:[0.0, 80.26380920410156]
ws:[4.6245961636304855, 12.24610060453415]
memory len:10000
memory used:3137.0
now epsilon is 0.13731218846855375, the reward is 194.33333333333334 with loss [27.069166660308838, 16.428245544433594] in episode 1489
Report: 
rewardSum:194.33333333333334
loss:[27.069166660308838, 16.428245544433594]
policies:[0, 4, 0]
qAverage:[0.0, 90.53367767333984]
ws:[5.351960802078247, 13.67932939529419]
memory len:10000
memory used:3137.0
now epsilon is 0.1371749277635744, the reward is 194.33333333333334 with loss [18.622974395751953, 18.93958953022957] in episode 1490
Report: 
rewardSum:194.33333333333334
loss:[18.622974395751953, 18.93958953022957]
policies:[1, 3, 0]
qAverage:[0.0, 75.71263313293457]
ws:[2.1349001675844193, 7.390474736690521]
memory len:10000
memory used:3138.0
now epsilon is 0.13703780426783588, the reward is 46.33333333333334 with loss [17.834007740020752, 25.121458768844604] in episode 1491
Report: 
rewardSum:46.33333333333334
loss:[17.834007740020752, 25.121458768844604]
policies:[0, 3, 1]
qAverage:[0.0, 78.63899421691895]
ws:[1.5344111621379852, 5.482164204120636]
memory len:10000
memory used:3138.0
now epsilon is 0.13690081784418034, the reward is 194.33333333333334 with loss [29.46911907196045, 23.455748558044434] in episode 1492
Report: 
rewardSum:194.33333333333334
loss:[29.46911907196045, 23.455748558044434]
policies:[0, 4, 0]
qAverage:[0.0, 89.03850860595703]
ws:[5.4052419543266295, 13.238251876831054]
memory len:10000
memory used:3137.0
now epsilon is 0.13676396835558713, the reward is 194.33333333333334 with loss [30.268903255462646, 25.86415457725525] in episode 1493
Report: 
rewardSum:194.33333333333334
loss:[30.268903255462646, 25.86415457725525]
policies:[1, 3, 0]
qAverage:[0.0, 79.56600952148438]
ws:[7.081099192301433, 15.407885869344076]
memory len:10000
memory used:3137.0
now epsilon is 0.13655895057654335, the reward is 192.33333333333334 with loss [36.648377418518066, 31.81963348388672] in episode 1494
Report: 
rewardSum:192.33333333333334
loss:[36.648377418518066, 31.81963348388672]
policies:[0, 5, 1]
qAverage:[0.0, 90.43276341756184]
ws:[5.826606750488281, 14.084993680318197]
memory len:10000
memory used:3137.0
now epsilon is 0.1364224428270389, the reward is 194.33333333333334 with loss [22.869940757751465, 20.974522590637207] in episode 1495
Report: 
rewardSum:194.33333333333334
loss:[22.869940757751465, 20.974522590637207]
policies:[0, 4, 0]
qAverage:[0.0, 89.23529205322265]
ws:[5.938737869262695, 13.54561939239502]
memory len:10000
memory used:3137.0
now epsilon is 0.13635424013202804, the reward is -1.0 with loss [10.04899549484253, 9.266779661178589] in episode 1496
Report: 
rewardSum:-1.0
loss:[10.04899549484253, 9.266779661178589]
policies:[0, 1, 1]
qAverage:[0.0, 45.65857696533203]
ws:[0.560677170753479, 2.1606321334838867]
memory len:10000
memory used:3137.0
now epsilon is 0.13621793701621449, the reward is 194.33333333333334 with loss [20.121704816818237, 17.88965630531311] in episode 1497
Report: 
rewardSum:194.33333333333334
loss:[20.121704816818237, 17.88965630531311]
policies:[0, 3, 1]
qAverage:[0.0, 85.48759269714355]
ws:[5.404080234467983, 13.101701378822327]
memory len:10000
memory used:3137.0
now epsilon is 0.13608177015241157, the reward is 194.33333333333334 with loss [20.038888454437256, 31.16590929031372] in episode 1498
Report: 
rewardSum:194.33333333333334
loss:[20.038888454437256, 31.16590929031372]
policies:[0, 4, 0]
qAverage:[0.0, 90.73920440673828]
ws:[5.120173501968384, 12.481394863128662]
memory len:10000
memory used:3137.0
now epsilon is 0.1359457394044184, the reward is 194.33333333333334 with loss [26.24635124206543, 20.824796199798584] in episode 1499
Report: 
rewardSum:194.33333333333334
loss:[26.24635124206543, 20.824796199798584]
policies:[0, 4, 0]
qAverage:[0.0, 85.76157188415527]
ws:[5.150290951132774, 12.770975589752197]
memory len:10000
memory used:3137.0
now epsilon is 0.13580984463617018, the reward is 194.33333333333334 with loss [29.431440830230713, 24.96946930885315] in episode 1500
Report: 
rewardSum:194.33333333333334
loss:[29.431440830230713, 24.96946930885315]
policies:[0, 4, 0]
qAverage:[0.0, 90.66296081542968]
ws:[4.393828147649765, 11.512883758544922]
memory len:10000
memory used:3138.0
now epsilon is 0.1356740857117382, the reward is 194.33333333333334 with loss [19.314159154891968, 21.616883277893066] in episode 1501
Report: 
rewardSum:194.33333333333334
loss:[19.314159154891968, 21.616883277893066]
policies:[0, 4, 0]
qAverage:[0.0, 88.76032104492188]
ws:[5.065774083137512, 13.465875959396362]
memory len:10000
memory used:3137.0
now epsilon is 0.1356062571485127, the reward is -1.0 with loss [11.153254508972168, 12.922132015228271] in episode 1502
Report: 
rewardSum:-1.0
loss:[11.153254508972168, 12.922132015228271]
policies:[0, 1, 1]
qAverage:[0.0, 45.40769577026367]
ws:[-0.17307306826114655, 1.5163484811782837]
memory len:10000
memory used:3137.0
now epsilon is 0.13547070173523576, the reward is 194.33333333333334 with loss [33.70407962799072, 27.004387855529785] in episode 1503
Report: 
rewardSum:194.33333333333334
loss:[33.70407962799072, 27.004387855529785]
policies:[0, 4, 0]
qAverage:[0.0, 88.92760162353515]
ws:[4.7100252449512485, 12.720140027999879]
memory len:10000
memory used:3138.0
now epsilon is 0.13533528182654728, the reward is 46.33333333333334 with loss [15.780156373977661, 24.825838088989258] in episode 1504
Report: 
rewardSum:46.33333333333334
loss:[15.780156373977661, 24.825838088989258]
policies:[0, 3, 1]
qAverage:[0.0, 79.25814628601074]
ws:[1.9660682380199432, 9.132011651992798]
memory len:10000
memory used:3138.0
now epsilon is 0.1351999972869935, the reward is 194.33333333333334 with loss [34.71865940093994, 22.747433185577393] in episode 1505
Report: 
rewardSum:194.33333333333334
loss:[34.71865940093994, 22.747433185577393]
policies:[2, 2, 0]
qAverage:[0.0, 74.74130757649739]
ws:[1.5116887887318928, 8.565355936686197]
memory len:10000
memory used:3138.0
now epsilon is 0.135064847981256, the reward is 194.33333333333334 with loss [17.15811252593994, 20.75328230857849] in episode 1506
Report: 
rewardSum:194.33333333333334
loss:[17.15811252593994, 20.75328230857849]
policies:[1, 2, 1]
qAverage:[0.0, 76.88026428222656]
ws:[2.6043110688527427, 10.977370262145996]
memory len:10000
memory used:3138.0
now epsilon is 0.13492983377415174, the reward is 194.33333333333334 with loss [26.744511127471924, 18.86576795578003] in episode 1507
Report: 
rewardSum:194.33333333333334
loss:[26.744511127471924, 18.86576795578003]
policies:[0, 4, 0]
qAverage:[0.0, 92.50632667541504]
ws:[6.73593270778656, 19.197054862976074]
memory len:10000
memory used:3138.0
now epsilon is 0.1347949545306327, the reward is 194.33333333333334 with loss [25.197314977645874, 22.389331579208374] in episode 1508
Report: 
rewardSum:194.33333333333334
loss:[25.197314977645874, 22.389331579208374]
policies:[0, 4, 0]
qAverage:[0.0, 90.81375312805176]
ws:[6.264250218868256, 18.92660403251648]
memory len:10000
memory used:3137.0
now epsilon is 0.13466021011578586, the reward is 194.33333333333334 with loss [16.997135639190674, 18.98746109008789] in episode 1509
Report: 
rewardSum:194.33333333333334
loss:[16.997135639190674, 18.98746109008789]
policies:[0, 3, 1]
qAverage:[0.0, 92.56902694702148]
ws:[4.690217852592468, 16.057800889015198]
memory len:10000
memory used:3137.0
now epsilon is 0.13452560039483313, the reward is 194.33333333333334 with loss [27.749002933502197, 22.750916004180908] in episode 1510
Report: 
rewardSum:194.33333333333334
loss:[27.749002933502197, 22.750916004180908]
policies:[1, 3, 0]
qAverage:[0.0, 79.16911506652832]
ws:[-0.27718204259872437, 4.939609557390213]
memory len:10000
memory used:3137.0
now epsilon is 0.13439112523313113, the reward is 194.33333333333334 with loss [23.203742504119873, 20.140941381454468] in episode 1511
Report: 
rewardSum:194.33333333333334
loss:[23.203742504119873, 20.140941381454468]
policies:[0, 4, 0]
qAverage:[0.0, 93.42141265869141]
ws:[5.2766563415527346, 15.965246391296386]
memory len:10000
memory used:3137.0
now epsilon is 0.13425678449617104, the reward is 194.33333333333334 with loss [33.191556215286255, 17.566771745681763] in episode 1512
Report: 
rewardSum:194.33333333333334
loss:[33.191556215286255, 17.566771745681763]
policies:[0, 3, 1]
qAverage:[0.0, 78.1811580657959]
ws:[1.9014069885015488, 7.710721731185913]
memory len:10000
memory used:3137.0
now epsilon is 0.13412257804957856, the reward is 194.33333333333334 with loss [35.00797653198242, 27.075809955596924] in episode 1513
Report: 
rewardSum:194.33333333333334
loss:[35.00797653198242, 27.075809955596924]
policies:[0, 4, 0]
qAverage:[0.0, 92.97238349914551]
ws:[8.820705652236938, 21.24938726425171]
memory len:10000
memory used:3137.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21*		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.1339885057591136, the reward is 194.33333333333334 with loss [18.642150402069092, 27.984558582305908] in episode 1514
Report: 
rewardSum:194.33333333333334
loss:[18.642150402069092, 27.984558582305908]
policies:[1, 3, 0]
qAverage:[0.0, 91.01922225952148]
ws:[8.99800556898117, 20.979498147964478]
memory len:10000
memory used:3137.0
now epsilon is 0.13378764857283557, the reward is 192.33333333333334 with loss [38.82820796966553, 39.71086502075195] in episode 1515
Report: 
rewardSum:192.33333333333334
loss:[38.82820796966553, 39.71086502075195]
policies:[0, 4, 2]
qAverage:[0.0, 94.04624786376954]
ws:[8.46652569770813, 19.971695709228516]
memory len:10000
memory used:3137.0
now epsilon is 0.13358709248409606, the reward is 192.33333333333334 with loss [30.9342200756073, 26.718517065048218] in episode 1516
Report: 
rewardSum:192.33333333333334
loss:[30.9342200756073, 26.718517065048218]
policies:[0, 4, 2]
qAverage:[0.0, 89.45186614990234]
ws:[4.144648395478725, 10.113677740097046]
memory len:10000
memory used:3137.0
now epsilon is 0.13345355547842297, the reward is 194.33333333333334 with loss [20.35461401939392, 26.513957977294922] in episode 1517
Report: 
rewardSum:194.33333333333334
loss:[20.35461401939392, 26.513957977294922]
policies:[0, 4, 0]
qAverage:[0.0, 93.92542572021485]
ws:[6.9851089239120485, 15.697943878173827]
memory len:10000
memory used:3137.0
now epsilon is 0.13315358507400424, the reward is 189.33333333333334 with loss [47.03603971004486, 44.89059400558472] in episode 1518
Report: 
rewardSum:189.33333333333334
loss:[47.03603971004486, 44.89059400558472]
policies:[1, 5, 3]
qAverage:[0.0, 86.17339172363282]
ws:[0.9529212474822998, 4.753770589828491]
memory len:10000
memory used:3137.0
now epsilon is 0.13302048141320308, the reward is 194.33333333333334 with loss [23.344184637069702, 15.975935935974121] in episode 1519
Report: 
rewardSum:194.33333333333334
loss:[23.344184637069702, 15.975935935974121]
policies:[2, 2, 0]
qAverage:[0.0, 71.46337127685547]
ws:[12.0684814453125, 22.69753074645996]
memory len:10000
memory used:3137.0
now epsilon is 0.13282107535622353, the reward is 192.33333333333334 with loss [30.869683265686035, 38.15373420715332] in episode 1520
Report: 
rewardSum:192.33333333333334
loss:[30.869683265686035, 38.15373420715332]
policies:[0, 5, 1]
qAverage:[0.0, 95.045716603597]
ws:[5.465728338807821, 13.491722504297892]
memory len:10000
memory used:3137.0
now epsilon is 0.1326883040804698, the reward is 194.33333333333334 with loss [17.39902663230896, 23.913830757141113] in episode 1521
Report: 
rewardSum:194.33333333333334
loss:[17.39902663230896, 23.913830757141113]
policies:[0, 3, 1]
qAverage:[0.0, 92.38008308410645]
ws:[7.01042440533638, 16.735917806625366]
memory len:10000
memory used:3137.0
now epsilon is 0.1325556655262109, the reward is 194.33333333333334 with loss [18.787668704986572, 23.072860717773438] in episode 1522
Report: 
rewardSum:194.33333333333334
loss:[18.787668704986572, 23.072860717773438]
policies:[0, 4, 0]
qAverage:[0.0, 92.99104614257813]
ws:[5.488514041900634, 13.749294674396515]
memory len:10000
memory used:3137.0
now epsilon is 0.13242315956077508, the reward is 194.33333333333334 with loss [22.237427711486816, 25.99354887008667] in episode 1523
Report: 
rewardSum:194.33333333333334
loss:[22.237427711486816, 25.99354887008667]
policies:[1, 3, 0]
qAverage:[0.0, 84.41179275512695]
ws:[7.55944687128067, 17.34843412041664]
memory len:10000
memory used:3137.0
now epsilon is 0.13229078605162323, the reward is 194.33333333333334 with loss [19.38282322883606, 47.82864952087402] in episode 1524
Report: 
rewardSum:194.33333333333334
loss:[19.38282322883606, 47.82864952087402]
policies:[0, 4, 0]
qAverage:[0.0, 93.2361831665039]
ws:[7.7705841064453125, 17.968450260162353]
memory len:10000
memory used:3138.0
now epsilon is 0.13215854486634873, the reward is 194.33333333333334 with loss [20.558513164520264, 21.259462356567383] in episode 1525
Report: 
rewardSum:194.33333333333334
loss:[20.558513164520264, 21.259462356567383]
policies:[1, 3, 0]
qAverage:[0.0, 90.68572998046875]
ws:[7.968387067317963, 16.502503156661987]
memory len:10000
memory used:3138.0
now epsilon is 0.13202643587267732, the reward is 194.33333333333334 with loss [17.985458850860596, 23.04016637802124] in episode 1526
Report: 
rewardSum:194.33333333333334
loss:[17.985458850860596, 23.04016637802124]
policies:[0, 4, 0]
qAverage:[0.0, 93.34308624267578]
ws:[4.898002505302429, 11.214417779445649]
memory len:10000
memory used:3138.0
now epsilon is 0.1318285199524014, the reward is 192.33333333333334 with loss [40.61807918548584, 38.380699157714844] in episode 1527
Report: 
rewardSum:192.33333333333334
loss:[40.61807918548584, 38.380699157714844]
policies:[1, 4, 1]
qAverage:[0.0, 85.82951354980469]
ws:[0.5972871422767639, 6.157405614852905]
memory len:10000
memory used:3138.0
now epsilon is 0.13169674085990524, the reward is 194.33333333333334 with loss [18.382112503051758, 21.242276906967163] in episode 1528
Report: 
rewardSum:194.33333333333334
loss:[18.382112503051758, 21.242276906967163]
policies:[0, 4, 0]
qAverage:[0.0, 81.52436065673828]
ws:[1.0656338036060333, 5.448790073394775]
memory len:10000
memory used:3138.0
now epsilon is 0.13153220222371836, the reward is 193.33333333333334 with loss [21.394566535949707, 27.51957678794861] in episode 1529
Report: 
rewardSum:193.33333333333334
loss:[21.394566535949707, 27.51957678794861]
policies:[0, 4, 1]
qAverage:[0.0, 92.02207946777344]
ws:[6.256059348583221, 15.421630907058717]
memory len:10000
memory used:3138.0
now epsilon is 0.1313350271907263, the reward is 192.33333333333334 with loss [31.807791709899902, 33.704591512680054] in episode 1530
Report: 
rewardSum:192.33333333333334
loss:[31.807791709899902, 33.704591512680054]
policies:[0, 5, 1]
qAverage:[0.0, 94.65141932169597]
ws:[6.4836549411217375, 15.623748779296875]
memory len:10000
memory used:3138.0
now epsilon is 0.13117094047061134, the reward is 193.33333333333334 with loss [20.050071001052856, 32.42014932632446] in episode 1531
Report: 
rewardSum:193.33333333333334
loss:[20.050071001052856, 32.42014932632446]
policies:[1, 2, 2]
qAverage:[0.0, 76.84156290690105]
ws:[9.388025244077047, 17.724337736765545]
memory len:10000
memory used:3138.0
now epsilon is 0.13103981871104578, the reward is 46.33333333333334 with loss [23.801130294799805, 29.684171676635742] in episode 1532
Report: 
rewardSum:46.33333333333334
loss:[23.801130294799805, 29.684171676635742]
policies:[0, 2, 2]
qAverage:[0.0, 67.41420491536458]
ws:[1.9945375521977742, 4.732921918233235]
memory len:10000
memory used:3138.0
now epsilon is 0.1309088280240773, the reward is 194.33333333333334 with loss [16.99044179916382, 20.0970139503479] in episode 1533
Report: 
rewardSum:194.33333333333334
loss:[16.99044179916382, 20.0970139503479]
policies:[0, 4, 0]
qAverage:[0.0, 90.96085357666016]
ws:[5.642042899131775, 13.121681761741637]
memory len:10000
memory used:3138.0
now epsilon is 0.13077796827868246, the reward is 194.33333333333334 with loss [14.931593894958496, 22.438918590545654] in episode 1534
Report: 
rewardSum:194.33333333333334
loss:[14.931593894958496, 22.438918590545654]
policies:[0, 4, 0]
qAverage:[0.0, 93.05751037597656]
ws:[6.545230793952942, 14.880343437194824]
memory len:10000
memory used:3137.0
now epsilon is 0.1306472393439688, the reward is 194.33333333333334 with loss [26.84260106086731, 38.34954595565796] in episode 1535
Report: 
rewardSum:194.33333333333334
loss:[26.84260106086731, 38.34954595565796]
policies:[0, 4, 0]
qAverage:[0.0, 90.42322845458985]
ws:[6.612817105650902, 15.988662481307983]
memory len:10000
memory used:3137.0
now epsilon is 0.13051664108917466, the reward is 194.33333333333334 with loss [30.85126781463623, 18.649394989013672] in episode 1536
Report: 
rewardSum:194.33333333333334
loss:[30.85126781463623, 18.649394989013672]
policies:[2, 2, 0]
qAverage:[0.0, 80.50400797526042]
ws:[7.132641086975734, 14.778250217437744]
memory len:10000
memory used:3137.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22*		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.13032098844611312, the reward is 192.33333333333334 with loss [31.360191226005554, 40.93201780319214] in episode 1537
Report: 
rewardSum:192.33333333333334
loss:[31.360191226005554, 40.93201780319214]
policies:[0, 3, 3]
qAverage:[0.0, 87.73588371276855]
ws:[2.036062315106392, 7.9262717962265015]
memory len:10000
memory used:3138.0
now epsilon is 0.13019071631989312, the reward is 194.33333333333334 with loss [30.170851230621338, 20.337256908416748] in episode 1538
Report: 
rewardSum:194.33333333333334
loss:[30.170851230621338, 20.337256908416748]
policies:[0, 4, 0]
qAverage:[0.0, 89.98003387451172]
ws:[3.412540686130524, 10.742544716596603]
memory len:10000
memory used:3138.0
now epsilon is 0.13006057441695543, the reward is 194.33333333333334 with loss [24.936418056488037, 30.734355926513672] in episode 1539
Report: 
rewardSum:194.33333333333334
loss:[24.936418056488037, 30.734355926513672]
policies:[0, 3, 1]
qAverage:[0.0, 88.79907035827637]
ws:[5.1002465188503265, 13.284592300653458]
memory len:10000
memory used:3138.0
now epsilon is 0.12993056260712563, the reward is 194.33333333333334 with loss [25.852449417114258, 24.572173833847046] in episode 1540
Report: 
rewardSum:194.33333333333334
loss:[25.852449417114258, 24.572173833847046]
policies:[1, 3, 0]
qAverage:[0.0, 82.49298858642578]
ws:[7.5626220703125, 18.132832209269207]
memory len:10000
memory used:3138.0
now epsilon is 0.12980068076035933, the reward is 194.33333333333334 with loss [28.963965892791748, 20.02829885482788] in episode 1541
Report: 
rewardSum:194.33333333333334
loss:[28.963965892791748, 20.02829885482788]
policies:[0, 4, 0]
qAverage:[0.0, 92.82207489013672]
ws:[4.920643532276154, 13.533442497253418]
memory len:10000
memory used:3138.0
now epsilon is 0.12960610138680193, the reward is 192.33333333333334 with loss [26.59474802017212, 28.087910413742065] in episode 1542
Report: 
rewardSum:192.33333333333334
loss:[26.59474802017212, 28.087910413742065]
policies:[0, 5, 1]
qAverage:[0.0, 95.00198745727539]
ws:[3.8983956774075827, 14.486115415891012]
memory len:10000
memory used:3138.0
now epsilon is 0.12941181369994748, the reward is 192.33333333333334 with loss [39.36460828781128, 34.64087629318237] in episode 1543
Report: 
rewardSum:192.33333333333334
loss:[39.36460828781128, 34.64087629318237]
policies:[1, 4, 1]
qAverage:[0.0, 86.68750762939453]
ws:[1.4394341349601745, 9.853580760955811]
memory len:10000
memory used:3138.0
now epsilon is 0.1292178172625393, the reward is 192.33333333333334 with loss [36.586111068725586, 31.635971546173096] in episode 1544
Report: 
rewardSum:192.33333333333334
loss:[36.586111068725586, 31.635971546173096]
policies:[0, 5, 1]
qAverage:[0.0, 94.46920649210612]
ws:[4.378593288362026, 15.017055789629618]
memory len:10000
memory used:3138.0
now epsilon is 0.12908864789388266, the reward is 194.33333333333334 with loss [25.04231548309326, 21.134040355682373] in episode 1545
Report: 
rewardSum:194.33333333333334
loss:[25.04231548309326, 21.134040355682373]
policies:[0, 4, 0]
qAverage:[0.0, 92.40908508300781]
ws:[5.4150652378797535, 15.219704818725585]
memory len:10000
memory used:3138.0
now epsilon is 0.1289596076461642, the reward is 194.33333333333334 with loss [22.014763832092285, 24.03787612915039] in episode 1546
Report: 
rewardSum:194.33333333333334
loss:[22.014763832092285, 24.03787612915039]
policies:[0, 4, 0]
qAverage:[0.0, 90.7520767211914]
ws:[5.601440337300301, 16.1596887588501]
memory len:10000
memory used:3138.0
now epsilon is 0.12883069639031144, the reward is 194.33333333333334 with loss [20.663572549819946, 20.843228340148926] in episode 1547
Report: 
rewardSum:194.33333333333334
loss:[20.663572549819946, 20.843228340148926]
policies:[0, 4, 0]
qAverage:[0.0, 82.24446678161621]
ws:[0.574752539396286, 4.815940678119659]
memory len:10000
memory used:3138.0
now epsilon is 0.1287019139973809, the reward is 46.33333333333334 with loss [18.921965837478638, 23.95963764190674] in episode 1548
Report: 
rewardSum:46.33333333333334
loss:[18.921965837478638, 23.95963764190674]
policies:[0, 3, 1]
qAverage:[0.0, 68.96571604410808]
ws:[0.43006420135498047, 3.1153258879979453]
memory len:10000
memory used:3138.0
now epsilon is 0.1285732603385579, the reward is 194.33333333333334 with loss [26.39091396331787, 22.445984840393066] in episode 1549
Report: 
rewardSum:194.33333333333334
loss:[26.39091396331787, 22.445984840393066]
policies:[0, 4, 0]
qAverage:[0.0, 91.21642608642578]
ws:[5.276547533273697, 14.973690724372863]
memory len:10000
memory used:3138.0
now epsilon is 0.12844473528515665, the reward is 194.33333333333334 with loss [16.525710344314575, 23.20028281211853] in episode 1550
Report: 
rewardSum:194.33333333333334
loss:[16.525710344314575, 23.20028281211853]
policies:[0, 4, 0]
qAverage:[0.0, 91.95436096191406]
ws:[5.457534235715866, 14.598756408691406]
memory len:10000
memory used:3138.0
now epsilon is 0.12831633870861991, the reward is 194.33333333333334 with loss [18.51110029220581, 24.015713214874268] in episode 1551
Report: 
rewardSum:194.33333333333334
loss:[18.51110029220581, 24.015713214874268]
policies:[0, 4, 0]
qAverage:[0.0, 91.65376434326171]
ws:[7.048821663856506, 17.30934085845947]
memory len:10000
memory used:3138.0
now epsilon is 0.12818807048051906, the reward is 194.33333333333334 with loss [22.76631236076355, 19.579881191253662] in episode 1552
Report: 
rewardSum:194.33333333333334
loss:[22.76631236076355, 19.579881191253662]
policies:[0, 4, 0]
qAverage:[0.0, 90.92066040039063]
ws:[7.458204889297486, 17.520746421813964]
memory len:10000
memory used:3138.0
now epsilon is 0.12799590851106313, the reward is 192.33333333333334 with loss [31.42629337310791, 33.877326011657715] in episode 1553
Report: 
rewardSum:192.33333333333334
loss:[31.42629337310791, 33.877326011657715]
policies:[0, 4, 2]
qAverage:[0.0, 90.31724090576172]
ws:[4.112550497055054, 13.702099990844726]
memory len:10000
memory used:3138.0
now epsilon is 0.1278679605930185, the reward is 194.33333333333334 with loss [25.615768432617188, 28.544095516204834] in episode 1554
Report: 
rewardSum:194.33333333333334
loss:[25.615768432617188, 28.544095516204834]
policies:[1, 3, 0]
qAverage:[0.0, 91.47253227233887]
ws:[3.2155047059059143, 11.3774693608284]
memory len:10000
memory used:3138.0
now epsilon is 0.12774014057491945, the reward is 194.33333333333334 with loss [20.947665691375732, 24.080241680145264] in episode 1555
Report: 
rewardSum:194.33333333333334
loss:[20.947665691375732, 24.080241680145264]
policies:[0, 3, 1]
qAverage:[0.0, 72.05685679117839]
ws:[-1.1141613225142162, 1.3267557223637898]
memory len:10000
memory used:3138.0
now epsilon is 0.127612448328914, the reward is 194.33333333333334 with loss [13.340441226959229, 18.42746663093567] in episode 1556
Report: 
rewardSum:194.33333333333334
loss:[13.340441226959229, 18.42746663093567]
policies:[0, 4, 0]
qAverage:[0.0, 92.88138427734376]
ws:[4.010699391365051, 11.139549362659455]
memory len:10000
memory used:3138.0
now epsilon is 0.12745301250634614, the reward is 193.33333333333334 with loss [25.823373794555664, 32.08337068557739] in episode 1557
Report: 
rewardSum:193.33333333333334
loss:[25.823373794555664, 32.08337068557739]
policies:[0, 4, 1]
qAverage:[0.0, 90.72526702880859]
ws:[4.058202648162842, 10.721842050552368]
memory len:10000
memory used:3138.0
now epsilon is 0.12732560728075415, the reward is 194.33333333333334 with loss [18.558969497680664, 26.160700798034668] in episode 1558
Report: 
rewardSum:194.33333333333334
loss:[18.558969497680664, 26.160700798034668]
policies:[1, 2, 1]
qAverage:[0.0, 79.01437886555989]
ws:[6.122204462687175, 16.571938196818035]
memory len:10000
memory used:3138.0
now epsilon is 0.12719832941261877, the reward is 194.33333333333334 with loss [25.94244146347046, 17.174935162067413] in episode 1559
Report: 
rewardSum:194.33333333333334
loss:[25.94244146347046, 17.174935162067413]
policies:[0, 4, 0]
qAverage:[0.0, 91.24910583496094]
ws:[3.4061078786849976, 9.546921968460083]
memory len:10000
memory used:3138.0
now epsilon is 0.12707117877463028, the reward is 194.33333333333334 with loss [17.245616912841797, 23.340484142303467] in episode 1560
Report: 
rewardSum:194.33333333333334
loss:[17.245616912841797, 23.340484142303467]
policies:[0, 4, 0]
qAverage:[0.0, 84.72062492370605]
ws:[3.7119125425815582, 10.21297574043274]
memory len:10000
memory used:3138.0
now epsilon is 0.12688069109599615, the reward is 192.33333333333334 with loss [39.24897384643555, 38.7017560005188] in episode 1561
Report: 
rewardSum:192.33333333333334
loss:[39.24897384643555, 38.7017560005188]
policies:[0, 5, 1]
qAverage:[0.0, 84.42713012695313]
ws:[3.6890378475189207, 9.226626825332641]
memory len:10000
memory used:3138.0
now epsilon is 0.12675385797722977, the reward is 194.33333333333334 with loss [22.141874074935913, 20.178965091705322] in episode 1562
Report: 
rewardSum:194.33333333333334
loss:[22.141874074935913, 20.178965091705322]
policies:[0, 4, 0]
qAverage:[0.0, 82.56822395324707]
ws:[1.313650757074356, 3.5982807874679565]
memory len:10000
memory used:3138.0
############# STATE ###############
0-		9-		18*		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.12656384598240267, the reward is 192.33333333333334 with loss [34.74856615066528, 29.362444639205933] in episode 1563
Report: 
rewardSum:192.33333333333334
loss:[34.74856615066528, 29.362444639205933]
policies:[1, 4, 1]
qAverage:[0.0, 89.83554382324219]
ws:[4.555655097961425, 11.068303108215332]
memory len:10000
memory used:3138.0
now epsilon is 0.12643732958995277, the reward is 194.33333333333334 with loss [20.240187883377075, 16.781081676483154] in episode 1564
Report: 
rewardSum:194.33333333333334
loss:[20.240187883377075, 16.781081676483154]
policies:[0, 4, 0]
qAverage:[0.0, 88.5916961669922]
ws:[3.761751341819763, 8.525148677825928]
memory len:10000
memory used:3138.0
now epsilon is 0.1263109396664596, the reward is 194.33333333333334 with loss [19.824087619781494, 17.759362936019897] in episode 1565
Report: 
rewardSum:194.33333333333334
loss:[19.824087619781494, 17.759362936019897]
policies:[2, 2, 0]
qAverage:[0.0, 81.29452006022136]
ws:[5.258582433064778, 12.067739168802897]
memory len:10000
memory used:3138.0
now epsilon is 0.1261846760855016, the reward is 194.33333333333334 with loss [17.670859336853027, 22.975138187408447] in episode 1566
Report: 
rewardSum:194.33333333333334
loss:[17.670859336853027, 22.975138187408447]
policies:[0, 4, 0]
qAverage:[0.0, 87.52269592285157]
ws:[4.312487626075745, 9.90105152130127]
memory len:10000
memory used:3138.0
now epsilon is 0.1259955173300819, the reward is 192.33333333333334 with loss [32.33297061920166, 26.15414535999298] in episode 1567
Report: 
rewardSum:192.33333333333334
loss:[32.33297061920166, 26.15414535999298]
policies:[0, 5, 1]
qAverage:[0.0, 91.51607259114583]
ws:[4.16228308280309, 10.214943567911783]
memory len:10000
memory used:3137.0
now epsilon is 0.1258695690531966, the reward is 194.33333333333334 with loss [29.54047727584839, 27.008501052856445] in episode 1568
Report: 
rewardSum:194.33333333333334
loss:[29.54047727584839, 27.008501052856445]
policies:[0, 4, 0]
qAverage:[0.0, 88.8783462524414]
ws:[2.9326592445373536, 7.7460180759429935]
memory len:10000
memory used:3137.0
now epsilon is 0.1257437466773655, the reward is 46.33333333333334 with loss [19.901493787765503, 22.45130157470703] in episode 1569
Report: 
rewardSum:46.33333333333334
loss:[19.901493787765503, 22.45130157470703]
policies:[0, 3, 1]
qAverage:[0.0, 79.32890319824219]
ws:[1.24058061465621, 5.219426155090332]
memory len:10000
memory used:3137.0
now epsilon is 0.12561805007673466, the reward is 194.33333333333334 with loss [21.567509651184082, 24.17133355140686] in episode 1570
Report: 
rewardSum:194.33333333333334
loss:[21.567509651184082, 24.17133355140686]
policies:[0, 3, 1]
qAverage:[0.0, 87.78308486938477]
ws:[3.89967742562294, 10.27125108242035]
memory len:10000
memory used:3137.0
now epsilon is 0.1254924791255761, the reward is 194.33333333333334 with loss [30.980223655700684, 19.93571186065674] in episode 1571
Report: 
rewardSum:194.33333333333334
loss:[30.980223655700684, 19.93571186065674]
policies:[0, 3, 1]
qAverage:[0.0, 79.29853057861328]
ws:[1.648349553346634, 5.010325193405151]
memory len:10000
memory used:3137.0
now epsilon is 0.12536703369828744, the reward is 194.33333333333334 with loss [27.210854530334473, 21.797142505645752] in episode 1572
Report: 
rewardSum:194.33333333333334
loss:[27.210854530334473, 21.797142505645752]
policies:[0, 4, 0]
qAverage:[0.0, 83.70350341796875]
ws:[4.860607624053955, 11.537148475646973]
memory len:10000
memory used:3137.0
now epsilon is 0.1251791006401643, the reward is 192.33333333333334 with loss [34.555949211120605, 26.839555501937866] in episode 1573
Report: 
rewardSum:192.33333333333334
loss:[34.555949211120605, 26.839555501937866]
policies:[2, 3, 1]
qAverage:[0.0, 76.27755165100098]
ws:[2.4911288619041443, 7.704915404319763]
memory len:10000
memory used:3137.0
now epsilon is 0.12499144930549977, the reward is 192.33333333333334 with loss [33.938544034957886, 38.87537431716919] in episode 1574
Report: 
rewardSum:192.33333333333334
loss:[33.938544034957886, 38.87537431716919]
policies:[0, 4, 2]
qAverage:[0.0, 78.71505355834961]
ws:[5.454759001731873, 12.579816281795502]
memory len:10000
memory used:3137.0
now epsilon is 0.12486650472017628, the reward is 194.33333333333334 with loss [21.349326610565186, 25.407875061035156] in episode 1575
Report: 
rewardSum:194.33333333333334
loss:[21.349326610565186, 25.407875061035156]
policies:[1, 3, 0]
qAverage:[0.0, 77.12853813171387]
ws:[0.782832071185112, 3.291412055492401]
memory len:10000
memory used:3137.0
now epsilon is 0.12474168503259173, the reward is 194.33333333333334 with loss [20.864159107208252, 30.144147396087646] in episode 1576
Report: 
rewardSum:194.33333333333334
loss:[20.864159107208252, 30.144147396087646]
policies:[0, 3, 1]
qAverage:[0.0, 81.38551712036133]
ws:[5.387803308665752, 14.044283032417297]
memory len:10000
memory used:3137.0
now epsilon is 0.12461699011789518, the reward is 194.33333333333334 with loss [26.148969173431396, 15.52780818939209] in episode 1577
Report: 
rewardSum:194.33333333333334
loss:[26.148969173431396, 15.52780818939209]
policies:[0, 4, 0]
qAverage:[0.0, 85.97649230957032]
ws:[4.199129447340965, 10.212101364135743]
memory len:10000
memory used:3137.0
now epsilon is 0.12449241985136052, the reward is 194.33333333333334 with loss [21.852229595184326, 20.235036849975586] in episode 1578
Report: 
rewardSum:194.33333333333334
loss:[21.852229595184326, 20.235036849975586]
policies:[0, 4, 0]
qAverage:[0.0, 78.19770431518555]
ws:[6.2146094143390656, 14.481238961219788]
memory len:10000
memory used:3137.0
now epsilon is 0.12433688211485923, the reward is 193.33333333333334 with loss [27.183295726776123, 24.927349090576172] in episode 1579
Report: 
rewardSum:193.33333333333334
loss:[27.183295726776123, 24.927349090576172]
policies:[0, 4, 1]
qAverage:[0.0, 84.76579437255859]
ws:[5.26532381772995, 12.869580364227295]
memory len:10000
memory used:3137.0
now epsilon is 0.12421259185130462, the reward is 194.33333333333334 with loss [22.44948959350586, 23.437886714935303] in episode 1580
Report: 
rewardSum:194.33333333333334
loss:[22.44948959350586, 23.437886714935303]
policies:[1, 3, 0]
qAverage:[0.0, 76.0386962890625]
ws:[1.6340750977396965, 6.023517608642578]
memory len:10000
memory used:3138.0
now epsilon is 0.12408842583141247, the reward is 194.33333333333334 with loss [24.0111403465271, 23.83942985534668] in episode 1581
Report: 
rewardSum:194.33333333333334
loss:[24.0111403465271, 23.83942985534668]
policies:[0, 4, 0]
qAverage:[0.0, 84.27913208007813]
ws:[3.2523687839508058, 9.845354789495468]
memory len:10000
memory used:3138.0
now epsilon is 0.1239643839309857, the reward is 194.33333333333334 with loss [21.310108184814453, 15.790454864501953] in episode 1582
Report: 
rewardSum:194.33333333333334
loss:[21.310108184814453, 15.790454864501953]
policies:[0, 4, 0]
qAverage:[0.0, 76.44612312316895]
ws:[0.3069896250963211, 3.323582887649536]
memory len:10000
memory used:3138.0
now epsilon is 0.12377855353296756, the reward is 44.33333333333334 with loss [34.58591032028198, 37.37299871444702] in episode 1583
Report: 
rewardSum:44.33333333333334
loss:[34.58591032028198, 37.37299871444702]
policies:[0, 3, 3]
qAverage:[0.0, 63.79739125569662]
ws:[1.8386285106341045, 6.708919127782186]
memory len:10000
memory used:3138.0
now epsilon is 0.12365482138865651, the reward is 194.33333333333334 with loss [20.050769329071045, 29.80250310897827] in episode 1584
Report: 
rewardSum:194.33333333333334
loss:[20.050769329071045, 29.80250310897827]
policies:[1, 2, 1]
qAverage:[0.0, 75.76751200358073]
ws:[6.072611371676127, 14.77244790395101]
memory len:10000
memory used:3138.0
now epsilon is 0.12353121293009794, the reward is 194.33333333333334 with loss [20.59068727493286, 25.45124578475952] in episode 1585
Report: 
rewardSum:194.33333333333334
loss:[20.59068727493286, 25.45124578475952]
policies:[0, 4, 0]
qAverage:[0.0, 81.74987335205078]
ws:[4.414766037464142, 13.12182822227478]
memory len:10000
memory used:3137.0
now epsilon is 0.12346945504433371, the reward is -1.0 with loss [12.81413984298706, 18.3897762298584] in episode 1586
Report: 
rewardSum:-1.0
loss:[12.81413984298706, 18.3897762298584]
policies:[0, 1, 1]
qAverage:[0.0, 45.962520599365234]
ws:[-0.2455165982246399, 0.8376719355583191]
memory len:10000
memory used:3137.0
now epsilon is 0.12334603188261867, the reward is 194.33333333333334 with loss [22.295037269592285, 24.041642665863037] in episode 1587
Report: 
rewardSum:194.33333333333334
loss:[22.295037269592285, 24.041642665863037]
policies:[0, 4, 0]
qAverage:[0.0, 82.89558258056641]
ws:[2.490288805961609, 10.544859606027604]
memory len:10000
memory used:3138.0
now epsilon is 0.12322273209778938, the reward is 194.33333333333334 with loss [18.07496190071106, 17.25400722026825] in episode 1588
Report: 
rewardSum:194.33333333333334
loss:[18.07496190071106, 17.25400722026825]
policies:[0, 4, 0]
qAverage:[0.0, 82.69327392578126]
ws:[1.426916813850403, 8.277066087722778]
memory len:10000
memory used:3138.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2*		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.12309955556651521, the reward is 194.33333333333334 with loss [18.96717882156372, 25.137794971466064] in episode 1589
Report: 
rewardSum:194.33333333333334
loss:[18.96717882156372, 25.137794971466064]
policies:[0, 4, 0]
qAverage:[0.0, 78.92328453063965]
ws:[2.5356415510177612, 10.253360599279404]
memory len:10000
memory used:3138.0
now epsilon is 0.1229457580400474, the reward is 193.33333333333334 with loss [26.93901300430298, 31.24319362640381] in episode 1590
Report: 
rewardSum:193.33333333333334
loss:[26.93901300430298, 31.24319362640381]
policies:[2, 2, 1]
qAverage:[0.0, 63.68632507324219]
ws:[-0.006499568621317546, 3.3028483390808105]
memory len:10000
memory used:3138.0
now epsilon is 0.122822858378983, the reward is 194.33333333333334 with loss [18.89720392227173, 16.01851725578308] in episode 1591
Report: 
rewardSum:194.33333333333334
loss:[18.89720392227173, 16.01851725578308]
policies:[0, 4, 0]
qAverage:[0.0, 79.13746452331543]
ws:[3.9695509262382984, 15.41280472278595]
memory len:10000
memory used:3138.0
now epsilon is 0.12270008157149999, the reward is 194.33333333333334 with loss [14.543597221374512, 20.4076189994812] in episode 1592
Report: 
rewardSum:194.33333333333334
loss:[14.543597221374512, 20.4076189994812]
policies:[0, 4, 0]
qAverage:[0.0, 83.70403900146485]
ws:[2.8871384024620057, 13.021020030975341]
memory len:10000
memory used:3138.0
now epsilon is 0.12257742749479082, the reward is 194.33333333333334 with loss [20.87393808364868, 20.321696758270264] in episode 1593
Report: 
rewardSum:194.33333333333334
loss:[20.87393808364868, 20.321696758270264]
policies:[1, 2, 1]
qAverage:[0.0, 71.56868998209636]
ws:[4.960190455118815, 17.960206985473633]
memory len:10000
memory used:3138.0
now epsilon is 0.12245489602617075, the reward is 194.33333333333334 with loss [19.850123405456543, 20.282920837402344] in episode 1594
Report: 
rewardSum:194.33333333333334
loss:[19.850123405456543, 20.282920837402344]
policies:[0, 4, 0]
qAverage:[0.0, 83.84186706542968]
ws:[2.399635463953018, 11.632578420639039]
memory len:10000
memory used:3138.0
now epsilon is 0.12227132844533657, the reward is 192.33333333333334 with loss [31.954273223876953, 26.44667959213257] in episode 1595
Report: 
rewardSum:192.33333333333334
loss:[31.954273223876953, 26.44667959213257]
policies:[0, 5, 1]
qAverage:[0.0, 77.24047241210937]
ws:[1.4781966805458069, 10.015018749237061]
memory len:10000
memory used:3138.0
now epsilon is 0.12214910296099792, the reward is 194.33333333333334 with loss [13.861085414886475, 18.154308795928955] in episode 1596
Report: 
rewardSum:194.33333333333334
loss:[13.861085414886475, 18.154308795928955]
policies:[0, 4, 0]
qAverage:[0.0, 80.5697250366211]
ws:[1.1281443953514099, 9.367330241203309]
memory len:10000
memory used:3138.0
now epsilon is 0.1220269996563167, the reward is 194.33333333333334 with loss [31.460434913635254, 22.615053176879883] in episode 1597
Report: 
rewardSum:194.33333333333334
loss:[31.460434913635254, 22.615053176879883]
policies:[0, 4, 0]
qAverage:[0.0, 78.66434173583984]
ws:[1.5247360706329345, 9.909823274612426]
memory len:10000
memory used:3138.0
now epsilon is 0.12190501840915906, the reward is 194.33333333333334 with loss [18.59313726425171, 17.858455657958984] in episode 1598
Report: 
rewardSum:194.33333333333334
loss:[18.59313726425171, 17.858455657958984]
policies:[0, 4, 0]
qAverage:[0.0, 79.92127075195313]
ws:[1.9475857466459274, 9.9944242477417]
memory len:10000
memory used:3138.0
now epsilon is 0.12178315909751322, the reward is 194.33333333333334 with loss [19.620256423950195, 27.28941559791565] in episode 1599
Report: 
rewardSum:194.33333333333334
loss:[19.620256423950195, 27.28941559791565]
policies:[1, 2, 1]
qAverage:[0.0, 63.307708740234375]
ws:[0.325271874666214, 3.0821495850880942]
memory len:10000
memory used:3138.0
now epsilon is 0.12166142159948941, the reward is 194.33333333333334 with loss [11.551688134670258, 16.462794303894043] in episode 1600
Report: 
rewardSum:194.33333333333334
loss:[11.551688134670258, 16.462794303894043]
policies:[0, 4, 0]
qAverage:[0.0, 79.50584869384765]
ws:[2.4576425552368164, 10.201838064193726]
memory len:10000
memory used:3138.0
now epsilon is 0.12153980579331966, the reward is 194.33333333333334 with loss [14.188216209411621, 23.620831966400146] in episode 1601
Report: 
rewardSum:194.33333333333334
loss:[14.188216209411621, 23.620831966400146]
policies:[1, 2, 1]
qAverage:[0.0, 46.981201171875]
ws:[2.2016401290893555, 8.685405731201172]
memory len:10000
memory used:3138.0
now epsilon is 0.12138795697946843, the reward is 193.33333333333334 with loss [30.155266284942627, 29.409305810928345] in episode 1602
Report: 
rewardSum:193.33333333333334
loss:[30.155266284942627, 29.409305810928345]
policies:[0, 4, 1]
qAverage:[0.0, 78.69928588867188]
ws:[3.1914291977882385, 11.550645112991333]
memory len:10000
memory used:3138.0
now epsilon is 0.12126661453538656, the reward is 194.33333333333334 with loss [25.042319774627686, 24.94485902786255] in episode 1603
Report: 
rewardSum:194.33333333333334
loss:[25.042319774627686, 24.94485902786255]
policies:[0, 4, 0]
qAverage:[0.0, 79.92010955810547]
ws:[2.0624205708503722, 8.480296754837036]
memory len:10000
memory used:3138.0
now epsilon is 0.12114539338825295, the reward is 194.33333333333334 with loss [13.34385061264038, 17.685031414031982] in episode 1604
Report: 
rewardSum:194.33333333333334
loss:[13.34385061264038, 17.685031414031982]
policies:[0, 4, 0]
qAverage:[0.0, 74.76556968688965]
ws:[1.533088881522417, 8.178924918174744]
memory len:10000
memory used:3138.0
now epsilon is 0.12102429341681611, the reward is 194.33333333333334 with loss [26.29658317565918, 18.168261528015137] in episode 1605
Report: 
rewardSum:194.33333333333334
loss:[26.29658317565918, 18.168261528015137]
policies:[0, 3, 1]
qAverage:[0.0, 73.9218521118164]
ws:[2.2823380306363106, 10.704515159130096]
memory len:10000
memory used:3138.0
now epsilon is 0.12084287039915298, the reward is 192.33333333333334 with loss [32.14392137527466, 35.53891205787659] in episode 1606
Report: 
rewardSum:192.33333333333334
loss:[32.14392137527466, 35.53891205787659]
policies:[0, 5, 1]
qAverage:[0.0, 81.9480209350586]
ws:[1.248960276444753, 8.248043874899546]
memory len:10000
memory used:3138.0
now epsilon is 0.1207824565166328, the reward is -1.0 with loss [11.853405952453613, 11.011045455932617] in episode 1607
Report: 
rewardSum:-1.0
loss:[11.853405952453613, 11.011045455932617]
policies:[0, 1, 1]
qAverage:[0.0, 43.02971649169922]
ws:[-0.029649335891008377, 1.1293039321899414]
memory len:10000
memory used:3138.0
now epsilon is 0.12072207283727802, the reward is -1.0 with loss [10.157562017440796, 12.515573978424072] in episode 1608
Report: 
rewardSum:-1.0
loss:[10.157562017440796, 12.515573978424072]
policies:[0, 1, 1]
qAverage:[0.0, 41.137794494628906]
ws:[-0.16701380908489227, 1.1496970653533936]
memory len:10000
memory used:3138.0
now epsilon is 0.12060139602767342, the reward is 194.33333333333334 with loss [21.750876903533936, 18.847383737564087] in episode 1609
Report: 
rewardSum:194.33333333333334
loss:[21.750876903533936, 18.847383737564087]
policies:[0, 4, 0]
qAverage:[0.0, 77.91065063476563]
ws:[1.6578184351325036, 9.612655305862427]
memory len:10000
memory used:3138.0
now epsilon is 0.12048083984963215, the reward is 194.33333333333334 with loss [33.779563426971436, 14.387413501739502] in episode 1610
Report: 
rewardSum:194.33333333333334
loss:[33.779563426971436, 14.387413501739502]
policies:[0, 3, 1]
qAverage:[0.0, 68.58533986409505]
ws:[1.7674657603104909, 10.313327153523764]
memory len:10000
memory used:3138.0
now epsilon is 0.12036040418256788, the reward is 194.33333333333334 with loss [25.65278434753418, 19.12414264678955] in episode 1611
Report: 
rewardSum:194.33333333333334
loss:[25.65278434753418, 19.12414264678955]
policies:[0, 4, 0]
qAverage:[0.0, 77.63359222412109]
ws:[1.6287030011415482, 9.66492726802826]
memory len:10000
memory used:3138.0
now epsilon is 0.12024008890601484, the reward is 194.33333333333334 with loss [22.476892471313477, 25.088109016418457] in episode 1612
Report: 
rewardSum:194.33333333333334
loss:[22.476892471313477, 25.088109016418457]
policies:[0, 4, 0]
qAverage:[0.0, 77.04821472167968]
ws:[1.8748840272426606, 10.382074403762818]
memory len:10000
memory used:3138.0
now epsilon is 0.12011989389962764, the reward is 194.33333333333334 with loss [30.650553941726685, 23.359006881713867] in episode 1613
Report: 
rewardSum:194.33333333333334
loss:[30.650553941726685, 23.359006881713867]
policies:[0, 4, 0]
qAverage:[0.0, 77.64507598876953]
ws:[1.87054186463356, 10.019500899314881]
memory len:10000
memory used:3138.0
now epsilon is 0.11999981904318122, the reward is 194.33333333333334 with loss [22.36066246032715, 21.308088779449463] in episode 1614
Report: 
rewardSum:194.33333333333334
loss:[22.36066246032715, 21.308088779449463]
policies:[1, 2, 1]
qAverage:[0.0, 53.51725769042969]
ws:[2.6095309257507324, 12.36105728149414]
memory len:10000
memory used:3138.0
now epsilon is 0.11981993177695391, the reward is 192.33333333333334 with loss [32.95168113708496, 24.399752855300903] in episode 1615
Report: 
rewardSum:192.33333333333334
loss:[32.95168113708496, 24.399752855300903]
policies:[0, 4, 2]
qAverage:[0.0, 74.24025268554688]
ws:[0.12291629612445831, 4.356841687858105]
memory len:10000
memory used:3138.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21*		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.11955060636809758, the reward is 189.33333333333334 with loss [48.08987760543823, 48.98827362060547] in episode 1616
Report: 
rewardSum:189.33333333333334
loss:[48.08987760543823, 48.98827362060547]
policies:[0, 6, 3]
qAverage:[0.0, 80.68997083391461]
ws:[0.7058009654283524, 6.263177667345319]
memory len:10000
memory used:3138.0
now epsilon is 0.11943110058573544, the reward is 194.33333333333334 with loss [17.144968509674072, 19.795321464538574] in episode 1617
Report: 
rewardSum:194.33333333333334
loss:[17.144968509674072, 19.795321464538574]
policies:[1, 3, 0]
qAverage:[0.0, 71.68469047546387]
ws:[1.7044076323509216, 10.962484121322632]
memory len:10000
memory used:3138.0
now epsilon is 0.11931171426434846, the reward is 194.33333333333334 with loss [20.055347323417664, 19.441405296325684] in episode 1618
Report: 
rewardSum:194.33333333333334
loss:[20.055347323417664, 19.441405296325684]
policies:[0, 4, 0]
qAverage:[0.0, 73.95420837402344]
ws:[0.2708040952682495, 8.785128498077393]
memory len:10000
memory used:3138.0
now epsilon is 0.11919244728452047, the reward is 194.33333333333334 with loss [19.95079517364502, 19.4010226726532] in episode 1619
Report: 
rewardSum:194.33333333333334
loss:[19.95079517364502, 19.4010226726532]
policies:[0, 3, 1]
qAverage:[0.0, 72.20293045043945]
ws:[0.4279235601425171, 10.289848446846008]
memory len:10000
memory used:3138.0
now epsilon is 0.11907329952695463, the reward is 194.33333333333334 with loss [19.61390733718872, 25.722145557403564] in episode 1620
Report: 
rewardSum:194.33333333333334
loss:[19.61390733718872, 25.722145557403564]
policies:[0, 4, 0]
qAverage:[0.0, 74.79424896240235]
ws:[0.1929262399673462, 8.41767339706421]
memory len:10000
memory used:3138.0
now epsilon is 0.11889480117167911, the reward is 192.33333333333334 with loss [27.780762434005737, 31.12914514541626] in episode 1621
Report: 
rewardSum:192.33333333333334
loss:[27.780762434005737, 31.12914514541626]
policies:[0, 5, 1]
qAverage:[0.0, 75.13643900553386]
ws:[-0.7145039737224579, 5.9886588255564375]
memory len:10000
memory used:3138.0
now epsilon is 0.11877595094862742, the reward is 194.33333333333334 with loss [19.90395736694336, 24.10731029510498] in episode 1622
Report: 
rewardSum:194.33333333333334
loss:[19.90395736694336, 24.10731029510498]
policies:[0, 4, 0]
qAverage:[0.0, 56.54301198323568]
ws:[-0.33035896221796673, 5.418916900952657]
memory len:10000
memory used:3138.0
now epsilon is 0.11865721953123737, the reward is 194.33333333333334 with loss [23.511833667755127, 20.414487838745117] in episode 1623
Report: 
rewardSum:194.33333333333334
loss:[23.511833667755127, 20.414487838745117]
policies:[0, 4, 0]
qAverage:[0.0, 74.35091552734374]
ws:[0.028012830764055252, 7.79420690536499]
memory len:10000
memory used:3138.0
now epsilon is 0.11853860680074785, the reward is 194.33333333333334 with loss [23.140430450439453, 15.854279041290283] in episode 1624
Report: 
rewardSum:194.33333333333334
loss:[23.140430450439453, 15.854279041290283]
policies:[1, 3, 0]
qAverage:[0.0, 66.02153587341309]
ws:[0.4084045756608248, 5.8783416748046875]
memory len:10000
memory used:3138.0
now epsilon is 0.11839050761035684, the reward is 193.33333333333334 with loss [27.509790897369385, 25.347709894180298] in episode 1625
Report: 
rewardSum:193.33333333333334
loss:[27.509790897369385, 25.347709894180298]
policies:[0, 4, 1]
qAverage:[0.0, 74.70032501220703]
ws:[0.7960978448390961, 10.17872633934021]
memory len:10000
memory used:3139.0
now epsilon is 0.11827216149178789, the reward is 194.33333333333334 with loss [23.019844949245453, 18.01115584373474] in episode 1626
Report: 
rewardSum:194.33333333333334
loss:[23.019844949245453, 18.01115584373474]
policies:[0, 4, 0]
qAverage:[0.0, 74.04780731201171]
ws:[0.14379740953445436, 9.673822498321533]
memory len:10000
memory used:3139.0
now epsilon is 0.11821303280305209, the reward is -1.0 with loss [8.685287714004517, 8.23723554611206] in episode 1627
Report: 
rewardSum:-1.0
loss:[8.685287714004517, 8.23723554611206]
policies:[0, 1, 1]
qAverage:[0.0, 41.37967300415039]
ws:[-0.7506704926490784, 1.3175677061080933]
memory len:10000
memory used:3139.0
now epsilon is 0.11809486409274851, the reward is 194.33333333333334 with loss [30.135690689086914, 25.28719997406006] in episode 1628
Report: 
rewardSum:194.33333333333334
loss:[30.135690689086914, 25.28719997406006]
policies:[0, 3, 1]
qAverage:[0.0, 71.71197509765625]
ws:[0.2186264842748642, 10.183645606040955]
memory len:10000
memory used:3139.0
now epsilon is 0.11797681350684933, the reward is 194.33333333333334 with loss [22.764580249786377, 17.7112398147583] in episode 1629
Report: 
rewardSum:194.33333333333334
loss:[22.764580249786377, 17.7112398147583]
policies:[0, 4, 0]
qAverage:[0.0, 73.22017288208008]
ws:[1.371942162513733, 12.8202805519104]
memory len:10000
memory used:3139.0
now epsilon is 0.11785888092727446, the reward is 194.33333333333334 with loss [21.127568244934082, 29.200483322143555] in episode 1630
Report: 
rewardSum:194.33333333333334
loss:[21.127568244934082, 29.200483322143555]
policies:[0, 2, 2]
qAverage:[0.0, 61.72304026285807]
ws:[0.2725357413291931, 9.763215700785318]
memory len:10000
memory used:3139.0
now epsilon is 0.1177999588529909, the reward is -1.0 with loss [21.96778106689453, 8.99911105632782] in episode 1631
Report: 
rewardSum:-1.0
loss:[21.96778106689453, 8.99911105632782]
policies:[0, 1, 1]
qAverage:[0.0, 39.17327880859375]
ws:[-0.296451598405838, 1.569692611694336]
memory len:10000
memory used:3139.0
now epsilon is 0.11768220306176047, the reward is 194.33333333333334 with loss [19.43747353553772, 25.53828525543213] in episode 1632
Report: 
rewardSum:194.33333333333334
loss:[19.43747353553772, 25.53828525543213]
policies:[0, 3, 1]
qAverage:[0.0, 71.32465553283691]
ws:[1.1707125455141068, 13.06022560596466]
memory len:10000
memory used:3139.0
now epsilon is 0.11753517384092466, the reward is 193.33333333333334 with loss [25.7497341632843, 30.866801738739014] in episode 1633
Report: 
rewardSum:193.33333333333334
loss:[25.7497341632843, 30.866801738739014]
policies:[0, 4, 1]
qAverage:[0.0, 71.80530090332032]
ws:[1.6532754957675935, 12.419549942016602]
memory len:10000
memory used:3140.0
now epsilon is 0.11741768273542845, the reward is 194.33333333333334 with loss [18.56810164451599, 24.03777837753296] in episode 1634
Report: 
rewardSum:194.33333333333334
loss:[18.56810164451599, 24.03777837753296]
policies:[0, 3, 1]
qAverage:[0.0, 64.94175910949707]
ws:[1.3266479074954987, 7.69902503490448]
memory len:10000
memory used:3139.0
now epsilon is 0.11730030907698591, the reward is 194.33333333333334 with loss [16.17038345336914, 16.827039003372192] in episode 1635
Report: 
rewardSum:194.33333333333334
loss:[16.17038345336914, 16.827039003372192]
policies:[0, 4, 0]
qAverage:[0.0, 71.34240264892578]
ws:[1.5270566880702972, 11.335858345031738]
memory len:10000
memory used:3139.0
now epsilon is 0.11718305274819403, the reward is 194.33333333333334 with loss [21.807992458343506, 25.80960750579834] in episode 1636
Report: 
rewardSum:194.33333333333334
loss:[21.807992458343506, 25.80960750579834]
policies:[1, 3, 0]
qAverage:[0.0, 64.90611457824707]
ws:[0.7046436155214906, 6.118040531873703]
memory len:10000
memory used:3139.0
now epsilon is 0.11706591363176716, the reward is 194.33333333333334 with loss [27.99503183364868, 21.091651439666748] in episode 1637
Report: 
rewardSum:194.33333333333334
loss:[27.99503183364868, 21.091651439666748]
policies:[1, 3, 0]
qAverage:[0.0, 62.195996602376304]
ws:[0.6428859755396843, 13.341344833374023]
memory len:10000
memory used:3139.0
now epsilon is 0.11694889161053684, the reward is 194.33333333333334 with loss [17.945165395736694, 16.33029055595398] in episode 1638
Report: 
rewardSum:194.33333333333334
loss:[17.945165395736694, 16.33029055595398]
policies:[0, 4, 0]
qAverage:[0.0, 72.22544403076172]
ws:[0.1777177035808563, 8.743701946735381]
memory len:10000
memory used:3139.0
now epsilon is 0.11683198656745182, the reward is 194.33333333333334 with loss [22.799221515655518, 17.160398960113525] in episode 1639
Report: 
rewardSum:194.33333333333334
loss:[22.799221515655518, 17.160398960113525]
policies:[0, 4, 0]
qAverage:[0.0, 66.16866874694824]
ws:[0.7046075165271759, 9.772717520594597]
memory len:10000
memory used:3139.0
now epsilon is 0.11671519838557781, the reward is 194.33333333333334 with loss [18.79246163368225, 25.20900249481201] in episode 1640
Report: 
rewardSum:194.33333333333334
loss:[18.79246163368225, 25.20900249481201]
policies:[0, 4, 0]
qAverage:[0.0, 71.87919006347656]
ws:[1.0825331330299377, 9.050681579113007]
memory len:10000
memory used:3139.0
now epsilon is 0.11659852694809739, the reward is 194.33333333333334 with loss [20.459572315216064, 19.29673457145691] in episode 1641
Report: 
rewardSum:194.33333333333334
loss:[20.459572315216064, 19.29673457145691]
policies:[0, 4, 0]
qAverage:[0.0, 72.42359161376953]
ws:[0.8177659031003713, 8.935188102722169]
memory len:10000
memory used:3139.0
now epsilon is 0.11642373843236406, the reward is 192.33333333333334 with loss [31.37079882621765, 31.800215482711792] in episode 1642
Report: 
rewardSum:192.33333333333334
loss:[31.37079882621765, 31.800215482711792]
policies:[1, 3, 2]
qAverage:[0.0, 64.65681076049805]
ws:[-0.5741971638053656, 1.9591951370239258]
memory len:10000
memory used:3139.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2*		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.11630735834555758, the reward is 194.33333333333334 with loss [24.212977409362793, 31.10826587677002] in episode 1643
Report: 
rewardSum:194.33333333333334
loss:[24.212977409362793, 31.10826587677002]
policies:[0, 4, 0]
qAverage:[0.0, 70.13429260253906]
ws:[0.35981945395469667, 8.79196639060974]
memory len:10000
memory used:3139.0
now epsilon is 0.11619109459520266, the reward is 194.33333333333334 with loss [23.08218288421631, 29.090813636779785] in episode 1644
Report: 
rewardSum:194.33333333333334
loss:[23.08218288421631, 29.090813636779785]
policies:[0, 4, 0]
qAverage:[0.0, 71.74125823974609]
ws:[0.15875869989395142, 8.146286535263062]
memory len:10000
memory used:3139.0
now epsilon is 0.11607494706500644, the reward is 194.33333333333334 with loss [24.48099994659424, 22.981434106826782] in episode 1645
Report: 
rewardSum:194.33333333333334
loss:[24.48099994659424, 22.981434106826782]
policies:[1, 3, 0]
qAverage:[0.0, 68.98653602600098]
ws:[0.33212025463581085, 8.975951313972473]
memory len:10000
memory used:3139.0
now epsilon is 0.11595891563879238, the reward is 194.33333333333334 with loss [20.053871154785156, 20.256909370422363] in episode 1646
Report: 
rewardSum:194.33333333333334
loss:[20.053871154785156, 20.256909370422363]
policies:[0, 4, 0]
qAverage:[0.0, 68.17678260803223]
ws:[0.21969637175789103, 6.161271274089813]
memory len:10000
memory used:3139.0
now epsilon is 0.1158430002005, the reward is 194.33333333333334 with loss [21.9228515625, 23.072887897491455] in episode 1647
Report: 
rewardSum:194.33333333333334
loss:[21.9228515625, 23.072887897491455]
policies:[0, 2, 2]
qAverage:[0.0, 59.713330586751304]
ws:[0.2982253432273865, 6.180524031321208]
memory len:10000
memory used:3140.0
now epsilon is 0.1156693442668178, the reward is 44.33333333333334 with loss [22.473273992538452, 30.22035813331604] in episode 1648
Report: 
rewardSum:44.33333333333334
loss:[22.473273992538452, 30.22035813331604]
policies:[0, 4, 2]
qAverage:[0.0, 67.65022888183594]
ws:[1.1043832898139954, 5.494903469085694]
memory len:10000
memory used:3140.0
now epsilon is 0.11555371829132621, the reward is 194.33333333333334 with loss [20.18309450149536, 24.191454887390137] in episode 1649
Report: 
rewardSum:194.33333333333334
loss:[20.18309450149536, 24.191454887390137]
policies:[0, 2, 2]
qAverage:[0.0, 61.43539683024088]
ws:[1.3933970133463542, 8.278156439463297]
memory len:10000
memory used:3140.0
now epsilon is 0.1154382078984576, the reward is 194.33333333333334 with loss [19.298747062683105, 13.462288856506348] in episode 1650
Report: 
rewardSum:194.33333333333334
loss:[19.298747062683105, 13.462288856506348]
policies:[0, 3, 1]
qAverage:[0.0, 61.63547007242838]
ws:[1.631285587946574, 8.555045445760092]
memory len:10000
memory used:3139.0
now epsilon is 0.11526515877386216, the reward is 192.33333333333334 with loss [40.43513298034668, 39.01321744918823] in episode 1651
Report: 
rewardSum:192.33333333333334
loss:[40.43513298034668, 39.01321744918823]
policies:[0, 5, 1]
qAverage:[0.0, 73.20884323120117]
ws:[1.4120461344718933, 7.202574809392293]
memory len:10000
memory used:3140.0
now epsilon is 0.11514993683231922, the reward is 194.33333333333334 with loss [21.837313652038574, 24.089195489883423] in episode 1652
Report: 
rewardSum:194.33333333333334
loss:[21.837313652038574, 24.089195489883423]
policies:[0, 4, 0]
qAverage:[0.0, 70.15955810546875]
ws:[0.7115905344486236, 7.122589540481568]
memory len:10000
memory used:3140.0
now epsilon is 0.11497731984415892, the reward is 192.33333333333334 with loss [24.933736085891724, 35.708125829696655] in episode 1653
Report: 
rewardSum:192.33333333333334
loss:[24.933736085891724, 35.708125829696655]
policies:[0, 5, 1]
qAverage:[0.0, 68.853564453125]
ws:[0.08439342081546783, 5.0518197298049925]
memory len:10000
memory used:3140.0
now epsilon is 0.11486238563362407, the reward is 194.33333333333334 with loss [19.54878330230713, 19.385059595108032] in episode 1654
Report: 
rewardSum:194.33333333333334
loss:[19.54878330230713, 19.385059595108032]
policies:[0, 4, 0]
qAverage:[0.0, 68.122802734375]
ws:[0.39385470747947693, 5.420807504653931]
memory len:10000
memory used:3140.0
now epsilon is 0.11480496161970635, the reward is -1.0 with loss [13.148925304412842, 11.369146823883057] in episode 1655
Report: 
rewardSum:-1.0
loss:[13.148925304412842, 11.369146823883057]
policies:[0, 1, 1]
qAverage:[0.0, 37.935157775878906]
ws:[0.08869989216327667, 0.8220708966255188]
memory len:10000
memory used:3139.0
now epsilon is 0.1146328617710585, the reward is 192.33333333333334 with loss [27.19308352470398, 32.48502254486084] in episode 1656
Report: 
rewardSum:192.33333333333334
loss:[27.19308352470398, 32.48502254486084]
policies:[0, 5, 1]
qAverage:[0.0, 70.37000783284505]
ws:[0.8081453194220861, 5.5343936284383135]
memory len:10000
memory used:3139.0
now epsilon is 0.11451827188944652, the reward is 194.33333333333334 with loss [26.55010175704956, 27.836167335510254] in episode 1657
Report: 
rewardSum:194.33333333333334
loss:[26.55010175704956, 27.836167335510254]
policies:[2, 2, 0]
qAverage:[0.0, 52.95134735107422]
ws:[1.0805885394414265, 5.104044993718465]
memory len:10000
memory used:3139.0
now epsilon is 0.11440379655475211, the reward is 194.33333333333334 with loss [28.234585762023926, 16.371424198150635] in episode 1658
Report: 
rewardSum:194.33333333333334
loss:[28.234585762023926, 16.371424198150635]
policies:[1, 3, 0]
qAverage:[0.0, 67.47142219543457]
ws:[1.4846650660037994, 9.945651769638062]
memory len:10000
memory used:3139.0
now epsilon is 0.11428943565247128, the reward is 194.33333333333334 with loss [15.64620852470398, 18.52490758895874] in episode 1659
Report: 
rewardSum:194.33333333333334
loss:[15.64620852470398, 18.52490758895874]
policies:[1, 3, 0]
qAverage:[0.0, 68.24593734741211]
ws:[1.5268933176994324, 9.922798752784729]
memory len:10000
memory used:3139.0
now epsilon is 0.11417518906821456, the reward is 194.33333333333334 with loss [25.06007671356201, 19.699389696121216] in episode 1660
Report: 
rewardSum:194.33333333333334
loss:[25.06007671356201, 19.699389696121216]
policies:[2, 2, 0]
qAverage:[0.0, 51.43720245361328]
ws:[0.3252194325129191, 3.215667804082235]
memory len:10000
memory used:3139.0
now epsilon is 0.11406105668770675, the reward is 194.33333333333334 with loss [23.77606773376465, 15.092581987380981] in episode 1661
Report: 
rewardSum:194.33333333333334
loss:[23.77606773376465, 15.092581987380981]
policies:[0, 4, 0]
qAverage:[0.0, 64.12037658691406]
ws:[1.1507295817136765, 9.967893660068512]
memory len:10000
memory used:3139.0
now epsilon is 0.11394703839678695, the reward is 194.33333333333334 with loss [18.075023651123047, 20.12336802482605] in episode 1662
Report: 
rewardSum:194.33333333333334
loss:[18.075023651123047, 20.12336802482605]
policies:[0, 4, 0]
qAverage:[0.0, 67.39990615844727]
ws:[1.0271298997104168, 10.302735328674316]
memory len:10000
memory used:3139.0
now epsilon is 0.11377622462893851, the reward is 192.33333333333334 with loss [47.99451303482056, 32.41491079330444] in episode 1663
Report: 
rewardSum:192.33333333333334
loss:[47.99451303482056, 32.41491079330444]
policies:[0, 5, 1]
qAverage:[0.0, 69.00940246582032]
ws:[0.18829310536384583, 7.057956552505493]
memory len:10000
memory used:3139.0
now epsilon is 0.11366249106328324, the reward is 194.33333333333334 with loss [20.435452461242676, 25.341933727264404] in episode 1664
Report: 
rewardSum:194.33333333333334
loss:[20.435452461242676, 25.341933727264404]
policies:[1, 3, 0]
qAverage:[0.0, 67.67815208435059]
ws:[0.37324832007288933, 8.708813190460205]
memory len:10000
memory used:3139.0
now epsilon is 0.11354887118855066, the reward is 194.33333333333334 with loss [24.244213581085205, 24.879600524902344] in episode 1665
Report: 
rewardSum:194.33333333333334
loss:[24.244213581085205, 24.879600524902344]
policies:[0, 4, 0]
qAverage:[0.0, 65.03056716918945]
ws:[-0.5513068195432425, 5.286781281232834]
memory len:10000
memory used:3139.0
now epsilon is 0.11343536489109246, the reward is 194.33333333333334 with loss [22.520031452178955, 21.207587957382202] in episode 1666
Report: 
rewardSum:194.33333333333334
loss:[22.520031452178955, 21.207587957382202]
policies:[0, 4, 0]
qAverage:[0.0, 66.26117095947265]
ws:[-0.2699819982051849, 6.75664873123169]
memory len:10000
memory used:3139.0
now epsilon is 0.11332197205737396, the reward is 194.33333333333334 with loss [19.213622331619263, 21.90341567993164] in episode 1667
Report: 
rewardSum:194.33333333333334
loss:[19.213622331619263, 21.90341567993164]
policies:[0, 4, 0]
qAverage:[0.0, 66.80879364013671]
ws:[-1.0312458992004394, 5.025847625732422]
memory len:10000
memory used:3139.0
now epsilon is 0.11320869257397392, the reward is 194.33333333333334 with loss [21.3515567779541, 16.851152539253235] in episode 1668
Report: 
rewardSum:194.33333333333334
loss:[21.3515567779541, 16.851152539253235]
policies:[0, 4, 0]
qAverage:[0.0, 61.44355392456055]
ws:[-0.5021710991859436, 7.1814849972724915]
memory len:10000
memory used:3146.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20*		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.11309552632758457, the reward is 194.33333333333334 with loss [19.395280361175537, 24.482916355133057] in episode 1669
Report: 
rewardSum:194.33333333333334
loss:[19.395280361175537, 24.482916355133057]
policies:[0, 4, 0]
qAverage:[0.0, 66.86456604003907]
ws:[-0.3545131877064705, 6.888531160354614]
memory len:10000
memory used:3146.0
now epsilon is 0.11298247320501136, the reward is 194.33333333333334 with loss [17.02388072013855, 27.88195490837097] in episode 1670
Report: 
rewardSum:194.33333333333334
loss:[17.02388072013855, 27.88195490837097]
policies:[0, 4, 0]
qAverage:[0.0, 66.50749359130859]
ws:[-0.6179707273840904, 5.536744832992554]
memory len:10000
memory used:3145.0
now epsilon is 0.11286953309317285, the reward is 194.33333333333334 with loss [18.640837907791138, 18.651702880859375] in episode 1671
Report: 
rewardSum:194.33333333333334
loss:[18.640837907791138, 18.651702880859375]
policies:[0, 4, 0]
qAverage:[0.0, 66.56581115722656]
ws:[-0.2634973406791687, 7.029904460906982]
memory len:10000
memory used:3118.0
now epsilon is 0.11275670587910071, the reward is 194.33333333333334 with loss [20.08944320678711, 15.617721796035767] in episode 1672
Report: 
rewardSum:194.33333333333334
loss:[20.08944320678711, 15.617721796035767]
policies:[0, 4, 0]
qAverage:[0.0, 66.7205795288086]
ws:[-1.042413729429245, 5.277885150909424]
memory len:10000
memory used:3124.0
now epsilon is 0.11258767649446397, the reward is 192.33333333333334 with loss [39.995381355285645, 33.55216646194458] in episode 1673
Report: 
rewardSum:192.33333333333334
loss:[39.995381355285645, 33.55216646194458]
policies:[2, 3, 1]
qAverage:[0.0, 59.7760066986084]
ws:[-0.2617553770542145, 4.327967792749405]
memory len:10000
memory used:3136.0
now epsilon is 0.1124751310313119, the reward is 194.33333333333334 with loss [18.343117713928223, 19.76241159439087] in episode 1674
Report: 
rewardSum:194.33333333333334
loss:[18.343117713928223, 19.76241159439087]
policies:[0, 4, 0]
qAverage:[0.0, 66.48577423095703]
ws:[-1.541028654575348, 4.544488048553466]
memory len:10000
memory used:3137.0
now epsilon is 0.11236269807142549, the reward is 194.33333333333334 with loss [23.613496780395508, 24.702863216400146] in episode 1675
Report: 
rewardSum:194.33333333333334
loss:[23.613496780395508, 24.702863216400146]
policies:[0, 3, 1]
qAverage:[0.0, 65.32144355773926]
ws:[-0.15115534514188766, 7.836023926734924]
memory len:10000
memory used:3137.0
now epsilon is 0.11225037750234362, the reward is 194.33333333333334 with loss [18.594377040863037, 17.871867418289185] in episode 1676
Report: 
rewardSum:194.33333333333334
loss:[18.594377040863037, 17.871867418289185]
policies:[0, 4, 0]
qAverage:[0.0, 66.06611328125]
ws:[-0.2249755859375, 6.9005210399627686]
memory len:10000
memory used:3137.0
now epsilon is 0.11213816921171765, the reward is 194.33333333333334 with loss [19.50828742980957, 29.624494314193726] in episode 1677
Report: 
rewardSum:194.33333333333334
loss:[19.50828742980957, 29.624494314193726]
policies:[0, 4, 0]
qAverage:[0.0, 67.29795532226562]
ws:[-0.047281515598297116, 6.910229110717774]
memory len:10000
memory used:3139.0
now epsilon is 0.1120260730873112, the reward is 194.33333333333334 with loss [22.945485591888428, 19.728925466537476] in episode 1678
Report: 
rewardSum:194.33333333333334
loss:[22.945485591888428, 19.728925466537476]
policies:[1, 3, 0]
qAverage:[0.0, 60.168718338012695]
ws:[-0.45848478376865387, 6.649809300899506]
memory len:10000
memory used:3139.0
now epsilon is 0.11191408901700012, the reward is 194.33333333333334 with loss [31.516890048980713, 22.934659481048584] in episode 1679
Report: 
rewardSum:194.33333333333334
loss:[31.516890048980713, 22.934659481048584]
policies:[0, 3, 1]
qAverage:[0.0, 60.18400955200195]
ws:[-0.8793419599533081, 5.374689996242523]
memory len:10000
memory used:3139.0
now epsilon is 0.1118022168887723, the reward is 194.33333333333334 with loss [20.387270092964172, 25.22285544872284] in episode 1680
Report: 
rewardSum:194.33333333333334
loss:[20.387270092964172, 25.22285544872284]
policies:[0, 3, 1]
qAverage:[0.0, 62.593475341796875]
ws:[-0.8292218074202538, 5.676695883274078]
memory len:10000
memory used:3139.0
now epsilon is 0.11169045659072767, the reward is 194.33333333333334 with loss [25.973591804504395, 20.596370697021484] in episode 1681
Report: 
rewardSum:194.33333333333334
loss:[25.973591804504395, 20.596370697021484]
policies:[0, 2, 2]
qAverage:[0.0, 54.07193501790365]
ws:[-1.6003840565681458, 4.847994804382324]
memory len:10000
memory used:3139.0
now epsilon is 0.11157880801107797, the reward is 194.33333333333334 with loss [20.152995109558105, 21.28729486465454] in episode 1682
Report: 
rewardSum:194.33333333333334
loss:[20.152995109558105, 21.28729486465454]
policies:[0, 4, 0]
qAverage:[0.0, 65.810546875]
ws:[-1.7185258269309998, 4.425232672691346]
memory len:10000
memory used:3139.0
now epsilon is 0.11146727103814667, the reward is 194.33333333333334 with loss [18.77864408493042, 27.097294330596924] in episode 1683
Report: 
rewardSum:194.33333333333334
loss:[18.77864408493042, 27.097294330596924]
policies:[2, 2, 0]
qAverage:[0.0, 54.92879740397135]
ws:[-2.153777758280436, 4.055550654729207]
memory len:10000
memory used:3139.0
now epsilon is 0.11135584556036891, the reward is 194.33333333333334 with loss [19.850796699523926, 25.4572491645813] in episode 1684
Report: 
rewardSum:194.33333333333334
loss:[19.850796699523926, 25.4572491645813]
policies:[1, 3, 0]
qAverage:[0.0, 54.32380167643229]
ws:[0.3052221971253554, 6.395767052968343]
memory len:10000
memory used:3139.0
now epsilon is 0.11124453146629133, the reward is 194.33333333333334 with loss [20.7156023979187, 19.226555109024048] in episode 1685
Report: 
rewardSum:194.33333333333334
loss:[20.7156023979187, 19.226555109024048]
policies:[1, 3, 0]
qAverage:[0.0, 64.1599235534668]
ws:[-0.7168941497802734, 7.351519227027893]
memory len:10000
memory used:3139.0
now epsilon is 0.11113332864457201, the reward is 194.33333333333334 with loss [31.17426347732544, 20.834675788879395] in episode 1686
Report: 
rewardSum:194.33333333333334
loss:[31.17426347732544, 20.834675788879395]
policies:[1, 1, 2]
qAverage:[0.0, 44.42790603637695]
ws:[-0.864736795425415, 9.829800605773926]
memory len:10000
memory used:3139.0
now epsilon is 0.1110222369839803, the reward is 194.33333333333334 with loss [23.05722188949585, 16.32655382156372] in episode 1687
Report: 
rewardSum:194.33333333333334
loss:[23.05722188949585, 16.32655382156372]
policies:[0, 3, 1]
qAverage:[0.0, 59.7580451965332]
ws:[0.29600269719958305, 4.1163212060928345]
memory len:10000
memory used:3139.0
now epsilon is 0.11091125637339674, the reward is 194.33333333333334 with loss [23.804049730300903, 21.387674808502197] in episode 1688
Report: 
rewardSum:194.33333333333334
loss:[23.804049730300903, 21.387674808502197]
policies:[0, 2, 2]
qAverage:[0.0, 57.867835998535156]
ws:[-0.3804064989089966, 8.068994045257568]
memory len:10000
memory used:3139.0
now epsilon is 0.11080038670181297, the reward is 194.33333333333334 with loss [19.707322597503662, 23.376925945281982] in episode 1689
Report: 
rewardSum:194.33333333333334
loss:[19.707322597503662, 23.376925945281982]
policies:[0, 4, 0]
qAverage:[0.0, 65.8250228881836]
ws:[-1.04958575963974, 5.376517236232758]
memory len:10000
memory used:3139.0
now epsilon is 0.11068962785833159, the reward is 194.33333333333334 with loss [20.46136713027954, 20.068716526031494] in episode 1690
Report: 
rewardSum:194.33333333333334
loss:[20.46136713027954, 20.068716526031494]
policies:[0, 4, 0]
qAverage:[0.0, 63.439903259277344]
ws:[-1.2832098662853242, 4.864566314220428]
memory len:10000
memory used:3139.0
now epsilon is 0.11057897973216604, the reward is 194.33333333333334 with loss [21.536068439483643, 20.891533851623535] in episode 1691
Report: 
rewardSum:194.33333333333334
loss:[21.536068439483643, 20.891533851623535]
policies:[0, 4, 0]
qAverage:[0.0, 61.82846374511719]
ws:[-0.2514374852180481, 7.128982903063298]
memory len:10000
memory used:3139.0
now epsilon is 0.11046844221264053, the reward is 194.33333333333334 with loss [35.173439025878906, 20.60428810119629] in episode 1692
Report: 
rewardSum:194.33333333333334
loss:[35.173439025878906, 20.60428810119629]
policies:[0, 4, 0]
qAverage:[0.0, 63.44156951904297]
ws:[-0.4735923171043396, 7.34023577272892]
memory len:10000
memory used:3139.0
now epsilon is 0.11030284307897124, the reward is 192.33333333333334 with loss [37.862549781799316, 22.084230661392212] in episode 1693
Report: 
rewardSum:192.33333333333334
loss:[37.862549781799316, 22.084230661392212]
policies:[1, 4, 1]
qAverage:[0.0, 62.97559356689453]
ws:[-0.987999114394188, 7.267041468620301]
memory len:10000
memory used:3139.0
now epsilon is 0.11019258159256495, the reward is 194.33333333333334 with loss [20.77016305923462, 23.568620204925537] in episode 1694
Report: 
rewardSum:194.33333333333334
loss:[20.77016305923462, 23.568620204925537]
policies:[0, 3, 1]
qAverage:[0.0, 58.01539993286133]
ws:[-1.8864309191703796, 4.864804327487946]
memory len:10000
memory used:3139.0
now epsilon is 0.1100824303263039, the reward is 194.33333333333334 with loss [29.69401788711548, 28.47969079017639] in episode 1695
Report: 
rewardSum:194.33333333333334
loss:[29.69401788711548, 28.47969079017639]
policies:[0, 3, 1]
qAverage:[0.0, 56.70031929016113]
ws:[-1.0827992260456085, 2.712043195962906]
memory len:10000
memory used:3139.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23*		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.10997238917000925, the reward is 194.33333333333334 with loss [25.43969440460205, 30.370285034179688] in episode 1696
Report: 
rewardSum:194.33333333333334
loss:[25.43969440460205, 30.370285034179688]
policies:[0, 4, 0]
qAverage:[0.0, 60.73860740661621]
ws:[-2.065843340009451, 5.600778341293335]
memory len:10000
memory used:3140.0
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.10986245801361236, the reward is 194.33333333333334 with loss [22.265451192855835, 20.43740177154541] in episode 1697
Report: 
rewardSum:194.33333333333334
loss:[22.265451192855835, 20.43740177154541]
policies:[0, 3, 1]
qAverage:[0.0, 56.542877197265625]
ws:[-2.6882012486457825, 1.3035565912723541]
memory len:10000
memory used:3139.0
now epsilon is 0.10975263674715455, the reward is 194.33333333333334 with loss [20.78460121154785, 21.659109592437744] in episode 1698
Report: 
rewardSum:194.33333333333334
loss:[20.78460121154785, 21.659109592437744]
policies:[0, 3, 1]
qAverage:[0.0, 59.5010871887207]
ws:[-2.675871342420578, 4.121705781668425]
memory len:10000
memory used:3140.0
now epsilon is 0.10964292526078709, the reward is 194.33333333333334 with loss [18.94350790977478, 23.263042449951172] in episode 1699
Report: 
rewardSum:194.33333333333334
loss:[18.94350790977478, 23.263042449951172]
policies:[0, 4, 0]
qAverage:[0.0, 62.44341468811035]
ws:[-1.6180462390184402, 5.7085089683532715]
memory len:10000
memory used:3139.0
now epsilon is 0.10953332344477103, the reward is 194.33333333333334 with loss [22.896786212921143, 19.235031127929688] in episode 1700
Report: 
rewardSum:194.33333333333334
loss:[22.896786212921143, 19.235031127929688]
policies:[0, 4, 0]
qAverage:[0.0, 62.81640625]
ws:[-0.798063313961029, 6.875232553482055]
memory len:10000
memory used:3139.0
now epsilon is 0.1093964752316798, the reward is 193.33333333333334 with loss [31.904601573944092, 38.98253536224365] in episode 1701
Report: 
rewardSum:193.33333333333334
loss:[31.904601573944092, 38.98253536224365]
policies:[0, 4, 1]
qAverage:[0.0, 63.84042663574219]
ws:[-2.1611414551734924, 4.295369005203247]
memory len:10000
memory used:3140.0
now epsilon is 0.10925979799334617, the reward is 193.33333333333334 with loss [22.114803552627563, 20.2524311542511] in episode 1702
Report: 
rewardSum:193.33333333333334
loss:[22.114803552627563, 20.2524311542511]
policies:[0, 4, 1]
qAverage:[0.0, 57.13606262207031]
ws:[-0.4808054566383362, 8.66684541106224]
memory len:10000
memory used:3140.0
now epsilon is 0.10915057916094878, the reward is 194.33333333333334 with loss [16.415466248989105, 22.60584306716919] in episode 1703
Report: 
rewardSum:194.33333333333334
loss:[16.415466248989105, 22.60584306716919]
policies:[0, 4, 0]
qAverage:[0.0, 62.77397918701172]
ws:[-0.8718977481126785, 5.864786005020141]
memory len:10000
memory used:3139.0
now epsilon is 0.10904146950643355, the reward is 194.33333333333334 with loss [16.871790885925293, 20.84166145324707] in episode 1704
Report: 
rewardSum:194.33333333333334
loss:[16.871790885925293, 20.84166145324707]
policies:[0, 4, 0]
qAverage:[0.0, 61.56615142822265]
ws:[-0.769180080294609, 6.576051568984985]
memory len:10000
memory used:3139.0
now epsilon is 0.10893246892066352, the reward is 194.33333333333334 with loss [34.643144607543945, 19.222389698028564] in episode 1705
Report: 
rewardSum:194.33333333333334
loss:[34.643144607543945, 19.222389698028564]
policies:[0, 4, 0]
qAverage:[0.0, 62.494371032714845]
ws:[-0.6304510645568371, 7.24660906791687]
memory len:10000
memory used:3140.0
now epsilon is 0.10882357729461085, the reward is 194.33333333333334 with loss [33.15594720840454, 21.054265022277832] in episode 1706
Report: 
rewardSum:194.33333333333334
loss:[33.15594720840454, 21.054265022277832]
policies:[0, 4, 0]
qAverage:[0.0, 61.72996063232422]
ws:[-1.3836367070674895, 6.605300712585449]
memory len:10000
memory used:3139.0
now epsilon is 0.1087147945193567, the reward is 194.33333333333334 with loss [30.376957893371582, 25.560492038726807] in episode 1707
Report: 
rewardSum:194.33333333333334
loss:[30.376957893371582, 25.560492038726807]
policies:[0, 4, 0]
qAverage:[0.0, 62.830482482910156]
ws:[-2.1649285435676573, 5.8312010049819945]
memory len:10000
memory used:3139.0
now epsilon is 0.10855182421373054, the reward is 192.33333333333334 with loss [28.203490257263184, 35.89209747314453] in episode 1708
Report: 
rewardSum:192.33333333333334
loss:[28.203490257263184, 35.89209747314453]
policies:[0, 5, 1]
qAverage:[0.0, 62.67348607381185]
ws:[-1.958101083834966, 4.672224670648575]
memory len:10000
memory used:3139.0
now epsilon is 0.10844331308966684, the reward is 194.33333333333334 with loss [28.833520889282227, 25.8660945892334] in episode 1709
Report: 
rewardSum:194.33333333333334
loss:[28.833520889282227, 25.8660945892334]
policies:[0, 3, 1]
qAverage:[0.0, 60.91079139709473]
ws:[-0.6410038098692894, 9.845833778381348]
memory len:10000
memory used:3140.0
now epsilon is 0.1083349104360423, the reward is 194.33333333333334 with loss [19.116066932678223, 24.885382652282715] in episode 1710
Report: 
rewardSum:194.33333333333334
loss:[19.116066932678223, 24.885382652282715]
policies:[1, 3, 0]
qAverage:[0.0, 55.14013862609863]
ws:[-1.350387454032898, 2.458002597093582]
memory len:10000
memory used:3140.0
now epsilon is 0.1081725096005185, the reward is 192.33333333333334 with loss [18.548351369798183, 27.182058930397034] in episode 1711
Report: 
rewardSum:192.33333333333334
loss:[18.548351369798183, 27.182058930397034]
policies:[0, 5, 1]
qAverage:[0.0, 62.927632649739586]
ws:[-2.824471910794576, 2.543738439679146]
memory len:10000
memory used:3140.0
now epsilon is 0.10806437764884874, the reward is 194.33333333333334 with loss [21.38135576248169, 23.933828830718994] in episode 1712
Report: 
rewardSum:194.33333333333334
loss:[21.38135576248169, 23.933828830718994]
policies:[0, 4, 0]
qAverage:[0.0, 62.526397705078125]
ws:[-2.19567414522171, 5.091072964668274]
memory len:10000
memory used:3140.0
now epsilon is 0.10795635378858794, the reward is 194.33333333333334 with loss [20.391932010650635, 22.932117462158203] in episode 1713
Report: 
rewardSum:194.33333333333334
loss:[20.391932010650635, 22.932117462158203]
policies:[0, 3, 1]
qAverage:[0.0, 59.74254608154297]
ws:[-1.3441722095012665, 7.4372042417526245]
memory len:10000
memory used:3139.0
now epsilon is 0.10782147580220726, the reward is 193.33333333333334 with loss [23.800418853759766, 20.730925917625427] in episode 1714
Report: 
rewardSum:193.33333333333334
loss:[23.800418853759766, 20.730925917625427]
policies:[1, 3, 1]
qAverage:[0.0, 59.90820121765137]
ws:[0.677602231502533, 11.44001317024231]
memory len:10000
memory used:3139.0
now epsilon is 0.10763292967629028, the reward is 191.33333333333334 with loss [40.104243516922, 37.97561550140381] in episode 1715
Report: 
rewardSum:191.33333333333334
loss:[40.104243516922, 37.97561550140381]
policies:[0, 5, 2]
qAverage:[0.0, 62.6822624206543]
ws:[-0.8169878249367079, 5.951812227567037]
memory len:10000
memory used:3139.0
now epsilon is 0.10752533710223598, the reward is 194.33333333333334 with loss [23.539682388305664, 17.857351541519165] in episode 1716
Report: 
rewardSum:194.33333333333334
loss:[23.539682388305664, 17.857351541519165]
policies:[1, 3, 0]
qAverage:[0.0, 56.06249237060547]
ws:[-0.8482847632840276, 8.012919068336487]
memory len:10000
memory used:3139.0
now epsilon is 0.10741785208041525, the reward is 194.33333333333334 with loss [18.59941816329956, 24.999305248260498] in episode 1717
Report: 
rewardSum:194.33333333333334
loss:[18.59941816329956, 24.999305248260498]
policies:[0, 4, 0]
qAverage:[0.0, 61.231797790527345]
ws:[-1.5789236545562744, 6.7337552309036255]
memory len:10000
memory used:3139.0
now epsilon is 0.10731047450331618, the reward is 194.33333333333334 with loss [21.83773708343506, 18.895536184310913] in episode 1718
Report: 
rewardSum:194.33333333333334
loss:[21.83773708343506, 18.895536184310913]
policies:[0, 4, 0]
qAverage:[0.0, 60.560813903808594]
ws:[-1.9652474999427796, 6.464167034626007]
memory len:10000
memory used:3140.0
now epsilon is 0.10720320426353433, the reward is 194.33333333333334 with loss [18.61848211288452, 22.687572956085205] in episode 1719
Report: 
rewardSum:194.33333333333334
loss:[18.61848211288452, 22.687572956085205]
policies:[0, 4, 0]
qAverage:[0.0, 61.46917724609375]
ws:[-2.289690560102463, 6.43576140999794]
memory len:10000
memory used:3140.0
now epsilon is 0.10709604125377263, the reward is 194.33333333333334 with loss [31.177547454833984, 29.06972074508667] in episode 1720
Report: 
rewardSum:194.33333333333334
loss:[31.177547454833984, 29.06972074508667]
policies:[0, 3, 1]
qAverage:[0.0, 57.82575607299805]
ws:[-3.1938223838806152, 4.165500111877918]
memory len:10000
memory used:3139.0
now epsilon is 0.10698898536684125, the reward is 194.33333333333334 with loss [26.453173637390137, 23.860605239868164] in episode 1721
Report: 
rewardSum:194.33333333333334
loss:[26.453173637390137, 23.860605239868164]
policies:[0, 4, 0]
qAverage:[0.0, 61.308197021484375]
ws:[-2.7670066177845003, 5.441013884544373]
memory len:10000
memory used:3140.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21*		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.10688203649565753, the reward is 194.33333333333334 with loss [27.295241832733154, 20.283015251159668] in episode 1722
Report: 
rewardSum:194.33333333333334
loss:[27.295241832733154, 20.283015251159668]
policies:[1, 3, 0]
qAverage:[0.0, 56.498050689697266]
ws:[-3.123836873099208, 5.32883457839489]
memory len:10000
memory used:3140.0
now epsilon is 0.10677519453324584, the reward is 194.33333333333334 with loss [18.65067684650421, 20.630016803741455] in episode 1723
Report: 
rewardSum:194.33333333333334
loss:[18.65067684650421, 20.630016803741455]
policies:[0, 4, 0]
qAverage:[0.0, 61.031822204589844]
ws:[-2.7220794171094895, 4.801184284687042]
memory len:10000
memory used:3140.0
now epsilon is 0.10666845937273751, the reward is 194.33333333333334 with loss [21.653642416000366, 22.336180210113525] in episode 1724
Report: 
rewardSum:194.33333333333334
loss:[21.653642416000366, 22.336180210113525]
policies:[1, 3, 0]
qAverage:[0.0, 60.9906120300293]
ws:[-2.1461852472275496, 7.381220817565918]
memory len:10000
memory used:3140.0
now epsilon is 0.10656183090737069, the reward is 194.33333333333334 with loss [25.215512990951538, 19.977545022964478] in episode 1725
Report: 
rewardSum:194.33333333333334
loss:[25.215512990951538, 19.977545022964478]
policies:[0, 4, 0]
qAverage:[0.0, 62.05337371826172]
ws:[-0.6183617353439331, 7.832399845123291]
memory len:10000
memory used:3140.0
now epsilon is 0.10645530903049023, the reward is 194.33333333333334 with loss [28.338875770568848, 22.08912467956543] in episode 1726
Report: 
rewardSum:194.33333333333334
loss:[28.338875770568848, 22.08912467956543]
policies:[0, 4, 0]
qAverage:[0.0, 62.014076232910156]
ws:[0.09025916457176208, 8.449131870269776]
memory len:10000
memory used:3140.0
now epsilon is 0.1063488936355476, the reward is 194.33333333333334 with loss [18.725632190704346, 24.817748069763184] in episode 1727
Report: 
rewardSum:194.33333333333334
loss:[18.725632190704346, 24.817748069763184]
policies:[1, 2, 1]
qAverage:[0.0, 54.82916514078776]
ws:[-1.5318556626637776, 9.791483561197916]
memory len:10000
memory used:3140.0
now epsilon is 0.10624258461610078, the reward is 194.33333333333334 with loss [23.957144737243652, 24.093745708465576] in episode 1728
Report: 
rewardSum:194.33333333333334
loss:[23.957144737243652, 24.093745708465576]
policies:[1, 3, 0]
qAverage:[0.0, 60.94789695739746]
ws:[-0.6761626601219177, 9.583428621292114]
memory len:10000
memory used:3139.0
now epsilon is 0.10613638186581417, the reward is 194.33333333333334 with loss [27.767301082611084, 23.149727821350098] in episode 1729
Report: 
rewardSum:194.33333333333334
loss:[27.767301082611084, 23.149727821350098]
policies:[0, 4, 0]
qAverage:[0.0, 62.240228271484376]
ws:[-0.9870064675807952, 7.020475769042969]
memory len:10000
memory used:3140.0
now epsilon is 0.10603028527845847, the reward is 194.33333333333334 with loss [17.65330743789673, 24.763598442077637] in episode 1730
Report: 
rewardSum:194.33333333333334
loss:[17.65330743789673, 24.763598442077637]
policies:[1, 2, 1]
qAverage:[0.0, 57.504191080729164]
ws:[-0.5485352675120035, 8.529639879862467]
memory len:10000
memory used:3140.0
now epsilon is 0.10592429474791051, the reward is 194.33333333333334 with loss [15.41767954826355, 20.724273204803467] in episode 1731
Report: 
rewardSum:194.33333333333334
loss:[15.41767954826355, 20.724273204803467]
policies:[0, 4, 0]
qAverage:[0.0, 62.35268707275391]
ws:[-0.17358307242393495, 7.552661800384522]
memory len:10000
memory used:3139.0
now epsilon is 0.1058184101681533, the reward is 194.33333333333334 with loss [15.5744868516922, 23.993343353271484] in episode 1732
Report: 
rewardSum:194.33333333333334
loss:[15.5744868516922, 23.993343353271484]
policies:[0, 2, 2]
qAverage:[0.0, 50.80706787109375]
ws:[1.2042837937672932, 4.790186723073323]
memory len:10000
memory used:3139.0
now epsilon is 0.10565978172459857, the reward is 192.33333333333334 with loss [38.40974688529968, 35.920634031295776] in episode 1733
Report: 
rewardSum:192.33333333333334
loss:[38.40974688529968, 35.920634031295776]
policies:[0, 4, 2]
qAverage:[0.0, 62.16024627685547]
ws:[0.462012779712677, 8.002865886688232]
memory len:10000
memory used:3151.0
now epsilon is 0.1055541615586888, the reward is 194.33333333333334 with loss [24.441696882247925, 19.932847023010254] in episode 1734
Report: 
rewardSum:194.33333333333334
loss:[24.441696882247925, 19.932847023010254]
policies:[0, 3, 1]
qAverage:[0.0, 61.86180114746094]
ws:[-1.776545912027359, 5.211810648441315]
memory len:10000
memory used:3151.0
now epsilon is 0.10544864697334398, the reward is 194.33333333333334 with loss [27.974571228027344, 23.621027946472168] in episode 1735
Report: 
rewardSum:194.33333333333334
loss:[27.974571228027344, 23.621027946472168]
policies:[0, 4, 0]
qAverage:[0.0, 61.60081481933594]
ws:[-1.1776115536689757, 5.057358208298683]
memory len:10000
memory used:3151.0
now epsilon is 0.10534323786302315, the reward is 194.33333333333334 with loss [27.945614337921143, 24.872833728790283] in episode 1736
Report: 
rewardSum:194.33333333333334
loss:[27.945614337921143, 24.872833728790283]
policies:[0, 4, 0]
qAverage:[0.0, 54.7463436126709]
ws:[0.48250423790887, 3.2824886441230774]
memory len:10000
memory used:3139.0
now epsilon is 0.1052379341222908, the reward is 194.33333333333334 with loss [31.37967348098755, 15.115484476089478] in episode 1737
Report: 
rewardSum:194.33333333333334
loss:[31.37967348098755, 15.115484476089478]
policies:[0, 4, 0]
qAverage:[0.0, 62.88739166259766]
ws:[0.5944027543067932, 8.0433584690094]
memory len:10000
memory used:3139.0
now epsilon is 0.10508017584878994, the reward is 192.33333333333334 with loss [29.069820880889893, 41.25819492340088] in episode 1738
Report: 
rewardSum:192.33333333333334
loss:[29.069820880889893, 41.25819492340088]
policies:[0, 5, 1]
qAverage:[0.0, 64.57837804158528]
ws:[-0.9524007712801298, 3.6180397868156433]
memory len:10000
memory used:3139.0
now epsilon is 0.10492265406485023, the reward is 192.33333333333334 with loss [32.70226514339447, 28.865004062652588] in episode 1739
Report: 
rewardSum:192.33333333333334
loss:[32.70226514339447, 28.865004062652588]
policies:[0, 4, 2]
qAverage:[0.0, 62.750675201416016]
ws:[-0.799943670630455, 7.275706887245178]
memory len:10000
memory used:3139.0
now epsilon is 0.1048177707502234, the reward is 194.33333333333334 with loss [29.326035022735596, 20.121431350708008] in episode 1740
Report: 
rewardSum:194.33333333333334
loss:[29.326035022735596, 20.121431350708008]
policies:[0, 4, 0]
qAverage:[0.0, 63.033973693847656]
ws:[0.2102745771408081, 7.946110343933105]
memory len:10000
memory used:3139.0
now epsilon is 0.10471299227958651, the reward is 194.33333333333334 with loss [16.616499662399292, 34.61862897872925] in episode 1741
Report: 
rewardSum:194.33333333333334
loss:[16.616499662399292, 34.61862897872925]
policies:[0, 4, 0]
qAverage:[0.0, 63.387725830078125]
ws:[0.011991739273071289, 6.742333126068115]
memory len:10000
memory used:3138.0
now epsilon is 0.10460831854813488, the reward is 194.33333333333334 with loss [25.80640983581543, 21.052549839019775] in episode 1742
Report: 
rewardSum:194.33333333333334
loss:[25.80640983581543, 21.052549839019775]
policies:[0, 4, 0]
qAverage:[0.0, 62.65092010498047]
ws:[0.3654069423675537, 8.16895513534546]
memory len:10000
memory used:3138.0
now epsilon is 0.10450374945116861, the reward is 194.33333333333334 with loss [21.526814937591553, 21.493937492370605] in episode 1743
Report: 
rewardSum:194.33333333333334
loss:[21.526814937591553, 21.493937492370605]
policies:[0, 3, 1]
qAverage:[0.0, 62.92802047729492]
ws:[-1.3506342321634293, 6.696290612220764]
memory len:10000
memory used:3139.0
now epsilon is 0.10439928488409242, the reward is 194.33333333333334 with loss [14.358565330505371, 23.76178216934204] in episode 1744
Report: 
rewardSum:194.33333333333334
loss:[14.358565330505371, 23.76178216934204]
policies:[1, 3, 0]
qAverage:[0.0, 62.54632377624512]
ws:[-1.943878747522831, 5.818629860877991]
memory len:10000
memory used:3140.0
now epsilon is 0.10429492474241563, the reward is 194.33333333333334 with loss [17.979619026184082, 26.280841827392578] in episode 1745
Report: 
rewardSum:194.33333333333334
loss:[17.979619026184082, 26.280841827392578]
policies:[0, 4, 0]
qAverage:[0.0, 63.217498779296875]
ws:[-2.0488165974617005, 4.638602331024595]
memory len:10000
memory used:3140.0
now epsilon is 0.10419066892175198, the reward is 194.33333333333334 with loss [25.207537412643433, 19.74292302131653] in episode 1746
Report: 
rewardSum:194.33333333333334
loss:[25.207537412643433, 19.74292302131653]
policies:[0, 4, 0]
qAverage:[0.0, 63.339299011230466]
ws:[-1.7937471687793731, 5.366604518890381]
memory len:10000
memory used:3140.0
now epsilon is 0.10406049568849011, the reward is 193.33333333333334 with loss [18.72272777557373, 29.52082061767578] in episode 1747
Report: 
rewardSum:193.33333333333334
loss:[18.72272777557373, 29.52082061767578]
policies:[0, 4, 1]
qAverage:[0.0, 62.69628753662109]
ws:[-1.7765146315097808, 6.112486457824707]
memory len:10000
memory used:3140.0
now epsilon is 0.10395647420898416, the reward is 194.33333333333334 with loss [27.16189479827881, 24.920358657836914] in episode 1748
Report: 
rewardSum:194.33333333333334
loss:[27.16189479827881, 24.920358657836914]
policies:[0, 3, 1]
qAverage:[0.0, 63.57241630554199]
ws:[-0.7077255547046661, 9.522407412528992]
memory len:10000
memory used:3140.0
############# STATE ###############
0-		9-		18*		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.10385255671195613, the reward is 194.33333333333334 with loss [23.61641502380371, 28.1259765625] in episode 1749
Report: 
rewardSum:194.33333333333334
loss:[23.61641502380371, 28.1259765625]
policies:[0, 4, 0]
qAverage:[0.0, 63.17349548339844]
ws:[-1.9813122972846031, 5.50020215511322]
memory len:10000
memory used:3140.0
now epsilon is 0.10364503324966387, the reward is 190.33333333333334 with loss [44.53118371963501, 29.575803756713867] in episode 1750
Report: 
rewardSum:190.33333333333334
loss:[44.53118371963501, 29.575803756713867]
policies:[0, 5, 3]
qAverage:[0.0, 65.54947408040364]
ws:[-1.7800369411706924, 7.0069712201754255]
memory len:10000
memory used:3140.0
now epsilon is 0.10354142707682427, the reward is 194.33333333333334 with loss [19.016154289245605, 30.642080307006836] in episode 1751
Report: 
rewardSum:194.33333333333334
loss:[19.016154289245605, 30.642080307006836]
policies:[1, 3, 0]
qAverage:[0.0, 62.87723350524902]
ws:[0.7139697074890137, 11.663493156433105]
memory len:10000
memory used:3140.0
now epsilon is 0.10338621197394629, the reward is 192.33333333333334 with loss [37.11238193511963, 29.902209997177124] in episode 1752
Report: 
rewardSum:192.33333333333334
loss:[37.11238193511963, 29.902209997177124]
policies:[0, 4, 2]
qAverage:[0.0, 65.75538330078125]
ws:[1.1289796471595763, 11.117842102050782]
memory len:10000
memory used:3140.0
now epsilon is 0.1032828645253406, the reward is 194.33333333333334 with loss [19.367854833602905, 23.044107913970947] in episode 1753
Report: 
rewardSum:194.33333333333334
loss:[19.367854833602905, 23.044107913970947]
policies:[1, 3, 0]
qAverage:[0.0, 55.10900688171387]
ws:[1.8313546478748322, 6.712974429130554]
memory len:10000
memory used:3140.0
now epsilon is 0.1031796203854347, the reward is 194.33333333333334 with loss [29.25196385383606, 27.16392946243286] in episode 1754
Report: 
rewardSum:194.33333333333334
loss:[29.25196385383606, 27.16392946243286]
policies:[0, 4, 0]
qAverage:[0.0, 64.4128662109375]
ws:[1.691953706741333, 11.144295120239258]
memory len:10000
memory used:3139.0
now epsilon is 0.1030764794509586, the reward is 46.33333333333334 with loss [17.643800020217896, 19.509127616882324] in episode 1755
Report: 
rewardSum:46.33333333333334
loss:[17.643800020217896, 19.509127616882324]
policies:[0, 3, 1]
qAverage:[0.0, 55.3286018371582]
ws:[1.6638007014989853, 6.87336391210556]
memory len:10000
memory used:3139.0
now epsilon is 0.10297344161874557, the reward is 194.33333333333334 with loss [20.404265880584717, 22.50217366218567] in episode 1756
Report: 
rewardSum:194.33333333333334
loss:[20.404265880584717, 22.50217366218567]
policies:[0, 3, 1]
qAverage:[0.0, 56.659921646118164]
ws:[0.33328912034630775, 5.3347769640386105]
memory len:10000
memory used:3139.0
now epsilon is 0.10287050678573201, the reward is 194.33333333333334 with loss [27.037955284118652, 19.776664972305298] in episode 1757
Report: 
rewardSum:194.33333333333334
loss:[27.037955284118652, 19.776664972305298]
policies:[1, 3, 0]
qAverage:[0.0, 60.72796440124512]
ws:[-2.0711823664605618, 5.009525630623102]
memory len:10000
memory used:3139.0
now epsilon is 0.10276767484895734, the reward is 194.33333333333334 with loss [13.225486159324646, 19.8671658039093] in episode 1758
Report: 
rewardSum:194.33333333333334
loss:[13.225486159324646, 19.8671658039093]
policies:[0, 4, 0]
qAverage:[0.0, 64.10887451171875]
ws:[-0.9604738593101502, 8.124471759796142]
memory len:10000
memory used:3139.0
now epsilon is 0.10266494570556386, the reward is 194.33333333333334 with loss [26.433908462524414, 22.081230401992798] in episode 1759
Report: 
rewardSum:194.33333333333334
loss:[26.433908462524414, 22.081230401992798]
policies:[1, 3, 0]
qAverage:[0.0, 61.125946044921875]
ws:[-1.0057999193668365, 7.097285866737366]
memory len:10000
memory used:3139.0
now epsilon is 0.1025366786729836, the reward is 193.33333333333334 with loss [32.139415979385376, 26.102606296539307] in episode 1760
Report: 
rewardSum:193.33333333333334
loss:[32.139415979385376, 26.102606296539307]
policies:[0, 4, 1]
qAverage:[0.0, 63.89908447265625]
ws:[0.12391969561576843, 9.275406837463379]
memory len:10000
memory used:3139.0
now epsilon is 0.10243418043915699, the reward is 194.33333333333334 with loss [38.09300518035889, 25.41266107559204] in episode 1761
Report: 
rewardSum:194.33333333333334
loss:[38.09300518035889, 25.41266107559204]
policies:[0, 4, 0]
qAverage:[0.0, 64.48961486816407]
ws:[-0.7328762441873551, 7.858987474441529]
memory len:10000
memory used:3140.0
now epsilon is 0.10233178466513378, the reward is 194.33333333333334 with loss [17.88544249534607, 22.77398920059204] in episode 1762
Report: 
rewardSum:194.33333333333334
loss:[17.88544249534607, 22.77398920059204]
policies:[0, 4, 0]
qAverage:[0.0, 63.701953125]
ws:[-1.1028879165649415, 8.136058616638184]
memory len:10000
memory used:3140.0
now epsilon is 0.10222949124849257, the reward is 194.33333333333334 with loss [22.853615522384644, 17.766589641571045] in episode 1763
Report: 
rewardSum:194.33333333333334
loss:[22.853615522384644, 17.766589641571045]
policies:[0, 3, 1]
qAverage:[0.0, 61.90432167053223]
ws:[-1.393152430653572, 7.068964123725891]
memory len:10000
memory used:3139.0
now epsilon is 0.10212730008691435, the reward is 194.33333333333334 with loss [23.802278995513916, 23.676711082458496] in episode 1764
Report: 
rewardSum:194.33333333333334
loss:[23.802278995513916, 23.676711082458496]
policies:[0, 4, 0]
qAverage:[0.0, 64.00314483642578]
ws:[-1.5165129005908966, 7.053477573394775]
memory len:10000
memory used:3139.0
now epsilon is 0.10202521107818242, the reward is 194.33333333333334 with loss [24.784464597702026, 30.860589504241943] in episode 1765
Report: 
rewardSum:194.33333333333334
loss:[24.784464597702026, 30.860589504241943]
policies:[0, 4, 0]
qAverage:[0.0, 59.44915580749512]
ws:[-1.5460435450077057, 9.259099662303925]
memory len:10000
memory used:3139.0
now epsilon is 0.10192322412018223, the reward is 46.33333333333334 with loss [11.492597579956055, 17.823134183883667] in episode 1766
Report: 
rewardSum:46.33333333333334
loss:[11.492597579956055, 17.823134183883667]
policies:[0, 3, 1]
qAverage:[0.0, 56.03626251220703]
ws:[0.3616649955511093, 7.036208868026733]
memory len:10000
memory used:3139.0
now epsilon is 0.1018213391109013, the reward is 194.33333333333334 with loss [17.171313047409058, 30.050463676452637] in episode 1767
Report: 
rewardSum:194.33333333333334
loss:[17.171313047409058, 30.050463676452637]
policies:[0, 3, 1]
qAverage:[0.0, 63.88126564025879]
ws:[-0.5988643169403076, 10.792179942131042]
memory len:10000
memory used:3139.0
now epsilon is 0.10164328535229519, the reward is 191.33333333333334 with loss [45.756948947906494, 34.06273102760315] in episode 1768
Report: 
rewardSum:191.33333333333334
loss:[45.756948947906494, 34.06273102760315]
policies:[0, 5, 2]
qAverage:[0.0, 64.23940404256184]
ws:[-0.5414182270566622, 7.00715430577596]
memory len:10000
memory used:3139.0
now epsilon is 0.10151629475677841, the reward is 193.33333333333334 with loss [25.09397268295288, 30.48829174041748] in episode 1769
Report: 
rewardSum:193.33333333333334
loss:[25.09397268295288, 30.48829174041748]
policies:[0, 4, 1]
qAverage:[0.0, 64.34208679199219]
ws:[1.6134339809417724, 12.940225219726562]
memory len:10000
memory used:3139.0
now epsilon is 0.10141481652428781, the reward is 194.33333333333334 with loss [21.21359920501709, 24.491951942443848] in episode 1770
Report: 
rewardSum:194.33333333333334
loss:[21.21359920501709, 24.491951942443848]
policies:[1, 3, 0]
qAverage:[0.0, 59.845136642456055]
ws:[-0.8008849918842316, 8.211585998535156]
memory len:10000
memory used:3139.0
now epsilon is 0.10131343973198172, the reward is 194.33333333333334 with loss [20.56642746925354, 22.486586332321167] in episode 1771
Report: 
rewardSum:194.33333333333334
loss:[20.56642746925354, 22.486586332321167]
policies:[0, 4, 0]
qAverage:[0.0, 64.83791656494141]
ws:[0.12637041807174682, 9.978556632995605]
memory len:10000
memory used:3139.0
now epsilon is 0.10116156452207899, the reward is 192.33333333333334 with loss [26.253826379776, 32.845181941986084] in episode 1772
Report: 
rewardSum:192.33333333333334
loss:[26.253826379776, 32.845181941986084]
policies:[0, 4, 2]
qAverage:[0.0, 55.30099678039551]
ws:[-0.13528946042060852, 3.625139892101288]
memory len:10000
memory used:3175.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20*		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.10100991698265557, the reward is 192.33333333333334 with loss [34.978665828704834, 31.76848602294922] in episode 1773
Report: 
rewardSum:192.33333333333334
loss:[34.978665828704834, 31.76848602294922]
policies:[0, 5, 1]
qAverage:[0.0, 58.67725524902344]
ws:[0.11681627631187438, 4.2032551765441895]
memory len:10000
memory used:3141.0
now epsilon is 0.10090894493807907, the reward is 194.33333333333334 with loss [19.96575927734375, 21.658278226852417] in episode 1774
Report: 
rewardSum:194.33333333333334
loss:[19.96575927734375, 21.658278226852417]
policies:[0, 4, 0]
qAverage:[0.0, 64.91294860839844]
ws:[1.181510829925537, 10.892317724227905]
memory len:10000
memory used:3141.0
now epsilon is 0.10080807382768894, the reward is 194.33333333333334 with loss [22.77632212638855, 19.06356579065323] in episode 1775
Report: 
rewardSum:194.33333333333334
loss:[22.77632212638855, 19.06356579065323]
policies:[0, 3, 1]
qAverage:[0.0, 65.0704517364502]
ws:[0.4911755621433258, 10.891717672348022]
memory len:10000
memory used:3141.0
now epsilon is 0.10070730355058885, the reward is 194.33333333333334 with loss [22.060200691223145, 28.82018804550171] in episode 1776
Report: 
rewardSum:194.33333333333334
loss:[22.060200691223145, 28.82018804550171]
policies:[0, 4, 0]
qAverage:[0.0, 64.86082611083984]
ws:[-1.7425024032592773, 3.6292305320501326]
memory len:10000
memory used:3141.0
now epsilon is 0.1006066340059833, the reward is 194.33333333333334 with loss [24.963576316833496, 24.021824836730957] in episode 1777
Report: 
rewardSum:194.33333333333334
loss:[24.963576316833496, 24.021824836730957]
policies:[2, 2, 0]
qAverage:[13.762590408325195, 45.993534088134766]
ws:[-2.8760749995708466, 2.2637895047664642]
memory len:10000
memory used:3142.0
now epsilon is 0.10050606509317755, the reward is 194.33333333333334 with loss [20.65500831604004, 26.802817344665527] in episode 1778
Report: 
rewardSum:194.33333333333334
loss:[20.65500831604004, 26.802817344665527]
policies:[1, 3, 0]
qAverage:[0.0, 61.76463317871094]
ws:[-1.7446000774701436, 6.0277055104573565]
memory len:10000
memory used:3141.0
now epsilon is 0.10040559671157757, the reward is 194.33333333333334 with loss [16.015214204788208, 23.675978422164917] in episode 1779
Report: 
rewardSum:194.33333333333334
loss:[16.015214204788208, 23.675978422164917]
policies:[0, 4, 0]
qAverage:[0.0, 57.20927619934082]
ws:[1.17840008251369, 5.991624355316162]
memory len:10000
memory used:3141.0
now epsilon is 0.10030522876068983, the reward is 194.33333333333334 with loss [17.093663275241852, 20.13628602027893] in episode 1780
Report: 
rewardSum:194.33333333333334
loss:[17.093663275241852, 20.13628602027893]
policies:[0, 4, 0]
qAverage:[0.0, 65.39894561767578]
ws:[0.43997693061828613, 9.084771251678466]
memory len:10000
memory used:3141.0
now epsilon is 0.10020496114012123, the reward is 194.33333333333334 with loss [28.551024913787842, 16.682469725608826] in episode 1781
Report: 
rewardSum:194.33333333333334
loss:[28.551024913787842, 16.682469725608826]
policies:[0, 4, 0]
qAverage:[0.0, 64.61003723144532]
ws:[0.2785316228866577, 8.556982135772705]
memory len:10000
memory used:3141.0
now epsilon is 0.10010479374957913, the reward is 194.33333333333334 with loss [23.848230600357056, 21.278679847717285] in episode 1782
Report: 
rewardSum:194.33333333333334
loss:[23.848230600357056, 21.278679847717285]
policies:[0, 3, 1]
qAverage:[0.0, 65.19076538085938]
ws:[0.0704958438873291, 9.614727258682251]
memory len:10000
memory used:3141.0
now epsilon is 0.10000472648887107, the reward is 194.33333333333334 with loss [26.07803201675415, 22.21070623397827] in episode 1783
Report: 
rewardSum:194.33333333333334
loss:[26.07803201675415, 22.21070623397827]
policies:[0, 4, 0]
qAverage:[0.0, 64.89271545410156]
ws:[0.3324976682662964, 9.352030897140503]
memory len:10000
memory used:3141.0
now epsilon is 0.09990475925790475, the reward is 194.33333333333334 with loss [20.03052282333374, 20.547757625579834] in episode 1784
Report: 
rewardSum:194.33333333333334
loss:[20.03052282333374, 20.547757625579834]
policies:[0, 4, 0]
qAverage:[0.0, 57.64877510070801]
ws:[0.8243076205253601, 5.6467732191085815]
memory len:10000
memory used:3141.0
now epsilon is 0.09980489195668792, the reward is 194.33333333333334 with loss [21.574716329574585, 28.66578722000122] in episode 1785
Report: 
rewardSum:194.33333333333334
loss:[21.574716329574585, 28.66578722000122]
policies:[0, 4, 0]
qAverage:[0.0, 65.24862670898438]
ws:[-0.20093834400177002, 8.417491483688355]
memory len:10000
memory used:3141.0
now epsilon is 0.09970512448532831, the reward is 194.33333333333334 with loss [20.628678798675537, 24.289283275604248] in episode 1786
Report: 
rewardSum:194.33333333333334
loss:[20.628678798675537, 24.289283275604248]
policies:[1, 3, 0]
qAverage:[0.0, 51.1073252360026]
ws:[0.5053598682085673, 3.169334967931112]
memory len:10000
memory used:3141.0
now epsilon is 0.0996054567440335, the reward is 194.33333333333334 with loss [13.744299292564392, 22.194628715515137] in episode 1787
Report: 
rewardSum:194.33333333333334
loss:[13.744299292564392, 22.194628715515137]
policies:[0, 4, 0]
qAverage:[0.0, 65.7356201171875]
ws:[0.3174685716629028, 9.44981427192688]
memory len:10000
memory used:3141.0
now epsilon is 0.0995058886331108, the reward is 194.33333333333334 with loss [14.45652961730957, 28.95690393447876] in episode 1788
Report: 
rewardSum:194.33333333333334
loss:[14.45652961730957, 28.95690393447876]
policies:[1, 3, 0]
qAverage:[0.0, 59.55508995056152]
ws:[0.034624502062797546, 10.17408013343811]
memory len:10000
memory used:3142.0
now epsilon is 0.0994064200529672, the reward is 194.33333333333334 with loss [25.489214420318604, 17.870328426361084] in episode 1789
Report: 
rewardSum:194.33333333333334
loss:[25.489214420318604, 17.870328426361084]
policies:[0, 4, 0]
qAverage:[0.0, 58.19949913024902]
ws:[1.4479453712701797, 6.931560397148132]
memory len:10000
memory used:3142.0
now epsilon is 0.09930705090410924, the reward is 194.33333333333334 with loss [23.332603454589844, 24.60219717025757] in episode 1790
Report: 
rewardSum:194.33333333333334
loss:[23.332603454589844, 24.60219717025757]
policies:[0, 4, 0]
qAverage:[0.0, 64.59100952148438]
ws:[-0.4241847097873688, 8.875388383865356]
memory len:10000
memory used:3141.0
now epsilon is 0.09920778108714293, the reward is 194.33333333333334 with loss [16.137812852859497, 22.32231903076172] in episode 1791
Report: 
rewardSum:194.33333333333334
loss:[16.137812852859497, 22.32231903076172]
policies:[0, 4, 0]
qAverage:[0.0, 65.20364074707031]
ws:[-0.6547030091285706, 7.921764826774597]
memory len:10000
memory used:3141.0
now epsilon is 0.09910861050277361, the reward is 194.33333333333334 with loss [31.14924192428589, 22.379817962646484] in episode 1792
Report: 
rewardSum:194.33333333333334
loss:[31.14924192428589, 22.379817962646484]
policies:[0, 3, 1]
qAverage:[0.0, 62.608036041259766]
ws:[-1.6629090309143066, 6.081758037209511]
memory len:10000
memory used:3142.0
now epsilon is 0.09896004047037618, the reward is 192.33333333333334 with loss [32.507580280303955, 36.255078077316284] in episode 1793
Report: 
rewardSum:192.33333333333334
loss:[32.507580280303955, 36.255078077316284]
policies:[0, 5, 1]
qAverage:[0.0, 67.10348892211914]
ws:[-1.079165627559026, 7.552976489067078]
memory len:10000
memory used:3142.0
now epsilon is 0.09886111753373637, the reward is 194.33333333333334 with loss [22.948639392852783, 17.288835763931274] in episode 1794
Report: 
rewardSum:194.33333333333334
loss:[22.948639392852783, 17.288835763931274]
policies:[0, 4, 0]
qAverage:[0.0, 65.64145965576172]
ws:[-0.9341095089912415, 8.595119762420655]
memory len:10000
memory used:3141.0
now epsilon is 0.0987622934829433, the reward is 194.33333333333334 with loss [20.575511693954468, 20.472066640853882] in episode 1795
Report: 
rewardSum:194.33333333333334
loss:[20.575511693954468, 20.472066640853882]
policies:[0, 4, 0]
qAverage:[0.0, 66.30617828369141]
ws:[-1.5735275983810424, 7.200440168380737]
memory len:10000
memory used:3141.0
now epsilon is 0.09866356821914816, the reward is 194.33333333333334 with loss [17.243476152420044, 25.482291221618652] in episode 1796
Report: 
rewardSum:194.33333333333334
loss:[17.243476152420044, 25.482291221618652]
policies:[0, 4, 0]
qAverage:[0.0, 65.32903289794922]
ws:[-0.8158167004585266, 8.785822248458862]
memory len:10000
memory used:3141.0
now epsilon is 0.09856494164360102, the reward is 194.33333333333334 with loss [17.50406241416931, 19.03244650363922] in episode 1797
Report: 
rewardSum:194.33333333333334
loss:[17.50406241416931, 19.03244650363922]
policies:[0, 4, 0]
qAverage:[0.0, 66.46826629638672]
ws:[-1.2376104891300201, 7.486723852157593]
memory len:10000
memory used:3141.0
now epsilon is 0.09844179705423622, the reward is 193.33333333333334 with loss [27.454345226287842, 27.182157516479492] in episode 1798
Report: 
rewardSum:193.33333333333334
loss:[27.454345226287842, 27.182157516479492]
policies:[0, 4, 1]
qAverage:[0.0, 65.8858139038086]
ws:[0.0706849068403244, 9.688339042663575]
memory len:10000
memory used:3141.0
now epsilon is 0.09829422661708233, the reward is 192.33333333333334 with loss [28.592097878456116, 35.71685242652893] in episode 1799
Report: 
rewardSum:192.33333333333334
loss:[28.592097878456116, 35.71685242652893]
policies:[0, 5, 1]
qAverage:[0.0, 67.70797602335612]
ws:[-1.1382336417833965, 7.628060857454936]
memory len:10000
memory used:3141.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21*		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.09819596924465723, the reward is 194.33333333333334 with loss [31.298198223114014, 25.68410873413086] in episode 1800
Report: 
rewardSum:194.33333333333334
loss:[31.298198223114014, 25.68410873413086]
policies:[0, 4, 0]
qAverage:[0.0, 65.57257843017578]
ws:[-2.6933905601501467, 5.270968537032604]
memory len:10000
memory used:3141.0
now epsilon is 0.09809781009276419, the reward is 194.33333333333334 with loss [24.08116102218628, 27.29370355606079] in episode 1801
Report: 
rewardSum:194.33333333333334
loss:[24.08116102218628, 27.29370355606079]
policies:[1, 2, 1]
qAverage:[0.0, 62.26396942138672]
ws:[-2.1723941763242087, 7.4292012850443525]
memory len:10000
memory used:3141.0
now epsilon is 0.0979997490632195, the reward is 194.33333333333334 with loss [27.189115524291992, 17.70773983001709] in episode 1802
Report: 
rewardSum:194.33333333333334
loss:[27.189115524291992, 17.70773983001709]
policies:[0, 4, 0]
qAverage:[0.0, 65.3356704711914]
ws:[-0.7831406474113465, 7.767021265625954]
memory len:10000
memory used:3141.0
now epsilon is 0.09790178605793758, the reward is 194.33333333333334 with loss [19.67775058746338, 23.282687425613403] in episode 1803
Report: 
rewardSum:194.33333333333334
loss:[19.67775058746338, 23.282687425613403]
policies:[0, 3, 1]
qAverage:[0.0, 63.11735153198242]
ws:[-0.9844955056905746, 6.859625160694122]
memory len:10000
memory used:3141.0
now epsilon is 0.09780392097893094, the reward is 194.33333333333334 with loss [20.781046390533447, 25.462618827819824] in episode 1804
Report: 
rewardSum:194.33333333333334
loss:[20.781046390533447, 25.462618827819824]
policies:[0, 4, 0]
qAverage:[0.0, 65.05821228027344]
ws:[0.14478557109832763, 7.315144205093384]
memory len:10000
memory used:3141.0
now epsilon is 0.09770615372831003, the reward is 194.33333333333334 with loss [19.494733095169067, 25.84345817565918] in episode 1805
Report: 
rewardSum:194.33333333333334
loss:[19.494733095169067, 25.84345817565918]
policies:[0, 4, 0]
qAverage:[0.0, 65.85591888427734]
ws:[0.16400991678237914, 7.228135347366333]
memory len:10000
memory used:3141.0
now epsilon is 0.09755968606670926, the reward is 192.33333333333334 with loss [30.82797861099243, 28.997592210769653] in episode 1806
Report: 
rewardSum:192.33333333333334
loss:[30.82797861099243, 28.997592210769653]
policies:[1, 4, 1]
qAverage:[0.0, 62.23986663818359]
ws:[2.0757035493850706, 7.054649400711059]
memory len:10000
memory used:3141.0
now epsilon is 0.09743779741868788, the reward is 193.33333333333334 with loss [22.627483010292053, 24.951101779937744] in episode 1807
Report: 
rewardSum:193.33333333333334
loss:[22.627483010292053, 24.951101779937744]
policies:[1, 3, 1]
qAverage:[0.0, 57.70594787597656]
ws:[1.1195367090404034, 6.991901099681854]
memory len:10000
memory used:3141.0
now epsilon is 0.09734039615435376, the reward is 194.33333333333334 with loss [19.793840646743774, 18.6790189743042] in episode 1808
Report: 
rewardSum:194.33333333333334
loss:[19.793840646743774, 18.6790189743042]
policies:[0, 4, 0]
qAverage:[0.0, 66.16305847167969]
ws:[-1.1979470521211624, 7.823867750167847]
memory len:10000
memory used:3141.0
now epsilon is 0.09724309225476459, the reward is 194.33333333333334 with loss [27.40691089630127, 16.59621787071228] in episode 1809
Report: 
rewardSum:194.33333333333334
loss:[27.40691089630127, 16.59621787071228]
policies:[0, 3, 1]
qAverage:[0.0, 65.69405364990234]
ws:[-1.546422228217125, 9.741344094276428]
memory len:10000
memory used:3141.0
now epsilon is 0.09714588562259213, the reward is 194.33333333333334 with loss [17.1853609085083, 29.79014778137207] in episode 1810
Report: 
rewardSum:194.33333333333334
loss:[17.1853609085083, 29.79014778137207]
policies:[1, 3, 0]
qAverage:[0.0, 60.702959060668945]
ws:[-1.7986695766448975, 8.242946565151215]
memory len:10000
memory used:3141.0
now epsilon is 0.09704877616060542, the reward is 194.33333333333334 with loss [25.118499755859375, 20.279279232025146] in episode 1811
Report: 
rewardSum:194.33333333333334
loss:[25.118499755859375, 20.279279232025146]
policies:[1, 3, 0]
qAverage:[0.0, 66.36612701416016]
ws:[-1.276743572205305, 9.533194303512573]
memory len:10000
memory used:3141.0
now epsilon is 0.09695176377167074, the reward is 194.33333333333334 with loss [18.551045656204224, 15.357752084732056] in episode 1812
Report: 
rewardSum:194.33333333333334
loss:[18.551045656204224, 15.357752084732056]
policies:[0, 3, 1]
qAverage:[0.0, 57.88873100280762]
ws:[0.7151642739772797, 7.326085686683655]
memory len:10000
memory used:3141.0
now epsilon is 0.09685484835875138, the reward is 194.33333333333334 with loss [24.701179027557373, 22.5557382106781] in episode 1813
Report: 
rewardSum:194.33333333333334
loss:[24.701179027557373, 22.5557382106781]
policies:[0, 4, 0]
qAverage:[0.0, 66.5699966430664]
ws:[-1.3110314548015594, 7.715636539459228]
memory len:10000
memory used:3141.0
now epsilon is 0.09675802982490772, the reward is 194.33333333333334 with loss [24.358073711395264, 22.885190963745117] in episode 1814
Report: 
rewardSum:194.33333333333334
loss:[24.358073711395264, 22.885190963745117]
policies:[0, 4, 0]
qAverage:[0.0, 65.5096664428711]
ws:[-0.5487296789884567, 9.698226165771484]
memory len:10000
memory used:3141.0
now epsilon is 0.09666130807329701, the reward is 194.33333333333334 with loss [23.26548671722412, 25.080627918243408] in episode 1815
Report: 
rewardSum:194.33333333333334
loss:[23.26548671722412, 25.080627918243408]
policies:[0, 4, 0]
qAverage:[0.0, 58.95694541931152]
ws:[-0.33921004831790924, 4.304280459880829]
memory len:10000
memory used:3141.0
now epsilon is 0.09656468300717329, the reward is 194.33333333333334 with loss [24.641979932785034, 29.37810516357422] in episode 1816
Report: 
rewardSum:194.33333333333334
loss:[24.641979932785034, 29.37810516357422]
policies:[0, 3, 1]
qAverage:[0.0, 60.29405212402344]
ws:[-2.970914840698242, 5.775387614965439]
memory len:10000
memory used:3141.0
now epsilon is 0.09646815452988734, the reward is -3.0 with loss [28.803449630737305, 20.54923701286316] in episode 1817
Report: 
rewardSum:-3.0
loss:[28.803449630737305, 20.54923701286316]
policies:[1, 1, 2]
qAverage:[0.0, 39.447044372558594]
ws:[-1.2360273599624634, 1.4966286420822144]
memory len:10000
memory used:3141.0
now epsilon is 0.09637172254488655, the reward is 194.33333333333334 with loss [22.405760765075684, 20.584150075912476] in episode 1818
Report: 
rewardSum:194.33333333333334
loss:[22.405760765075684, 20.584150075912476]
policies:[1, 3, 0]
qAverage:[0.0, 60.348836263020836]
ws:[-3.440190076828003, 7.007144927978516]
memory len:10000
memory used:3141.0
now epsilon is 0.09627538695571476, the reward is 194.33333333333334 with loss [23.19355320930481, 26.329153776168823] in episode 1819
Report: 
rewardSum:194.33333333333334
loss:[23.19355320930481, 26.329153776168823]
policies:[0, 4, 0]
qAverage:[0.0, 65.58394165039063]
ws:[-1.7941069625318051, 7.8273831605911255]
memory len:10000
memory used:3141.0
now epsilon is 0.09617914766601234, the reward is 194.33333333333334 with loss [23.756489753723145, 36.90075349807739] in episode 1820
Report: 
rewardSum:194.33333333333334
loss:[23.756489753723145, 36.90075349807739]
policies:[1, 2, 1]
qAverage:[0.0, 54.84547424316406]
ws:[-0.17535299062728882, 4.320867538452148]
memory len:10000
memory used:3141.0
now epsilon is 0.09608300457951588, the reward is 194.33333333333334 with loss [26.939104557037354, 18.17754864692688] in episode 1821
Report: 
rewardSum:194.33333333333334
loss:[26.939104557037354, 18.17754864692688]
policies:[0, 3, 1]
qAverage:[0.0, 59.619712829589844]
ws:[-2.6170745889345803, 6.736011028289795]
memory len:10000
memory used:3141.0
now epsilon is 0.09598695760005828, the reward is 194.33333333333334 with loss [17.172231197357178, 20.132373809814453] in episode 1822
Report: 
rewardSum:194.33333333333334
loss:[17.172231197357178, 20.132373809814453]
policies:[0, 3, 1]
qAverage:[0.0, 61.312774658203125]
ws:[-3.6084412038326263, 3.8234879821538925]
memory len:10000
memory used:3140.0
now epsilon is 0.0958910066315685, the reward is 194.33333333333334 with loss [21.46925640106201, 16.821572065353394] in episode 1823
Report: 
rewardSum:194.33333333333334
loss:[21.46925640106201, 16.821572065353394]
policies:[0, 3, 1]
qAverage:[0.0, 64.32113456726074]
ws:[-2.7448373697698116, 6.79969596862793]
memory len:10000
memory used:3140.0
now epsilon is 0.09579515157807163, the reward is 194.33333333333334 with loss [17.682499885559082, 20.6340069770813] in episode 1824
Report: 
rewardSum:194.33333333333334
loss:[17.682499885559082, 20.6340069770813]
policies:[1, 3, 0]
qAverage:[0.0, 59.02421569824219]
ws:[-0.17856276035308838, 4.222306072711945]
memory len:10000
memory used:3141.0
now epsilon is 0.09565154862872878, the reward is 192.33333333333334 with loss [45.36516237258911, 24.421476364135742] in episode 1825
Report: 
rewardSum:192.33333333333334
loss:[45.36516237258911, 24.421476364135742]
policies:[0, 5, 1]
qAverage:[0.0, 66.85921986897786]
ws:[-0.9613151624798775, 7.0917560656865435]
memory len:10000
memory used:3141.0
now epsilon is 0.09555593294345295, the reward is 194.33333333333334 with loss [20.038825035095215, 20.795664072036743] in episode 1826
Report: 
rewardSum:194.33333333333334
loss:[20.038825035095215, 20.795664072036743]
policies:[0, 4, 0]
qAverage:[0.0, 65.73628387451171]
ws:[-3.6237646102905274, 4.7037365436553955]
memory len:10000
memory used:3140.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22*		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.09546041283801247, the reward is 194.33333333333334 with loss [23.805986881256104, 18.468659043312073] in episode 1827
Report: 
rewardSum:194.33333333333334
loss:[23.805986881256104, 18.468659043312073]
policies:[0, 4, 0]
qAverage:[0.0, 66.3067611694336]
ws:[-3.7706051111221313, 5.300231552124023]
memory len:10000
memory used:3141.0
now epsilon is 0.09536498821686339, the reward is 194.33333333333334 with loss [24.511959552764893, 20.767723083496094] in episode 1828
Report: 
rewardSum:194.33333333333334
loss:[24.511959552764893, 20.767723083496094]
policies:[0, 4, 0]
qAverage:[0.0, 66.61578826904297]
ws:[-3.47524778842926, 4.064682412147522]
memory len:10000
memory used:3141.0
now epsilon is 0.09526965898455718, the reward is 194.33333333333334 with loss [20.22929358482361, 31.80966854095459] in episode 1829
Report: 
rewardSum:194.33333333333334
loss:[20.22929358482361, 31.80966854095459]
policies:[0, 4, 0]
qAverage:[0.0, 66.79018859863281]
ws:[-1.505979299545288, 8.025601983070374]
memory len:10000
memory used:3141.0
now epsilon is 0.09517442504574078, the reward is 194.33333333333334 with loss [24.62619376182556, 19.194129467010498] in episode 1830
Report: 
rewardSum:194.33333333333334
loss:[24.62619376182556, 19.194129467010498]
policies:[0, 4, 0]
qAverage:[0.0, 67.17731475830078]
ws:[-1.3010609984397887, 6.761875915527344]
memory len:10000
memory used:3141.0
now epsilon is 0.09507928630515641, the reward is 194.33333333333334 with loss [19.633856296539307, 22.468288898468018] in episode 1831
Report: 
rewardSum:194.33333333333334
loss:[19.633856296539307, 22.468288898468018]
policies:[0, 4, 0]
qAverage:[0.0, 66.3732681274414]
ws:[-1.142504060268402, 6.972924661636353]
memory len:10000
memory used:3141.0
now epsilon is 0.09498424266764155, the reward is 194.33333333333334 with loss [23.554293632507324, 27.211337089538574] in episode 1832
Report: 
rewardSum:194.33333333333334
loss:[23.554293632507324, 27.211337089538574]
policies:[1, 3, 0]
qAverage:[0.0, 60.83805274963379]
ws:[-1.9690368371084332, 5.164228081703186]
memory len:10000
memory used:3141.0
now epsilon is 0.09488929403812878, the reward is 194.33333333333334 with loss [22.938969612121582, 24.015470266342163] in episode 1833
Report: 
rewardSum:194.33333333333334
loss:[22.938969612121582, 24.015470266342163]
policies:[0, 3, 1]
qAverage:[0.0, 65.83933067321777]
ws:[-1.9884368665516376, 6.436080574989319]
memory len:10000
memory used:3141.0
now epsilon is 0.09479444032164572, the reward is 46.33333333333334 with loss [17.909127235412598, 26.44052529335022] in episode 1834
Report: 
rewardSum:46.33333333333334
loss:[17.909127235412598, 26.44052529335022]
policies:[0, 3, 1]
qAverage:[0.0, 58.991960525512695]
ws:[-0.10100512206554413, 3.2040976881980896]
memory len:10000
memory used:3140.0
now epsilon is 0.09469968142331493, the reward is 194.33333333333334 with loss [25.271379470825195, 17.067886114120483] in episode 1835
Report: 
rewardSum:194.33333333333334
loss:[25.271379470825195, 17.067886114120483]
policies:[0, 4, 0]
qAverage:[0.0, 66.74175109863282]
ws:[-2.529589241743088, 4.961123305559158]
memory len:10000
memory used:3140.0
now epsilon is 0.09455772065254321, the reward is 192.33333333333334 with loss [34.66352415084839, 30.764489889144897] in episode 1836
Report: 
rewardSum:192.33333333333334
loss:[34.66352415084839, 30.764489889144897]
policies:[1, 3, 2]
qAverage:[0.0, 56.040122985839844]
ws:[1.4227339426676433, 8.288889567057291]
memory len:10000
memory used:3140.0
now epsilon is 0.09436877060453712, the reward is 190.33333333333334 with loss [37.60821318626404, 43.724421977996826] in episode 1837
Report: 
rewardSum:190.33333333333334
loss:[37.60821318626404, 43.724421977996826]
policies:[0, 6, 2]
qAverage:[0.0, 69.67385864257812]
ws:[-0.5348566531514128, 10.96550965309143]
memory len:10000
memory used:3140.0
now epsilon is 0.09427443721632389, the reward is -3.0 with loss [30.58678936958313, 18.506057262420654] in episode 1838
Report: 
rewardSum:-3.0
loss:[30.58678936958313, 18.506057262420654]
policies:[1, 1, 2]
qAverage:[0.0, 38.525089263916016]
ws:[-0.5171777606010437, 3.191476345062256]
memory len:10000
memory used:3140.0
now epsilon is 0.09415665307659822, the reward is 193.33333333333334 with loss [45.845778942108154, 28.89674472808838] in episode 1839
Report: 
rewardSum:193.33333333333334
loss:[45.845778942108154, 28.89674472808838]
policies:[0, 4, 1]
qAverage:[0.0, 62.8384895324707]
ws:[-2.7266529351472855, 7.823047965764999]
memory len:10000
memory used:3140.0
now epsilon is 0.09406253172638211, the reward is 194.33333333333334 with loss [26.799575805664062, 19.94587016105652] in episode 1840
Report: 
rewardSum:194.33333333333334
loss:[26.799575805664062, 19.94587016105652]
policies:[0, 4, 0]
qAverage:[0.0, 66.22465362548829]
ws:[-1.7474162578582764, 8.915939903259277]
memory len:10000
memory used:3140.0
now epsilon is 0.09396850446222658, the reward is 194.33333333333334 with loss [17.160377740859985, 27.022440910339355] in episode 1841
Report: 
rewardSum:194.33333333333334
loss:[17.160377740859985, 27.022440910339355]
policies:[0, 4, 0]
qAverage:[0.0, 59.93239974975586]
ws:[-2.817425474524498, 7.324872553348541]
memory len:10000
memory used:3140.0
now epsilon is 0.09387457119008089, the reward is 194.33333333333334 with loss [21.510837078094482, 18.64149522781372] in episode 1842
Report: 
rewardSum:194.33333333333334
loss:[21.510837078094482, 18.64149522781372]
policies:[1, 3, 0]
qAverage:[0.0, 61.0808048248291]
ws:[-3.184077709913254, 6.421955913305283]
memory len:10000
memory used:3140.0
now epsilon is 0.09378073181598821, the reward is 194.33333333333334 with loss [25.026769638061523, 18.534260988235474] in episode 1843
Report: 
rewardSum:194.33333333333334
loss:[25.026769638061523, 18.534260988235474]
policies:[0, 4, 0]
qAverage:[0.0, 64.98588714599609]
ws:[-0.951263815164566, 10.155789947509765]
memory len:10000
memory used:3140.0
now epsilon is 0.09368698624608573, the reward is 194.33333333333334 with loss [17.976179122924805, 19.499576926231384] in episode 1844
Report: 
rewardSum:194.33333333333334
loss:[17.976179122924805, 19.499576926231384]
policies:[0, 4, 0]
qAverage:[0.0, 65.74791412353515]
ws:[-0.6914589464664459, 10.398393917083741]
memory len:10000
memory used:3141.0
now epsilon is 0.09359333438660443, the reward is 194.33333333333334 with loss [19.80845022201538, 34.33853244781494] in episode 1845
Report: 
rewardSum:194.33333333333334
loss:[19.80845022201538, 34.33853244781494]
policies:[0, 4, 0]
qAverage:[0.0, 65.43033142089844]
ws:[-1.0528979837894439, 10.096915817260742]
memory len:10000
memory used:3141.0
now epsilon is 0.09349977614386902, the reward is 194.33333333333334 with loss [21.849416732788086, 26.619511604309082] in episode 1846
Report: 
rewardSum:194.33333333333334
loss:[21.849416732788086, 26.619511604309082]
policies:[0, 4, 0]
qAverage:[0.0, 65.49261932373047]
ws:[-1.2125254273414612, 10.644884395599366]
memory len:10000
memory used:3141.0
now epsilon is 0.09340631142429784, the reward is 194.33333333333334 with loss [28.03164291381836, 20.10884952545166] in episode 1847
Report: 
rewardSum:194.33333333333334
loss:[28.03164291381836, 20.10884952545166]
policies:[0, 3, 1]
qAverage:[0.0, 65.2287826538086]
ws:[-2.5877165719866753, 9.211790561676025]
memory len:10000
memory used:3141.0
now epsilon is 0.0933129401344028, the reward is 194.33333333333334 with loss [21.341872692108154, 14.086474269628525] in episode 1848
Report: 
rewardSum:194.33333333333334
loss:[21.341872692108154, 14.086474269628525]
policies:[0, 3, 1]
qAverage:[0.0, 61.96800422668457]
ws:[-2.5268549248576164, 7.5187495946884155]
memory len:10000
memory used:3141.0
now epsilon is 0.09321966218078927, the reward is 194.33333333333334 with loss [28.248404264450073, 25.68177080154419] in episode 1849
Report: 
rewardSum:194.33333333333334
loss:[28.248404264450073, 25.68177080154419]
policies:[1, 3, 0]
qAverage:[0.0, 61.14752197265625]
ws:[-3.117902100086212, 6.271990358829498]
memory len:10000
memory used:3141.0
now epsilon is 0.09312647747015594, the reward is 194.33333333333334 with loss [21.81520390510559, 16.161848545074463] in episode 1850
Report: 
rewardSum:194.33333333333334
loss:[21.81520390510559, 16.161848545074463]
policies:[0, 3, 1]
qAverage:[0.0, 57.759521484375]
ws:[-0.9555879514664412, 4.0113025438040495]
memory len:10000
memory used:3141.0
now epsilon is 0.09303338590929482, the reward is 194.33333333333334 with loss [24.3681902885437, 21.901905059814453] in episode 1851
Report: 
rewardSum:194.33333333333334
loss:[24.3681902885437, 21.901905059814453]
policies:[0, 4, 0]
qAverage:[0.0, 58.23285102844238]
ws:[-1.213195726275444, 3.722660705447197]
memory len:10000
memory used:3141.0
now epsilon is 0.09294038740509103, the reward is 194.33333333333334 with loss [19.571422815322876, 21.04987096786499] in episode 1852
Report: 
rewardSum:194.33333333333334
loss:[19.571422815322876, 21.04987096786499]
policies:[1, 3, 0]
qAverage:[0.0, 57.89297294616699]
ws:[-0.9408541582524776, 4.320006258785725]
memory len:10000
memory used:3141.0
now epsilon is 0.09284748186452281, the reward is 194.33333333333334 with loss [21.934924602508545, 23.832947254180908] in episode 1853
Report: 
rewardSum:194.33333333333334
loss:[21.934924602508545, 23.832947254180908]
policies:[0, 4, 0]
qAverage:[0.0, 66.46263275146484]
ws:[-3.1624194152653216, 5.615237322449684]
memory len:10000
memory used:3140.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11*		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.09275466919466138, the reward is 194.33333333333334 with loss [14.658746719360352, 31.17116403579712] in episode 1854
Report: 
rewardSum:194.33333333333334
loss:[14.658746719360352, 31.17116403579712]
policies:[0, 4, 0]
qAverage:[0.0, 67.0375762939453]
ws:[-2.5896256625652314, 6.242837047576904]
memory len:10000
memory used:3140.0
now epsilon is 0.09266194930267087, the reward is 194.33333333333334 with loss [25.998368740081787, 21.45037317276001] in episode 1855
Report: 
rewardSum:194.33333333333334
loss:[25.998368740081787, 21.45037317276001]
policies:[1, 3, 0]
qAverage:[0.0, 61.32658386230469]
ws:[-2.8754713386297226, 5.325509488582611]
memory len:10000
memory used:3140.0
now epsilon is 0.09256932209580818, the reward is 194.33333333333334 with loss [23.87512707710266, 21.801769733428955] in episode 1856
Report: 
rewardSum:194.33333333333334
loss:[23.87512707710266, 21.801769733428955]
policies:[0, 3, 1]
qAverage:[0.0, 58.63662910461426]
ws:[1.8197375982999802, 8.066279888153076]
memory len:10000
memory used:3140.0
now epsilon is 0.09247678748142295, the reward is 194.33333333333334 with loss [15.26656699180603, 19.048806190490723] in episode 1857
Report: 
rewardSum:194.33333333333334
loss:[15.26656699180603, 19.048806190490723]
policies:[0, 4, 0]
qAverage:[0.0, 58.6738026936849]
ws:[0.16661739349365234, 14.791635513305664]
memory len:10000
memory used:3140.0
now epsilon is 0.0923843453669574, the reward is 194.33333333333334 with loss [23.77136516571045, 15.08367669582367] in episode 1858
Report: 
rewardSum:194.33333333333334
loss:[23.77136516571045, 15.08367669582367]
policies:[0, 4, 0]
qAverage:[0.0, 66.54545288085937]
ws:[0.9575173377990722, 10.092222976684571]
memory len:10000
memory used:3140.0
now epsilon is 0.09229199565994631, the reward is 194.33333333333334 with loss [22.065138816833496, 19.589802265167236] in episode 1859
Report: 
rewardSum:194.33333333333334
loss:[22.065138816833496, 19.589802265167236]
policies:[0, 4, 0]
qAverage:[0.0, 66.17936515808105]
ws:[-0.07776546478271484, 11.088528633117676]
memory len:10000
memory used:3140.0
now epsilon is 0.09219973826801688, the reward is 194.33333333333334 with loss [24.799235343933105, 26.68186926841736] in episode 1860
Report: 
rewardSum:194.33333333333334
loss:[24.799235343933105, 26.68186926841736]
policies:[1, 3, 0]
qAverage:[0.0, 65.73168563842773]
ws:[-2.2197638377547264, 6.350340306758881]
memory len:10000
memory used:3140.0
now epsilon is 0.09210757309888859, the reward is 194.33333333333334 with loss [20.42188858985901, 21.936333179473877] in episode 1861
Report: 
rewardSum:194.33333333333334
loss:[20.42188858985901, 21.936333179473877]
policies:[0, 3, 1]
qAverage:[0.0, 61.33778381347656]
ws:[-1.6257075816392899, 8.353548046201468]
memory len:10000
memory used:3140.0
now epsilon is 0.09201550006037325, the reward is 194.33333333333334 with loss [16.057169198989868, 19.35037660598755] in episode 1862
Report: 
rewardSum:194.33333333333334
loss:[16.057169198989868, 19.35037660598755]
policies:[1, 3, 0]
qAverage:[0.0, 64.82779693603516]
ws:[-2.207658424973488, 5.716159217059612]
memory len:10000
memory used:3140.0
now epsilon is 0.0919235190603748, the reward is 194.33333333333334 with loss [16.020965576171875, 24.68133020401001] in episode 1863
Report: 
rewardSum:194.33333333333334
loss:[16.020965576171875, 24.68133020401001]
policies:[0, 4, 0]
qAverage:[0.0, 66.12555084228515]
ws:[-1.5121794827282429, 6.509891533851624]
memory len:10000
memory used:3140.0
now epsilon is 0.09183163000688922, the reward is 194.33333333333334 with loss [23.128296852111816, 23.27518367767334] in episode 1864
Report: 
rewardSum:194.33333333333334
loss:[23.128296852111816, 23.27518367767334]
policies:[1, 3, 0]
qAverage:[0.0, 61.01489448547363]
ws:[-0.4055050192400813, 10.657100558280945]
memory len:10000
memory used:3140.0
now epsilon is 0.09173983280800448, the reward is 194.33333333333334 with loss [31.84375524520874, 26.80245065689087] in episode 1865
Report: 
rewardSum:194.33333333333334
loss:[31.84375524520874, 26.80245065689087]
policies:[0, 4, 0]
qAverage:[0.0, 65.12078399658203]
ws:[-0.017006266117095947, 8.92318344116211]
memory len:10000
memory used:3140.0
now epsilon is 0.0916023090362224, the reward is 192.33333333333334 with loss [37.19245481491089, 34.98814630508423] in episode 1866
Report: 
rewardSum:192.33333333333334
loss:[37.19245481491089, 34.98814630508423]
policies:[0, 3, 3]
qAverage:[0.0, 58.15831756591797]
ws:[-0.15545520186424255, 3.8167154788970947]
memory len:10000
memory used:3146.0
now epsilon is 0.0915107410723273, the reward is 194.33333333333334 with loss [19.907598972320557, 28.352816820144653] in episode 1867
Report: 
rewardSum:194.33333333333334
loss:[19.907598972320557, 28.352816820144653]
policies:[0, 3, 1]
qAverage:[0.0, 62.142862955729164]
ws:[-1.3331647713979085, 9.680045445760092]
memory len:10000
memory used:3146.0
now epsilon is 0.09141926464206382, the reward is 194.33333333333334 with loss [17.883378982543945, 24.557758808135986] in episode 1868
Report: 
rewardSum:194.33333333333334
loss:[17.883378982543945, 24.557758808135986]
policies:[1, 3, 0]
qAverage:[0.0, 65.79355430603027]
ws:[-0.7816585004329681, 10.64937162399292]
memory len:10000
memory used:3146.0
now epsilon is 0.09123658601652597, the reward is 190.33333333333334 with loss [42.55341553688049, 45.71025037765503] in episode 1869
Report: 
rewardSum:190.33333333333334
loss:[42.55341553688049, 45.71025037765503]
policies:[0, 6, 2]
qAverage:[0.0, 68.28939056396484]
ws:[-0.5988339508573214, 9.513296167055765]
memory len:10000
memory used:3158.0
now epsilon is 0.09114538363852728, the reward is 194.33333333333334 with loss [22.091783046722412, 17.576223373413086] in episode 1870
Report: 
rewardSum:194.33333333333334
loss:[22.091783046722412, 17.576223373413086]
policies:[0, 4, 0]
qAverage:[0.0, 66.6024887084961]
ws:[-0.4808568835258484, 9.364368772506714]
memory len:10000
memory used:3158.0
now epsilon is 0.09105427242871139, the reward is 194.33333333333334 with loss [18.6822292804718, 22.83925700187683] in episode 1871
Report: 
rewardSum:194.33333333333334
loss:[18.6822292804718, 22.83925700187683]
policies:[0, 4, 0]
qAverage:[0.0, 66.2459701538086]
ws:[-0.252286422252655, 9.64897599220276]
memory len:10000
memory used:3158.0
now epsilon is 0.0909632522959443, the reward is 194.33333333333334 with loss [20.167945861816406, 21.08105230331421] in episode 1872
Report: 
rewardSum:194.33333333333334
loss:[20.167945861816406, 21.08105230331421]
policies:[0, 3, 1]
qAverage:[0.0, 65.84875679016113]
ws:[-0.46452832221984863, 11.078891277313232]
memory len:10000
memory used:3158.0
now epsilon is 0.09087232314918313, the reward is -3.0 with loss [26.542178630828857, 30.61850595474243] in episode 1873
Report: 
rewardSum:-3.0
loss:[26.542178630828857, 30.61850595474243]
policies:[0, 2, 2]
qAverage:[0.0, 48.022570292154946]
ws:[1.0409838457902272, 7.022174596786499]
memory len:10000
memory used:3158.0
now epsilon is 0.09078148489747598, the reward is 194.33333333333334 with loss [17.00020933151245, 17.65270185470581] in episode 1874
Report: 
rewardSum:194.33333333333334
loss:[17.00020933151245, 17.65270185470581]
policies:[1, 2, 1]
qAverage:[0.0, 62.43519592285156]
ws:[-2.3978451689084372, 7.931481679280599]
memory len:10000
memory used:3158.0
now epsilon is 0.09069073744996187, the reward is 194.33333333333334 with loss [24.95648467540741, 21.980576038360596] in episode 1875
Report: 
rewardSum:194.33333333333334
loss:[24.95648467540741, 21.980576038360596]
policies:[0, 4, 0]
qAverage:[0.0, 60.90595626831055]
ws:[-1.402127481997013, 9.551109552383423]
memory len:10000
memory used:3158.0
now epsilon is 0.09055478633801775, the reward is 192.33333333333334 with loss [42.401466846466064, 28.470252752304077] in episode 1876
Report: 
rewardSum:192.33333333333334
loss:[42.401466846466064, 28.470252752304077]
policies:[0, 5, 1]
qAverage:[0.0, 68.15774154663086]
ws:[-0.13479125748078027, 10.828820864359537]
memory len:10000
memory used:3160.0
now epsilon is 0.0904642655040653, the reward is 194.33333333333334 with loss [22.329024076461792, 19.248162508010864] in episode 1877
Report: 
rewardSum:194.33333333333334
loss:[22.329024076461792, 19.248162508010864]
policies:[0, 4, 0]
qAverage:[0.0, 66.15827178955078]
ws:[-0.9444472253322601, 9.358849430084229]
memory len:10000
memory used:3160.0
now epsilon is 0.09037383515700714, the reward is 194.33333333333334 with loss [26.461801052093506, 19.209407329559326] in episode 1878
Report: 
rewardSum:194.33333333333334
loss:[26.461801052093506, 19.209407329559326]
policies:[0, 3, 1]
qAverage:[0.0, 64.21556282043457]
ws:[-2.629558239132166, 6.4891403913497925]
memory len:10000
memory used:3160.0
now epsilon is 0.09028349520639031, the reward is 194.33333333333334 with loss [19.896090507507324, 22.513243675231934] in episode 1879
Report: 
rewardSum:194.33333333333334
loss:[19.896090507507324, 22.513243675231934]
policies:[0, 4, 0]
qAverage:[0.0, 66.33223114013671]
ws:[-3.0034065008163453, 7.067740535736084]
memory len:10000
memory used:3160.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21*		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.09019324556185226, the reward is 194.33333333333334 with loss [27.83012866973877, 19.36028552055359] in episode 1880
Report: 
rewardSum:194.33333333333334
loss:[27.83012866973877, 19.36028552055359]
policies:[0, 4, 0]
qAverage:[0.0, 66.65739288330079]
ws:[-1.8519264698028564, 10.52402400970459]
memory len:10000
memory used:3160.0
now epsilon is 0.09010308613312078, the reward is 194.33333333333334 with loss [23.985687732696533, 25.529762268066406] in episode 1881
Report: 
rewardSum:194.33333333333334
loss:[23.985687732696533, 25.529762268066406]
policies:[0, 4, 0]
qAverage:[0.0, 66.8186538696289]
ws:[-1.5402072072029114, 10.698212146759033]
memory len:10000
memory used:3159.0
now epsilon is 0.09001301683001388, the reward is 194.33333333333334 with loss [19.672427892684937, 19.877113580703735] in episode 1882
Report: 
rewardSum:194.33333333333334
loss:[19.672427892684937, 19.877113580703735]
policies:[0, 4, 0]
qAverage:[0.0, 66.50516510009766]
ws:[-1.974155104160309, 7.95352954864502]
memory len:10000
memory used:3160.0
now epsilon is 0.08992303756243973, the reward is 46.33333333333334 with loss [22.400153160095215, 28.32261085510254] in episode 1883
Report: 
rewardSum:46.33333333333334
loss:[22.400153160095215, 28.32261085510254]
policies:[0, 2, 2]
qAverage:[0.0, 55.400464375813804]
ws:[0.510505293806394, 5.627732594807942]
memory len:10000
memory used:3160.0
now epsilon is 0.08978823728084812, the reward is 192.33333333333334 with loss [38.58197259902954, 40.365952491760254] in episode 1884
Report: 
rewardSum:192.33333333333334
loss:[38.58197259902954, 40.365952491760254]
policies:[0, 4, 2]
qAverage:[0.0, 67.5468032836914]
ws:[-1.836745622754097, 5.643244791030884]
memory len:10000
memory used:3160.0
now epsilon is 0.08967605808786772, the reward is 193.33333333333334 with loss [21.525916576385498, 32.78497791290283] in episode 1885
Report: 
rewardSum:193.33333333333334
loss:[21.525916576385498, 32.78497791290283]
policies:[0, 4, 1]
qAverage:[0.0, 67.03489685058594]
ws:[-1.0487871527671815, 9.840020489692687]
memory len:10000
memory used:3160.0
now epsilon is 0.08958641565269725, the reward is 194.33333333333334 with loss [15.43464982509613, 22.01440191268921] in episode 1886
Report: 
rewardSum:194.33333333333334
loss:[15.43464982509613, 22.01440191268921]
policies:[0, 4, 0]
qAverage:[0.0, 66.72210388183593]
ws:[-1.4077085316181184, 8.882500153779983]
memory len:10000
memory used:3159.0
now epsilon is 0.08945211998849238, the reward is 192.33333333333334 with loss [38.744205951690674, 31.656855940818787] in episode 1887
Report: 
rewardSum:192.33333333333334
loss:[38.744205951690674, 31.656855940818787]
policies:[1, 4, 1]
qAverage:[0.0, 68.05964088439941]
ws:[-1.3012857735157013, 8.77922797203064]
memory len:10000
memory used:3160.0
now epsilon is 0.08936270140745849, the reward is 194.33333333333334 with loss [22.035531520843506, 19.07917809486389] in episode 1888
Report: 
rewardSum:194.33333333333334
loss:[22.035531520843506, 19.07917809486389]
policies:[0, 3, 1]
qAverage:[0.0, 67.65248107910156]
ws:[-1.197147250175476, 9.94583010673523]
memory len:10000
memory used:3160.0
now epsilon is 0.08927337221147923, the reward is 194.33333333333334 with loss [24.227391242980957, 19.912971019744873] in episode 1889
Report: 
rewardSum:194.33333333333334
loss:[24.227391242980957, 19.912971019744873]
policies:[0, 4, 0]
qAverage:[0.0, 68.0763916015625]
ws:[-1.1506651699543, 9.189337158203125]
memory len:10000
memory used:3160.0
now epsilon is 0.08918413231120309, the reward is 194.33333333333334 with loss [18.11171007156372, 26.125582218170166] in episode 1890
Report: 
rewardSum:194.33333333333334
loss:[18.11171007156372, 26.125582218170166]
policies:[0, 4, 0]
qAverage:[0.0, 67.98883666992188]
ws:[-0.477157324552536, 12.239964771270753]
memory len:10000
memory used:3160.0
now epsilon is 0.08909498161736785, the reward is 194.33333333333334 with loss [21.59024429321289, 18.07326579093933] in episode 1891
Report: 
rewardSum:194.33333333333334
loss:[21.59024429321289, 18.07326579093933]
policies:[0, 3, 1]
qAverage:[0.0, 62.41181755065918]
ws:[-1.43955896794796, 12.26146525144577]
memory len:10000
memory used:3160.0
now epsilon is 0.08900592004080052, the reward is 194.33333333333334 with loss [25.186399459838867, 23.553709030151367] in episode 1892
Report: 
rewardSum:194.33333333333334
loss:[25.186399459838867, 23.553709030151367]
policies:[0, 4, 0]
qAverage:[0.0, 67.98751068115234]
ws:[-1.583044296503067, 11.169319486618042]
memory len:10000
memory used:3160.0
now epsilon is 0.08891694749241721, the reward is 194.33333333333334 with loss [21.191304683685303, 29.26339817047119] in episode 1893
Report: 
rewardSum:194.33333333333334
loss:[21.191304683685303, 29.26339817047119]
policies:[0, 4, 0]
qAverage:[0.0, 67.7377700805664]
ws:[-1.5236506700515746, 12.654715633392334]
memory len:10000
memory used:3160.0
now epsilon is 0.08882806388322316, the reward is 194.33333333333334 with loss [17.879598379135132, 16.860534191131592] in episode 1894
Report: 
rewardSum:194.33333333333334
loss:[17.879598379135132, 16.860534191131592]
policies:[0, 3, 1]
qAverage:[0.0, 67.43869590759277]
ws:[-2.530184142291546, 12.28097677230835]
memory len:10000
memory used:3160.0
now epsilon is 0.08871708430703142, the reward is 193.33333333333334 with loss [26.693135738372803, 21.920682191848755] in episode 1895
Report: 
rewardSum:193.33333333333334
loss:[26.693135738372803, 21.920682191848755]
policies:[0, 4, 1]
qAverage:[0.0, 67.29089546203613]
ws:[-1.7409799247980118, 14.564371347427368]
memory len:10000
memory used:3160.0
now epsilon is 0.08862840048608654, the reward is 194.33333333333334 with loss [17.280640125274658, 30.94787359237671] in episode 1896
Report: 
rewardSum:194.33333333333334
loss:[17.280640125274658, 30.94787359237671]
policies:[0, 4, 0]
qAverage:[0.0, 68.09170684814453]
ws:[-2.865642136335373, 7.932581949234009]
memory len:10000
memory used:3161.0
now epsilon is 0.08853980531571172, the reward is 194.33333333333334 with loss [16.299026489257812, 26.51065731048584] in episode 1897
Report: 
rewardSum:194.33333333333334
loss:[16.299026489257812, 26.51065731048584]
policies:[1, 1, 2]
qAverage:[0.0, 38.219337463378906]
ws:[0.46827489137649536, 8.624701499938965]
memory len:10000
memory used:3161.0
now epsilon is 0.08840707858614215, the reward is 192.33333333333334 with loss [33.43953490257263, 27.163711071014404] in episode 1898
Report: 
rewardSum:192.33333333333334
loss:[33.43953490257263, 27.163711071014404]
policies:[0, 5, 1]
qAverage:[0.0, 69.38780212402344]
ws:[-2.1155416816473007, 9.22761340936025]
memory len:10000
memory used:3161.0
now epsilon is 0.08831870465468537, the reward is 194.33333333333334 with loss [27.78809404373169, 28.114150524139404] in episode 1899
Report: 
rewardSum:194.33333333333334
loss:[27.78809404373169, 28.114150524139404]
policies:[0, 4, 0]
qAverage:[0.0, 69.3265609741211]
ws:[-3.430056428909302, 6.784399524331093]
memory len:10000
memory used:3161.0
now epsilon is 0.08823041906402536, the reward is 46.33333333333334 with loss [29.72557830810547, 25.381797790527344] in episode 1900
Report: 
rewardSum:46.33333333333334
loss:[29.72557830810547, 25.381797790527344]
policies:[0, 3, 1]
qAverage:[0.0, 58.54240798950195]
ws:[-1.0422491282224655, 3.728301614522934]
memory len:10000
memory used:3160.0
now epsilon is 0.08814222172585442, the reward is 194.33333333333334 with loss [19.563573837280273, 23.94902014732361] in episode 1901
Report: 
rewardSum:194.33333333333334
loss:[19.563573837280273, 23.94902014732361]
policies:[1, 3, 0]
qAverage:[0.0, 66.4118423461914]
ws:[-2.581805467605591, 9.480097234249115]
memory len:10000
memory used:3160.0
now epsilon is 0.08805411255195318, the reward is 194.33333333333334 with loss [18.98507833480835, 17.083052396774292] in episode 1902
Report: 
rewardSum:194.33333333333334
loss:[18.98507833480835, 17.083052396774292]
policies:[0, 4, 0]
qAverage:[0.0, 68.05009613037109]
ws:[-1.2965408891439438, 10.056647872924804]
memory len:10000
memory used:3160.0
now epsilon is 0.0879660914541904, the reward is 194.33333333333334 with loss [25.358476638793945, 17.680165767669678] in episode 1903
Report: 
rewardSum:194.33333333333334
loss:[25.358476638793945, 17.680165767669678]
policies:[0, 4, 0]
qAverage:[0.0, 68.43672485351563]
ws:[-0.40043067932128906, 11.99337100982666]
memory len:10000
memory used:3160.0
now epsilon is 0.08787815834452299, the reward is 194.33333333333334 with loss [21.834644079208374, 24.854981899261475] in episode 1904
Report: 
rewardSum:194.33333333333334
loss:[21.834644079208374, 24.854981899261475]
policies:[2, 1, 1]
qAverage:[0.0, 40.00962448120117]
ws:[1.4416499137878418, 9.629926681518555]
memory len:10000
memory used:3159.0
now epsilon is 0.08779031313499583, the reward is 194.33333333333334 with loss [22.427522897720337, 17.82616949081421] in episode 1905
Report: 
rewardSum:194.33333333333334
loss:[22.427522897720337, 17.82616949081421]
policies:[0, 4, 0]
qAverage:[0.0, 67.8944076538086]
ws:[-0.6647938385605812, 12.273197746276855]
memory len:10000
memory used:3159.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23*		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.08770255573774172, the reward is 194.33333333333334 with loss [15.1173095703125, 21.39761781692505] in episode 1906
Report: 
rewardSum:194.33333333333334
loss:[15.1173095703125, 21.39761781692505]
policies:[0, 4, 0]
qAverage:[0.0, 68.68627014160157]
ws:[-0.6070865452289581, 10.299148607254029]
memory len:10000
memory used:3159.0
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.08763679526379727, the reward is -2.0 with loss [16.99525213241577, 17.08214235305786] in episode 1907
Report: 
rewardSum:-2.0
loss:[16.99525213241577, 17.08214235305786]
policies:[0, 1, 2]
qAverage:[0.0, 34.23942947387695]
ws:[0.4906679689884186, 1.310583472251892]
memory len:10000
memory used:3160.0
now epsilon is 0.08754919132685475, the reward is 194.33333333333334 with loss [23.2288236618042, 19.79919719696045] in episode 1908
Report: 
rewardSum:194.33333333333334
loss:[23.2288236618042, 19.79919719696045]
policies:[0, 4, 0]
qAverage:[0.0, 68.56215057373046]
ws:[-1.7330735146999359, 6.433923590183258]
memory len:10000
memory used:3160.0
now epsilon is 0.08746167496100317, the reward is 194.33333333333334 with loss [25.842340230941772, 17.342087984085083] in episode 1909
Report: 
rewardSum:194.33333333333334
loss:[25.842340230941772, 17.342087984085083]
policies:[1, 3, 0]
qAverage:[10.225447082519532, 54.44466705322266]
ws:[-2.418073308467865, 3.9579540967941282]
memory len:10000
memory used:3160.0
now epsilon is 0.08737424607870428, the reward is 194.33333333333334 with loss [24.39894199371338, 17.643510341644287] in episode 1910
Report: 
rewardSum:194.33333333333334
loss:[24.39894199371338, 17.643510341644287]
policies:[1, 3, 0]
qAverage:[10.374678802490234, 54.12700500488281]
ws:[-2.165791612863541, 5.709980130195618]
memory len:10000
memory used:3160.0
now epsilon is 0.08728690459250732, the reward is 194.33333333333334 with loss [24.07511329650879, 24.523673057556152] in episode 1911
Report: 
rewardSum:194.33333333333334
loss:[24.07511329650879, 24.523673057556152]
policies:[0, 4, 0]
qAverage:[0.0, 68.95744934082032]
ws:[-2.5722394555807115, 4.6389355778694155]
memory len:10000
memory used:3160.0
now epsilon is 0.08719965041504896, the reward is 194.33333333333334 with loss [26.483814239501953, 15.552289485931396] in episode 1912
Report: 
rewardSum:194.33333333333334
loss:[26.483814239501953, 15.552289485931396]
policies:[0, 4, 0]
qAverage:[0.0, 60.331329345703125]
ws:[-0.7307268530130386, 2.882684201002121]
memory len:10000
memory used:3160.0
now epsilon is 0.08711248345905319, the reward is 194.33333333333334 with loss [18.881361484527588, 21.64222812652588] in episode 1913
Report: 
rewardSum:194.33333333333334
loss:[18.881361484527588, 21.64222812652588]
policies:[0, 3, 1]
qAverage:[0.0, 66.2634162902832]
ws:[-3.112124104052782, 4.952757805585861]
memory len:10000
memory used:3160.0
now epsilon is 0.08702540363733124, the reward is 194.33333333333334 with loss [22.889028072357178, 29.03121280670166] in episode 1914
Report: 
rewardSum:194.33333333333334
loss:[22.889028072357178, 29.03121280670166]
policies:[0, 4, 0]
qAverage:[0.0, 68.32765655517578]
ws:[-2.7448984030634165, 5.089660120010376]
memory len:10000
memory used:3160.0
now epsilon is 0.08693841086278155, the reward is 194.33333333333334 with loss [16.818801164627075, 21.50155782699585] in episode 1915
Report: 
rewardSum:194.33333333333334
loss:[16.818801164627075, 21.50155782699585]
policies:[0, 4, 0]
qAverage:[0.0, 66.51712608337402]
ws:[-2.5925426185131073, 5.21883237361908]
memory len:10000
memory used:3160.0
now epsilon is 0.08685150504838955, the reward is 194.33333333333334 with loss [29.284427642822266, 20.163620471954346] in episode 1916
Report: 
rewardSum:194.33333333333334
loss:[29.284427642822266, 20.163620471954346]
policies:[0, 4, 0]
qAverage:[0.0, 68.51313591003418]
ws:[-2.126510441303253, 7.958506464958191]
memory len:10000
memory used:3160.0
now epsilon is 0.08676468610722769, the reward is 194.33333333333334 with loss [24.067559719085693, 28.59306812286377] in episode 1917
Report: 
rewardSum:194.33333333333334
loss:[24.067559719085693, 28.59306812286377]
policies:[1, 3, 0]
qAverage:[0.0, 60.889503479003906]
ws:[0.20231249928474426, 4.889808177947998]
memory len:10000
memory used:3160.0
now epsilon is 0.0866779539524553, the reward is 194.33333333333334 with loss [27.44601821899414, 31.0013427734375] in episode 1918
Report: 
rewardSum:194.33333333333334
loss:[27.44601821899414, 31.0013427734375]
policies:[0, 4, 0]
qAverage:[0.0, 69.15209045410157]
ws:[-1.6476645350456238, 7.5158916234970095]
memory len:10000
memory used:3160.0
now epsilon is 0.08659130849731855, the reward is 194.33333333333334 with loss [21.564881563186646, 18.680331230163574] in episode 1919
Report: 
rewardSum:194.33333333333334
loss:[21.564881563186646, 18.680331230163574]
policies:[0, 4, 0]
qAverage:[0.0, 68.62464294433593]
ws:[-0.9748378083109855, 8.876937961578369]
memory len:10000
memory used:3160.0
now epsilon is 0.08650474965515031, the reward is 194.33333333333334 with loss [20.17621350288391, 17.986825466156006] in episode 1920
Report: 
rewardSum:194.33333333333334
loss:[20.17621350288391, 17.986825466156006]
policies:[0, 4, 0]
qAverage:[0.0, 69.44701766967773]
ws:[-0.8377740979194641, 11.584786415100098]
memory len:10000
memory used:3160.0
now epsilon is 0.08641827733937009, the reward is 194.33333333333334 with loss [22.394415378570557, 22.336969256401062] in episode 1921
Report: 
rewardSum:194.33333333333334
loss:[22.394415378570557, 22.336969256401062]
policies:[0, 4, 0]
qAverage:[0.0, 68.85174102783203]
ws:[-0.4797572374343872, 10.359918689727783]
memory len:10000
memory used:3161.0
now epsilon is 0.08633189146348393, the reward is 194.33333333333334 with loss [24.94666290283203, 26.04402732849121] in episode 1922
Report: 
rewardSum:194.33333333333334
loss:[24.94666290283203, 26.04402732849121]
policies:[0, 4, 0]
qAverage:[0.0, 69.49519195556641]
ws:[-1.0593899130821227, 10.107607364654541]
memory len:10000
memory used:3161.0
now epsilon is 0.08624559194108435, the reward is 194.33333333333334 with loss [18.394015073776245, 21.90494441986084] in episode 1923
Report: 
rewardSum:194.33333333333334
loss:[18.394015073776245, 21.90494441986084]
policies:[0, 4, 0]
qAverage:[0.0, 69.70737152099609]
ws:[-2.0122028589248657, 8.729485988616943]
memory len:10000
memory used:3161.0
now epsilon is 0.0862024745354633, the reward is -1.0 with loss [10.257333755493164, 12.775551795959473] in episode 1924
Report: 
rewardSum:-1.0
loss:[10.257333755493164, 12.775551795959473]
policies:[0, 1, 1]
qAverage:[0.0, 37.3360481262207]
ws:[-0.7310596704483032, 0.5503310561180115]
memory len:10000
memory used:3161.0
now epsilon is 0.08611630438146849, the reward is 194.33333333333334 with loss [20.260164976119995, 21.317821621894836] in episode 1925
Report: 
rewardSum:194.33333333333334
loss:[20.260164976119995, 21.317821621894836]
policies:[0, 4, 0]
qAverage:[0.0, 70.6192642211914]
ws:[-2.9360965967178343, 8.04797334074974]
memory len:10000
memory used:3161.0
now epsilon is 0.08603022036531925, the reward is 194.33333333333334 with loss [20.901491165161133, 22.65735673904419] in episode 1926
Report: 
rewardSum:194.33333333333334
loss:[20.901491165161133, 22.65735673904419]
policies:[0, 4, 0]
qAverage:[0.0, 68.66322135925293]
ws:[-3.3231063559651375, 7.033925838768482]
memory len:10000
memory used:3161.0
now epsilon is 0.08594422240091003, the reward is 194.33333333333334 with loss [25.909931421279907, 16.296769380569458] in episode 1927
Report: 
rewardSum:194.33333333333334
loss:[25.909931421279907, 16.296769380569458]
policies:[0, 3, 1]
qAverage:[0.0, 68.09000015258789]
ws:[-2.4772385954856873, 8.411333441734314]
memory len:10000
memory used:3161.0
now epsilon is 0.08581538661316465, the reward is 192.33333333333334 with loss [34.89394283294678, 32.352068185806274] in episode 1928
Report: 
rewardSum:192.33333333333334
loss:[34.89394283294678, 32.352068185806274]
policies:[0, 5, 1]
qAverage:[0.0, 70.67925135294597]
ws:[-1.1886223157246907, 10.320496479670206]
memory len:10000
memory used:3161.0
now epsilon is 0.08572960340195834, the reward is 194.33333333333334 with loss [21.858819484710693, 25.118417501449585] in episode 1929
Report: 
rewardSum:194.33333333333334
loss:[21.858819484710693, 25.118417501449585]
policies:[1, 3, 0]
qAverage:[0.0, 62.917152404785156]
ws:[0.5519533045589924, 6.6397786140441895]
memory len:10000
memory used:3161.0
now epsilon is 0.0856439059417999, the reward is 194.33333333333334 with loss [21.172495126724243, 21.14047646522522] in episode 1930
Report: 
rewardSum:194.33333333333334
loss:[21.172495126724243, 21.14047646522522]
policies:[0, 4, 0]
qAverage:[0.0, 69.80228118896484]
ws:[-0.9789071798324585, 10.52494010925293]
memory len:10000
memory used:3161.0
now epsilon is 0.08555829414697042, the reward is 194.33333333333334 with loss [26.066402912139893, 20.642685890197754] in episode 1931
Report: 
rewardSum:194.33333333333334
loss:[26.066402912139893, 20.642685890197754]
policies:[0, 4, 0]
qAverage:[0.0, 71.93516235351562]
ws:[-0.341542649269104, 10.759604454040527]
memory len:10000
memory used:3161.0
now epsilon is 0.0854727679318367, the reward is 194.33333333333334 with loss [29.914336681365967, 17.095794320106506] in episode 1932
Report: 
rewardSum:194.33333333333334
loss:[29.914336681365967, 17.095794320106506]
policies:[0, 4, 0]
qAverage:[0.0, 70.02348022460937]
ws:[0.24600446224212646, 12.381496715545655]
memory len:10000
memory used:3161.0
now epsilon is 0.08538732721085113, the reward is 194.33333333333334 with loss [22.442960262298584, 16.784653186798096] in episode 1933
Report: 
rewardSum:194.33333333333334
loss:[22.442960262298584, 16.784653186798096]
policies:[0, 4, 0]
qAverage:[0.0, 71.1520751953125]
ws:[-0.8870308220386505, 9.67238974571228]
memory len:10000
memory used:3161.0
now epsilon is 0.08530197189855161, the reward is 194.33333333333334 with loss [20.160655975341797, 28.118637561798096] in episode 1934
Report: 
rewardSum:194.33333333333334
loss:[20.160655975341797, 28.118637561798096]
policies:[0, 4, 0]
qAverage:[0.0, 70.44954223632813]
ws:[-1.2171200703829528, 10.546840286254882]
memory len:10000
memory used:3161.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2*		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.08517409888465059, the reward is 192.33333333333334 with loss [33.84575796127319, 29.166795253753662] in episode 1935
Report: 
rewardSum:192.33333333333334
loss:[33.84575796127319, 29.166795253753662]
policies:[0, 5, 1]
qAverage:[0.0, 72.72993723551433]
ws:[-1.335091640551885, 9.272576769193014]
memory len:10000
memory used:3161.0
now epsilon is 0.08508895672072998, the reward is 194.33333333333334 with loss [22.370052814483643, 21.530592918395996] in episode 1936
Report: 
rewardSum:194.33333333333334
loss:[22.370052814483643, 21.530592918395996]
policies:[0, 4, 0]
qAverage:[0.0, 71.6622528076172]
ws:[-1.5179777443408966, 9.100830274820328]
memory len:10000
memory used:3161.0
now epsilon is 0.08500389966705031, the reward is 194.33333333333334 with loss [21.179793119430542, 26.784149169921875] in episode 1937
Report: 
rewardSum:194.33333333333334
loss:[21.179793119430542, 26.784149169921875]
policies:[0, 4, 0]
qAverage:[0.0, 71.69027862548828]
ws:[-1.5905158519744873, 8.318185102939605]
memory len:10000
memory used:3161.0
now epsilon is 0.08491892763853325, the reward is 194.33333333333334 with loss [25.169665813446045, 24.56067132949829] in episode 1938
Report: 
rewardSum:194.33333333333334
loss:[25.169665813446045, 24.56067132949829]
policies:[0, 4, 0]
qAverage:[0.0, 71.89896240234376]
ws:[-0.3392774939537048, 11.573169279098511]
memory len:10000
memory used:3162.0
now epsilon is 0.0848340405501855, the reward is 194.33333333333334 with loss [24.056126594543457, 20.995422840118408] in episode 1939
Report: 
rewardSum:194.33333333333334
loss:[24.056126594543457, 20.995422840118408]
policies:[0, 4, 0]
qAverage:[0.0, 71.4449462890625]
ws:[-0.2574652910232544, 11.276374626159669]
memory len:10000
memory used:3161.0
now epsilon is 0.08474923831709874, the reward is 194.33333333333334 with loss [23.983969688415527, 22.0547776222229] in episode 1940
Report: 
rewardSum:194.33333333333334
loss:[23.983969688415527, 22.0547776222229]
policies:[0, 4, 0]
qAverage:[0.0, 71.56363220214844]
ws:[0.2191493272781372, 11.942271041870118]
memory len:10000
memory used:3161.0
now epsilon is 0.08466452085444953, the reward is 194.33333333333334 with loss [36.96513509750366, 19.119859218597412] in episode 1941
Report: 
rewardSum:194.33333333333334
loss:[36.96513509750366, 19.119859218597412]
policies:[0, 4, 0]
qAverage:[0.0, 72.11041717529297]
ws:[0.5014304876327514, 13.317493152618407]
memory len:10000
memory used:3162.0
now epsilon is 0.08457988807749921, the reward is 194.33333333333334 with loss [19.59312629699707, 15.39501953125] in episode 1942
Report: 
rewardSum:194.33333333333334
loss:[19.59312629699707, 15.39501953125]
policies:[0, 4, 0]
qAverage:[0.0, 71.88088836669922]
ws:[-0.7052377998828888, 10.324403381347656]
memory len:10000
memory used:3162.0
now epsilon is 0.08449533990159383, the reward is 194.33333333333334 with loss [31.59874153137207, 24.65968155860901] in episode 1943
Report: 
rewardSum:194.33333333333334
loss:[31.59874153137207, 24.65968155860901]
policies:[1, 3, 0]
qAverage:[0.0, 62.75107955932617]
ws:[0.1105516105890274, 4.544637620449066]
memory len:10000
memory used:3162.0
now epsilon is 0.08441087624216409, the reward is 194.33333333333334 with loss [21.89824414253235, 28.34985065460205] in episode 1944
Report: 
rewardSum:194.33333333333334
loss:[21.89824414253235, 28.34985065460205]
policies:[1, 3, 0]
qAverage:[0.0, 69.04501342773438]
ws:[-2.0683980733156204, 10.055688202381134]
memory len:10000
memory used:3162.0
now epsilon is 0.08432649701472518, the reward is 194.33333333333334 with loss [22.44642162322998, 17.973726630210876] in episode 1945
Report: 
rewardSum:194.33333333333334
loss:[22.44642162322998, 17.973726630210876]
policies:[0, 4, 0]
qAverage:[0.0, 71.16002197265625]
ws:[-2.016201901435852, 9.680265092849732]
memory len:10000
memory used:3162.0
now epsilon is 0.08424220213487676, the reward is 194.33333333333334 with loss [23.25848913192749, 18.6453857421875] in episode 1946
Report: 
rewardSum:194.33333333333334
loss:[23.25848913192749, 18.6453857421875]
policies:[1, 3, 0]
qAverage:[0.0, 71.48439025878906]
ws:[-2.126979246735573, 10.962644815444946]
memory len:10000
memory used:3162.0
now epsilon is 0.0841159177824182, the reward is 192.33333333333334 with loss [33.46436834335327, 36.919615745544434] in episode 1947
Report: 
rewardSum:192.33333333333334
loss:[33.46436834335327, 36.919615745544434]
policies:[0, 4, 2]
qAverage:[0.0, 65.24683990478516]
ws:[-1.4383380264043808, 2.07257042825222]
memory len:10000
memory used:3162.0
now epsilon is 0.08403183340284805, the reward is 194.33333333333334 with loss [16.251264572143555, 32.4146523475647] in episode 1948
Report: 
rewardSum:194.33333333333334
loss:[16.251264572143555, 32.4146523475647]
policies:[0, 4, 0]
qAverage:[0.0, 73.47053527832031]
ws:[-2.2413827240467072, 9.117189359664916]
memory len:10000
memory used:3162.0
now epsilon is 0.08394783307613107, the reward is 194.33333333333334 with loss [22.31954026222229, 22.07462787628174] in episode 1949
Report: 
rewardSum:194.33333333333334
loss:[22.31954026222229, 22.07462787628174]
policies:[0, 4, 0]
qAverage:[0.0, 72.95354919433593]
ws:[-2.5719441697001457, 7.421082282066346]
memory len:10000
memory used:3162.0
now epsilon is 0.08386391671824595, the reward is 194.33333333333334 with loss [22.592893600463867, 18.348734855651855] in episode 1950
Report: 
rewardSum:194.33333333333334
loss:[22.592893600463867, 18.348734855651855]
policies:[0, 4, 0]
qAverage:[0.0, 73.9502960205078]
ws:[-2.6888288035988808, 8.412460947036744]
memory len:10000
memory used:3162.0
now epsilon is 0.0837800842452553, the reward is 194.33333333333334 with loss [20.571480751037598, 16.106630086898804] in episode 1951
Report: 
rewardSum:194.33333333333334
loss:[20.571480751037598, 16.106630086898804]
policies:[0, 4, 0]
qAverage:[0.0, 73.23699340820312]
ws:[-3.761562871932983, 5.7704182982444765]
memory len:10000
memory used:3162.0
now epsilon is 0.08369633557330572, the reward is 194.33333333333334 with loss [18.90451991558075, 27.239223957061768] in episode 1952
Report: 
rewardSum:194.33333333333334
loss:[18.90451991558075, 27.239223957061768]
policies:[1, 3, 0]
qAverage:[0.0, 72.76128578186035]
ws:[-3.1966671030968428, 9.543453097343445]
memory len:10000
memory used:3162.0
now epsilon is 0.08361267061862757, the reward is 194.33333333333334 with loss [26.010383129119873, 29.07819890975952] in episode 1953
Report: 
rewardSum:194.33333333333334
loss:[26.010383129119873, 29.07819890975952]
policies:[0, 4, 0]
qAverage:[0.0, 73.76471710205078]
ws:[-3.0251758486032485, 5.1246275186538695]
memory len:10000
memory used:3161.0
now epsilon is 0.08352908929753496, the reward is 194.33333333333334 with loss [21.844433546066284, 23.69197130203247] in episode 1954
Report: 
rewardSum:194.33333333333334
loss:[21.844433546066284, 23.69197130203247]
policies:[0, 4, 0]
qAverage:[0.0, 68.10297584533691]
ws:[-3.3027687668800354, 6.596675306558609]
memory len:10000
memory used:3161.0
now epsilon is 0.08344559152642568, the reward is 194.33333333333334 with loss [17.417808294296265, 32.12026405334473] in episode 1955
Report: 
rewardSum:194.33333333333334
loss:[17.417808294296265, 32.12026405334473]
policies:[1, 2, 1]
qAverage:[0.0, 61.08476257324219]
ws:[0.7944237540165583, 5.467131932576497]
memory len:10000
memory used:3161.0
now epsilon is 0.08336217722178108, the reward is 194.33333333333334 with loss [18.58682417869568, 22.382896423339844] in episode 1956
Report: 
rewardSum:194.33333333333334
loss:[18.58682417869568, 22.382896423339844]
policies:[1, 2, 1]
qAverage:[0.0, 56.88424428304037]
ws:[1.2881110509236653, 3.8958539168039956]
memory len:10000
memory used:3161.0
now epsilon is 0.08321640277892328, the reward is 191.33333333333334 with loss [38.225396513938904, 36.916074991226196] in episode 1957
Report: 
rewardSum:191.33333333333334
loss:[38.225396513938904, 36.916074991226196]
policies:[0, 5, 2]
qAverage:[0.0, 74.89088694254558]
ws:[-0.867117140442133, 9.297609368960062]
memory len:10000
memory used:3161.0
now epsilon is 0.08313321757709471, the reward is 194.33333333333334 with loss [19.31682538986206, 18.60418701171875] in episode 1958
Report: 
rewardSum:194.33333333333334
loss:[19.31682538986206, 18.60418701171875]
policies:[0, 4, 0]
qAverage:[0.0, 74.77682342529297]
ws:[-0.9914122462272644, 6.71183967590332]
memory len:10000
memory used:3161.0
now epsilon is 0.08305011552927873, the reward is 194.33333333333334 with loss [17.790061473846436, 23.287757873535156] in episode 1959
Report: 
rewardSum:194.33333333333334
loss:[17.790061473846436, 23.287757873535156]
policies:[0, 4, 0]
qAverage:[0.0, 72.51647186279297]
ws:[-0.2766645669937134, 9.08179497718811]
memory len:10000
memory used:3161.0
now epsilon is 0.08296709655235247, the reward is 194.33333333333334 with loss [22.9406840801239, 26.26382875442505] in episode 1960
Report: 
rewardSum:194.33333333333334
loss:[22.9406840801239, 26.26382875442505]
policies:[0, 4, 0]
qAverage:[0.0, 74.9687271118164]
ws:[-0.5117404460906982, 6.8355247497558596]
memory len:10000
memory used:3161.0
now epsilon is 0.08288416056327622, the reward is 194.33333333333334 with loss [20.31453275680542, 23.950767517089844] in episode 1961
Report: 
rewardSum:194.33333333333334
loss:[20.31453275680542, 23.950767517089844]
policies:[0, 4, 0]
qAverage:[0.0, 73.7146499633789]
ws:[-0.01996424198150635, 8.423105430603027]
memory len:10000
memory used:3161.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19*		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.08280130747909321, the reward is 194.33333333333334 with loss [18.893845319747925, 17.04323148727417] in episode 1962
Report: 
rewardSum:194.33333333333334
loss:[18.893845319747925, 17.04323148727417]
policies:[0, 4, 0]
qAverage:[0.0, 65.0109920501709]
ws:[1.1533953845500946, 4.187567174434662]
memory len:10000
memory used:3161.0
now epsilon is 0.0826771831182298, the reward is 192.33333333333334 with loss [40.22497606277466, 37.390907764434814] in episode 1963
Report: 
rewardSum:192.33333333333334
loss:[40.22497606277466, 37.390907764434814]
policies:[0, 5, 1]
qAverage:[0.0, 70.33223724365234]
ws:[-0.3545822620391846, 8.49778447151184]
memory len:10000
memory used:3161.0
now epsilon is 0.08259453693388825, the reward is 194.33333333333334 with loss [18.116984248161316, 22.010616302490234] in episode 1964
Report: 
rewardSum:194.33333333333334
loss:[18.116984248161316, 22.010616302490234]
policies:[0, 4, 0]
qAverage:[0.0, 72.95413208007812]
ws:[0.0503920316696167, 8.180171632766724]
memory len:10000
memory used:3161.0
now epsilon is 0.08251197336474389, the reward is 194.33333333333334 with loss [20.121695518493652, 19.48165249824524] in episode 1965
Report: 
rewardSum:194.33333333333334
loss:[20.121695518493652, 19.48165249824524]
policies:[0, 4, 0]
qAverage:[0.0, 74.4468002319336]
ws:[0.6905331134796142, 8.90766921043396]
memory len:10000
memory used:3160.0
now epsilon is 0.08242949232821249, the reward is 194.33333333333334 with loss [21.155669927597046, 27.670044898986816] in episode 1966
Report: 
rewardSum:194.33333333333334
loss:[21.155669927597046, 27.670044898986816]
policies:[2, 2, 0]
qAverage:[13.67507553100586, 45.699819564819336]
ws:[2.5438947081565857, 5.178077816963196]
memory len:10000
memory used:3160.0
now epsilon is 0.08234709374179237, the reward is 194.33333333333334 with loss [26.767725467681885, 28.981398105621338] in episode 1967
Report: 
rewardSum:194.33333333333334
loss:[26.767725467681885, 28.981398105621338]
policies:[0, 3, 1]
qAverage:[0.0, 72.70962333679199]
ws:[0.1430978775024414, 13.607261419296265]
memory len:10000
memory used:3160.0
now epsilon is 0.08226477752306438, the reward is 194.33333333333334 with loss [16.743643522262573, 22.920437335968018] in episode 1968
Report: 
rewardSum:194.33333333333334
loss:[16.743643522262573, 22.920437335968018]
policies:[0, 4, 0]
qAverage:[0.0, 73.98211822509765]
ws:[-0.5097795486450195, 9.512636995315551]
memory len:10000
memory used:3160.0
now epsilon is 0.08218254358969167, the reward is 194.33333333333334 with loss [26.14457607269287, 23.072336196899414] in episode 1969
Report: 
rewardSum:194.33333333333334
loss:[26.14457607269287, 23.072336196899414]
policies:[0, 4, 0]
qAverage:[0.0, 73.50308685302734]
ws:[-0.8021397709846496, 9.371926832199097]
memory len:10000
memory used:3160.0
now epsilon is 0.08210039185941975, the reward is 194.33333333333334 with loss [18.117721796035767, 22.816990852355957] in episode 1970
Report: 
rewardSum:194.33333333333334
loss:[18.117721796035767, 22.816990852355957]
policies:[0, 4, 0]
qAverage:[0.0, 73.6555679321289]
ws:[-0.5332701623439788, 11.327253556251526]
memory len:10000
memory used:3160.0
now epsilon is 0.08201832225007633, the reward is 194.33333333333334 with loss [18.352800130844116, 22.43165636062622] in episode 1971
Report: 
rewardSum:194.33333333333334
loss:[18.352800130844116, 22.43165636062622]
policies:[0, 3, 1]
qAverage:[0.0, 71.46148109436035]
ws:[-1.3873344883322716, 9.227969974279404]
memory len:10000
memory used:3160.0
now epsilon is 0.0819158505959014, the reward is 193.33333333333334 with loss [27.47221803665161, 26.427347421646118] in episode 1972
Report: 
rewardSum:193.33333333333334
loss:[27.47221803665161, 26.427347421646118]
policies:[0, 4, 1]
qAverage:[0.0, 74.4285873413086]
ws:[-0.7815948583185672, 8.618825125694276]
memory len:10000
memory used:3160.0
now epsilon is 0.08183396545863006, the reward is 46.33333333333334 with loss [22.113568782806396, 22.141295433044434] in episode 1973
Report: 
rewardSum:46.33333333333334
loss:[22.113568782806396, 22.141295433044434]
policies:[0, 3, 1]
qAverage:[0.0, 66.14936065673828]
ws:[1.2146039009094238, 5.319833993911743]
memory len:10000
memory used:3160.0
now epsilon is 0.08175216217579419, the reward is 194.33333333333334 with loss [25.616203784942627, 25.557283401489258] in episode 1974
Report: 
rewardSum:194.33333333333334
loss:[25.616203784942627, 25.557283401489258]
policies:[0, 3, 1]
qAverage:[0.0, 71.81204414367676]
ws:[0.15678143501281738, 12.98056936264038]
memory len:10000
memory used:3160.0
now epsilon is 0.08167044066557004, the reward is 194.33333333333334 with loss [21.819186210632324, 19.08586597442627] in episode 1975
Report: 
rewardSum:194.33333333333334
loss:[21.819186210632324, 19.08586597442627]
policies:[0, 4, 0]
qAverage:[0.0, 74.44762878417968]
ws:[0.43806943893432615, 12.456396198272705]
memory len:10000
memory used:3160.0
now epsilon is 0.08158880084621566, the reward is 194.33333333333334 with loss [20.13315486907959, 28.690165519714355] in episode 1976
Report: 
rewardSum:194.33333333333334
loss:[20.13315486907959, 28.690165519714355]
policies:[1, 3, 0]
qAverage:[0.0, 66.00489234924316]
ws:[2.418510138988495, 10.133427858352661]
memory len:10000
memory used:3160.0
now epsilon is 0.08150724263607079, the reward is 194.33333333333334 with loss [22.702481746673584, 21.125441551208496] in episode 1977
Report: 
rewardSum:194.33333333333334
loss:[22.702481746673584, 21.125441551208496]
policies:[0, 4, 0]
qAverage:[0.0, 74.01507263183593]
ws:[1.2911341667175293, 14.653024673461914]
memory len:10000
memory used:3160.0
now epsilon is 0.08142576595355684, the reward is 194.33333333333334 with loss [33.43311643600464, 31.229578495025635] in episode 1978
Report: 
rewardSum:194.33333333333334
loss:[33.43311643600464, 31.229578495025635]
policies:[0, 4, 0]
qAverage:[0.0, 76.25376892089844]
ws:[0.11004104614257812, 11.04903016090393]
memory len:10000
memory used:3160.0
now epsilon is 0.08134437071717673, the reward is 194.33333333333334 with loss [21.657676458358765, 26.453634023666382] in episode 1979
Report: 
rewardSum:194.33333333333334
loss:[21.657676458358765, 26.453634023666382]
policies:[0, 4, 0]
qAverage:[0.0, 64.95559120178223]
ws:[1.3937786519527435, 6.3234753012657166]
memory len:10000
memory used:3160.0
now epsilon is 0.0812630568455149, the reward is 194.33333333333334 with loss [12.611199259757996, 27.82813024520874] in episode 1980
Report: 
rewardSum:194.33333333333334
loss:[12.611199259757996, 27.82813024520874]
policies:[1, 3, 0]
qAverage:[11.307261657714843, 59.867652893066406]
ws:[-0.900624006986618, 7.552040219306946]
memory len:10000
memory used:3161.0
now epsilon is 0.08118182425723708, the reward is 194.33333333333334 with loss [30.934058666229248, 19.532753348350525] in episode 1981
Report: 
rewardSum:194.33333333333334
loss:[30.934058666229248, 19.532753348350525]
policies:[0, 4, 0]
qAverage:[0.0, 74.91071014404297]
ws:[0.5591077580116689, 12.226741403341293]
memory len:10000
memory used:3161.0
now epsilon is 0.0811006728710904, the reward is 194.33333333333334 with loss [27.418971300125122, 23.83144974708557] in episode 1982
Report: 
rewardSum:194.33333333333334
loss:[27.418971300125122, 23.83144974708557]
policies:[0, 4, 0]
qAverage:[0.0, 74.3826400756836]
ws:[1.771989220380783, 13.917863750457764]
memory len:10000
memory used:3161.0
now epsilon is 0.08101960260590317, the reward is 194.33333333333334 with loss [25.6991286277771, 24.558210849761963] in episode 1983
Report: 
rewardSum:194.33333333333334
loss:[25.6991286277771, 24.558210849761963]
policies:[0, 4, 0]
qAverage:[0.0, 72.47319602966309]
ws:[0.9739170968532562, 12.885364651679993]
memory len:10000
memory used:3160.0
now epsilon is 0.08093861338058486, the reward is 194.33333333333334 with loss [27.860363483428955, 24.539279460906982] in episode 1984
Report: 
rewardSum:194.33333333333334
loss:[27.860363483428955, 24.539279460906982]
policies:[0, 4, 0]
qAverage:[0.0, 77.55673675537109]
ws:[1.2774717569351197, 13.985447120666503]
memory len:10000
memory used:3160.0
now epsilon is 0.08085770511412595, the reward is 194.33333333333334 with loss [15.922601103782654, 26.064833641052246] in episode 1985
Report: 
rewardSum:194.33333333333334
loss:[15.922601103782654, 26.064833641052246]
policies:[0, 4, 0]
qAverage:[0.0, 69.91300582885742]
ws:[-0.6280274093151093, 10.47553277015686]
memory len:10000
memory used:3160.0
now epsilon is 0.08077687772559797, the reward is 194.33333333333334 with loss [26.218130588531494, 19.32334852218628] in episode 1986
Report: 
rewardSum:194.33333333333334
loss:[26.218130588531494, 19.32334852218628]
policies:[0, 4, 0]
qAverage:[0.0, 71.4956169128418]
ws:[-0.8844329861458391, 12.588505327701569]
memory len:10000
memory used:3161.0
now epsilon is 0.0806961311341533, the reward is 194.33333333333334 with loss [22.93710994720459, 15.590378761291504] in episode 1987
Report: 
rewardSum:194.33333333333334
loss:[22.93710994720459, 15.590378761291504]
policies:[0, 4, 0]
qAverage:[0.0, 76.05252532958984]
ws:[-1.7118203520774842, 8.829198479652405]
memory len:10000
memory used:3160.0
now epsilon is 0.08061546525902515, the reward is 194.33333333333334 with loss [20.947649478912354, 26.692143440246582] in episode 1988
Report: 
rewardSum:194.33333333333334
loss:[20.947649478912354, 26.692143440246582]
policies:[0, 3, 1]
qAverage:[0.0, 68.91105270385742]
ws:[-0.7711090743541718, 2.718638151884079]
memory len:10000
memory used:3160.0
now epsilon is 0.08053488001952745, the reward is 194.33333333333334 with loss [22.506844997406006, 27.92779016494751] in episode 1989
Report: 
rewardSum:194.33333333333334
loss:[22.506844997406006, 27.92779016494751]
policies:[0, 4, 0]
qAverage:[0.0, 76.47790908813477]
ws:[-1.9329500496387482, 9.74006974697113]
memory len:10000
memory used:3161.0
############# STATE ###############
0-		9*		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.08045437533505483, the reward is 194.33333333333334 with loss [20.116384029388428, 20.93178129196167] in episode 1990
Report: 
rewardSum:194.33333333333334
loss:[20.116384029388428, 20.93178129196167]
policies:[0, 3, 1]
qAverage:[0.0, 71.50184440612793]
ws:[-2.0711628049612045, 7.833377589471638]
memory len:10000
memory used:3160.0
now epsilon is 0.08037395112508244, the reward is 194.33333333333334 with loss [20.689522743225098, 26.34014654159546] in episode 1991
Report: 
rewardSum:194.33333333333334
loss:[20.689522743225098, 26.34014654159546]
policies:[0, 4, 0]
qAverage:[0.0, 74.85557746887207]
ws:[-1.4715502858161926, 8.393442429602146]
memory len:10000
memory used:3161.0
now epsilon is 0.08029360730916599, the reward is 194.33333333333334 with loss [39.64012145996094, 25.831390857696533] in episode 1992
Report: 
rewardSum:194.33333333333334
loss:[39.64012145996094, 25.831390857696533]
policies:[0, 4, 0]
qAverage:[0.0, 76.22196655273437]
ws:[-0.785649299621582, 8.689948391914367]
memory len:10000
memory used:3161.0
now epsilon is 0.08017324214837206, the reward is 192.33333333333334 with loss [42.17883777618408, 33.661499977111816] in episode 1993
Report: 
rewardSum:192.33333333333334
loss:[42.17883777618408, 33.661499977111816]
policies:[0, 5, 1]
qAverage:[0.0, 78.83673095703125]
ws:[-0.21327157566944757, 8.165849049886068]
memory len:10000
memory used:3160.0
now epsilon is 0.08009309896617899, the reward is 194.33333333333334 with loss [19.75815713405609, 21.158036708831787] in episode 1994
Report: 
rewardSum:194.33333333333334
loss:[19.75815713405609, 21.158036708831787]
policies:[1, 3, 0]
qAverage:[0.0, 64.49028778076172]
ws:[1.0319807529449463, 5.169039090474446]
memory len:10000
memory used:3160.0
now epsilon is 0.08001303589711943, the reward is 194.33333333333334 with loss [22.04778814315796, 20.587363719940186] in episode 1995
Report: 
rewardSum:194.33333333333334
loss:[22.04778814315796, 20.587363719940186]
policies:[0, 4, 0]
qAverage:[0.0, 76.6696060180664]
ws:[-0.12270009517669678, 9.00861496925354]
memory len:10000
memory used:3161.0
now epsilon is 0.07993305286111028, the reward is 46.33333333333334 with loss [22.596501350402832, 26.624195098876953] in episode 1996
Report: 
rewardSum:46.33333333333334
loss:[22.596501350402832, 26.624195098876953]
policies:[1, 2, 1]
qAverage:[0.0, 56.561299641927086]
ws:[0.32181990146636963, 2.0822678804397583]
memory len:10000
memory used:3161.0
now epsilon is 0.07985314977814852, the reward is 194.33333333333334 with loss [26.90050220489502, 21.820277452468872] in episode 1997
Report: 
rewardSum:194.33333333333334
loss:[26.90050220489502, 21.820277452468872]
policies:[1, 3, 0]
qAverage:[0.0, 66.9762134552002]
ws:[0.07005861483048648, 3.1776815354824066]
memory len:10000
memory used:3160.0
now epsilon is 0.07977332656831104, the reward is 194.33333333333334 with loss [23.200071334838867, 29.33242130279541] in episode 1998
Report: 
rewardSum:194.33333333333334
loss:[23.200071334838867, 29.33242130279541]
policies:[0, 4, 0]
qAverage:[0.0, 76.35962677001953]
ws:[-0.6794645637273788, 12.963573455810547]
memory len:10000
memory used:3160.0
now epsilon is 0.07969358315175468, the reward is 194.33333333333334 with loss [28.699525833129883, 27.006137371063232] in episode 1999
Report: 
rewardSum:194.33333333333334
loss:[28.699525833129883, 27.006137371063232]
policies:[0, 3, 1]
qAverage:[0.0, 73.35611343383789]
ws:[-1.5435553714632988, 9.980392694473267]
memory len:10000
memory used:3162.0
now epsilon is 0.07961391944871608, the reward is 194.33333333333334 with loss [37.40054702758789, 25.573345184326172] in episode 2000
Report: 
rewardSum:194.33333333333334
loss:[37.40054702758789, 25.573345184326172]
policies:[0, 4, 0]
qAverage:[0.0, 76.702490234375]
ws:[-0.9037043511867523, 9.971878147125244]
memory len:10000
memory used:3162.0
now epsilon is 0.07953433537951161, the reward is 194.33333333333334 with loss [17.849934577941895, 28.519163608551025] in episode 2001
Report: 
rewardSum:194.33333333333334
loss:[17.849934577941895, 28.519163608551025]
policies:[1, 3, 0]
qAverage:[0.0, 73.70808982849121]
ws:[-0.6853375658392906, 10.565969467163086]
memory len:10000
memory used:3162.0
now epsilon is 0.07945483086453729, the reward is 194.33333333333334 with loss [29.85170555114746, 28.468820095062256] in episode 2002
Report: 
rewardSum:194.33333333333334
loss:[29.85170555114746, 28.468820095062256]
policies:[0, 4, 0]
qAverage:[0.0, 74.98644256591797]
ws:[-0.6676473394036293, 11.18101155757904]
memory len:10000
memory used:3162.0
now epsilon is 0.07937540582426873, the reward is 194.33333333333334 with loss [29.150283813476562, 22.910300731658936] in episode 2003
Report: 
rewardSum:194.33333333333334
loss:[29.150283813476562, 22.910300731658936]
policies:[0, 4, 0]
qAverage:[0.0, 76.74556579589844]
ws:[-0.08212855234742164, 11.93974140956998]
memory len:10000
memory used:3162.0
now epsilon is 0.079296060179261, the reward is 194.33333333333334 with loss [18.250207901000977, 22.14258575439453] in episode 2004
Report: 
rewardSum:194.33333333333334
loss:[18.250207901000977, 22.14258575439453]
policies:[0, 3, 1]
qAverage:[0.0, 66.93930244445801]
ws:[0.4459444284439087, 4.6451422572135925]
memory len:10000
memory used:3161.0
now epsilon is 0.07921679385014861, the reward is 194.33333333333334 with loss [17.51891827583313, 22.49123525619507] in episode 2005
Report: 
rewardSum:194.33333333333334
loss:[17.51891827583313, 22.49123525619507]
policies:[0, 4, 0]
qAverage:[0.0, 67.68418884277344]
ws:[1.2183908820152283, 7.581799149513245]
memory len:10000
memory used:3161.0
now epsilon is 0.07913760675764542, the reward is 194.33333333333334 with loss [26.29189157485962, 21.63208508491516] in episode 2006
Report: 
rewardSum:194.33333333333334
loss:[26.29189157485962, 21.63208508491516]
policies:[0, 4, 0]
qAverage:[0.0, 76.90310363769531]
ws:[-0.7704450398683548, 10.377978587150574]
memory len:10000
memory used:3161.0
now epsilon is 0.07905849882254452, the reward is 194.33333333333334 with loss [26.365731239318848, 17.6732177734375] in episode 2007
Report: 
rewardSum:194.33333333333334
loss:[26.365731239318848, 17.6732177734375]
policies:[0, 4, 0]
qAverage:[0.0, 76.2723388671875]
ws:[-0.7885320901870727, 11.524996614456176]
memory len:10000
memory used:3161.0
now epsilon is 0.0789399851669522, the reward is 192.33333333333334 with loss [31.088923931121826, 29.028503894805908] in episode 2008
Report: 
rewardSum:192.33333333333334
loss:[31.088923931121826, 29.028503894805908]
policies:[0, 5, 1]
qAverage:[0.0, 79.61444473266602]
ws:[-0.3410886066655318, 11.036204814910889]
memory len:10000
memory used:3161.0
now epsilon is 0.07886107477934626, the reward is 194.33333333333334 with loss [21.735836029052734, 20.19156265258789] in episode 2009
Report: 
rewardSum:194.33333333333334
loss:[21.735836029052734, 20.19156265258789]
policies:[0, 4, 0]
qAverage:[0.0, 68.83395576477051]
ws:[-0.16286537237465382, 3.877544552087784]
memory len:10000
memory used:3160.0
now epsilon is 0.07876254771172332, the reward is 45.33333333333334 with loss [22.124534845352173, 26.080634117126465] in episode 2010
Report: 
rewardSum:45.33333333333334
loss:[22.124534845352173, 26.080634117126465]
policies:[0, 3, 2]
qAverage:[0.0, 67.7744140625]
ws:[-0.867267295718193, 3.4822215884923935]
memory len:10000
memory used:3160.0
now epsilon is 0.07868381469504465, the reward is 194.33333333333334 with loss [22.227450370788574, 23.462671279907227] in episode 2011
Report: 
rewardSum:194.33333333333334
loss:[22.227450370788574, 23.462671279907227]
policies:[0, 2, 2]
qAverage:[0.0, 68.45532989501953]
ws:[-2.6113454302152, 13.116335233052572]
memory len:10000
memory used:3161.0
now epsilon is 0.07860516038186269, the reward is 194.33333333333334 with loss [15.61527943611145, 26.573623180389404] in episode 2012
Report: 
rewardSum:194.33333333333334
loss:[15.61527943611145, 26.573623180389404]
policies:[0, 4, 0]
qAverage:[0.0, 78.82366638183593]
ws:[-2.680435371398926, 7.563532453775406]
memory len:10000
memory used:3161.0
now epsilon is 0.07852658469350347, the reward is 194.33333333333334 with loss [17.632245659828186, 25.31455683708191] in episode 2013
Report: 
rewardSum:194.33333333333334
loss:[17.632245659828186, 25.31455683708191]
policies:[1, 3, 0]
qAverage:[0.0, 68.36734962463379]
ws:[-0.44405829533934593, 4.937714397907257]
memory len:10000
memory used:3161.0
now epsilon is 0.07844808755137163, the reward is 194.33333333333334 with loss [34.682934284210205, 23.49450445175171] in episode 2014
Report: 
rewardSum:194.33333333333334
loss:[34.682934284210205, 23.49450445175171]
policies:[0, 4, 0]
qAverage:[0.0, 78.2369598388672]
ws:[-1.500386607646942, 9.310969161987305]
memory len:10000
memory used:3161.0
now epsilon is 0.0783696688769504, the reward is 194.33333333333334 with loss [28.249838829040527, 22.95791792869568] in episode 2015
Report: 
rewardSum:194.33333333333334
loss:[28.249838829040527, 22.95791792869568]
policies:[0, 3, 1]
qAverage:[0.0, 68.70022392272949]
ws:[0.28579176450148225, 5.547780871391296]
memory len:10000
memory used:3161.0
now epsilon is 0.07829132859180149, the reward is 194.33333333333334 with loss [20.530012845993042, 26.036683082580566] in episode 2016
Report: 
rewardSum:194.33333333333334
loss:[20.530012845993042, 26.036683082580566]
policies:[0, 4, 0]
qAverage:[0.0, 77.43227081298828]
ws:[-0.5869207739830017, 11.169615221023559]
memory len:10000
memory used:3161.0
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.07819351335091064, the reward is 193.33333333333334 with loss [27.356934547424316, 31.018222332000732] in episode 2017
Report: 
rewardSum:193.33333333333334
loss:[27.356934547424316, 31.018222332000732]
policies:[0, 4, 1]
qAverage:[0.0, 72.18465232849121]
ws:[-1.3314663507044315, 11.443226397037506]
memory len:10000
memory used:3161.0
now epsilon is 0.07811534915524045, the reward is 194.33333333333334 with loss [24.204793691635132, 17.5121750831604] in episode 2018
Report: 
rewardSum:194.33333333333334
loss:[24.204793691635132, 17.5121750831604]
policies:[0, 4, 0]
qAverage:[0.0, 77.62262420654297]
ws:[-1.0988873481750487, 10.773310708999634]
memory len:10000
memory used:3161.0
now epsilon is 0.07799824934024095, the reward is 192.33333333333334 with loss [26.351704835891724, 40.13350296020508] in episode 2019
Report: 
rewardSum:192.33333333333334
loss:[26.351704835891724, 40.13350296020508]
policies:[0, 5, 1]
qAverage:[0.0, 77.30914611816407]
ws:[-0.36540172100067136, 13.989929437637329]
memory len:10000
memory used:3161.0
now epsilon is 0.07792028033536963, the reward is 194.33333333333334 with loss [22.691264152526855, 23.541614532470703] in episode 2020
Report: 
rewardSum:194.33333333333334
loss:[22.691264152526855, 23.541614532470703]
policies:[0, 4, 0]
qAverage:[0.0, 78.7742416381836]
ws:[-0.6771397709846496, 11.215393161773681]
memory len:10000
memory used:3161.0
now epsilon is 0.07784238927026968, the reward is 194.33333333333334 with loss [20.386174201965332, 31.440373420715332] in episode 2021
Report: 
rewardSum:194.33333333333334
loss:[20.386174201965332, 31.440373420715332]
policies:[0, 4, 0]
qAverage:[0.0, 70.68798637390137]
ws:[-0.767099916934967, 11.852925181388855]
memory len:10000
memory used:3161.0
now epsilon is 0.07776457606703055, the reward is 194.33333333333334 with loss [30.18008542060852, 20.046961069107056] in episode 2022
Report: 
rewardSum:194.33333333333334
loss:[30.18008542060852, 20.046961069107056]
policies:[0, 4, 0]
qAverage:[0.0, 69.73871803283691]
ws:[0.7348184287548065, 6.118881940841675]
memory len:10000
memory used:3161.0
now epsilon is 0.07768684064781958, the reward is 194.33333333333334 with loss [25.89467716217041, 24.559901237487793] in episode 2023
Report: 
rewardSum:194.33333333333334
loss:[25.89467716217041, 24.559901237487793]
policies:[0, 3, 1]
qAverage:[0.0, 60.89080047607422]
ws:[0.861089825630188, 4.460461457570394]
memory len:10000
memory used:3161.0
now epsilon is 0.07760918293488189, the reward is 194.33333333333334 with loss [23.855610609054565, 25.433265686035156] in episode 2024
Report: 
rewardSum:194.33333333333334
loss:[23.855610609054565, 25.433265686035156]
policies:[0, 4, 0]
qAverage:[0.0, 77.50987701416015]
ws:[-0.742231285572052, 9.732801389694213]
memory len:10000
memory used:3161.0
now epsilon is 0.07753160285054034, the reward is 194.33333333333334 with loss [20.918336629867554, 22.35297679901123] in episode 2025
Report: 
rewardSum:194.33333333333334
loss:[20.918336629867554, 22.35297679901123]
policies:[0, 4, 0]
qAverage:[0.0, 78.06383666992187]
ws:[-1.485003298521042, 8.435139220952987]
memory len:10000
memory used:3161.0
now epsilon is 0.07745410031719545, the reward is 194.33333333333334 with loss [23.219894409179688, 23.320096015930176] in episode 2026
Report: 
rewardSum:194.33333333333334
loss:[23.219894409179688, 23.320096015930176]
policies:[0, 4, 0]
qAverage:[0.0, 73.95614624023438]
ws:[-1.6050007343292236, 9.860050350427628]
memory len:10000
memory used:3161.0
now epsilon is 0.07733799175573886, the reward is 192.33333333333334 with loss [34.0896942615509, 31.2861328125] in episode 2027
Report: 
rewardSum:192.33333333333334
loss:[34.0896942615509, 31.2861328125]
policies:[2, 3, 1]
qAverage:[0.0, 76.68739700317383]
ws:[-0.9556699395179749, 12.590080499649048]
memory len:10000
memory used:3161.0
now epsilon is 0.0772606827608967, the reward is 194.33333333333334 with loss [25.1444890499115, 24.41756772994995] in episode 2028
Report: 
rewardSum:194.33333333333334
loss:[25.1444890499115, 24.41756772994995]
policies:[0, 4, 0]
qAverage:[0.0, 77.40704193115235]
ws:[-0.6156891822814942, 11.841834354400635]
memory len:10000
memory used:3161.0
now epsilon is 0.07718345104606336, the reward is 194.33333333333334 with loss [28.888371467590332, 18.37874698638916] in episode 2029
Report: 
rewardSum:194.33333333333334
loss:[28.888371467590332, 18.37874698638916]
policies:[0, 4, 0]
qAverage:[0.0, 77.30696563720703]
ws:[-0.586622828245163, 14.0137469291687]
memory len:10000
memory used:3161.0
now epsilon is 0.07710629653398779, the reward is 194.33333333333334 with loss [20.498499393463135, 27.39208173751831] in episode 2030
Report: 
rewardSum:194.33333333333334
loss:[20.498499393463135, 27.39208173751831]
policies:[0, 4, 0]
qAverage:[0.0, 77.88053894042969]
ws:[-1.796573132276535, 10.051777791976928]
memory len:10000
memory used:3160.0
now epsilon is 0.07702921914749616, the reward is 194.33333333333334 with loss [19.935847282409668, 24.81917428970337] in episode 2031
Report: 
rewardSum:194.33333333333334
loss:[19.935847282409668, 24.81917428970337]
policies:[0, 4, 0]
qAverage:[0.0, 70.66841125488281]
ws:[-2.1733152270317078, 9.509055495262146]
memory len:10000
memory used:3161.0
now epsilon is 0.07695221880949184, the reward is 194.33333333333334 with loss [21.097270727157593, 20.935776233673096] in episode 2032
Report: 
rewardSum:194.33333333333334
loss:[21.097270727157593, 20.935776233673096]
policies:[0, 4, 0]
qAverage:[0.0, 76.80274505615235]
ws:[-2.3752318620681763, 8.430788922309876]
memory len:10000
memory used:3160.0
now epsilon is 0.0768752954429552, the reward is 194.33333333333334 with loss [26.510432720184326, 23.924924850463867] in episode 2033
Report: 
rewardSum:194.33333333333334
loss:[26.510432720184326, 23.924924850463867]
policies:[0, 4, 0]
qAverage:[0.0, 75.3841609954834]
ws:[-2.274327903985977, 10.765790700912476]
memory len:10000
memory used:3160.0
now epsilon is 0.07679844897094365, the reward is 194.33333333333334 with loss [22.069188594818115, 16.45397162437439] in episode 2034
Report: 
rewardSum:194.33333333333334
loss:[22.069188594818115, 16.45397162437439]
policies:[0, 4, 0]
qAverage:[0.0, 67.85794830322266]
ws:[0.7570407018065453, 8.87471753358841]
memory len:10000
memory used:3161.0
now epsilon is 0.07672167931659146, the reward is 194.33333333333334 with loss [18.60879874229431, 21.53855323791504] in episode 2035
Report: 
rewardSum:194.33333333333334
loss:[18.60879874229431, 21.53855323791504]
policies:[0, 4, 0]
qAverage:[0.0, 77.33104705810547]
ws:[-1.030790787935257, 11.606877994537353]
memory len:10000
memory used:3161.0
now epsilon is 0.07664498640310981, the reward is 194.33333333333334 with loss [17.222074270248413, 17.00455904006958] in episode 2036
Report: 
rewardSum:194.33333333333334
loss:[17.222074270248413, 17.00455904006958]
policies:[0, 4, 0]
qAverage:[0.0, 75.96785278320313]
ws:[-0.1633739471435547, 15.42019281387329]
memory len:10000
memory used:3160.0
now epsilon is 0.0765683701537866, the reward is 194.33333333333334 with loss [22.90031909942627, 28.92466449737549] in episode 2037
Report: 
rewardSum:194.33333333333334
loss:[22.90031909942627, 28.92466449737549]
policies:[0, 4, 0]
qAverage:[0.0, 77.27815399169921]
ws:[-0.06393544673919678, 15.347999382019044]
memory len:10000
memory used:3160.0
now epsilon is 0.07649183049198642, the reward is 194.33333333333334 with loss [26.946226119995117, 23.654929637908936] in episode 2038
Report: 
rewardSum:194.33333333333334
loss:[26.946226119995117, 23.654929637908936]
policies:[0, 4, 0]
qAverage:[0.0, 76.16680145263672]
ws:[-0.4264840245246887, 13.997315597534179]
memory len:10000
memory used:3160.0
now epsilon is 0.07641536734115044, the reward is 194.33333333333334 with loss [28.62186098098755, 26.18156385421753] in episode 2039
Report: 
rewardSum:194.33333333333334
loss:[28.62186098098755, 26.18156385421753]
policies:[0, 4, 0]
qAverage:[0.0, 76.50909881591797]
ws:[-1.1825670719146728, 12.362185287475587]
memory len:10000
memory used:3160.0
now epsilon is 0.07633898062479638, the reward is 194.33333333333334 with loss [25.985612869262695, 26.403464317321777] in episode 2040
Report: 
rewardSum:194.33333333333334
loss:[25.985612869262695, 26.403464317321777]
policies:[0, 4, 0]
qAverage:[0.0, 72.73995399475098]
ws:[-2.467148393392563, 8.216039925813675]
memory len:10000
memory used:3161.0
now epsilon is 0.07626267026651844, the reward is 46.33333333333334 with loss [26.976219654083252, 24.54188346862793] in episode 2041
Report: 
rewardSum:46.33333333333334
loss:[26.976219654083252, 24.54188346862793]
policies:[0, 3, 1]
qAverage:[0.0, 69.17300224304199]
ws:[-0.8384012877941132, 1.3172326236963272]
memory len:10000
memory used:3161.0
now epsilon is 0.07618643618998716, the reward is 46.33333333333334 with loss [27.15607786178589, 23.27982258796692] in episode 2042
Report: 
rewardSum:46.33333333333334
loss:[27.15607786178589, 23.27982258796692]
policies:[1, 2, 1]
qAverage:[0.0, 63.32288106282552]
ws:[0.27856027086575824, 6.032692273457845]
memory len:10000
memory used:3161.0
now epsilon is 0.0761102783189494, the reward is 194.33333333333334 with loss [30.161144733428955, 23.719415187835693] in episode 2043
Report: 
rewardSum:194.33333333333334
loss:[30.161144733428955, 23.719415187835693]
policies:[0, 3, 1]
qAverage:[0.0, 72.87539863586426]
ws:[-2.704626888036728, 6.4334516525268555]
memory len:10000
memory used:3161.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19*		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.07603419657722824, the reward is 194.33333333333334 with loss [30.530858516693115, 26.349604606628418] in episode 2044
Report: 
rewardSum:194.33333333333334
loss:[30.530858516693115, 26.349604606628418]
policies:[0, 4, 0]
qAverage:[0.0, 76.43327331542969]
ws:[-2.381558692455292, 6.752026402950287]
memory len:10000
memory used:3161.0
now epsilon is 0.07595819088872291, the reward is 194.33333333333334 with loss [22.625813961029053, 23.91284990310669] in episode 2045
Report: 
rewardSum:194.33333333333334
loss:[22.625813961029053, 23.91284990310669]
policies:[0, 4, 0]
qAverage:[0.0, 76.01091766357422]
ws:[-2.211295801401138, 9.350289106369019]
memory len:10000
memory used:3161.0
now epsilon is 0.07588226117740868, the reward is 194.33333333333334 with loss [25.801753520965576, 23.16053533554077] in episode 2046
Report: 
rewardSum:194.33333333333334
loss:[25.801753520965576, 23.16053533554077]
policies:[0, 4, 0]
qAverage:[0.0, 77.11274108886718]
ws:[-2.6857110023498536, 7.176796507835388]
memory len:10000
memory used:3160.0
now epsilon is 0.07578745576549505, the reward is 193.33333333333334 with loss [27.14206075668335, 32.46230745315552] in episode 2047
Report: 
rewardSum:193.33333333333334
loss:[27.14206075668335, 32.46230745315552]
policies:[0, 4, 1]
qAverage:[0.0, 76.48264617919922]
ws:[-1.45614046305418, 11.107176780700684]
memory len:10000
memory used:3160.0
now epsilon is 0.07571169672528905, the reward is 194.33333333333334 with loss [27.57939577102661, 17.995517015457153] in episode 2048
Report: 
rewardSum:194.33333333333334
loss:[27.57939577102661, 17.995517015457153]
policies:[0, 4, 0]
qAverage:[0.0, 76.45081787109375]
ws:[-0.5587739631533623, 10.34576644897461]
memory len:10000
memory used:3160.0
now epsilon is 0.07559820013626134, the reward is 192.33333333333334 with loss [29.96484088897705, 37.2827787399292] in episode 2049
Report: 
rewardSum:192.33333333333334
loss:[29.96484088897705, 37.2827787399292]
policies:[0, 5, 1]
qAverage:[0.0, 77.01645787556966]
ws:[1.2536065777142842, 9.019136468569437]
memory len:10000
memory used:3161.0
now epsilon is 0.07550374962315537, the reward is 193.33333333333334 with loss [26.590994358062744, 25.9229793548584] in episode 2050
Report: 
rewardSum:193.33333333333334
loss:[26.590994358062744, 25.9229793548584]
policies:[0, 4, 1]
qAverage:[0.0, 76.08751220703125]
ws:[2.3304412841796873, 12.896369457244873]
memory len:10000
memory used:3160.0
now epsilon is 0.07542827418271963, the reward is 194.33333333333334 with loss [19.493303775787354, 21.116584062576294] in episode 2051
Report: 
rewardSum:194.33333333333334
loss:[19.493303775787354, 21.116584062576294]
policies:[0, 4, 0]
qAverage:[0.0, 75.86736145019532]
ws:[1.6545516014099122, 11.045904731750488]
memory len:10000
memory used:3161.0
now epsilon is 0.07535287418942577, the reward is 194.33333333333334 with loss [25.101611614227295, 25.996234893798828] in episode 2052
Report: 
rewardSum:194.33333333333334
loss:[25.101611614227295, 25.996234893798828]
policies:[1, 3, 0]
qAverage:[12.197187042236328, 59.4497787475586]
ws:[0.5636107206344605, 9.684885787963868]
memory len:10000
memory used:3162.0
now epsilon is 0.0752775495678549, the reward is 194.33333333333334 with loss [26.997660160064697, 19.972129344940186] in episode 2053
Report: 
rewardSum:194.33333333333334
loss:[26.997660160064697, 19.972129344940186]
policies:[1, 3, 0]
qAverage:[12.091000366210938, 59.38357849121094]
ws:[0.08636424541473389, 9.454367125034333]
memory len:10000
memory used:3161.0
now epsilon is 0.0752023002426636, the reward is 194.33333333333334 with loss [19.932612419128418, 23.65420126914978] in episode 2054
Report: 
rewardSum:194.33333333333334
loss:[19.932612419128418, 23.65420126914978]
policies:[1, 3, 0]
qAverage:[15.038554191589355, 49.64183044433594]
ws:[0.169293612241745, 11.560800969600677]
memory len:10000
memory used:3161.0
now epsilon is 0.07512712613858369, the reward is 194.33333333333334 with loss [19.83735680580139, 22.666590213775635] in episode 2055
Report: 
rewardSum:194.33333333333334
loss:[19.83735680580139, 22.666590213775635]
policies:[0, 4, 0]
qAverage:[0.0, 66.6944408416748]
ws:[2.116098552942276, 8.206950604915619]
memory len:10000
memory used:3161.0
now epsilon is 0.07505202718042228, the reward is 194.33333333333334 with loss [17.71575927734375, 23.31501579284668] in episode 2056
Report: 
rewardSum:194.33333333333334
loss:[17.71575927734375, 23.31501579284668]
policies:[0, 4, 0]
qAverage:[0.0, 75.17947692871094]
ws:[0.9394149303436279, 10.316164541244508]
memory len:10000
memory used:3161.0
now epsilon is 0.0749770032930616, the reward is 194.33333333333334 with loss [20.679224491119385, 21.000176906585693] in episode 2057
Report: 
rewardSum:194.33333333333334
loss:[20.679224491119385, 21.000176906585693]
policies:[0, 4, 0]
qAverage:[0.0, 74.33544464111328]
ws:[1.5067461490631104, 12.471387386322021]
memory len:10000
memory used:3161.0
now epsilon is 0.07490205440145901, the reward is 194.33333333333334 with loss [33.62981605529785, 23.26621150970459] in episode 2058
Report: 
rewardSum:194.33333333333334
loss:[33.62981605529785, 23.26621150970459]
policies:[0, 4, 0]
qAverage:[0.0, 69.53384971618652]
ws:[0.06139039993286133, 9.56424731016159]
memory len:10000
memory used:3162.0
now epsilon is 0.07482718043064689, the reward is 194.33333333333334 with loss [23.8300678730011, 21.570838451385498] in episode 2059
Report: 
rewardSum:194.33333333333334
loss:[23.8300678730011, 21.570838451385498]
policies:[1, 3, 0]
qAverage:[0.0, 61.60119374593099]
ws:[0.09789980947971344, 3.2898642222086587]
memory len:10000
memory used:3162.0
now epsilon is 0.0747523813057325, the reward is 194.33333333333334 with loss [19.26971673965454, 24.52322816848755] in episode 2060
Report: 
rewardSum:194.33333333333334
loss:[19.26971673965454, 24.52322816848755]
policies:[1, 3, 0]
qAverage:[12.144617462158203, 58.51481475830078]
ws:[-2.0948437452316284, 5.109023571014404]
memory len:10000
memory used:3162.0
now epsilon is 0.07467765695189804, the reward is 194.33333333333334 with loss [19.91349506378174, 24.663599967956543] in episode 2061
Report: 
rewardSum:194.33333333333334
loss:[19.91349506378174, 24.663599967956543]
policies:[0, 3, 1]
qAverage:[0.0, 74.30110549926758]
ws:[-2.903731346130371, 5.850482881069183]
memory len:10000
memory used:3162.0
now epsilon is 0.07460300729440045, the reward is 194.33333333333334 with loss [23.0719256401062, 25.40441393852234] in episode 2062
Report: 
rewardSum:194.33333333333334
loss:[23.0719256401062, 25.40441393852234]
policies:[1, 2, 1]
qAverage:[0.0, 67.72652180989583]
ws:[-3.5424915154774985, 7.138072649637858]
memory len:10000
memory used:3162.0
now epsilon is 0.07452843225857139, the reward is 194.33333333333334 with loss [22.127197742462158, 27.988193035125732] in episode 2063
Report: 
rewardSum:194.33333333333334
loss:[22.127197742462158, 27.988193035125732]
policies:[1, 3, 0]
qAverage:[12.024242401123047, 59.35243377685547]
ws:[-2.765953516960144, 4.842457342147827]
memory len:10000
memory used:3162.0
now epsilon is 0.07441670945730301, the reward is 192.33333333333334 with loss [33.30678582191467, 34.06920051574707] in episode 2064
Report: 
rewardSum:192.33333333333334
loss:[33.30678582191467, 34.06920051574707]
policies:[1, 4, 1]
qAverage:[0.0, 71.63367004394532]
ws:[-0.6906091884244233, 2.7530836544930937]
memory len:10000
memory used:3162.0
now epsilon is 0.07434232064946102, the reward is 194.33333333333334 with loss [19.323036193847656, 22.65955114364624] in episode 2065
Report: 
rewardSum:194.33333333333334
loss:[19.323036193847656, 22.65955114364624]
policies:[0, 4, 0]
qAverage:[0.0, 74.65509643554688]
ws:[-1.0472206162288784, 7.503553652763367]
memory len:10000
memory used:3183.0
now epsilon is 0.07426800620253571, the reward is 194.33333333333334 with loss [23.62989902496338, 25.207314491271973] in episode 2066
Report: 
rewardSum:194.33333333333334
loss:[23.62989902496338, 25.207314491271973]
policies:[0, 4, 0]
qAverage:[0.0, 75.4309799194336]
ws:[-0.7923337996006012, 7.720591831207275]
memory len:10000
memory used:3196.0
now epsilon is 0.07419376604219405, the reward is 194.33333333333334 with loss [20.52851366996765, 26.811436891555786] in episode 2067
Report: 
rewardSum:194.33333333333334
loss:[20.52851366996765, 26.811436891555786]
policies:[0, 4, 0]
qAverage:[0.0, 73.3030776977539]
ws:[-1.1449387401342392, 6.743454551696777]
memory len:10000
memory used:3162.0
now epsilon is 0.07411960009417731, the reward is 194.33333333333334 with loss [30.596927642822266, 23.75442409515381] in episode 2068
Report: 
rewardSum:194.33333333333334
loss:[30.596927642822266, 23.75442409515381]
policies:[0, 4, 0]
qAverage:[0.0, 73.69564208984374]
ws:[-1.1100200459361076, 8.044667494297027]
memory len:10000
memory used:3162.0
now epsilon is 0.07398998803546362, the reward is 191.33333333333334 with loss [46.03930711746216, 41.12029552459717] in episode 2069
Report: 
rewardSum:191.33333333333334
loss:[46.03930711746216, 41.12029552459717]
policies:[0, 5, 2]
qAverage:[0.0, 74.48895009358723]
ws:[-1.1747264315684636, 7.507428954044978]
memory len:10000
memory used:3161.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19*		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.0739160257890496, the reward is 194.33333333333334 with loss [22.760611534118652, 21.363699913024902] in episode 2070
Report: 
rewardSum:194.33333333333334
loss:[22.760611534118652, 21.363699913024902]
policies:[0, 3, 1]
qAverage:[0.0, 64.51763343811035]
ws:[0.8502360284328461, 4.294105887413025]
memory len:10000
memory used:3162.0
now epsilon is 0.07384213747715075, the reward is 194.33333333333334 with loss [26.02991533279419, 22.78969144821167] in episode 2071
Report: 
rewardSum:194.33333333333334
loss:[26.02991533279419, 22.78969144821167]
policies:[0, 4, 0]
qAverage:[0.0, 74.07993774414062]
ws:[0.036410045623779294, 7.892454338073731]
memory len:10000
memory used:3161.0
now epsilon is 0.07376832302586032, the reward is 194.33333333333334 with loss [25.09385919570923, 12.815983533859253] in episode 2072
Report: 
rewardSum:194.33333333333334
loss:[25.09385919570923, 12.815983533859253]
policies:[1, 2, 1]
qAverage:[0.0, 58.104248046875]
ws:[0.8239157199859619, 3.0149383544921875]
memory len:10000
memory used:3161.0
now epsilon is 0.07369458236134538, the reward is 194.33333333333334 with loss [18.028843879699707, 19.995983004570007] in episode 2073
Report: 
rewardSum:194.33333333333334
loss:[18.028843879699707, 19.995983004570007]
policies:[0, 4, 0]
qAverage:[0.0, 73.20643768310546]
ws:[-0.5316555500030518, 9.020113873481751]
memory len:10000
memory used:3161.0
now epsilon is 0.07362091540984682, the reward is 194.33333333333334 with loss [24.83950662612915, 24.98625898361206] in episode 2074
Report: 
rewardSum:194.33333333333334
loss:[24.83950662612915, 24.98625898361206]
policies:[0, 4, 0]
qAverage:[0.0, 73.87652435302735]
ws:[-1.504255441389978, 6.873913741111755]
memory len:10000
memory used:3161.0
now epsilon is 0.07354732209767924, the reward is 194.33333333333334 with loss [27.805274963378906, 17.647450923919678] in episode 2075
Report: 
rewardSum:194.33333333333334
loss:[27.805274963378906, 17.647450923919678]
policies:[0, 4, 0]
qAverage:[0.0, 73.01170959472657]
ws:[-1.7765651285648345, 7.136140060424805]
memory len:10000
memory used:3162.0
now epsilon is 0.07343707004216796, the reward is 192.33333333333334 with loss [31.623537302017212, 37.339118003845215] in episode 2076
Report: 
rewardSum:192.33333333333334
loss:[31.623537302017212, 37.339118003845215]
policies:[0, 4, 2]
qAverage:[0.0, 66.78364944458008]
ws:[-1.9814751595258713, 4.91880738735199]
memory len:10000
memory used:3162.0
now epsilon is 0.07332698326141311, the reward is 192.33333333333334 with loss [35.41007375717163, 35.441317558288574] in episode 2077
Report: 
rewardSum:192.33333333333334
loss:[35.41007375717163, 35.441317558288574]
policies:[0, 4, 2]
qAverage:[0.0, 68.11168975830078]
ws:[-1.407238280773163, 2.77510666847229]
memory len:10000
memory used:3162.0
now epsilon is 0.07325368377118778, the reward is 194.33333333333334 with loss [28.529456615447998, 25.047666549682617] in episode 2078
Report: 
rewardSum:194.33333333333334
loss:[28.529456615447998, 25.047666549682617]
policies:[1, 3, 0]
qAverage:[0.0, 68.45793914794922]
ws:[-2.4549480378627777, 7.166953295469284]
memory len:10000
memory used:3161.0
now epsilon is 0.07318045755296994, the reward is 194.33333333333334 with loss [23.77954864501953, 22.92310619354248] in episode 2079
Report: 
rewardSum:194.33333333333334
loss:[23.77954864501953, 22.92310619354248]
policies:[0, 4, 0]
qAverage:[0.0, 71.45951843261719]
ws:[-1.795392596721649, 9.726256513595581]
memory len:10000
memory used:3161.0
now epsilon is 0.07307075545045486, the reward is 192.33333333333334 with loss [31.34846329689026, 26.085326671600342] in episode 2080
Report: 
rewardSum:192.33333333333334
loss:[31.34846329689026, 26.085326671600342]
policies:[0, 5, 1]
qAverage:[0.0, 72.52871704101562]
ws:[-1.474897136290868, 9.813773314158121]
memory len:10000
memory used:3161.0
now epsilon is 0.07303422463965185, the reward is -1.0 with loss [11.248536109924316, 17.454256057739258] in episode 2081
Report: 
rewardSum:-1.0
loss:[11.248536109924316, 17.454256057739258]
policies:[0, 1, 1]
qAverage:[0.0, 37.94499969482422]
ws:[-0.11078255623579025, 1.6921581029891968]
memory len:10000
memory used:3161.0
now epsilon is 0.07296121779828209, the reward is 194.33333333333334 with loss [18.414804935455322, 24.698631286621094] in episode 2082
Report: 
rewardSum:194.33333333333334
loss:[18.414804935455322, 24.698631286621094]
policies:[0, 4, 0]
qAverage:[0.0, 71.72061920166016]
ws:[-1.3279279828071595, 11.188499259948731]
memory len:10000
memory used:3161.0
now epsilon is 0.0728882839363807, the reward is 46.33333333333334 with loss [20.821045875549316, 15.743590593338013] in episode 2083
Report: 
rewardSum:46.33333333333334
loss:[20.821045875549316, 15.743590593338013]
policies:[0, 3, 1]
qAverage:[0.0, 63.22135353088379]
ws:[0.5179457068443298, 7.671465754508972]
memory len:10000
memory used:3161.0
now epsilon is 0.07281542298099557, the reward is 194.33333333333334 with loss [24.903067588806152, 20.524170398712158] in episode 2084
Report: 
rewardSum:194.33333333333334
loss:[24.903067588806152, 20.524170398712158]
policies:[0, 4, 0]
qAverage:[0.0, 72.49674682617187]
ws:[-1.5486455723643302, 10.522112131118774]
memory len:10000
memory used:3160.0
now epsilon is 0.07274263485924752, the reward is 194.33333333333334 with loss [18.318517684936523, 19.77150058746338] in episode 2085
Report: 
rewardSum:194.33333333333334
loss:[18.318517684936523, 19.77150058746338]
policies:[0, 4, 0]
qAverage:[0.0, 62.81960678100586]
ws:[-0.09123077243566513, 6.396695703268051]
memory len:10000
memory used:3160.0
now epsilon is 0.07266991949833021, the reward is 194.33333333333334 with loss [31.17435359954834, 25.635652542114258] in episode 2086
Report: 
rewardSum:194.33333333333334
loss:[31.17435359954834, 25.635652542114258]
policies:[0, 4, 0]
qAverage:[0.0, 72.86125793457032]
ws:[-2.1900646090507507, 9.467367380857468]
memory len:10000
memory used:3160.0
now epsilon is 0.07259727682551012, the reward is 194.33333333333334 with loss [19.305238485336304, 23.84173560142517] in episode 2087
Report: 
rewardSum:194.33333333333334
loss:[19.305238485336304, 23.84173560142517]
policies:[1, 3, 0]
qAverage:[0.0, 68.25854301452637]
ws:[-3.3680909872055054, 7.662144124507904]
memory len:10000
memory used:3160.0
now epsilon is 0.07252470676812638, the reward is 194.33333333333334 with loss [27.67245388031006, 29.220215797424316] in episode 2088
Report: 
rewardSum:194.33333333333334
loss:[27.67245388031006, 29.220215797424316]
policies:[1, 2, 1]
qAverage:[0.0, 63.177886962890625]
ws:[-4.133867263793945, 9.982796669006348]
memory len:10000
memory used:3160.0
now epsilon is 0.07245220925359079, the reward is 194.33333333333334 with loss [21.618155479431152, 20.04884672164917] in episode 2089
Report: 
rewardSum:194.33333333333334
loss:[21.618155479431152, 20.04884672164917]
policies:[1, 3, 0]
qAverage:[0.0, 62.94614028930664]
ws:[-0.5366441160440445, 7.53893381357193]
memory len:10000
memory used:3160.0
now epsilon is 0.0723797842093877, the reward is 194.33333333333334 with loss [24.416282415390015, 30.364033699035645] in episode 2090
Report: 
rewardSum:194.33333333333334
loss:[24.416282415390015, 30.364033699035645]
policies:[0, 4, 0]
qAverage:[0.0, 71.53747863769532]
ws:[-2.3771107316017153, 7.768560409545898]
memory len:10000
memory used:3160.0
now epsilon is 0.07230743156307395, the reward is 194.33333333333334 with loss [17.230883359909058, 23.0285587310791] in episode 2091
Report: 
rewardSum:194.33333333333334
loss:[17.230883359909058, 23.0285587310791]
policies:[0, 4, 0]
qAverage:[0.0, 71.65030975341797]
ws:[-1.3593117594718933, 11.191958856582641]
memory len:10000
memory used:3160.0
now epsilon is 0.0722351512422788, the reward is 194.33333333333334 with loss [21.8783438205719, 19.363232612609863] in episode 2092
Report: 
rewardSum:194.33333333333334
loss:[21.8783438205719, 19.363232612609863]
policies:[0, 3, 1]
qAverage:[0.0, 65.80635452270508]
ws:[-1.7585681602358818, 11.25355440378189]
memory len:10000
memory used:3160.0
now epsilon is 0.07216294317470383, the reward is 194.33333333333334 with loss [20.33634638786316, 23.78652334213257] in episode 2093
Report: 
rewardSum:194.33333333333334
loss:[20.33634638786316, 23.78652334213257]
policies:[1, 2, 1]
qAverage:[0.0, 59.64807383219401]
ws:[0.31539098421732586, 4.978732426961263]
memory len:10000
memory used:3161.0
now epsilon is 0.07209080728812292, the reward is 194.33333333333334 with loss [20.69835901260376, 25.488890647888184] in episode 2094
Report: 
rewardSum:194.33333333333334
loss:[20.69835901260376, 25.488890647888184]
policies:[0, 4, 0]
qAverage:[0.0, 69.43377876281738]
ws:[-0.8151739537715912, 12.023193001747131]
memory len:10000
memory used:3160.0
now epsilon is 0.07201874351038214, the reward is 194.33333333333334 with loss [28.981359481811523, 24.2306170463562] in episode 2095
Report: 
rewardSum:194.33333333333334
loss:[28.981359481811523, 24.2306170463562]
policies:[0, 4, 0]
qAverage:[0.0, 71.55494689941406]
ws:[-1.257457610964775, 6.241988635063171]
memory len:10000
memory used:3160.0
now epsilon is 0.07194675176939969, the reward is 194.33333333333334 with loss [29.247402667999268, 32.50944757461548] in episode 2096
Report: 
rewardSum:194.33333333333334
loss:[29.247402667999268, 32.50944757461548]
policies:[1, 3, 0]
qAverage:[0.0, 67.56461143493652]
ws:[-1.826663971412927, 6.388103783130646]
memory len:10000
memory used:3160.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22*		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.07187483199316581, the reward is 194.33333333333334 with loss [26.58184814453125, 24.351080894470215] in episode 2097
Report: 
rewardSum:194.33333333333334
loss:[26.58184814453125, 24.351080894470215]
policies:[0, 4, 0]
qAverage:[0.0, 71.07028045654297]
ws:[-1.5287657529115677, 8.458156538009643]
memory len:10000
memory used:3161.0
now epsilon is 0.07180298410974276, the reward is 194.33333333333334 with loss [18.762187719345093, 27.66972541809082] in episode 2098
Report: 
rewardSum:194.33333333333334
loss:[18.762187719345093, 27.66972541809082]
policies:[0, 4, 0]
qAverage:[0.0, 71.30084075927735]
ws:[-1.8600806981325149, 8.269168138504028]
memory len:10000
memory used:3161.0
now epsilon is 0.07173120804726466, the reward is 194.33333333333334 with loss [26.687191009521484, 20.229453086853027] in episode 2099
Report: 
rewardSum:194.33333333333334
loss:[26.687191009521484, 20.229453086853027]
policies:[0, 4, 0]
qAverage:[0.0, 70.01339912414551]
ws:[-2.7825810350477695, 9.71650505065918]
memory len:10000
memory used:3160.0
now epsilon is 0.0716595037339375, the reward is 194.33333333333334 with loss [21.58849334716797, 16.002009391784668] in episode 2100
Report: 
rewardSum:194.33333333333334
loss:[21.58849334716797, 16.002009391784668]
policies:[0, 4, 0]
qAverage:[0.0, 71.65719757080078]
ws:[-3.467481231689453, 5.785447883605957]
memory len:10000
memory used:3160.0
now epsilon is 0.07158787109803902, the reward is 194.33333333333334 with loss [19.485080003738403, 20.884395599365234] in episode 2101
Report: 
rewardSum:194.33333333333334
loss:[19.485080003738403, 20.884395599365234]
policies:[0, 4, 0]
qAverage:[0.0, 71.11619415283204]
ws:[-3.0969393968582155, 8.006492686271667]
memory len:10000
memory used:3159.0
now epsilon is 0.07151631006791868, the reward is 194.33333333333334 with loss [25.501946926116943, 22.46418285369873] in episode 2102
Report: 
rewardSum:194.33333333333334
loss:[25.501946926116943, 22.46418285369873]
policies:[0, 3, 1]
qAverage:[0.0, 64.15947151184082]
ws:[-1.0835434943437576, 5.496320813894272]
memory len:10000
memory used:3159.0
now epsilon is 0.07144482057199757, the reward is 46.33333333333334 with loss [24.40212655067444, 34.00537586212158] in episode 2103
Report: 
rewardSum:46.33333333333334
loss:[24.40212655067444, 34.00537586212158]
policies:[0, 3, 1]
qAverage:[0.0, 62.9720516204834]
ws:[-0.7864170223474503, 5.602139845490456]
memory len:10000
memory used:3159.0
now epsilon is 0.07137340253876828, the reward is 194.33333333333334 with loss [24.841847896575928, 24.541691303253174] in episode 2104
Report: 
rewardSum:194.33333333333334
loss:[24.841847896575928, 24.541691303253174]
policies:[0, 4, 0]
qAverage:[0.0, 66.6350154876709]
ws:[-3.4634569734334946, 8.118667930364609]
memory len:10000
memory used:3160.0
now epsilon is 0.0713020558967949, the reward is 194.33333333333334 with loss [26.370725631713867, 21.867413997650146] in episode 2105
Report: 
rewardSum:194.33333333333334
loss:[26.370725631713867, 21.867413997650146]
policies:[0, 2, 2]
qAverage:[0.0, 60.516309102376304]
ws:[-4.419668873151143, 5.383783598740895]
memory len:10000
memory used:3159.0
now epsilon is 0.071266409325225, the reward is -1.0 with loss [6.920800685882568, 10.201966285705566] in episode 2106
Report: 
rewardSum:-1.0
loss:[6.920800685882568, 10.201966285705566]
policies:[0, 1, 1]
qAverage:[0.0, 38.199337005615234]
ws:[-1.006901741027832, 0.4959632456302643]
memory len:10000
memory used:3159.0
now epsilon is 0.07119516963634942, the reward is 194.33333333333334 with loss [25.51425790786743, 22.668850898742676] in episode 2107
Report: 
rewardSum:194.33333333333334
loss:[25.51425790786743, 22.668850898742676]
policies:[0, 4, 0]
qAverage:[0.0, 71.48776245117188]
ws:[-3.1822680056095125, 7.487761688232422]
memory len:10000
memory used:3160.0
now epsilon is 0.07110622016016216, the reward is 193.33333333333334 with loss [23.61069107055664, 34.10940456390381] in episode 2108
Report: 
rewardSum:193.33333333333334
loss:[23.61069107055664, 34.10940456390381]
policies:[0, 4, 1]
qAverage:[0.0, 71.62118835449219]
ws:[-2.0396529376506805, 9.319724869728088]
memory len:10000
memory used:3159.0
now epsilon is 0.0710351406003907, the reward is 194.33333333333334 with loss [21.690825700759888, 19.559935092926025] in episode 2109
Report: 
rewardSum:194.33333333333334
loss:[21.690825700759888, 19.559935092926025]
policies:[1, 3, 0]
qAverage:[0.0, 63.58603858947754]
ws:[0.6437163259834051, 6.486280262470245]
memory len:10000
memory used:3160.0
now epsilon is 0.07096413209352863, the reward is 194.33333333333334 with loss [22.07421326637268, 20.806873321533203] in episode 2110
Report: 
rewardSum:194.33333333333334
loss:[22.07421326637268, 20.806873321533203]
policies:[1, 3, 0]
qAverage:[0.0, 70.37326431274414]
ws:[-1.6804963946342468, 10.549741268157959]
memory len:10000
memory used:3160.0
now epsilon is 0.07089319456854967, the reward is 194.33333333333334 with loss [23.087435960769653, 16.00252413749695] in episode 2111
Report: 
rewardSum:194.33333333333334
loss:[23.087435960769653, 16.00252413749695]
policies:[0, 4, 0]
qAverage:[0.0, 71.8546875]
ws:[-0.9802336990833282, 9.158634328842163]
memory len:10000
memory used:3160.0
now epsilon is 0.07082232795449855, the reward is 194.33333333333334 with loss [20.28803539276123, 19.312501430511475] in episode 2112
Report: 
rewardSum:194.33333333333334
loss:[20.28803539276123, 19.312501430511475]
policies:[0, 4, 0]
qAverage:[0.0, 71.58816223144531]
ws:[-0.6123544216156006, 9.01983699798584]
memory len:10000
memory used:3160.0
now epsilon is 0.07075153218049092, the reward is 194.33333333333334 with loss [18.39604425430298, 20.26363515853882] in episode 2113
Report: 
rewardSum:194.33333333333334
loss:[18.39604425430298, 20.26363515853882]
policies:[1, 3, 0]
qAverage:[11.589530181884765, 55.910464477539065]
ws:[0.018688678741455078, 9.007370615005494]
memory len:10000
memory used:3160.0
now epsilon is 0.0706808071757133, the reward is 194.33333333333334 with loss [20.795570611953735, 22.706937789916992] in episode 2114
Report: 
rewardSum:194.33333333333334
loss:[20.795570611953735, 22.706937789916992]
policies:[1, 3, 0]
qAverage:[11.282234954833985, 56.88806915283203]
ws:[0.08818411827087402, 6.958572006225586]
memory len:10000
memory used:3160.0
now epsilon is 0.07061015286942302, the reward is 46.33333333333334 with loss [21.413672924041748, 19.15015983581543] in episode 2115
Report: 
rewardSum:46.33333333333334
loss:[21.413672924041748, 19.15015983581543]
policies:[1, 2, 1]
qAverage:[14.468131065368652, 44.00595283508301]
ws:[2.2715904712677, 6.4129598736763]
memory len:10000
memory used:3161.0
now epsilon is 0.07053956919094806, the reward is 194.33333333333334 with loss [17.142976999282837, 20.613637447357178] in episode 2116
Report: 
rewardSum:194.33333333333334
loss:[17.142976999282837, 20.613637447357178]
policies:[0, 4, 0]
qAverage:[0.0, 70.78627166748046]
ws:[-0.5812581539154053, 6.813720989227295]
memory len:10000
memory used:3161.0
now epsilon is 0.07046905606968712, the reward is 194.33333333333334 with loss [20.598997116088867, 20.510694980621338] in episode 2117
Report: 
rewardSum:194.33333333333334
loss:[20.598997116088867, 20.510694980621338]
policies:[1, 3, 0]
qAverage:[11.43734130859375, 55.642478942871094]
ws:[-0.05494349002838135, 6.964200830459594]
memory len:10000
memory used:3161.0
now epsilon is 0.07039861343510942, the reward is 194.33333333333334 with loss [16.81664228439331, 21.470877528190613] in episode 2118
Report: 
rewardSum:194.33333333333334
loss:[16.81664228439331, 21.470877528190613]
policies:[1, 3, 0]
qAverage:[11.307677459716796, 55.896646118164064]
ws:[0.2690704822540283, 8.657661199569702]
memory len:10000
memory used:3161.0
now epsilon is 0.07032824121675471, the reward is 194.33333333333334 with loss [22.55854868888855, 15.082326769828796] in episode 2119
Report: 
rewardSum:194.33333333333334
loss:[22.55854868888855, 15.082326769828796]
policies:[1, 3, 0]
qAverage:[11.363803100585937, 56.19776916503906]
ws:[-1.0001037955284118, 5.750488674640655]
memory len:10000
memory used:3161.0
now epsilon is 0.07025793934423318, the reward is 194.33333333333334 with loss [17.67316807806492, 23.957518577575684] in episode 2120
Report: 
rewardSum:194.33333333333334
loss:[17.67316807806492, 23.957518577575684]
policies:[0, 4, 0]
qAverage:[0.0, 70.46957397460938]
ws:[-1.2311150223016738, 7.849553573131561]
memory len:10000
memory used:3161.0
now epsilon is 0.0701526182800835, the reward is 192.33333333333334 with loss [37.02160596847534, 35.31666278839111] in episode 2121
Report: 
rewardSum:192.33333333333334
loss:[37.02160596847534, 35.31666278839111]
policies:[0, 5, 1]
qAverage:[0.0, 70.70216522216796]
ws:[-1.6743399947881699, 8.397286522388459]
memory len:10000
memory used:3162.0
now epsilon is 0.07008249196465101, the reward is 194.33333333333334 with loss [20.066673040390015, 22.041521787643433] in episode 2122
Report: 
rewardSum:194.33333333333334
loss:[20.066673040390015, 22.041521787643433]
policies:[0, 4, 0]
qAverage:[0.0, 70.90632476806641]
ws:[-1.4738075476139785, 6.750265979766846]
memory len:10000
memory used:3162.0
now epsilon is 0.07001243574924097, the reward is 194.33333333333334 with loss [21.237688064575195, 19.40832805633545] in episode 2123
Report: 
rewardSum:194.33333333333334
loss:[21.237688064575195, 19.40832805633545]
policies:[0, 4, 0]
qAverage:[0.0, 70.56409606933593]
ws:[-1.5057142209261656, 6.984444308280945]
memory len:10000
memory used:3162.0
now epsilon is 0.0699249639513887, the reward is 193.33333333333334 with loss [24.70085573196411, 28.83000373840332] in episode 2124
Report: 
rewardSum:193.33333333333334
loss:[24.70085573196411, 28.83000373840332]
policies:[0, 4, 1]
qAverage:[0.0, 70.78106384277343]
ws:[-1.0892111465334893, 9.510802078247071]
memory len:10000
memory used:3162.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22*		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.06985506520492876, the reward is 194.33333333333334 with loss [33.455074310302734, 17.558905601501465] in episode 2125
Report: 
rewardSum:194.33333333333334
loss:[33.455074310302734, 17.558905601501465]
policies:[0, 4, 0]
qAverage:[0.0, 70.68235931396484]
ws:[-1.4415245354175568, 7.204887008666992]
memory len:10000
memory used:3162.0
now epsilon is 0.06978523633100762, the reward is 194.33333333333334 with loss [26.777635097503662, 22.22611904144287] in episode 2126
Report: 
rewardSum:194.33333333333334
loss:[26.777635097503662, 22.22611904144287]
policies:[0, 4, 0]
qAverage:[0.0, 70.81384735107422]
ws:[-1.3884170189499856, 8.694113183021546]
memory len:10000
memory used:3162.0
now epsilon is 0.06971547725977893, the reward is 194.33333333333334 with loss [22.333843231201172, 20.867398500442505] in episode 2127
Report: 
rewardSum:194.33333333333334
loss:[22.333843231201172, 20.867398500442505]
policies:[0, 4, 0]
qAverage:[0.0, 69.97362365722657]
ws:[-1.9903661794960499, 6.293447124958038]
memory len:10000
memory used:3162.0
now epsilon is 0.0696457879214662, the reward is 194.33333333333334 with loss [20.183874368667603, 18.169514179229736] in episode 2128
Report: 
rewardSum:194.33333333333334
loss:[20.183874368667603, 18.169514179229736]
policies:[0, 4, 0]
qAverage:[0.0, 69.50118408203124]
ws:[-1.540931725502014, 8.24887574315071]
memory len:10000
memory used:3162.0
now epsilon is 0.06961096938036722, the reward is -1.0 with loss [13.833106994628906, 13.43613052368164] in episode 2129
Report: 
rewardSum:-1.0
loss:[13.833106994628906, 13.43613052368164]
policies:[0, 1, 1]
qAverage:[0.0, 37.5191650390625]
ws:[-0.3377480208873749, 0.27842405438423157]
memory len:10000
memory used:3162.0
now epsilon is 0.06950661816483113, the reward is 192.33333333333334 with loss [32.075955867767334, 35.818331480026245] in episode 2130
Report: 
rewardSum:192.33333333333334
loss:[32.075955867767334, 35.818331480026245]
policies:[0, 5, 1]
qAverage:[0.0, 61.58383750915527]
ws:[-0.12015628814697266, 5.100556991994381]
memory len:10000
memory used:3162.0
now epsilon is 0.06943713760730422, the reward is 194.33333333333334 with loss [19.83057737350464, 18.946588039398193] in episode 2131
Report: 
rewardSum:194.33333333333334
loss:[19.83057737350464, 18.946588039398193]
policies:[0, 4, 0]
qAverage:[0.0, 70.08476867675782]
ws:[-1.2539800643920898, 9.454407429695129]
memory len:10000
memory used:3162.0
now epsilon is 0.06936772650428395, the reward is 194.33333333333334 with loss [25.563889503479004, 19.863893508911133] in episode 2132
Report: 
rewardSum:194.33333333333334
loss:[25.563889503479004, 19.863893508911133]
policies:[0, 4, 0]
qAverage:[0.0, 70.37200622558593]
ws:[-1.1528191704303026, 9.759614181518554]
memory len:10000
memory used:3161.0
now epsilon is 0.0692983847863419, the reward is 194.33333333333334 with loss [22.39829182624817, 22.309799909591675] in episode 2133
Report: 
rewardSum:194.33333333333334
loss:[22.39829182624817, 22.309799909591675]
policies:[0, 4, 0]
qAverage:[0.0, 69.89580078125]
ws:[-0.8790878474712371, 10.643223190307618]
memory len:10000
memory used:3161.0
now epsilon is 0.06922911238411898, the reward is 194.33333333333334 with loss [22.583282470703125, 24.937204837799072] in episode 2134
Report: 
rewardSum:194.33333333333334
loss:[22.583282470703125, 24.937204837799072]
policies:[0, 4, 0]
qAverage:[0.0, 70.643505859375]
ws:[-0.9080549657344819, 10.698676538467407]
memory len:10000
memory used:3161.0
now epsilon is 0.06915990922832546, the reward is 194.33333333333334 with loss [26.78612995147705, 23.848989963531494] in episode 2135
Report: 
rewardSum:194.33333333333334
loss:[26.78612995147705, 23.848989963531494]
policies:[1, 3, 0]
qAverage:[0.0, 69.04157066345215]
ws:[-1.7090040743350983, 11.1486074924469]
memory len:10000
memory used:3161.0
now epsilon is 0.06909077524974087, the reward is 194.33333333333334 with loss [15.456834316253662, 21.431018114089966] in episode 2136
Report: 
rewardSum:194.33333333333334
loss:[15.456834316253662, 21.431018114089966]
policies:[0, 4, 0]
qAverage:[0.0, 70.16662902832032]
ws:[-2.0258657187223434, 7.4672336876392365]
memory len:10000
memory used:3161.0
now epsilon is 0.06902171037921395, the reward is 194.33333333333334 with loss [24.100135564804077, 24.30965805053711] in episode 2137
Report: 
rewardSum:194.33333333333334
loss:[24.100135564804077, 24.30965805053711]
policies:[0, 4, 0]
qAverage:[0.0, 70.48529510498047]
ws:[-2.7415445387363433, 5.691648697853088]
memory len:10000
memory used:3161.0
now epsilon is 0.06895271454766255, the reward is 194.33333333333334 with loss [23.37159824371338, 25.123168468475342] in episode 2138
Report: 
rewardSum:194.33333333333334
loss:[23.37159824371338, 25.123168468475342]
policies:[1, 3, 0]
qAverage:[11.346241760253907, 54.97900390625]
ws:[-3.2499144077301025, 4.4618632078170775]
memory len:10000
memory used:3161.0
now epsilon is 0.06886656673915206, the reward is 193.33333333333334 with loss [23.32244610786438, 22.688333988189697] in episode 2139
Report: 
rewardSum:193.33333333333334
loss:[23.32244610786438, 22.688333988189697]
policies:[1, 3, 1]
qAverage:[11.347689819335937, 55.09974212646485]
ws:[-3.102263295650482, 6.0553874492645265]
memory len:10000
memory used:3161.0
now epsilon is 0.0687633314299329, the reward is 192.33333333333334 with loss [34.05360794067383, 37.8019757270813] in episode 2140
Report: 
rewardSum:192.33333333333334
loss:[34.05360794067383, 37.8019757270813]
policies:[0, 5, 1]
qAverage:[0.0, 72.56380081176758]
ws:[-2.5244966546694436, 8.218948915600777]
memory len:10000
memory used:3161.0
now epsilon is 0.06869459388045485, the reward is 194.33333333333334 with loss [20.613342761993408, 22.743905067443848] in episode 2141
Report: 
rewardSum:194.33333333333334
loss:[20.613342761993408, 22.743905067443848]
policies:[0, 4, 0]
qAverage:[0.0, 70.223486328125]
ws:[-2.171105805784464, 7.823993778228759]
memory len:10000
memory used:3161.0
now epsilon is 0.06862592504275396, the reward is 194.33333333333334 with loss [28.982154369354248, 20.951136589050293] in episode 2142
Report: 
rewardSum:194.33333333333334
loss:[28.982154369354248, 20.951136589050293]
policies:[0, 4, 0]
qAverage:[0.0, 70.90809478759766]
ws:[-2.0784278869628907, 6.1574160099029545]
memory len:10000
memory used:3161.0
now epsilon is 0.06855732484814425, the reward is 194.33333333333334 with loss [26.32863426208496, 23.67108964920044] in episode 2143
Report: 
rewardSum:194.33333333333334
loss:[26.32863426208496, 23.67108964920044]
policies:[1, 3, 0]
qAverage:[0.0, 64.44455909729004]
ws:[-2.0629149079322815, 5.607248842716217]
memory len:10000
memory used:3162.0
now epsilon is 0.06848879322800837, the reward is 194.33333333333334 with loss [28.18903875350952, 17.255014657974243] in episode 2144
Report: 
rewardSum:194.33333333333334
loss:[28.18903875350952, 17.255014657974243]
policies:[0, 4, 0]
qAverage:[0.0, 70.50112915039062]
ws:[-0.3270514249801636, 7.885805654525757]
memory len:10000
memory used:3162.0
now epsilon is 0.06842033011379754, the reward is 194.33333333333334 with loss [29.942772388458252, 26.964646339416504] in episode 2145
Report: 
rewardSum:194.33333333333334
loss:[29.942772388458252, 26.964646339416504]
policies:[0, 4, 0]
qAverage:[0.0, 70.45824737548828]
ws:[-0.2397674560546875, 6.40227689743042]
memory len:10000
memory used:3171.0
now epsilon is 0.06835193543703154, the reward is 194.33333333333334 with loss [21.889224529266357, 24.63617706298828] in episode 2146
Report: 
rewardSum:194.33333333333334
loss:[21.889224529266357, 24.63617706298828]
policies:[0, 4, 0]
qAverage:[0.0, 69.95422821044922]
ws:[-0.4256284713745117, 7.715816831588745]
memory len:10000
memory used:3171.0
now epsilon is 0.06821535112225525, the reward is 190.33333333333334 with loss [49.42377805709839, 44.7300865650177] in episode 2147
Report: 
rewardSum:190.33333333333334
loss:[49.42377805709839, 44.7300865650177]
policies:[0, 5, 3]
qAverage:[0.0, 69.97137196858723]
ws:[-1.9547674705584843, 4.5673730572064715]
memory len:10000
memory used:3171.0
now epsilon is 0.0681471613476265, the reward is 194.33333333333334 with loss [21.913589477539062, 21.37610960006714] in episode 2148
Report: 
rewardSum:194.33333333333334
loss:[21.913589477539062, 21.37610960006714]
policies:[0, 4, 0]
qAverage:[0.0, 70.20185546875]
ws:[-0.778356921672821, 5.827534151077271]
memory len:10000
memory used:3182.0
now epsilon is 0.06804500447227682, the reward is 192.33333333333334 with loss [34.51786160469055, 38.92170476913452] in episode 2149
Report: 
rewardSum:192.33333333333334
loss:[34.51786160469055, 38.92170476913452]
policies:[0, 4, 2]
qAverage:[0.0, 65.25660552978516]
ws:[1.9571696758270263, 6.906728458404541]
memory len:10000
memory used:3182.0
now epsilon is 0.06797698498042869, the reward is 194.33333333333334 with loss [22.64208745956421, 21.838615894317627] in episode 2150
Report: 
rewardSum:194.33333333333334
loss:[22.64208745956421, 21.838615894317627]
policies:[0, 4, 0]
qAverage:[0.0, 67.76658935546875]
ws:[0.7208132743835449, 6.957558250427246]
memory len:10000
memory used:3182.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38*		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.06787508321014266, the reward is 192.33333333333334 with loss [42.41256523132324, 34.69243764877319] in episode 2151
Report: 
rewardSum:192.33333333333334
loss:[42.41256523132324, 34.69243764877319]
policies:[0, 5, 1]
qAverage:[0.0, 71.18657811482747]
ws:[0.35330794254938763, 7.16157591342926]
memory len:10000
memory used:3170.0
now epsilon is 0.0678072335758468, the reward is 194.33333333333334 with loss [21.44447374343872, 28.5142502784729] in episode 2152
Report: 
rewardSum:194.33333333333334
loss:[21.44447374343872, 28.5142502784729]
policies:[0, 4, 0]
qAverage:[0.0, 65.3978500366211]
ws:[-1.1623508036136627, 7.283750653266907]
memory len:10000
memory used:3169.0
now epsilon is 0.06773945176574586, the reward is 194.33333333333334 with loss [20.447606563568115, 29.998391151428223] in episode 2153
Report: 
rewardSum:194.33333333333334
loss:[20.447606563568115, 29.998391151428223]
policies:[0, 4, 0]
qAverage:[0.0, 69.72442016601562]
ws:[-0.9558453917503357, 8.853727173805236]
memory len:10000
memory used:3169.0
now epsilon is 0.06765481977761308, the reward is 193.33333333333334 with loss [26.906365156173706, 31.93522548675537] in episode 2154
Report: 
rewardSum:193.33333333333334
loss:[26.906365156173706, 31.93522548675537]
policies:[0, 4, 1]
qAverage:[0.0, 69.23430480957032]
ws:[-1.512117898464203, 8.872389805316924]
memory len:10000
memory used:3169.0
now epsilon is 0.06758719032416473, the reward is 194.33333333333334 with loss [20.73826265335083, 20.983920097351074] in episode 2155
Report: 
rewardSum:194.33333333333334
loss:[20.73826265335083, 20.983920097351074]
policies:[1, 3, 0]
qAverage:[0.0, 67.83450126647949]
ws:[-2.4443260729312897, 10.30294680595398]
memory len:10000
memory used:3169.0
now epsilon is 0.067519628474813, the reward is 194.33333333333334 with loss [29.82109546661377, 21.592405319213867] in episode 2156
Report: 
rewardSum:194.33333333333334
loss:[29.82109546661377, 21.592405319213867]
policies:[0, 4, 0]
qAverage:[0.0, 69.28983306884766]
ws:[-1.7349622786045074, 9.574642872810363]
memory len:10000
memory used:3170.0
now epsilon is 0.06741841231065657, the reward is 192.33333333333334 with loss [38.5016131401062, 30.240418434143066] in episode 2157
Report: 
rewardSum:192.33333333333334
loss:[38.5016131401062, 30.240418434143066]
policies:[0, 5, 1]
qAverage:[0.0, 69.35979843139648]
ws:[-1.6033136521776516, 7.435572226842244]
memory len:10000
memory used:3170.0
now epsilon is 0.06735101917603714, the reward is 194.33333333333334 with loss [23.910726070404053, 26.32679581642151] in episode 2158
Report: 
rewardSum:194.33333333333334
loss:[23.910726070404053, 26.32679581642151]
policies:[0, 4, 0]
qAverage:[0.0, 67.76046295166016]
ws:[-0.6719399690628052, 10.715330171585084]
memory len:10000
memory used:3170.0
now epsilon is 0.06728369340928411, the reward is 194.33333333333334 with loss [23.218887329101562, 24.38193941116333] in episode 2159
Report: 
rewardSum:194.33333333333334
loss:[23.218887329101562, 24.38193941116333]
policies:[0, 4, 0]
qAverage:[0.0, 68.91495361328126]
ws:[-1.2418160319328309, 8.683173656463623]
memory len:10000
memory used:3170.0
now epsilon is 0.06721643494305489, the reward is 194.33333333333334 with loss [24.410011291503906, 20.790473699569702] in episode 2160
Report: 
rewardSum:194.33333333333334
loss:[24.410011291503906, 20.790473699569702]
policies:[0, 4, 0]
qAverage:[0.0, 67.99299621582031]
ws:[-0.8965072989463806, 11.612574577331543]
memory len:10000
memory used:3170.0
now epsilon is 0.06714924371007418, the reward is 194.33333333333334 with loss [21.223514080047607, 21.848857879638672] in episode 2161
Report: 
rewardSum:194.33333333333334
loss:[21.223514080047607, 21.848857879638672]
policies:[1, 3, 0]
qAverage:[0.0, 63.40092468261719]
ws:[-1.883844032883644, 11.47976678609848]
memory len:10000
memory used:3166.0
now epsilon is 0.06708211964313393, the reward is 194.33333333333334 with loss [23.549992561340332, 17.285882472991943] in episode 2162
Report: 
rewardSum:194.33333333333334
loss:[23.549992561340332, 17.285882472991943]
policies:[0, 4, 0]
qAverage:[0.0, 67.22671661376953]
ws:[-2.5663947463035583, 7.572946691513062]
memory len:10000
memory used:3166.0
now epsilon is 0.0670150626750933, the reward is 194.33333333333334 with loss [21.396301746368408, 21.69997787475586] in episode 2163
Report: 
rewardSum:194.33333333333334
loss:[21.396301746368408, 21.69997787475586]
policies:[0, 4, 0]
qAverage:[0.0, 69.0223403930664]
ws:[-2.528483247756958, 9.077916312217713]
memory len:10000
memory used:3167.0
now epsilon is 0.06691460288676364, the reward is 192.33333333333334 with loss [33.97121858596802, 38.98967123031616] in episode 2164
Report: 
rewardSum:192.33333333333334
loss:[33.97121858596802, 38.98967123031616]
policies:[1, 4, 1]
qAverage:[0.0, 67.8487777709961]
ws:[-2.498929113149643, 6.6813640832901005]
memory len:10000
memory used:3167.0
now epsilon is 0.06684771337267106, the reward is 194.33333333333334 with loss [25.856754302978516, 19.386112689971924] in episode 2165
Report: 
rewardSum:194.33333333333334
loss:[25.856754302978516, 19.386112689971924]
policies:[0, 4, 0]
qAverage:[0.0, 68.50783233642578]
ws:[-0.9870194494724274, 10.777705574035645]
memory len:10000
memory used:3167.0
now epsilon is 0.06678089072301319, the reward is 194.33333333333334 with loss [22.760191440582275, 19.12995982170105] in episode 2166
Report: 
rewardSum:194.33333333333334
loss:[22.760191440582275, 19.12995982170105]
policies:[0, 4, 0]
qAverage:[0.0, 67.81266937255859]
ws:[-1.4618345379829407, 8.267243766784668]
memory len:10000
memory used:3167.0
now epsilon is 0.06671413487095065, the reward is 194.33333333333334 with loss [22.97264862060547, 21.862567901611328] in episode 2167
Report: 
rewardSum:194.33333333333334
loss:[22.97264862060547, 21.862567901611328]
policies:[0, 4, 0]
qAverage:[0.0, 68.32027587890624]
ws:[-1.5125138252973556, 9.683419775962829]
memory len:10000
memory used:3167.0
now epsilon is 0.0666474457497109, the reward is 194.33333333333334 with loss [24.691181182861328, 27.4555721282959] in episode 2168
Report: 
rewardSum:194.33333333333334
loss:[24.691181182861328, 27.4555721282959]
policies:[0, 4, 0]
qAverage:[0.0, 67.97896270751953]
ws:[-2.5805296301841736, 7.6709885597229]
memory len:10000
memory used:3167.0
now epsilon is 0.06658082329258816, the reward is 194.33333333333334 with loss [25.498210430145264, 21.547343254089355] in episode 2169
Report: 
rewardSum:194.33333333333334
loss:[25.498210430145264, 21.547343254089355]
policies:[0, 4, 0]
qAverage:[0.0, 68.22722473144532]
ws:[-2.357022815942764, 9.252886295318604]
memory len:10000
memory used:3167.0
now epsilon is 0.06651426743294328, the reward is 194.33333333333334 with loss [22.58070182800293, 17.54357933998108] in episode 2170
Report: 
rewardSum:194.33333333333334
loss:[22.58070182800293, 17.54357933998108]
policies:[0, 4, 0]
qAverage:[0.0, 68.75917510986328]
ws:[-1.4364785730838776, 12.516844654083252]
memory len:10000
memory used:3166.0
now epsilon is 0.06644777810420374, the reward is 194.33333333333334 with loss [24.508649826049805, 22.366771697998047] in episode 2171
Report: 
rewardSum:194.33333333333334
loss:[24.508649826049805, 22.366771697998047]
policies:[0, 3, 1]
qAverage:[0.0, 55.4927012125651]
ws:[-0.2339046224951744, 5.9708404541015625]
memory len:10000
memory used:3166.0
now epsilon is 0.0663813552398636, the reward is 194.33333333333334 with loss [29.427915573120117, 21.46525287628174] in episode 2172
Report: 
rewardSum:194.33333333333334
loss:[29.427915573120117, 21.46525287628174]
policies:[0, 4, 0]
qAverage:[0.0, 69.28466033935547]
ws:[-1.426375722885132, 13.021328258514405]
memory len:10000
memory used:3166.0
now epsilon is 0.06631499877348337, the reward is 194.33333333333334 with loss [24.86748957633972, 27.093166828155518] in episode 2173
Report: 
rewardSum:194.33333333333334
loss:[24.86748957633972, 27.093166828155518]
policies:[0, 4, 0]
qAverage:[0.0, 66.32528686523438]
ws:[-2.3657130271196367, 9.52351360321045]
memory len:10000
memory used:3166.0
now epsilon is 0.06624870863869002, the reward is 194.33333333333334 with loss [23.902207851409912, 21.079453945159912] in episode 2174
Report: 
rewardSum:194.33333333333334
loss:[23.902207851409912, 21.079453945159912]
policies:[0, 3, 1]
qAverage:[0.0, 64.58405876159668]
ws:[-2.3787869960069656, 12.809143900871277]
memory len:10000
memory used:3166.0
now epsilon is 0.06611632709870326, the reward is 190.33333333333334 with loss [41.866615533828735, 51.049431800842285] in episode 2175
Report: 
rewardSum:190.33333333333334
loss:[41.866615533828735, 51.049431800842285]
policies:[1, 4, 3]
qAverage:[0.0, 64.81455535888672]
ws:[-2.7205756679177284, 5.755936026573181]
memory len:10000
memory used:3166.0
now epsilon is 0.06605023556109522, the reward is 194.33333333333334 with loss [27.92441415786743, 20.61034345626831] in episode 2176
Report: 
rewardSum:194.33333333333334
loss:[27.92441415786743, 20.61034345626831]
policies:[0, 3, 1]
qAverage:[0.0, 63.98604774475098]
ws:[-2.149048238992691, 12.191128373146057]
memory len:10000
memory used:3165.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2*		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.06598421009024459, the reward is 194.33333333333334 with loss [30.449320316314697, 27.41904354095459] in episode 2177
Report: 
rewardSum:194.33333333333334
loss:[30.449320316314697, 27.41904354095459]
policies:[1, 3, 0]
qAverage:[0.0, 58.75326919555664]
ws:[-0.38494373857975006, 5.213672757148743]
memory len:10000
memory used:3165.0
now epsilon is 0.06591825062010938, the reward is 194.33333333333334 with loss [23.68626880645752, 25.68933129310608] in episode 2178
Report: 
rewardSum:194.33333333333334
loss:[23.68626880645752, 25.68933129310608]
policies:[0, 4, 0]
qAverage:[0.0, 66.02582702636718]
ws:[-3.1794061362743378, 7.185820126533509]
memory len:10000
memory used:3165.0
now epsilon is 0.06585235708471364, the reward is 194.33333333333334 with loss [11.542646527290344, 21.67272424697876] in episode 2179
Report: 
rewardSum:194.33333333333334
loss:[11.542646527290344, 21.67272424697876]
policies:[0, 4, 0]
qAverage:[0.0, 60.722341537475586]
ws:[-3.437427803874016, 8.3884879052639]
memory len:10000
memory used:3165.0
now epsilon is 0.06578652941814732, the reward is 194.33333333333334 with loss [17.727487087249756, 26.397103548049927] in episode 2180
Report: 
rewardSum:194.33333333333334
loss:[17.727487087249756, 26.397103548049927]
policies:[0, 4, 0]
qAverage:[0.0, 66.00458068847657]
ws:[-2.0722256511449815, 8.737273454666138]
memory len:10000
memory used:3165.0
now epsilon is 0.065687911278337, the reward is 192.33333333333334 with loss [26.347429513931274, 35.93746089935303] in episode 2181
Report: 
rewardSum:192.33333333333334
loss:[26.347429513931274, 35.93746089935303]
policies:[0, 5, 1]
qAverage:[0.0, 66.27152404785156]
ws:[-0.5155503034591675, 7.744469547271729]
memory len:10000
memory used:3165.0
now epsilon is 0.06562224799592015, the reward is 46.33333333333334 with loss [29.46949815750122, 20.847986936569214] in episode 2182
Report: 
rewardSum:46.33333333333334
loss:[29.46949815750122, 20.847986936569214]
policies:[0, 3, 1]
qAverage:[0.0, 58.43034362792969]
ws:[1.2235510647296906, 5.889544665813446]
memory len:10000
memory used:3165.0
now epsilon is 0.06555665035216611, the reward is 194.33333333333334 with loss [16.36824083328247, 22.878818035125732] in episode 2183
Report: 
rewardSum:194.33333333333334
loss:[16.36824083328247, 22.878818035125732]
policies:[0, 4, 0]
qAverage:[0.0, 65.65389556884766]
ws:[-1.9887403026223183, 7.954602291435004]
memory len:10000
memory used:3165.0
now epsilon is 0.0654911182814608, the reward is 194.33333333333334 with loss [29.043351650238037, 32.658090591430664] in episode 2184
Report: 
rewardSum:194.33333333333334
loss:[29.043351650238037, 32.658090591430664]
policies:[1, 3, 0]
qAverage:[0.0, 57.25991566975912]
ws:[-3.330090284347534, 11.674009323120117]
memory len:10000
memory used:3164.0
now epsilon is 0.06542565171825576, the reward is 194.33333333333334 with loss [22.34657573699951, 20.48837399482727] in episode 2185
Report: 
rewardSum:194.33333333333334
loss:[22.34657573699951, 20.48837399482727]
policies:[0, 4, 0]
qAverage:[0.0, 64.43463439941407]
ws:[-2.2095690727233888, 8.785767126083375]
memory len:10000
memory used:3164.0
now epsilon is 0.06536025059706807, the reward is 194.33333333333334 with loss [25.52099847793579, 18.009740829467773] in episode 2186
Report: 
rewardSum:194.33333333333334
loss:[25.52099847793579, 18.009740829467773]
policies:[0, 3, 1]
qAverage:[0.0, 58.6077880859375]
ws:[-0.004604697227478027, 6.5872330367565155]
memory len:10000
memory used:3164.0
now epsilon is 0.06529491485248022, the reward is 194.33333333333334 with loss [17.361578941345215, 15.001444101333618] in episode 2187
Report: 
rewardSum:194.33333333333334
loss:[17.361578941345215, 15.001444101333618]
policies:[0, 4, 0]
qAverage:[0.0, 64.35881042480469]
ws:[-2.1167196646798403, 9.177879667282104]
memory len:10000
memory used:3165.0
now epsilon is 0.06522964441914014, the reward is 194.33333333333334 with loss [24.021647214889526, 23.231971263885498] in episode 2188
Report: 
rewardSum:194.33333333333334
loss:[24.021647214889526, 23.231971263885498]
policies:[0, 4, 0]
qAverage:[0.0, 60.52701759338379]
ws:[-2.7413573563098907, 9.111157834529877]
memory len:10000
memory used:3165.0
now epsilon is 0.06516443923176106, the reward is 194.33333333333334 with loss [25.119207859039307, 19.714149951934814] in episode 2189
Report: 
rewardSum:194.33333333333334
loss:[25.119207859039307, 19.714149951934814]
policies:[0, 4, 0]
qAverage:[0.0, 64.49361877441406]
ws:[-2.021751180291176, 7.734438133239746]
memory len:10000
memory used:3165.0
now epsilon is 0.06509929922512149, the reward is 194.33333333333334 with loss [21.264645099639893, 26.918753147125244] in episode 2190
Report: 
rewardSum:194.33333333333334
loss:[21.264645099639893, 26.918753147125244]
policies:[0, 4, 0]
qAverage:[0.0, 65.33739013671875]
ws:[-2.390580379962921, 5.453120470046997]
memory len:10000
memory used:3165.0
now epsilon is 0.06503422433406514, the reward is 194.33333333333334 with loss [32.887407302856445, 22.670988082885742] in episode 2191
Report: 
rewardSum:194.33333333333334
loss:[32.887407302856445, 22.670988082885742]
policies:[1, 3, 0]
qAverage:[10.513274383544921, 51.266171264648435]
ws:[-2.586820524930954, 4.5802295207977295]
memory len:10000
memory used:3164.0
now epsilon is 0.06496921449350082, the reward is 194.33333333333334 with loss [24.558481693267822, 17.360467433929443] in episode 2192
Report: 
rewardSum:194.33333333333334
loss:[24.558481693267822, 17.360467433929443]
policies:[0, 3, 1]
qAverage:[0.0, 63.65424728393555]
ws:[-3.026545003056526, 3.0766146779060364]
memory len:10000
memory used:3165.0
now epsilon is 0.06490426963840244, the reward is 194.33333333333334 with loss [21.04508066177368, 17.649728059768677] in episode 2193
Report: 
rewardSum:194.33333333333334
loss:[21.04508066177368, 17.649728059768677]
policies:[0, 4, 0]
qAverage:[0.0, 59.578633626302086]
ws:[-2.8926152984301248, 4.648931344350179]
memory len:10000
memory used:3165.0
now epsilon is 0.0648393897038089, the reward is 194.33333333333334 with loss [23.78183937072754, 23.198058605194092] in episode 2194
Report: 
rewardSum:194.33333333333334
loss:[23.78183937072754, 23.198058605194092]
policies:[0, 4, 0]
qAverage:[0.0, 65.04004821777343]
ws:[-1.1719574928283691, 6.522715675830841]
memory len:10000
memory used:3165.0
now epsilon is 0.06477457462482401, the reward is 194.33333333333334 with loss [23.052437782287598, 26.29001808166504] in episode 2195
Report: 
rewardSum:194.33333333333334
loss:[23.052437782287598, 26.29001808166504]
policies:[0, 4, 0]
qAverage:[0.0, 63.86971473693848]
ws:[-1.393044352531433, 8.897189140319824]
memory len:10000
memory used:3165.0
now epsilon is 0.06470982433661653, the reward is 194.33333333333334 with loss [24.405646800994873, 23.585737705230713] in episode 2196
Report: 
rewardSum:194.33333333333334
loss:[24.405646800994873, 23.585737705230713]
policies:[0, 4, 0]
qAverage:[0.0, 64.95067291259765]
ws:[-1.3891347348690033, 7.6282188415527346]
memory len:10000
memory used:3165.0
now epsilon is 0.06464513877441994, the reward is 194.33333333333334 with loss [28.42772626876831, 25.737815856933594] in episode 2197
Report: 
rewardSum:194.33333333333334
loss:[28.42772626876831, 25.737815856933594]
policies:[0, 4, 0]
qAverage:[0.0, 64.24917449951172]
ws:[-1.7839937955141068, 4.962570333480835]
memory len:10000
memory used:3164.0
now epsilon is 0.06454823165087811, the reward is 192.33333333333334 with loss [27.069855451583862, 29.978501319885254] in episode 2198
Report: 
rewardSum:192.33333333333334
loss:[27.069855451583862, 29.978501319885254]
policies:[0, 5, 1]
qAverage:[0.0, 65.9646479288737]
ws:[-1.4060750206311543, 5.849110841751099]
memory len:10000
memory used:3165.0
now epsilon is 0.06448370762078011, the reward is 194.33333333333334 with loss [19.71729826927185, 21.254047393798828] in episode 2199
Report: 
rewardSum:194.33333333333334
loss:[19.71729826927185, 21.254047393798828]
policies:[0, 4, 0]
qAverage:[0.0, 64.66416625976562]
ws:[-1.7493025243282319, 6.2891830682754515]
memory len:10000
memory used:3165.0
now epsilon is 0.0644192480905197, the reward is 194.33333333333334 with loss [22.031184911727905, 25.642420768737793] in episode 2200
Report: 
rewardSum:194.33333333333334
loss:[22.031184911727905, 25.642420768737793]
policies:[0, 4, 0]
qAverage:[0.0, 64.33844451904297]
ws:[-1.6182965636253357, 6.312512922286987]
memory len:10000
memory used:3165.0
now epsilon is 0.06435485299562127, the reward is 194.33333333333334 with loss [28.23724126815796, 19.10258722305298] in episode 2201
Report: 
rewardSum:194.33333333333334
loss:[28.23724126815796, 19.10258722305298]
policies:[1, 3, 0]
qAverage:[0.0, 51.10979207356771]
ws:[0.32775684508184594, 1.8581195225318272]
memory len:10000
memory used:3165.0
now epsilon is 0.06429052227167362, the reward is 194.33333333333334 with loss [18.210554122924805, 28.3826584815979] in episode 2202
Report: 
rewardSum:194.33333333333334
loss:[18.210554122924805, 28.3826584815979]
policies:[0, 4, 0]
qAverage:[0.0, 64.67820587158204]
ws:[-1.2800194770097733, 6.735146033763885]
memory len:10000
memory used:3165.0
now epsilon is 0.0642262558543299, the reward is 46.33333333333334 with loss [18.930425763130188, 20.041109561920166] in episode 2203
Report: 
rewardSum:46.33333333333334
loss:[18.930425763130188, 20.041109561920166]
policies:[0, 3, 1]
qAverage:[0.0, 56.887460708618164]
ws:[1.194621965289116, 5.699342906475067]
memory len:10000
memory used:3166.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21*		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.06412997666259632, the reward is 192.33333333333334 with loss [35.26399564743042, 28.554146766662598] in episode 2204
Report: 
rewardSum:192.33333333333334
loss:[35.26399564743042, 28.554146766662598]
policies:[0, 4, 2]
qAverage:[0.0, 63.63714447021484]
ws:[-1.1578406810760498, 3.8728466987609864]
memory len:10000
memory used:3165.0
now epsilon is 0.0640658707306671, the reward is 194.33333333333334 with loss [24.9843807220459, 27.4218487739563] in episode 2205
Report: 
rewardSum:194.33333333333334
loss:[24.9843807220459, 27.4218487739563]
policies:[1, 3, 0]
qAverage:[0.0, 57.240186055501304]
ws:[-3.0436009565989175, 9.526930491129557]
memory len:10000
memory used:3166.0
now epsilon is 0.06400182888063408, the reward is 194.33333333333334 with loss [26.727843761444092, 26.21997630596161] in episode 2206
Report: 
rewardSum:194.33333333333334
loss:[26.727843761444092, 26.21997630596161]
policies:[0, 4, 0]
qAverage:[0.0, 64.82910919189453]
ws:[-1.7572060227394104, 8.316529464721679]
memory len:10000
memory used:3166.0
now epsilon is 0.06393785104843941, the reward is 194.33333333333334 with loss [22.604376792907715, 19.02647829055786] in episode 2207
Report: 
rewardSum:194.33333333333334
loss:[22.604376792907715, 19.02647829055786]
policies:[0, 4, 0]
qAverage:[0.0, 64.98519897460938]
ws:[-1.4947620153427124, 8.998131942749023]
memory len:10000
memory used:3166.0
now epsilon is 0.06387393717008925, the reward is 194.33333333333334 with loss [22.924042224884033, 25.02188205718994] in episode 2208
Report: 
rewardSum:194.33333333333334
loss:[22.924042224884033, 25.02188205718994]
policies:[0, 4, 0]
qAverage:[0.0, 62.76833724975586]
ws:[-2.632017657160759, 7.171132445335388]
memory len:10000
memory used:3166.0
now epsilon is 0.06381008718165374, the reward is 194.33333333333334 with loss [22.782870054244995, 22.909212589263916] in episode 2209
Report: 
rewardSum:194.33333333333334
loss:[22.782870054244995, 22.909212589263916]
policies:[0, 4, 0]
qAverage:[0.0, 66.04464874267578]
ws:[-1.7899655938148498, 9.774007797241211]
memory len:10000
memory used:3165.0
now epsilon is 0.0637463010192669, the reward is 194.33333333333334 with loss [29.733988761901855, 23.179973602294922] in episode 2210
Report: 
rewardSum:194.33333333333334
loss:[29.733988761901855, 23.179973602294922]
policies:[0, 4, 0]
qAverage:[0.0, 65.39865264892578]
ws:[-1.2914170056581498, 10.335219383239746]
memory len:10000
memory used:3165.0
now epsilon is 0.06368257861912663, the reward is 194.33333333333334 with loss [22.54222822189331, 16.159343242645264] in episode 2211
Report: 
rewardSum:194.33333333333334
loss:[22.54222822189331, 16.159343242645264]
policies:[0, 4, 0]
qAverage:[0.0, 65.80118255615234]
ws:[-1.7238690853118896, 9.774058151245118]
memory len:10000
memory used:3165.0
now epsilon is 0.06361891991749458, the reward is 194.33333333333334 with loss [20.16702890396118, 17.75001811981201] in episode 2212
Report: 
rewardSum:194.33333333333334
loss:[20.16702890396118, 17.75001811981201]
policies:[0, 4, 0]
qAverage:[0.0, 59.9881649017334]
ws:[-2.657662034034729, 10.20584750175476]
memory len:10000
memory used:3165.0
now epsilon is 0.06355532485069613, the reward is 194.33333333333334 with loss [26.54868221282959, 23.203468322753906] in episode 2213
Report: 
rewardSum:194.33333333333334
loss:[26.54868221282959, 23.203468322753906]
policies:[0, 4, 0]
qAverage:[0.0, 65.34362487792968]
ws:[-2.2323552697896956, 9.768184423446655]
memory len:10000
memory used:3165.0
now epsilon is 0.0634917933551203, the reward is 194.33333333333334 with loss [21.88973379135132, 22.625961780548096] in episode 2214
Report: 
rewardSum:194.33333333333334
loss:[21.88973379135132, 22.625961780548096]
policies:[0, 4, 0]
qAverage:[0.0, 65.47256927490234]
ws:[-2.2931863009929656, 10.044595575332641]
memory len:10000
memory used:3165.0
now epsilon is 0.0634283253672197, the reward is 194.33333333333334 with loss [29.328316688537598, 23.40245294570923] in episode 2215
Report: 
rewardSum:194.33333333333334
loss:[29.328316688537598, 23.40245294570923]
policies:[0, 4, 0]
qAverage:[0.0, 65.14200744628906]
ws:[-2.3900462865829466, 7.814395570755005]
memory len:10000
memory used:3165.0
now epsilon is 0.06336492082351047, the reward is 194.33333333333334 with loss [21.169607877731323, 23.23153305053711] in episode 2216
Report: 
rewardSum:194.33333333333334
loss:[21.169607877731323, 23.23153305053711]
policies:[0, 4, 0]
qAverage:[0.0, 65.52662658691406]
ws:[-1.6603435792028904, 9.327901935577392]
memory len:10000
memory used:3165.0
now epsilon is 0.06330157966057222, the reward is 194.33333333333334 with loss [24.749052047729492, 24.62571096420288] in episode 2217
Report: 
rewardSum:194.33333333333334
loss:[24.749052047729492, 24.62571096420288]
policies:[0, 4, 0]
qAverage:[0.0, 65.61046752929687]
ws:[-2.3782642513513563, 6.295674562454224]
memory len:10000
memory used:3165.0
now epsilon is 0.06323830181504793, the reward is 194.33333333333334 with loss [30.208569526672363, 25.278212547302246] in episode 2218
Report: 
rewardSum:194.33333333333334
loss:[30.208569526672363, 25.278212547302246]
policies:[0, 3, 1]
qAverage:[0.0, 64.63510704040527]
ws:[-2.3226781114935875, 10.195074319839478]
memory len:10000
memory used:3165.0
now epsilon is 0.06317508722364391, the reward is 194.33333333333334 with loss [28.258492469787598, 31.539097785949707] in episode 2219
Report: 
rewardSum:194.33333333333334
loss:[28.258492469787598, 31.539097785949707]
policies:[0, 4, 0]
qAverage:[0.0, 60.56704902648926]
ws:[-3.168072670698166, 6.22112450003624]
memory len:10000
memory used:3165.0
now epsilon is 0.06311193582312978, the reward is 194.33333333333334 with loss [17.527453899383545, 22.278815269470215] in episode 2220
Report: 
rewardSum:194.33333333333334
loss:[17.527453899383545, 22.278815269470215]
policies:[0, 4, 0]
qAverage:[0.0, 65.33047027587891]
ws:[-1.7851525127887726, 8.887953543663025]
memory len:10000
memory used:3164.0
now epsilon is 0.06304884755033834, the reward is 194.33333333333334 with loss [21.21567416191101, 22.161264896392822] in episode 2221
Report: 
rewardSum:194.33333333333334
loss:[21.21567416191101, 22.161264896392822]
policies:[0, 4, 0]
qAverage:[0.0, 66.80617065429688]
ws:[-1.7876296669244767, 8.721933031082154]
memory len:10000
memory used:3165.0
now epsilon is 0.06298582234216553, the reward is 194.33333333333334 with loss [25.34577989578247, 23.611818552017212] in episode 2222
Report: 
rewardSum:194.33333333333334
loss:[25.34577989578247, 23.611818552017212]
policies:[0, 4, 0]
qAverage:[0.0, 65.57485656738281]
ws:[-1.862194448709488, 8.613120794296265]
memory len:10000
memory used:3164.0
now epsilon is 0.06292286013557039, the reward is 194.33333333333334 with loss [25.721928596496582, 23.056190967559814] in episode 2223
Report: 
rewardSum:194.33333333333334
loss:[25.721928596496582, 23.056190967559814]
policies:[0, 4, 0]
qAverage:[0.0, 66.4625473022461]
ws:[-2.192587684839964, 6.577392625808716]
memory len:10000
memory used:3164.0
now epsilon is 0.06285996086757495, the reward is 194.33333333333334 with loss [26.10887050628662, 23.114693641662598] in episode 2224
Report: 
rewardSum:194.33333333333334
loss:[26.10887050628662, 23.114693641662598]
policies:[0, 4, 0]
qAverage:[0.0, 65.4740478515625]
ws:[-1.653354274854064, 8.92491250038147]
memory len:10000
memory used:3164.0
now epsilon is 0.06279712447526421, the reward is 194.33333333333334 with loss [27.69382381439209, 27.19588279724121] in episode 2225
Report: 
rewardSum:194.33333333333334
loss:[27.69382381439209, 27.19588279724121]
policies:[0, 4, 0]
qAverage:[0.0, 66.28023071289063]
ws:[-1.4637798875570298, 9.10573935508728]
memory len:10000
memory used:3164.0
now epsilon is 0.06273435089578606, the reward is 194.33333333333334 with loss [19.80081844329834, 24.86822009086609] in episode 2226
Report: 
rewardSum:194.33333333333334
loss:[19.80081844329834, 24.86822009086609]
policies:[0, 4, 0]
qAverage:[0.0, 60.15393257141113]
ws:[-2.261983498930931, 6.6025015115737915]
memory len:10000
memory used:3165.0
now epsilon is 0.06267164006635123, the reward is 194.33333333333334 with loss [29.18390130996704, 22.98062562942505] in episode 2227
Report: 
rewardSum:194.33333333333334
loss:[29.18390130996704, 22.98062562942505]
policies:[0, 4, 0]
qAverage:[0.0, 65.80442352294922]
ws:[-0.919092345237732, 8.93358678817749]
memory len:10000
memory used:3165.0
now epsilon is 0.06260899192423318, the reward is 194.33333333333334 with loss [23.334097385406494, 17.126267194747925] in episode 2228
Report: 
rewardSum:194.33333333333334
loss:[23.334097385406494, 17.126267194747925]
policies:[0, 4, 0]
qAverage:[0.0, 66.47888488769532]
ws:[-2.1252066045999527, 8.482304787635803]
memory len:10000
memory used:3164.0
now epsilon is 0.0625464064067681, the reward is 194.33333333333334 with loss [29.466312408447266, 19.46509099006653] in episode 2229
Report: 
rewardSum:194.33333333333334
loss:[29.466312408447266, 19.46509099006653]
policies:[0, 4, 0]
qAverage:[0.0, 65.83476638793945]
ws:[-3.2410254266578704, 10.269664883613586]
memory len:10000
memory used:3165.0
now epsilon is 0.062483883451354846, the reward is 194.33333333333334 with loss [25.75261116027832, 19.763773679733276] in episode 2230
Report: 
rewardSum:194.33333333333334
loss:[25.75261116027832, 19.763773679733276]
policies:[0, 4, 0]
qAverage:[0.0, 66.70907897949219]
ws:[-2.6187020599842072, 8.639381432533265]
memory len:10000
memory used:3166.0
now epsilon is 0.06242142299545479, the reward is 194.33333333333334 with loss [15.593913555145264, 19.71468448638916] in episode 2231
Report: 
rewardSum:194.33333333333334
loss:[15.593913555145264, 19.71468448638916]
policies:[0, 4, 0]
qAverage:[0.0, 63.945674896240234]
ws:[-2.9335888251662254, 6.834848761558533]
memory len:10000
memory used:3166.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21*		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.062359024976591876, the reward is 194.33333333333334 with loss [26.56000280380249, 22.288819313049316] in episode 2232
Report: 
rewardSum:194.33333333333334
loss:[26.56000280380249, 22.288819313049316]
policies:[0, 4, 0]
qAverage:[0.0, 66.70162658691406]
ws:[-1.586619257926941, 8.655141305923461]
memory len:10000
memory used:3166.0
now epsilon is 0.06229668933235246, the reward is 194.33333333333334 with loss [25.451311826705933, 24.408347606658936] in episode 2233
Report: 
rewardSum:194.33333333333334
loss:[25.451311826705933, 24.408347606658936]
policies:[0, 3, 1]
qAverage:[0.0, 67.6192512512207]
ws:[-3.05676656216383, 7.3629913330078125]
memory len:10000
memory used:3166.0
now epsilon is 0.06223441600038531, the reward is 194.33333333333334 with loss [19.063525438308716, 21.66063165664673] in episode 2234
Report: 
rewardSum:194.33333333333334
loss:[19.063525438308716, 21.66063165664673]
policies:[0, 4, 0]
qAverage:[0.0, 69.6947998046875]
ws:[-1.882946851849556, 8.858309173583985]
memory len:10000
memory used:3166.0
now epsilon is 0.06217220491840153, the reward is 194.33333333333334 with loss [30.391400814056396, 20.611207485198975] in episode 2235
Report: 
rewardSum:194.33333333333334
loss:[30.391400814056396, 20.611207485198975]
policies:[1, 3, 0]
qAverage:[0.0, 66.6994857788086]
ws:[-2.4927422180771828, 6.784285008907318]
memory len:10000
memory used:3166.0
now epsilon is 0.062110056024174465, the reward is 194.33333333333334 with loss [23.26716661453247, 22.12774920463562] in episode 2236
Report: 
rewardSum:194.33333333333334
loss:[23.26716661453247, 22.12774920463562]
policies:[0, 4, 0]
qAverage:[0.0, 68.63509216308594]
ws:[-1.6149291820824145, 8.899832487106323]
memory len:10000
memory used:3166.0
now epsilon is 0.062047969255539674, the reward is 194.33333333333334 with loss [21.263335704803467, 27.452335834503174] in episode 2237
Report: 
rewardSum:194.33333333333334
loss:[21.263335704803467, 27.452335834503174]
policies:[0, 4, 0]
qAverage:[0.0, 69.34393768310547]
ws:[-2.440553216636181, 6.413231945037841]
memory len:10000
memory used:3166.0
now epsilon is 0.061985944550394856, the reward is 194.33333333333334 with loss [27.366050720214844, 23.958295583724976] in episode 2238
Report: 
rewardSum:194.33333333333334
loss:[27.366050720214844, 23.958295583724976]
policies:[0, 4, 0]
qAverage:[0.0, 67.97696380615234]
ws:[-1.7471269011497497, 8.572695302963258]
memory len:10000
memory used:3166.0
now epsilon is 0.06192398184669979, the reward is 194.33333333333334 with loss [18.80643057823181, 30.137210369110107] in episode 2239
Report: 
rewardSum:194.33333333333334
loss:[18.80643057823181, 30.137210369110107]
policies:[0, 4, 0]
qAverage:[0.0, 69.54176635742188]
ws:[-2.3223159492015837, 5.668254661560058]
memory len:10000
memory used:3166.0
now epsilon is 0.06186208108247628, the reward is 194.33333333333334 with loss [24.028679847717285, 13.106634378433228] in episode 2240
Report: 
rewardSum:194.33333333333334
loss:[24.028679847717285, 13.106634378433228]
policies:[0, 4, 0]
qAverage:[0.0, 62.523000717163086]
ws:[-2.297844134271145, 8.166036456823349]
memory len:10000
memory used:3166.0
now epsilon is 0.06180024219580808, the reward is 194.33333333333334 with loss [22.43508291244507, 31.623459339141846] in episode 2241
Report: 
rewardSum:194.33333333333334
loss:[22.43508291244507, 31.623459339141846]
policies:[0, 4, 0]
qAverage:[0.0, 69.090234375]
ws:[-1.8585924059152603, 5.956203365325928]
memory len:10000
memory used:3166.0
now epsilon is 0.06173846512484083, the reward is 194.33333333333334 with loss [24.920093536376953, 25.349190711975098] in episode 2242
Report: 
rewardSum:194.33333333333334
loss:[24.920093536376953, 25.349190711975098]
policies:[0, 4, 0]
qAverage:[0.0, 68.55584564208985]
ws:[-1.0979179859161377, 8.252926683425903]
memory len:10000
memory used:3166.0
now epsilon is 0.061676749807782004, the reward is 194.33333333333334 with loss [22.523276805877686, 22.566400289535522] in episode 2243
Report: 
rewardSum:194.33333333333334
loss:[22.523276805877686, 22.566400289535522]
policies:[0, 4, 0]
qAverage:[0.0, 68.18537139892578]
ws:[-1.3156616270542145, 8.829083895683288]
memory len:10000
memory used:3166.0
now epsilon is 0.06161509618290085, the reward is 194.33333333333334 with loss [22.457370281219482, 28.24367046356201] in episode 2244
Report: 
rewardSum:194.33333333333334
loss:[22.457370281219482, 28.24367046356201]
policies:[0, 4, 0]
qAverage:[0.0, 69.27454376220703]
ws:[-1.7945001438260078, 6.866828441619873]
memory len:10000
memory used:3166.0
now epsilon is 0.06155350418852833, the reward is 194.33333333333334 with loss [16.1656711101532, 20.09541940689087] in episode 2245
Report: 
rewardSum:194.33333333333334
loss:[16.1656711101532, 20.09541940689087]
policies:[0, 4, 0]
qAverage:[0.0, 65.94109916687012]
ws:[-2.2369840256869793, 6.996496200561523]
memory len:10000
memory used:3166.0
now epsilon is 0.061491973763057026, the reward is 194.33333333333334 with loss [22.316729068756104, 23.934005737304688] in episode 2246
Report: 
rewardSum:194.33333333333334
loss:[22.316729068756104, 23.934005737304688]
policies:[0, 4, 0]
qAverage:[0.0, 71.11343688964844]
ws:[-0.866162633895874, 9.164357137680053]
memory len:10000
memory used:3165.0
now epsilon is 0.06143050484494113, the reward is 194.33333333333334 with loss [28.135246753692627, 27.832597255706787] in episode 2247
Report: 
rewardSum:194.33333333333334
loss:[28.135246753692627, 27.832597255706787]
policies:[0, 4, 0]
qAverage:[0.0, 69.44535217285156]
ws:[-0.9407125115394592, 6.773139905929566]
memory len:10000
memory used:3165.0
now epsilon is 0.06136909737269634, the reward is 194.33333333333334 with loss [32.95793151855469, 23.13663148880005] in episode 2248
Report: 
rewardSum:194.33333333333334
loss:[32.95793151855469, 23.13663148880005]
policies:[0, 4, 0]
qAverage:[0.0, 70.62528686523437]
ws:[-0.8519519090652465, 7.370757913589477]
memory len:10000
memory used:3166.0
now epsilon is 0.061307751284899833, the reward is 46.33333333333334 with loss [29.390475749969482, 18.77638530731201] in episode 2249
Report: 
rewardSum:46.33333333333334
loss:[29.390475749969482, 18.77638530731201]
policies:[0, 3, 1]
qAverage:[0.0, 61.4333553314209]
ws:[1.5187989175319672, 8.144882440567017]
memory len:10000
memory used:3166.0
now epsilon is 0.061246466520190175, the reward is 194.33333333333334 with loss [16.140379667282104, 24.038992404937744] in episode 2250
Report: 
rewardSum:194.33333333333334
loss:[16.140379667282104, 24.038992404937744]
policies:[0, 4, 0]
qAverage:[0.0, 69.90661315917968]
ws:[-1.244571363925934, 7.065238094329834]
memory len:10000
memory used:3165.0
now epsilon is 0.06118524301726727, the reward is 194.33333333333334 with loss [24.22349452972412, 27.85543918609619] in episode 2251
Report: 
rewardSum:194.33333333333334
loss:[24.22349452972412, 27.85543918609619]
policies:[0, 4, 0]
qAverage:[0.0, 70.15999145507813]
ws:[-0.6726847052574157, 9.46302490234375]
memory len:10000
memory used:3165.0
now epsilon is 0.0611240807148923, the reward is 194.33333333333334 with loss [15.67336106300354, 27.18317461013794] in episode 2252
Report: 
rewardSum:194.33333333333334
loss:[15.67336106300354, 27.18317461013794]
policies:[0, 4, 0]
qAverage:[0.0, 70.54907531738282]
ws:[-0.7400936841964721, 9.172315073013305]
memory len:10000
memory used:3166.0
now epsilon is 0.06106297955188767, the reward is 194.33333333333334 with loss [27.954650402069092, 23.769075870513916] in episode 2253
Report: 
rewardSum:194.33333333333334
loss:[27.954650402069092, 23.769075870513916]
policies:[0, 4, 0]
qAverage:[0.0, 69.64854888916015]
ws:[-1.1485351026058197, 7.151462459564209]
memory len:10000
memory used:3166.0
now epsilon is 0.06100193946713692, the reward is 194.33333333333334 with loss [24.938252925872803, 22.998826026916504] in episode 2254
Report: 
rewardSum:194.33333333333334
loss:[24.938252925872803, 22.998826026916504]
policies:[0, 4, 0]
qAverage:[0.0, 70.60103454589844]
ws:[-0.3436567306518555, 10.462427234649658]
memory len:10000
memory used:3166.0
now epsilon is 0.06094096039958471, the reward is 194.33333333333334 with loss [27.820574283599854, 25.969104766845703] in episode 2255
Report: 
rewardSum:194.33333333333334
loss:[27.820574283599854, 25.969104766845703]
policies:[0, 4, 0]
qAverage:[0.0, 69.70907287597656]
ws:[-0.8506156444549561, 8.156823301315308]
memory len:10000
memory used:3165.0
now epsilon is 0.06088004228823671, the reward is 194.33333333333334 with loss [20.436012744903564, 21.48641014099121] in episode 2256
Report: 
rewardSum:194.33333333333334
loss:[20.436012744903564, 21.48641014099121]
policies:[1, 3, 0]
qAverage:[0.0, 70.5318489074707]
ws:[-0.5382999181747437, 12.312733888626099]
memory len:10000
memory used:3165.0
now epsilon is 0.06081918507215957, the reward is 194.33333333333334 with loss [22.883952140808105, 20.550530672073364] in episode 2257
Report: 
rewardSum:194.33333333333334
loss:[22.883952140808105, 20.550530672073364]
policies:[0, 4, 0]
qAverage:[0.0, 69.96361846923828]
ws:[-0.8689605236053467, 7.847943639755249]
memory len:10000
memory used:3165.0
now epsilon is 0.060758388690480856, the reward is 194.33333333333334 with loss [24.141875743865967, 22.337307453155518] in episode 2258
Report: 
rewardSum:194.33333333333334
loss:[24.141875743865967, 22.337307453155518]
policies:[0, 4, 0]
qAverage:[0.0, 71.08824615478515]
ws:[-0.3653113603591919, 10.116781520843507]
memory len:10000
memory used:3165.0
now epsilon is 0.06069765308238899, the reward is 194.33333333333334 with loss [21.084073066711426, 21.95089316368103] in episode 2259
Report: 
rewardSum:194.33333333333334
loss:[21.084073066711426, 21.95089316368103]
policies:[0, 4, 0]
qAverage:[0.0, 72.61363220214844]
ws:[-0.3895374834537506, 9.437475109100342]
memory len:10000
memory used:3165.0
now epsilon is 0.06063697818713314, the reward is 194.33333333333334 with loss [20.62279462814331, 19.802120685577393] in episode 2260
Report: 
rewardSum:194.33333333333334
loss:[20.62279462814331, 19.802120685577393]
policies:[0, 4, 0]
qAverage:[0.0, 72.17047729492188]
ws:[-0.5611687183380127, 7.417824697494507]
memory len:10000
memory used:3165.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2*		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.06057636394402326, the reward is 194.33333333333334 with loss [20.10438323020935, 25.823634147644043] in episode 2261
Report: 
rewardSum:194.33333333333334
loss:[20.10438323020935, 25.823634147644043]
policies:[0, 4, 0]
qAverage:[0.0, 72.86885070800781]
ws:[0.37930276393890383, 10.489348411560059]
memory len:10000
memory used:3165.0
now epsilon is 0.06048555616952187, the reward is 192.33333333333334 with loss [35.49195861816406, 37.97301912307739] in episode 2262
Report: 
rewardSum:192.33333333333334
loss:[35.49195861816406, 37.97301912307739]
policies:[2, 3, 1]
qAverage:[0.0, 70.14286041259766]
ws:[-0.22667625546455383, 8.872585415840149]
memory len:10000
memory used:3165.0
now epsilon is 0.06042509329165581, the reward is 194.33333333333334 with loss [24.317969799041748, 21.890223026275635] in episode 2263
Report: 
rewardSum:194.33333333333334
loss:[24.317969799041748, 21.890223026275635]
policies:[1, 3, 0]
qAverage:[0.0, 70.9576530456543]
ws:[0.3359023928642273, 13.117231130599976]
memory len:10000
memory used:3165.0
now epsilon is 0.06036469085399781, the reward is 194.33333333333334 with loss [26.67534589767456, 22.894593238830566] in episode 2264
Report: 
rewardSum:194.33333333333334
loss:[26.67534589767456, 22.894593238830566]
policies:[1, 3, 0]
qAverage:[0.0, 72.72555160522461]
ws:[-0.7495991438627243, 9.736127972602844]
memory len:10000
memory used:3164.0
now epsilon is 0.060304348796130336, the reward is 194.33333333333334 with loss [23.49767017364502, 21.98110866546631] in episode 2265
Report: 
rewardSum:194.33333333333334
loss:[23.49767017364502, 21.98110866546631]
policies:[0, 4, 0]
qAverage:[0.0, 72.17870025634765]
ws:[-0.283985435962677, 10.544207429885864]
memory len:10000
memory used:3164.0
now epsilon is 0.06024406705769622, the reward is 194.33333333333334 with loss [22.569310188293457, 22.446858882904053] in episode 2266
Report: 
rewardSum:194.33333333333334
loss:[22.569310188293457, 22.446858882904053]
policies:[0, 4, 0]
qAverage:[0.0, 73.01678771972657]
ws:[-1.1888616874814033, 8.199840593338013]
memory len:10000
memory used:3164.0
now epsilon is 0.06018384557839867, the reward is 194.33333333333334 with loss [27.332500457763672, 24.92331600189209] in episode 2267
Report: 
rewardSum:194.33333333333334
loss:[27.332500457763672, 24.92331600189209]
policies:[0, 4, 0]
qAverage:[0.0, 73.14562377929687]
ws:[-1.483772373199463, 8.455795574188233]
memory len:10000
memory used:3164.0
now epsilon is 0.060123684298001114, the reward is 194.33333333333334 with loss [16.216634273529053, 19.83292531967163] in episode 2268
Report: 
rewardSum:194.33333333333334
loss:[16.216634273529053, 19.83292531967163]
policies:[0, 4, 0]
qAverage:[0.0, 73.00292510986328]
ws:[-1.511542022228241, 8.894009971618653]
memory len:10000
memory used:3164.0
now epsilon is 0.06006358315632724, the reward is 194.33333333333334 with loss [23.214065074920654, 22.57719087600708] in episode 2269
Report: 
rewardSum:194.33333333333334
loss:[23.214065074920654, 22.57719087600708]
policies:[1, 3, 0]
qAverage:[0.0, 60.53122202555338]
ws:[0.09007152418295543, 5.968074798583984]
memory len:10000
memory used:3165.0
now epsilon is 0.060003542093260864, the reward is 194.33333333333334 with loss [24.01396131515503, 31.283591747283936] in episode 2270
Report: 
rewardSum:194.33333333333334
loss:[24.01396131515503, 31.283591747283936]
policies:[0, 4, 0]
qAverage:[0.0, 73.30721893310547]
ws:[-1.3805776685476303, 8.737226152420044]
memory len:10000
memory used:3164.0
now epsilon is 0.05994356104874591, the reward is 194.33333333333334 with loss [17.945008873939514, 24.53706693649292] in episode 2271
Report: 
rewardSum:194.33333333333334
loss:[17.945008873939514, 24.53706693649292]
policies:[0, 4, 0]
qAverage:[0.0, 77.8970703125]
ws:[-1.59688857793808, 7.550177812576294]
memory len:10000
memory used:3164.0
now epsilon is 0.059883639962786325, the reward is 194.33333333333334 with loss [26.78523015975952, 27.008948802947998] in episode 2272
Report: 
rewardSum:194.33333333333334
loss:[26.78523015975952, 27.008948802947998]
policies:[0, 4, 0]
qAverage:[0.0, 74.15834197998046]
ws:[-0.8724996387958527, 9.454529237747192]
memory len:10000
memory used:3165.0
now epsilon is 0.05982377877544604, the reward is 194.33333333333334 with loss [24.688077926635742, 30.281699180603027] in episode 2273
Report: 
rewardSum:194.33333333333334
loss:[24.688077926635742, 30.281699180603027]
policies:[0, 4, 0]
qAverage:[0.0, 65.46051788330078]
ws:[0.5130550786852837, 5.99784392118454]
memory len:10000
memory used:3165.0
now epsilon is 0.059763977426848885, the reward is 194.33333333333334 with loss [18.237857818603516, 23.641475915908813] in episode 2274
Report: 
rewardSum:194.33333333333334
loss:[18.237857818603516, 23.641475915908813]
policies:[0, 4, 0]
qAverage:[0.0, 74.50722045898438]
ws:[-0.9997123718261719, 6.43612402677536]
memory len:10000
memory used:3165.0
now epsilon is 0.05970423585717856, the reward is 194.33333333333334 with loss [25.22602891921997, 21.9987690448761] in episode 2275
Report: 
rewardSum:194.33333333333334
loss:[25.22602891921997, 21.9987690448761]
policies:[0, 4, 0]
qAverage:[0.0, 75.28908996582031]
ws:[0.20812322497367858, 9.702812457084656]
memory len:10000
memory used:3165.0
now epsilon is 0.05964455400667855, the reward is 194.33333333333334 with loss [27.85370397567749, 25.033449172973633] in episode 2276
Report: 
rewardSum:194.33333333333334
loss:[27.85370397567749, 25.033449172973633]
policies:[0, 4, 0]
qAverage:[0.0, 73.83990173339843]
ws:[0.028338134288787842, 8.167983484268188]
memory len:10000
memory used:3164.0
now epsilon is 0.059584931815652074, the reward is 194.33333333333334 with loss [20.834019660949707, 25.40541362762451] in episode 2277
Report: 
rewardSum:194.33333333333334
loss:[20.834019660949707, 25.40541362762451]
policies:[0, 4, 0]
qAverage:[0.0, 75.81253051757812]
ws:[0.11990205049514771, 8.758901071548461]
memory len:10000
memory used:3165.0
now epsilon is 0.059525369224462034, the reward is 194.33333333333334 with loss [24.546192169189453, 19.8992977142334] in episode 2278
Report: 
rewardSum:194.33333333333334
loss:[24.546192169189453, 19.8992977142334]
policies:[0, 4, 0]
qAverage:[0.0, 73.96815338134766]
ws:[0.7129692792892456, 10.917737436294555]
memory len:10000
memory used:3165.0
now epsilon is 0.059495610260185385, the reward is -1.0 with loss [11.916759490966797, 11.016748905181885] in episode 2279
Report: 
rewardSum:-1.0
loss:[11.916759490966797, 11.016748905181885]
policies:[0, 1, 1]
qAverage:[0.0, 38.51227951049805]
ws:[0.40355053544044495, 1.393900990486145]
memory len:10000
memory used:3164.0
now epsilon is 0.05943613695706081, the reward is 194.33333333333334 with loss [21.92565107345581, 27.263375759124756] in episode 2280
Report: 
rewardSum:194.33333333333334
loss:[21.92565107345581, 27.263375759124756]
policies:[0, 4, 0]
qAverage:[0.0, 74.1078384399414]
ws:[0.7249902069568634, 12.085790014266967]
memory len:10000
memory used:3164.0
now epsilon is 0.05937672310494059, the reward is 194.33333333333334 with loss [21.971797466278076, 30.948218822479248] in episode 2281
Report: 
rewardSum:194.33333333333334
loss:[21.971797466278076, 30.948218822479248]
policies:[0, 4, 0]
qAverage:[0.0, 75.30772094726562]
ws:[1.2102393627166748, 12.79518747329712]
memory len:10000
memory used:3165.0
now epsilon is 0.059317368644396004, the reward is 194.33333333333334 with loss [26.80285930633545, 27.860158443450928] in episode 2282
Report: 
rewardSum:194.33333333333334
loss:[26.80285930633545, 27.860158443450928]
policies:[0, 4, 0]
qAverage:[0.0, 73.61275482177734]
ws:[1.1701778173446655, 12.961019515991211]
memory len:10000
memory used:3165.0
now epsilon is 0.05928771366740935, the reward is -1.0 with loss [10.637090682983398, 13.805519104003906] in episode 2283
Report: 
rewardSum:-1.0
loss:[10.637090682983398, 13.805519104003906]
policies:[0, 1, 1]
qAverage:[0.0, 38.12467575073242]
ws:[0.5227702856063843, 2.1859660148620605]
memory len:10000
memory used:3165.0
now epsilon is 0.05922844818292932, the reward is 194.33333333333334 with loss [20.092723608016968, 23.738248348236084] in episode 2284
Report: 
rewardSum:194.33333333333334
loss:[20.092723608016968, 23.738248348236084]
policies:[0, 4, 0]
qAverage:[0.0, 76.58200531005859]
ws:[0.8389039859175682, 12.958035659790038]
memory len:10000
memory used:3166.0
now epsilon is 0.05916924194171292, the reward is 194.33333333333334 with loss [24.601473331451416, 23.923874855041504] in episode 2285
Report: 
rewardSum:194.33333333333334
loss:[24.601473331451416, 23.923874855041504]
policies:[0, 4, 0]
qAverage:[0.0, 74.15786170959473]
ws:[-0.07841810584068298, 9.996084094047546]
memory len:10000
memory used:3165.0
now epsilon is 0.05911009488453909, the reward is 194.33333333333334 with loss [22.153082370758057, 24.66562795639038] in episode 2286
Report: 
rewardSum:194.33333333333334
loss:[22.153082370758057, 24.66562795639038]
policies:[0, 4, 0]
qAverage:[0.0, 77.58696136474609]
ws:[-0.3211417317390442, 9.415006637573242]
memory len:10000
memory used:3165.0
now epsilon is 0.05905100695224599, the reward is 194.33333333333334 with loss [18.057420253753662, 21.290833473205566] in episode 2287
Report: 
rewardSum:194.33333333333334
loss:[18.057420253753662, 21.290833473205566]
policies:[0, 4, 0]
qAverage:[0.0, 77.29139709472656]
ws:[-0.02812908887863159, 12.404631567001342]
memory len:10000
memory used:3165.0
now epsilon is 0.058991978085730895, the reward is 194.33333333333334 with loss [23.693002223968506, 22.480243682861328] in episode 2288
Report: 
rewardSum:194.33333333333334
loss:[23.693002223968506, 22.480243682861328]
policies:[0, 4, 0]
qAverage:[0.0, 77.50867767333985]
ws:[-0.7981515064835548, 9.696287202835084]
memory len:10000
memory used:3165.0
now epsilon is 0.05893300822595018, the reward is 194.33333333333334 with loss [27.175761699676514, 25.058542728424072] in episode 2289
Report: 
rewardSum:194.33333333333334
loss:[27.175761699676514, 25.058542728424072]
policies:[0, 4, 0]
qAverage:[0.0, 77.33353729248047]
ws:[0.17530832588672637, 13.455191326141357]
memory len:10000
memory used:3165.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19*		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.05887409731391924, the reward is 194.33333333333334 with loss [19.39763641357422, 28.05272388458252] in episode 2290
Report: 
rewardSum:194.33333333333334
loss:[19.39763641357422, 28.05272388458252]
policies:[0, 4, 0]
qAverage:[0.0, 77.55406799316407]
ws:[-0.6688369330018759, 9.66234312057495]
memory len:10000
memory used:3166.0
now epsilon is 0.05881524529071242, the reward is 194.33333333333334 with loss [15.875866174697876, 15.434340715408325] in episode 2291
Report: 
rewardSum:194.33333333333334
loss:[15.875866174697876, 15.434340715408325]
policies:[0, 4, 0]
qAverage:[0.0, 77.50285491943359]
ws:[-0.1544252574443817, 12.38372540473938]
memory len:10000
memory used:3165.0
now epsilon is 0.05875645209746298, the reward is 194.33333333333334 with loss [24.806354999542236, 19.046664476394653] in episode 2292
Report: 
rewardSum:194.33333333333334
loss:[24.806354999542236, 19.046664476394653]
policies:[0, 4, 0]
qAverage:[0.0, 77.26792449951172]
ws:[-0.8615709635429084, 8.978808975219726]
memory len:10000
memory used:3165.0
now epsilon is 0.05869771767536302, the reward is 194.33333333333334 with loss [24.73242998123169, 20.86140775680542] in episode 2293
Report: 
rewardSum:194.33333333333334
loss:[24.73242998123169, 20.86140775680542]
policies:[0, 4, 0]
qAverage:[0.0, 74.02567863464355]
ws:[-0.9780747927725315, 9.664134740829468]
memory len:10000
memory used:3166.0
now epsilon is 0.058639041965663406, the reward is 194.33333333333334 with loss [21.14777112007141, 19.305373191833496] in episode 2294
Report: 
rewardSum:194.33333333333334
loss:[21.14777112007141, 19.305373191833496]
policies:[0, 4, 0]
qAverage:[0.0, 78.02727508544922]
ws:[-1.0460385888814927, 9.797068691253662]
memory len:10000
memory used:3166.0
now epsilon is 0.05858042490967378, the reward is 194.33333333333334 with loss [28.994542598724365, 21.411727905273438] in episode 2295
Report: 
rewardSum:194.33333333333334
loss:[28.994542598724365, 21.411727905273438]
policies:[0, 4, 0]
qAverage:[0.0, 77.4682632446289]
ws:[-1.1440700173377991, 9.277110052108764]
memory len:10000
memory used:3166.0
now epsilon is 0.058521866448762415, the reward is 194.33333333333334 with loss [19.833367824554443, 21.063849449157715] in episode 2296
Report: 
rewardSum:194.33333333333334
loss:[19.833367824554443, 21.063849449157715]
policies:[0, 4, 0]
qAverage:[0.0, 71.84713935852051]
ws:[-0.7012363374233246, 11.993932008743286]
memory len:10000
memory used:3166.0
now epsilon is 0.05846336652435619, the reward is 194.33333333333334 with loss [24.149555206298828, 22.16278314590454] in episode 2297
Report: 
rewardSum:194.33333333333334
loss:[24.149555206298828, 22.16278314590454]
policies:[0, 4, 0]
qAverage:[0.0, 80.65285949707031]
ws:[-0.12417801022529602, 11.634947395324707]
memory len:10000
memory used:3165.0
now epsilon is 0.05840492507794056, the reward is 194.33333333333334 with loss [19.581396341323853, 16.940234184265137] in episode 2298
Report: 
rewardSum:194.33333333333334
loss:[19.581396341323853, 16.940234184265137]
policies:[2, 2, 0]
qAverage:[0.0, 65.11477915445964]
ws:[1.4454154173533122, 10.528117815653482]
memory len:10000
memory used:3166.0
now epsilon is 0.05834654205105944, the reward is 194.33333333333334 with loss [23.28475856781006, 26.972190380096436] in episode 2299
Report: 
rewardSum:194.33333333333334
loss:[23.28475856781006, 26.972190380096436]
policies:[1, 3, 0]
qAverage:[0.0, 73.85001564025879]
ws:[-0.2780143916606903, 9.29892873764038]
memory len:10000
memory used:3166.0
now epsilon is 0.05828821738531523, the reward is 194.33333333333334 with loss [21.453621864318848, 27.36794090270996] in episode 2300
Report: 
rewardSum:194.33333333333334
loss:[21.453621864318848, 27.36794090270996]
policies:[0, 4, 0]
qAverage:[0.0, 78.86632537841797]
ws:[0.6143257975578308, 12.15733470916748]
memory len:10000
memory used:3165.0
now epsilon is 0.05822995102236865, the reward is 194.33333333333334 with loss [26.30841302871704, 21.607595205307007] in episode 2301
Report: 
rewardSum:194.33333333333334
loss:[26.30841302871704, 21.607595205307007]
policies:[0, 4, 0]
qAverage:[0.0, 79.6142562866211]
ws:[-0.1464024782180786, 9.081356430053711]
memory len:10000
memory used:3165.0
now epsilon is 0.05817174290393878, the reward is 194.33333333333334 with loss [26.082817792892456, 22.476327180862427] in episode 2302
Report: 
rewardSum:194.33333333333334
loss:[26.082817792892456, 22.476327180862427]
policies:[0, 4, 0]
qAverage:[0.0, 79.47901153564453]
ws:[-0.031858548521995544, 10.824224948883057]
memory len:10000
memory used:3165.0
now epsilon is 0.05811359297180293, the reward is 194.33333333333334 with loss [26.986165523529053, 22.8723726272583] in episode 2303
Report: 
rewardSum:194.33333333333334
loss:[26.986165523529053, 22.8723726272583]
policies:[0, 4, 0]
qAverage:[0.0, 73.43482971191406]
ws:[-1.047777984291315, 7.651016682386398]
memory len:10000
memory used:3165.0
now epsilon is 0.058055501167796626, the reward is 194.33333333333334 with loss [17.922851085662842, 15.268584370613098] in episode 2304
Report: 
rewardSum:194.33333333333334
loss:[17.922851085662842, 15.268584370613098]
policies:[0, 4, 0]
qAverage:[0.0, 79.0270278930664]
ws:[-0.32897637486457826, 9.76191337108612]
memory len:10000
memory used:3166.0
now epsilon is 0.05799746743381353, the reward is 194.33333333333334 with loss [16.04957103729248, 28.725971221923828] in episode 2305
Report: 
rewardSum:194.33333333333334
loss:[16.04957103729248, 28.725971221923828]
policies:[0, 4, 0]
qAverage:[0.0, 79.87445983886718]
ws:[-0.3434015572071075, 9.761319589614867]
memory len:10000
memory used:3166.0
now epsilon is 0.057939491711805395, the reward is 194.33333333333334 with loss [15.8815416097641, 21.035412788391113] in episode 2306
Report: 
rewardSum:194.33333333333334
loss:[15.8815416097641, 21.035412788391113]
policies:[0, 4, 0]
qAverage:[0.0, 79.45762634277344]
ws:[-0.1744919538497925, 9.964608192443848]
memory len:10000
memory used:3166.0
now epsilon is 0.057881573943782, the reward is 194.33333333333334 with loss [16.046123504638672, 25.661024570465088] in episode 2307
Report: 
rewardSum:194.33333333333334
loss:[16.046123504638672, 25.661024570465088]
policies:[0, 4, 0]
qAverage:[0.0, 79.13325653076171]
ws:[-0.8376859366893769, 6.9793387174606325]
memory len:10000
memory used:3166.0
now epsilon is 0.05782371407181108, the reward is 194.33333333333334 with loss [26.58846640586853, 30.912782907485962] in episode 2308
Report: 
rewardSum:194.33333333333334
loss:[26.58846640586853, 30.912782907485962]
policies:[0, 4, 0]
qAverage:[0.0, 80.09084777832031]
ws:[-0.27447277754545213, 8.52129831314087]
memory len:10000
memory used:3166.0
now epsilon is 0.05776591203801829, the reward is 194.33333333333334 with loss [19.8133282661438, 24.08314037322998] in episode 2309
Report: 
rewardSum:194.33333333333334
loss:[19.8133282661438, 24.08314037322998]
policies:[1, 3, 0]
qAverage:[0.0, 79.08285903930664]
ws:[-0.7615432590246201, 6.329909235239029]
memory len:10000
memory used:3165.0
now epsilon is 0.05770816778458715, the reward is 194.33333333333334 with loss [23.033625602722168, 28.05483102798462] in episode 2310
Report: 
rewardSum:194.33333333333334
loss:[23.033625602722168, 28.05483102798462]
policies:[0, 4, 0]
qAverage:[0.0, 81.36133117675782]
ws:[-0.28851900100708006, 7.292321825027466]
memory len:10000
memory used:3166.0
now epsilon is 0.057650481253758955, the reward is 194.33333333333334 with loss [20.199758768081665, 25.340558528900146] in episode 2311
Report: 
rewardSum:194.33333333333334
loss:[20.199758768081665, 25.340558528900146]
policies:[0, 4, 0]
qAverage:[0.0, 82.11589660644532]
ws:[0.3671349465847015, 10.583765459060668]
memory len:10000
memory used:3166.0
now epsilon is 0.057592852387832745, the reward is 194.33333333333334 with loss [29.74684476852417, 24.82834005355835] in episode 2312
Report: 
rewardSum:194.33333333333334
loss:[29.74684476852417, 24.82834005355835]
policies:[0, 4, 0]
qAverage:[0.0, 82.02522430419921]
ws:[0.14688458442687988, 9.406928157806396]
memory len:10000
memory used:3166.0
now epsilon is 0.057535281129165235, the reward is 194.33333333333334 with loss [23.43044924736023, 20.877848625183105] in episode 2313
Report: 
rewardSum:194.33333333333334
loss:[23.43044924736023, 20.877848625183105]
policies:[0, 4, 0]
qAverage:[0.0, 81.70227355957032]
ws:[0.7519224286079407, 11.793640232086181]
memory len:10000
memory used:3166.0
now epsilon is 0.05747776742017077, the reward is 194.33333333333334 with loss [25.224344491958618, 17.802599906921387] in episode 2314
Report: 
rewardSum:194.33333333333334
loss:[25.224344491958618, 17.802599906921387]
policies:[0, 4, 0]
qAverage:[0.0, 72.01879692077637]
ws:[0.7786742597818375, 6.680094838142395]
memory len:10000
memory used:3166.0
now epsilon is 0.05742031120332126, the reward is 194.33333333333334 with loss [24.652210235595703, 24.39549970626831] in episode 2315
Report: 
rewardSum:194.33333333333334
loss:[24.652210235595703, 24.39549970626831]
policies:[0, 4, 0]
qAverage:[0.0, 81.38991241455078]
ws:[1.4758498817682266, 13.681828498840332]
memory len:10000
memory used:3165.0
now epsilon is 0.0573629124211461, the reward is 194.33333333333334 with loss [19.581832885742188, 23.348084926605225] in episode 2316
Report: 
rewardSum:194.33333333333334
loss:[19.581832885742188, 23.348084926605225]
policies:[0, 4, 0]
qAverage:[0.0, 75.68728637695312]
ws:[1.4865527153015137, 14.348108410835266]
memory len:10000
memory used:3166.0
now epsilon is 0.057305571016232154, the reward is 194.33333333333334 with loss [27.457703351974487, 30.46931791305542] in episode 2317
Report: 
rewardSum:194.33333333333334
loss:[27.457703351974487, 30.46931791305542]
policies:[0, 4, 0]
qAverage:[0.0, 81.87886505126953]
ws:[1.3051963090896606, 12.394955062866211]
memory len:10000
memory used:3166.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23*		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.057248286931223684, the reward is 194.33333333333334 with loss [25.81722593307495, 26.178675174713135] in episode 2318
Report: 
rewardSum:194.33333333333334
loss:[25.81722593307495, 26.178675174713135]
policies:[0, 3, 1]
qAverage:[0.0, 80.46243476867676]
ws:[1.591049998998642, 15.752854108810425]
memory len:10000
memory used:3166.0
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.05719106010882227, the reward is 194.33333333333334 with loss [21.359827518463135, 22.462920427322388] in episode 2319
Report: 
rewardSum:194.33333333333334
loss:[21.359827518463135, 22.462920427322388]
policies:[0, 4, 0]
qAverage:[0.0, 82.0644359588623]
ws:[3.0943450927734375, 20.407216548919678]
memory len:10000
memory used:3166.0
now epsilon is 0.05713389049178678, the reward is 194.33333333333334 with loss [28.093021392822266, 23.736087799072266] in episode 2320
Report: 
rewardSum:194.33333333333334
loss:[28.093021392822266, 23.736087799072266]
policies:[2, 2, 0]
qAverage:[0.0, 68.48093159993489]
ws:[1.975380261739095, 14.247615973154703]
memory len:10000
memory used:3166.0
now epsilon is 0.05707677802293329, the reward is 194.33333333333334 with loss [28.288838386535645, 20.695995330810547] in episode 2321
Report: 
rewardSum:194.33333333333334
loss:[28.288838386535645, 20.695995330810547]
policies:[1, 3, 0]
qAverage:[0.0, 82.48360824584961]
ws:[3.3975948095321655, 18.205504655838013]
memory len:10000
memory used:3166.0
now epsilon is 0.057019722645135044, the reward is 194.33333333333334 with loss [29.933745861053467, 21.394243240356445] in episode 2322
Report: 
rewardSum:194.33333333333334
loss:[29.933745861053467, 21.394243240356445]
policies:[0, 4, 0]
qAverage:[0.0, 83.5681381225586]
ws:[2.1431090354919435, 10.92555899620056]
memory len:10000
memory used:3166.0
now epsilon is 0.0569627243013224, the reward is 194.33333333333334 with loss [28.67545223236084, 29.91417646408081] in episode 2323
Report: 
rewardSum:194.33333333333334
loss:[28.67545223236084, 29.91417646408081]
policies:[0, 4, 0]
qAverage:[0.0, 84.14120788574219]
ws:[1.8983363509178162, 9.968226718902589]
memory len:10000
memory used:3167.0
now epsilon is 0.05690578293448275, the reward is 194.33333333333334 with loss [16.22351336479187, 18.02173376083374] in episode 2324
Report: 
rewardSum:194.33333333333334
loss:[16.22351336479187, 18.02173376083374]
policies:[0, 3, 1]
qAverage:[0.0, 73.25261688232422]
ws:[1.7838627323508263, 8.742252796888351]
memory len:10000
memory used:3166.0
now epsilon is 0.05684889848766048, the reward is 194.33333333333334 with loss [26.263291358947754, 31.590430736541748] in episode 2325
Report: 
rewardSum:194.33333333333334
loss:[26.263291358947754, 31.590430736541748]
policies:[0, 4, 0]
qAverage:[0.0, 84.33945159912109]
ws:[1.24010888338089, 11.694890356063842]
memory len:10000
memory used:3165.0
now epsilon is 0.05676367841800939, the reward is 192.33333333333334 with loss [39.85527849197388, 35.13179969787598] in episode 2326
Report: 
rewardSum:192.33333333333334
loss:[39.85527849197388, 35.13179969787598]
policies:[0, 5, 1]
qAverage:[0.0, 84.09394454956055]
ws:[1.3156402955452602, 14.501442670822144]
memory len:10000
memory used:3166.0
now epsilon is 0.05670693602242328, the reward is 194.33333333333334 with loss [26.596019744873047, 30.560749053955078] in episode 2327
Report: 
rewardSum:194.33333333333334
loss:[26.596019744873047, 30.560749053955078]
policies:[1, 3, 0]
qAverage:[0.0, 82.25138092041016]
ws:[0.24378225952386856, 12.152361154556274]
memory len:10000
memory used:3166.0
now epsilon is 0.05665025034795791, the reward is 194.33333333333334 with loss [26.02112913131714, 24.224855184555054] in episode 2328
Report: 
rewardSum:194.33333333333334
loss:[26.02112913131714, 24.224855184555054]
policies:[0, 4, 0]
qAverage:[0.0, 84.2154769897461]
ws:[-0.19200003147125244, 11.553521990776062]
memory len:10000
memory used:3167.0
now epsilon is 0.0565653280643458, the reward is 192.33333333333334 with loss [36.29895639419556, 41.41202116012573] in episode 2329
Report: 
rewardSum:192.33333333333334
loss:[36.29895639419556, 41.41202116012573]
policies:[0, 4, 2]
qAverage:[0.0, 80.95792999267579]
ws:[-0.8055861532688141, 7.963703966140747]
memory len:10000
memory used:3167.0
now epsilon is 0.05650878394474438, the reward is 194.33333333333334 with loss [30.33554172515869, 24.680916786193848] in episode 2330
Report: 
rewardSum:194.33333333333334
loss:[30.33554172515869, 24.680916786193848]
policies:[0, 4, 0]
qAverage:[0.0, 82.85640869140624]
ws:[0.3822296380996704, 13.981245803833009]
memory len:10000
memory used:3166.0
now epsilon is 0.056452296348062043, the reward is 194.33333333333334 with loss [21.651328086853027, 20.882713794708252] in episode 2331
Report: 
rewardSum:194.33333333333334
loss:[21.651328086853027, 20.882713794708252]
policies:[0, 4, 0]
qAverage:[0.0, 83.37234039306641]
ws:[0.6173701856285334, 12.614108562469482]
memory len:10000
memory used:3166.0
now epsilon is 0.05636767080992974, the reward is 192.33333333333334 with loss [30.780399322509766, 35.10064506530762] in episode 2332
Report: 
rewardSum:192.33333333333334
loss:[30.780399322509766, 35.10064506530762]
policies:[0, 4, 2]
qAverage:[0.0, 76.39679260253907]
ws:[0.07632651329040527, 6.1267777442932125]
memory len:10000
memory used:3166.0
now epsilon is 0.056311324273473615, the reward is 194.33333333333334 with loss [22.82107162475586, 24.281798839569092] in episode 2333
Report: 
rewardSum:194.33333333333334
loss:[22.82107162475586, 24.281798839569092]
policies:[0, 4, 0]
qAverage:[0.0, 85.16156616210938]
ws:[-0.4570711076259613, 8.855857944488525]
memory len:10000
memory used:3166.0
now epsilon is 0.05622691006133592, the reward is 192.33333333333334 with loss [39.247788429260254, 41.850223541259766] in episode 2334
Report: 
rewardSum:192.33333333333334
loss:[39.247788429260254, 41.850223541259766]
policies:[0, 5, 1]
qAverage:[0.0, 81.9799591064453]
ws:[-0.32095459699630735, 10.41525685787201]
memory len:10000
memory used:3166.0
now epsilon is 0.056142622391404495, the reward is 192.33333333333334 with loss [30.589710474014282, 43.446731090545654] in episode 2335
Report: 
rewardSum:192.33333333333334
loss:[30.589710474014282, 43.446731090545654]
policies:[1, 4, 1]
qAverage:[0.0, 79.80248107910157]
ws:[-0.02988034635782242, 4.290902960300445]
memory len:10000
memory used:3166.0
now epsilon is 0.0560865008189878, the reward is 194.33333333333334 with loss [20.77516746520996, 25.28399133682251] in episode 2336
Report: 
rewardSum:194.33333333333334
loss:[20.77516746520996, 25.28399133682251]
policies:[1, 3, 0]
qAverage:[0.0, 70.16240692138672]
ws:[0.7061510483423868, 7.509096781412761]
memory len:10000
memory used:3166.0
now epsilon is 0.056030435347101434, the reward is 194.33333333333334 with loss [19.803736925125122, 20.476969957351685] in episode 2337
Report: 
rewardSum:194.33333333333334
loss:[19.803736925125122, 20.476969957351685]
policies:[0, 4, 0]
qAverage:[0.0, 84.18508453369141]
ws:[1.6468632578849793, 17.764719676971435]
memory len:10000
memory used:3166.0
now epsilon is 0.05597442591966591, the reward is 194.33333333333334 with loss [22.37966823577881, 24.18064260482788] in episode 2338
Report: 
rewardSum:194.33333333333334
loss:[22.37966823577881, 24.18064260482788]
policies:[0, 4, 0]
qAverage:[0.0, 84.70513305664062]
ws:[1.4992716312408447, 14.860593795776367]
memory len:10000
memory used:3166.0
now epsilon is 0.055890516739321994, the reward is 192.33333333333334 with loss [37.50642967224121, 38.11805820465088] in episode 2339
Report: 
rewardSum:192.33333333333334
loss:[37.50642967224121, 38.11805820465088]
policies:[0, 4, 2]
qAverage:[0.0, 82.53629760742187]
ws:[0.5984356015920639, 10.70392198562622]
memory len:10000
memory used:3165.0
now epsilon is 0.05583464717803351, the reward is 194.33333333333334 with loss [20.637736797332764, 23.097254753112793] in episode 2340
Report: 
rewardSum:194.33333333333334
loss:[20.637736797332764, 23.097254753112793]
policies:[0, 3, 1]
qAverage:[0.0, 84.47628211975098]
ws:[0.22302312776446342, 11.57250189781189]
memory len:10000
memory used:3166.0
now epsilon is 0.05577883346535873, the reward is 194.33333333333334 with loss [22.59940242767334, 27.107951164245605] in episode 2341
Report: 
rewardSum:194.33333333333334
loss:[22.59940242767334, 27.107951164245605]
policies:[0, 4, 0]
qAverage:[0.0, 83.20659484863282]
ws:[-0.37822816595435144, 8.176077926158905]
memory len:10000
memory used:3166.0
now epsilon is 0.055723075545469965, the reward is 194.33333333333334 with loss [27.911570072174072, 23.888197898864746] in episode 2342
Report: 
rewardSum:194.33333333333334
loss:[27.911570072174072, 23.888197898864746]
policies:[0, 4, 0]
qAverage:[0.0, 85.37240447998047]
ws:[-0.48821750059723856, 9.72166121006012]
memory len:10000
memory used:3167.0
now epsilon is 0.055667373362595356, the reward is 194.33333333333334 with loss [21.879759788513184, 29.405210494995117] in episode 2343
Report: 
rewardSum:194.33333333333334
loss:[21.879759788513184, 29.405210494995117]
policies:[0, 4, 0]
qAverage:[0.0, 82.55863494873047]
ws:[-0.3936600714921951, 10.090538740158081]
memory len:10000
memory used:3166.0
############# STATE ###############
0-		9*		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.05561172686101878, the reward is 194.33333333333334 with loss [25.32313346862793, 24.601423263549805] in episode 2344
Report: 
rewardSum:194.33333333333334
loss:[25.32313346862793, 24.601423263549805]
policies:[0, 4, 0]
qAverage:[0.0, 83.97914428710938]
ws:[-0.48404319770634174, 9.566667652130127]
memory len:10000
memory used:3166.0
now epsilon is 0.05555613598507983, the reward is 194.33333333333334 with loss [24.111363887786865, 25.10976219177246] in episode 2345
Report: 
rewardSum:194.33333333333334
loss:[24.111363887786865, 25.10976219177246]
policies:[0, 4, 0]
qAverage:[0.0, 82.37957153320312]
ws:[0.4662096560001373, 13.783488035202026]
memory len:10000
memory used:3166.0
now epsilon is 0.05550060067917371, the reward is 194.33333333333334 with loss [23.57017183303833, 26.068665027618408] in episode 2346
Report: 
rewardSum:194.33333333333334
loss:[23.57017183303833, 26.068665027618408]
policies:[0, 4, 0]
qAverage:[0.0, 83.54356079101562]
ws:[-0.1829814463853836, 10.085311269760131]
memory len:10000
memory used:3166.0
now epsilon is 0.05544512088775123, the reward is 194.33333333333334 with loss [23.984222888946533, 20.769703149795532] in episode 2347
Report: 
rewardSum:194.33333333333334
loss:[23.984222888946533, 20.769703149795532]
policies:[0, 4, 0]
qAverage:[0.0, 83.60061492919922]
ws:[-0.3746697599533945, 9.578222417831421]
memory len:10000
memory used:3166.0
now epsilon is 0.05538969655531871, the reward is 194.33333333333334 with loss [19.19688391685486, 22.524248600006104] in episode 2348
Report: 
rewardSum:194.33333333333334
loss:[19.19688391685486, 22.524248600006104]
policies:[0, 4, 0]
qAverage:[0.0, 80.70280456542969]
ws:[-0.1695495245512575, 14.721622705459595]
memory len:10000
memory used:3166.0
now epsilon is 0.05533432762643797, the reward is 194.33333333333334 with loss [27.729968547821045, 26.122098922729492] in episode 2349
Report: 
rewardSum:194.33333333333334
loss:[27.729968547821045, 26.122098922729492]
policies:[0, 4, 0]
qAverage:[0.0, 83.74967498779297]
ws:[-0.4828660959377885, 8.80588357448578]
memory len:10000
memory used:3166.0
now epsilon is 0.055279014045726216, the reward is 194.33333333333334 with loss [30.404990196228027, 22.917901515960693] in episode 2350
Report: 
rewardSum:194.33333333333334
loss:[30.404990196228027, 22.917901515960693]
policies:[0, 4, 0]
qAverage:[0.0, 81.61134033203125]
ws:[0.16609606444835662, 12.250096821784973]
memory len:10000
memory used:3166.0
now epsilon is 0.055223755757856034, the reward is 194.33333333333334 with loss [25.907586097717285, 22.62412977218628] in episode 2351
Report: 
rewardSum:194.33333333333334
loss:[25.907586097717285, 22.62412977218628]
policies:[0, 3, 1]
qAverage:[0.0, 80.1070442199707]
ws:[-0.1415754584595561, 9.525093257427216]
memory len:10000
memory used:3166.0
now epsilon is 0.05516855270755532, the reward is 194.33333333333334 with loss [29.341068744659424, 24.119921684265137] in episode 2352
Report: 
rewardSum:194.33333333333334
loss:[29.341068744659424, 24.119921684265137]
policies:[0, 4, 0]
qAverage:[0.0, 81.51459503173828]
ws:[0.6946543045341969, 12.738795924186707]
memory len:10000
memory used:3166.0
now epsilon is 0.05511340483960723, the reward is 194.33333333333334 with loss [13.071045160293579, 17.507855892181396] in episode 2353
Report: 
rewardSum:194.33333333333334
loss:[13.071045160293579, 17.507855892181396]
policies:[0, 4, 0]
qAverage:[0.0, 84.07551574707031]
ws:[0.8842157244682312, 12.993537759780883]
memory len:10000
memory used:3166.0
now epsilon is 0.05505831209885007, the reward is 194.33333333333334 with loss [25.2830867767334, 25.96993398666382] in episode 2354
Report: 
rewardSum:194.33333333333334
loss:[25.2830867767334, 25.96993398666382]
policies:[0, 4, 0]
qAverage:[0.0, 81.09862976074218]
ws:[1.0095699727535248, 12.51365613937378]
memory len:10000
memory used:3166.0
now epsilon is 0.055003274430177336, the reward is 194.33333333333334 with loss [24.853516101837158, 17.002039432525635] in episode 2355
Report: 
rewardSum:194.33333333333334
loss:[24.853516101837158, 17.002039432525635]
policies:[0, 4, 0]
qAverage:[0.0, 83.938525390625]
ws:[0.6454344689846039, 8.597701501846313]
memory len:10000
memory used:3166.0
now epsilon is 0.054948291778537585, the reward is 194.33333333333334 with loss [24.144025325775146, 19.251063108444214] in episode 2356
Report: 
rewardSum:194.33333333333334
loss:[24.144025325775146, 19.251063108444214]
policies:[0, 4, 0]
qAverage:[0.0, 80.573583984375]
ws:[0.49955891743302344, 7.461926615238189]
memory len:10000
memory used:3166.0
now epsilon is 0.05489336408893441, the reward is 194.33333333333334 with loss [23.737329959869385, 25.40104579925537] in episode 2357
Report: 
rewardSum:194.33333333333334
loss:[23.737329959869385, 25.40104579925537]
policies:[1, 3, 0]
qAverage:[13.964912414550781, 64.88000030517578]
ws:[0.1633590381592512, 6.727625572681427]
memory len:10000
memory used:3166.0
now epsilon is 0.054838491306426394, the reward is 194.33333333333334 with loss [19.67180585861206, 26.84700298309326] in episode 2358
Report: 
rewardSum:194.33333333333334
loss:[19.67180585861206, 26.84700298309326]
policies:[1, 3, 0]
qAverage:[13.78909912109375, 63.24932098388672]
ws:[0.10112500190734863, 6.353215372562408]
memory len:10000
memory used:3166.0
now epsilon is 0.05478367337612703, the reward is 194.33333333333334 with loss [30.139720916748047, 25.537074089050293] in episode 2359
Report: 
rewardSum:194.33333333333334
loss:[30.139720916748047, 25.537074089050293]
policies:[1, 3, 0]
qAverage:[14.251734924316406, 64.11404113769531]
ws:[0.008832895755767822, 6.280491828918457]
memory len:10000
memory used:3166.0
now epsilon is 0.05470154920863996, the reward is 192.33333333333334 with loss [34.890448570251465, 34.036933183670044] in episode 2360
Report: 
rewardSum:192.33333333333334
loss:[34.890448570251465, 34.036933183670044]
policies:[1, 4, 1]
qAverage:[11.401217142740885, 68.9908816019694]
ws:[-0.34630538026491803, 4.815487335125606]
memory len:10000
memory used:3165.0
now epsilon is 0.05464686816909365, the reward is 194.33333333333334 with loss [19.604493856430054, 24.864123821258545] in episode 2361
Report: 
rewardSum:194.33333333333334
loss:[19.604493856430054, 24.864123821258545]
policies:[0, 4, 0]
qAverage:[0.0, 72.14118957519531]
ws:[-0.11312353983521461, 2.7704484090209007]
memory len:10000
memory used:3165.0
now epsilon is 0.054592241790084914, the reward is 194.33333333333334 with loss [23.655758380889893, 27.48377752304077] in episode 2362
Report: 
rewardSum:194.33333333333334
loss:[23.655758380889893, 27.48377752304077]
policies:[0, 3, 1]
qAverage:[0.0, 79.72649002075195]
ws:[0.5252994671463966, 12.678212285041809]
memory len:10000
memory used:3165.0
now epsilon is 0.054537670016973704, the reward is 194.33333333333334 with loss [22.029078483581543, 25.00998544692993] in episode 2363
Report: 
rewardSum:194.33333333333334
loss:[22.029078483581543, 25.00998544692993]
policies:[0, 4, 0]
qAverage:[0.0, 81.62885284423828]
ws:[0.31593224927783015, 8.672876238822937]
memory len:10000
memory used:3167.0
now epsilon is 0.05451040459056959, the reward is -1.0 with loss [13.558242321014404, 10.366374492645264] in episode 2364
Report: 
rewardSum:-1.0
loss:[13.558242321014404, 10.366374492645264]
policies:[0, 1, 1]
qAverage:[0.0, 45.07085418701172]
ws:[0.21843695640563965, 0.6777103543281555]
memory len:10000
memory used:3167.0
now epsilon is 0.05445591462397407, the reward is 194.33333333333334 with loss [24.739137649536133, 25.31827688217163] in episode 2365
Report: 
rewardSum:194.33333333333334
loss:[24.739137649536133, 25.31827688217163]
policies:[0, 4, 0]
qAverage:[0.0, 80.49787139892578]
ws:[0.2878685712814331, 9.09130311012268]
memory len:10000
memory used:3166.0
now epsilon is 0.0544014791269148, the reward is 194.33333333333334 with loss [22.494566440582275, 22.88569736480713] in episode 2366
Report: 
rewardSum:194.33333333333334
loss:[22.494566440582275, 22.88569736480713]
policies:[0, 4, 0]
qAverage:[0.0, 82.24953002929688]
ws:[0.3965004563331604, 10.087867069244385]
memory len:10000
memory used:3166.0
now epsilon is 0.05434709804494268, the reward is 194.33333333333334 with loss [21.724586963653564, 27.879316329956055] in episode 2367
Report: 
rewardSum:194.33333333333334
loss:[21.724586963653564, 27.879316329956055]
policies:[0, 4, 0]
qAverage:[0.0, 81.369677734375]
ws:[0.12064361572265625, 9.907228040695191]
memory len:10000
memory used:3166.0
now epsilon is 0.054292771323663024, the reward is 194.33333333333334 with loss [25.578233242034912, 25.360500812530518] in episode 2368
Report: 
rewardSum:194.33333333333334
loss:[25.578233242034912, 25.360500812530518]
policies:[0, 4, 0]
qAverage:[0.0, 80.5697738647461]
ws:[0.2627686597406864, 12.898561668395995]
memory len:10000
memory used:3166.0
now epsilon is 0.05423849890873552, the reward is 194.33333333333334 with loss [19.666678428649902, 23.105262279510498] in episode 2369
Report: 
rewardSum:194.33333333333334
loss:[19.666678428649902, 23.105262279510498]
policies:[0, 4, 0]
qAverage:[0.0, 80.41192932128907]
ws:[-0.16446372750215232, 10.348175096511842]
memory len:10000
memory used:3166.0
now epsilon is 0.05418428074587419, the reward is 194.33333333333334 with loss [18.538731575012207, 21.447201251983643] in episode 2370
Report: 
rewardSum:194.33333333333334
loss:[18.538731575012207, 21.447201251983643]
policies:[0, 4, 0]
qAverage:[0.0, 79.82375946044922]
ws:[0.22875554263591766, 13.63439416885376]
memory len:10000
memory used:3166.0
now epsilon is 0.0541301167808473, the reward is 194.33333333333334 with loss [19.17234206199646, 22.630983352661133] in episode 2371
Report: 
rewardSum:194.33333333333334
loss:[19.17234206199646, 22.630983352661133]
policies:[0, 4, 0]
qAverage:[0.0, 81.14864349365234]
ws:[-0.3693983983248472, 9.23191270828247]
memory len:10000
memory used:3167.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21*		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.054076006959477334, the reward is 194.33333333333334 with loss [24.13135528564453, 32.56491661071777] in episode 2372
Report: 
rewardSum:194.33333333333334
loss:[24.13135528564453, 32.56491661071777]
policies:[0, 4, 0]
qAverage:[0.0, 79.45677490234375]
ws:[-0.25077065229415896, 11.30655841231346]
memory len:10000
memory used:3166.0
now epsilon is 0.05402195122764093, the reward is 194.33333333333334 with loss [18.92284369468689, 27.28067445755005] in episode 2373
Report: 
rewardSum:194.33333333333334
loss:[18.92284369468689, 27.28067445755005]
policies:[0, 4, 0]
qAverage:[0.0, 80.54820404052734]
ws:[-0.6339358627796173, 7.588689954578877]
memory len:10000
memory used:3166.0
now epsilon is 0.05396794953126884, the reward is 194.33333333333334 with loss [30.434555053710938, 23.511754989624023] in episode 2374
Report: 
rewardSum:194.33333333333334
loss:[30.434555053710938, 23.511754989624023]
policies:[0, 4, 0]
qAverage:[0.0, 78.40889549255371]
ws:[-0.750631220638752, 9.066007018089294]
memory len:10000
memory used:3166.0
now epsilon is 0.05391400181634586, the reward is 194.33333333333334 with loss [27.489601612091064, 31.360888481140137] in episode 2375
Report: 
rewardSum:194.33333333333334
loss:[27.489601612091064, 31.360888481140137]
policies:[0, 4, 0]
qAverage:[0.0, 79.5116958618164]
ws:[-0.7139738857746124, 6.561046481132507]
memory len:10000
memory used:3166.0
now epsilon is 0.053833181341153086, the reward is 192.33333333333334 with loss [36.333462715148926, 42.80637454986572] in episode 2376
Report: 
rewardSum:192.33333333333334
loss:[36.333462715148926, 42.80637454986572]
policies:[1, 4, 1]
qAverage:[11.592915852864584, 68.20887120564778]
ws:[-0.777142326037089, 4.188486993312836]
memory len:10000
memory used:3166.0
now epsilon is 0.053779368343890584, the reward is 194.33333333333334 with loss [29.620174884796143, 28.022231101989746] in episode 2377
Report: 
rewardSum:194.33333333333334
loss:[29.620174884796143, 28.022231101989746]
policies:[1, 3, 0]
qAverage:[13.858143615722657, 62.43730163574219]
ws:[-0.6952857732772827, 5.53932511806488]
memory len:10000
memory used:3166.0
now epsilon is 0.053725609139448835, the reward is 194.33333333333334 with loss [23.583329677581787, 26.79497766494751] in episode 2378
Report: 
rewardSum:194.33333333333334
loss:[23.583329677581787, 26.79497766494751]
policies:[1, 3, 0]
qAverage:[14.208572387695312, 61.87129669189453]
ws:[-0.7699276745319367, 5.736678051948547]
memory len:10000
memory used:3166.0
now epsilon is 0.05367190367405518, the reward is 194.33333333333334 with loss [23.31711435317993, 21.10963201522827] in episode 2379
Report: 
rewardSum:194.33333333333334
loss:[23.31711435317993, 21.10963201522827]
policies:[1, 3, 0]
qAverage:[13.996707153320312, 62.36368255615234]
ws:[-0.8073317170143127, 5.808058178424835]
memory len:10000
memory used:3166.0
now epsilon is 0.05361825189399072, the reward is 194.33333333333334 with loss [29.234028816223145, 24.78508472442627] in episode 2380
Report: 
rewardSum:194.33333333333334
loss:[29.234028816223145, 24.78508472442627]
policies:[1, 3, 0]
qAverage:[14.107656860351563, 60.85048370361328]
ws:[-0.9100099325180053, 5.959259879589081]
memory len:10000
memory used:3166.0
now epsilon is 0.05356465374559026, the reward is 194.33333333333334 with loss [31.061144828796387, 22.19257664680481] in episode 2381
Report: 
rewardSum:194.33333333333334
loss:[31.061144828796387, 22.19257664680481]
policies:[0, 4, 0]
qAverage:[0.0, 76.11343841552734]
ws:[-0.9756913006305694, 6.277745416760444]
memory len:10000
memory used:3166.0
now epsilon is 0.053484356965098956, the reward is 192.33333333333334 with loss [39.230424880981445, 33.85677361488342] in episode 2382
Report: 
rewardSum:192.33333333333334
loss:[39.230424880981445, 33.85677361488342]
policies:[0, 4, 2]
qAverage:[0.0, 77.47597961425781]
ws:[-1.376288777589798, 5.779604423046112]
memory len:10000
memory used:3166.0
now epsilon is 0.05343089266142516, the reward is 194.33333333333334 with loss [20.713634252548218, 28.34019947052002] in episode 2383
Report: 
rewardSum:194.33333333333334
loss:[20.713634252548218, 28.34019947052002]
policies:[1, 3, 0]
qAverage:[0.0, 74.07039451599121]
ws:[-1.2049692906439304, 7.3590730875730515]
memory len:10000
memory used:3166.0
now epsilon is 0.05337748180200926, the reward is 194.33333333333334 with loss [24.27028751373291, 25.528536319732666] in episode 2384
Report: 
rewardSum:194.33333333333334
loss:[24.27028751373291, 25.528536319732666]
policies:[0, 4, 0]
qAverage:[0.0, 77.32978668212891]
ws:[-0.6109658256173134, 9.81826446056366]
memory len:10000
memory used:3166.0
now epsilon is 0.05332412433342704, the reward is 194.33333333333334 with loss [25.178821563720703, 23.09670114517212] in episode 2385
Report: 
rewardSum:194.33333333333334
loss:[25.178821563720703, 23.09670114517212]
policies:[0, 4, 0]
qAverage:[0.0, 77.16810302734375]
ws:[-1.0274345606565476, 6.884375858306885]
memory len:10000
memory used:3166.0
now epsilon is 0.053270820202307695, the reward is 194.33333333333334 with loss [26.109375, 24.551953315734863] in episode 2386
Report: 
rewardSum:194.33333333333334
loss:[26.109375, 24.551953315734863]
policies:[0, 3, 1]
qAverage:[0.0, 75.35329055786133]
ws:[-0.9286212874576449, 12.04341435432434]
memory len:10000
memory used:3167.0
now epsilon is 0.053217569355333755, the reward is 194.33333333333334 with loss [25.493748664855957, 20.033201694488525] in episode 2387
Report: 
rewardSum:194.33333333333334
loss:[25.493748664855957, 20.033201694488525]
policies:[0, 4, 0]
qAverage:[0.0, 71.24324226379395]
ws:[-1.3647950440645218, 7.0800648629665375]
memory len:10000
memory used:3167.0
now epsilon is 0.05316437173924104, the reward is 194.33333333333334 with loss [19.531163692474365, 22.216875553131104] in episode 2388
Report: 
rewardSum:194.33333333333334
loss:[19.531163692474365, 22.216875553131104]
policies:[0, 4, 0]
qAverage:[0.0, 77.5937271118164]
ws:[-0.6450611323118209, 9.445554196834564]
memory len:10000
memory used:3167.0
now epsilon is 0.05311122730081864, the reward is 194.33333333333334 with loss [18.38193392753601, 19.254032850265503] in episode 2389
Report: 
rewardSum:194.33333333333334
loss:[18.38193392753601, 19.254032850265503]
policies:[0, 4, 0]
qAverage:[0.0, 77.53494720458984]
ws:[-0.45775972604751586, 9.602467679977417]
memory len:10000
memory used:3166.0
now epsilon is 0.05305813598690882, the reward is 194.33333333333334 with loss [16.458556175231934, 25.85543203353882] in episode 2390
Report: 
rewardSum:194.33333333333334
loss:[16.458556175231934, 25.85543203353882]
policies:[0, 4, 0]
qAverage:[0.0, 68.91666603088379]
ws:[0.634345144033432, 7.133005321025848]
memory len:10000
memory used:3167.0
now epsilon is 0.05300509774440698, the reward is 194.33333333333334 with loss [21.716590881347656, 28.1972713470459] in episode 2391
Report: 
rewardSum:194.33333333333334
loss:[21.716590881347656, 28.1972713470459]
policies:[0, 3, 1]
qAverage:[0.0, 71.91974067687988]
ws:[-0.5993848741054535, 7.252878546714783]
memory len:10000
memory used:3166.0
now epsilon is 0.052952112520261635, the reward is 194.33333333333334 with loss [23.047557830810547, 24.968146324157715] in episode 2392
Report: 
rewardSum:194.33333333333334
loss:[23.047557830810547, 24.968146324157715]
policies:[0, 4, 0]
qAverage:[0.0, 75.15069274902343]
ws:[-0.12894118428230286, 9.790887689590454]
memory len:10000
memory used:3166.0
now epsilon is 0.05289918026147428, the reward is 194.33333333333334 with loss [23.333463668823242, 25.563629865646362] in episode 2393
Report: 
rewardSum:194.33333333333334
loss:[23.333463668823242, 25.563629865646362]
policies:[0, 4, 0]
qAverage:[0.0, 73.46139678955078]
ws:[-0.8167425408959389, 6.535467958450317]
memory len:10000
memory used:3167.0
now epsilon is 0.05284630091509942, the reward is 194.33333333333334 with loss [20.021726608276367, 22.319024324417114] in episode 2394
Report: 
rewardSum:194.33333333333334
loss:[20.021726608276367, 22.319024324417114]
policies:[0, 4, 0]
qAverage:[0.0, 75.33004150390624]
ws:[-0.5026090525090694, 9.466395103931427]
memory len:10000
memory used:3167.0
now epsilon is 0.05279347442824448, the reward is 194.33333333333334 with loss [28.235311031341553, 22.181368350982666] in episode 2395
Report: 
rewardSum:194.33333333333334
loss:[28.235311031341553, 22.181368350982666]
policies:[0, 4, 0]
qAverage:[0.0, 73.76199188232422]
ws:[-1.02844790071249, 5.592914700508118]
memory len:10000
memory used:3167.0
now epsilon is 0.05274070074806977, the reward is 194.33333333333334 with loss [26.556854248046875, 21.755464792251587] in episode 2396
Report: 
rewardSum:194.33333333333334
loss:[26.556854248046875, 21.755464792251587]
policies:[1, 3, 0]
qAverage:[13.295176696777343, 58.163082885742185]
ws:[-0.6659346610307694, 8.006002122163773]
memory len:10000
memory used:3166.0
now epsilon is 0.052687979821788404, the reward is 194.33333333333334 with loss [27.50882339477539, 26.980623245239258] in episode 2397
Report: 
rewardSum:194.33333333333334
loss:[27.50882339477539, 26.980623245239258]
policies:[0, 4, 0]
qAverage:[0.0, 74.50074310302735]
ws:[-1.1752935141324996, 4.952264210581779]
memory len:10000
memory used:3166.0
now epsilon is 0.05263531159666626, the reward is 194.33333333333334 with loss [32.246798515319824, 24.127686023712158] in episode 2398
Report: 
rewardSum:194.33333333333334
loss:[32.246798515319824, 24.127686023712158]
policies:[1, 3, 0]
qAverage:[13.393275451660156, 57.5390380859375]
ws:[-1.276546561717987, 4.804324162006378]
memory len:10000
memory used:3166.0
now epsilon is 0.05258269602002195, the reward is 194.33333333333334 with loss [23.96956157684326, 22.59649634361267] in episode 2399
Report: 
rewardSum:194.33333333333334
loss:[23.96956157684326, 22.59649634361267]
policies:[0, 4, 0]
qAverage:[0.0, 74.6868881225586]
ws:[-1.2849361956119538, 5.132897180318833]
memory len:10000
memory used:3166.0
############# STATE ###############
0-		9-		18-		27-		36-		
1*		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.05255640795843044, the reward is -1.0 with loss [12.29491639137268, 12.80180549621582] in episode 2400
Report: 
rewardSum:-1.0
loss:[12.29491639137268, 12.80180549621582]
policies:[0, 1, 1]
qAverage:[0.0, 41.277305603027344]
ws:[-0.135518878698349, 0.14396080374717712]
memory len:10000
memory used:3166.0
now epsilon is 0.05250387125584043, the reward is 194.33333333333334 with loss [25.53273916244507, 20.784915924072266] in episode 2401
Report: 
rewardSum:194.33333333333334
loss:[25.53273916244507, 20.784915924072266]
policies:[0, 4, 0]
qAverage:[0.0, 73.82037048339843]
ws:[-1.817829144001007, 5.539929342269898]
memory len:10000
memory used:3166.0
now epsilon is 0.05245138707025504, the reward is 194.33333333333334 with loss [25.415019035339355, 18.828807830810547] in episode 2402
Report: 
rewardSum:194.33333333333334
loss:[25.415019035339355, 18.828807830810547]
policies:[0, 4, 0]
qAverage:[0.0, 74.20460052490235]
ws:[-1.7311495423316956, 6.381324076652527]
memory len:10000
memory used:3166.0
now epsilon is 0.052398955349176934, the reward is 194.33333333333334 with loss [24.70284605026245, 25.093921184539795] in episode 2403
Report: 
rewardSum:194.33333333333334
loss:[24.70284605026245, 25.093921184539795]
policies:[0, 4, 0]
qAverage:[0.0, 74.41091461181641]
ws:[-1.6512001514434815, 6.97635850906372]
memory len:10000
memory used:3166.0
now epsilon is 0.05234657604016129, the reward is 194.33333333333334 with loss [20.473512172698975, 29.051000595092773] in episode 2404
Report: 
rewardSum:194.33333333333334
loss:[20.473512172698975, 29.051000595092773]
policies:[0, 4, 0]
qAverage:[0.0, 74.15716094970703]
ws:[-1.6834900021553039, 6.70546178817749]
memory len:10000
memory used:3167.0
now epsilon is 0.052281175528542986, the reward is 193.33333333333334 with loss [26.97641658782959, 28.69127130508423] in episode 2405
Report: 
rewardSum:193.33333333333334
loss:[26.97641658782959, 28.69127130508423]
policies:[0, 4, 1]
qAverage:[0.0, 73.68205108642579]
ws:[-1.2472991943359375, 10.412178421020508]
memory len:10000
memory used:3166.0
now epsilon is 0.0522289139551879, the reward is 194.33333333333334 with loss [21.302100658416748, 20.371472358703613] in episode 2406
Report: 
rewardSum:194.33333333333334
loss:[21.302100658416748, 20.371472358703613]
policies:[0, 4, 0]
qAverage:[0.0, 72.70860137939454]
ws:[-0.8145712079480291, 10.350075292587281]
memory len:10000
memory used:3167.0
now epsilon is 0.052176704623811354, the reward is 194.33333333333334 with loss [23.42714834213257, 26.874945163726807] in episode 2407
Report: 
rewardSum:194.33333333333334
loss:[23.42714834213257, 26.874945163726807]
policies:[0, 4, 0]
qAverage:[0.0, 74.18467254638672]
ws:[-0.8661686763167381, 9.702360439300538]
memory len:10000
memory used:3167.0
now epsilon is 0.05212454748219093, the reward is 194.33333333333334 with loss [22.49599599838257, 33.01900672912598] in episode 2408
Report: 
rewardSum:194.33333333333334
loss:[22.49599599838257, 33.01900672912598]
policies:[2, 2, 0]
qAverage:[0.0, 66.67334747314453]
ws:[-2.366100527346134, 6.3431142171223955]
memory len:10000
memory used:3166.0
now epsilon is 0.05207244247815647, the reward is 194.33333333333334 with loss [18.068085432052612, 22.778457164764404] in episode 2409
Report: 
rewardSum:194.33333333333334
loss:[18.068085432052612, 22.778457164764404]
policies:[0, 3, 1]
qAverage:[0.0, 72.00312614440918]
ws:[-2.1133480295538902, 5.119099259376526]
memory len:10000
memory used:3166.0
now epsilon is 0.052020389559589916, the reward is 194.33333333333334 with loss [23.87177801132202, 16.281516313552856] in episode 2410
Report: 
rewardSum:194.33333333333334
loss:[23.87177801132202, 16.281516313552856]
policies:[2, 2, 0]
qAverage:[15.763153076171875, 45.32832717895508]
ws:[-0.4728049337863922, 1.75501748919487]
memory len:10000
memory used:3166.0
now epsilon is 0.05196838867442534, the reward is 194.33333333333334 with loss [24.69951319694519, 17.57033085823059] in episode 2411
Report: 
rewardSum:194.33333333333334
loss:[24.69951319694519, 17.57033085823059]
policies:[1, 3, 0]
qAverage:[13.06156005859375, 57.01925354003906]
ws:[-1.3823134005069733, 7.372955620288849]
memory len:10000
memory used:3166.0
now epsilon is 0.05191643977064885, the reward is 194.33333333333334 with loss [25.194308757781982, 22.682931900024414] in episode 2412
Report: 
rewardSum:194.33333333333334
loss:[25.194308757781982, 22.682931900024414]
policies:[0, 4, 0]
qAverage:[0.0, 73.5025421142578]
ws:[-1.9862883687019348, 5.023218548297882]
memory len:10000
memory used:3166.0
now epsilon is 0.05186454279629855, the reward is 194.33333333333334 with loss [23.611236095428467, 20.699886560440063] in episode 2413
Report: 
rewardSum:194.33333333333334
loss:[23.611236095428467, 20.699886560440063]
policies:[0, 4, 0]
qAverage:[0.0, 72.7513412475586]
ws:[-1.5092846632003785, 9.078317999839783]
memory len:10000
memory used:3166.0
now epsilon is 0.05181269769946447, the reward is 194.33333333333334 with loss [24.1662654876709, 24.765320301055908] in episode 2414
Report: 
rewardSum:194.33333333333334
loss:[24.1662654876709, 24.765320301055908]
policies:[0, 4, 0]
qAverage:[0.0, 73.801171875]
ws:[-1.8927345596253873, 6.420748853683472]
memory len:10000
memory used:3166.0
now epsilon is 0.051760904428288554, the reward is 194.33333333333334 with loss [23.392615795135498, 23.68977117538452] in episode 2415
Report: 
rewardSum:194.33333333333334
loss:[23.392615795135498, 23.68977117538452]
policies:[0, 4, 0]
qAverage:[0.0, 72.60650177001953]
ws:[-2.174934196472168, 5.842307734489441]
memory len:10000
memory used:3166.0
now epsilon is 0.05170916293096457, the reward is 194.33333333333334 with loss [27.03806495666504, 27.11367702484131] in episode 2416
Report: 
rewardSum:194.33333333333334
loss:[27.03806495666504, 27.11367702484131]
policies:[0, 4, 0]
qAverage:[0.0, 73.78235321044922]
ws:[-1.962736466526985, 6.2116045475006105]
memory len:10000
memory used:3166.0
now epsilon is 0.05165747315573809, the reward is 194.33333333333334 with loss [27.664964199066162, 17.510437726974487] in episode 2417
Report: 
rewardSum:194.33333333333334
loss:[27.664964199066162, 17.510437726974487]
policies:[0, 4, 0]
qAverage:[0.0, 72.24227294921874]
ws:[-1.8151534467935562, 6.539555883407592]
memory len:10000
memory used:3165.0
now epsilon is 0.05160583505090641, the reward is 194.33333333333334 with loss [22.225877285003662, 19.73187828063965] in episode 2418
Report: 
rewardSum:194.33333333333334
loss:[22.225877285003662, 19.73187828063965]
policies:[0, 4, 0]
qAverage:[0.0, 70.8125991821289]
ws:[-1.847362107038498, 6.389840984344483]
memory len:10000
memory used:3166.0
now epsilon is 0.05155424856481849, the reward is 194.33333333333334 with loss [21.38602924346924, 21.03614044189453] in episode 2419
Report: 
rewardSum:194.33333333333334
loss:[21.38602924346924, 21.03614044189453]
policies:[0, 4, 0]
qAverage:[0.0, 71.10323486328124]
ws:[-1.2172580033540725, 9.862962770462037]
memory len:10000
memory used:3166.0
now epsilon is 0.05150271364587494, the reward is 194.33333333333334 with loss [17.734041690826416, 27.570329666137695] in episode 2420
Report: 
rewardSum:194.33333333333334
loss:[17.734041690826416, 27.570329666137695]
policies:[0, 4, 0]
qAverage:[0.0, 70.52175750732422]
ws:[-1.4776229918003083, 6.891678810119629]
memory len:10000
memory used:3166.0
now epsilon is 0.05145123024252797, the reward is 194.33333333333334 with loss [22.64039945602417, 16.73457145690918] in episode 2421
Report: 
rewardSum:194.33333333333334
loss:[22.64039945602417, 16.73457145690918]
policies:[0, 4, 0]
qAverage:[0.0, 70.84402313232422]
ws:[-0.819952130317688, 10.1855872631073]
memory len:10000
memory used:3166.0
now epsilon is 0.05139979830328129, the reward is 194.33333333333334 with loss [16.163352966308594, 20.422029495239258] in episode 2422
Report: 
rewardSum:194.33333333333334
loss:[16.163352966308594, 20.422029495239258]
policies:[0, 4, 0]
qAverage:[0.0, 70.46068878173828]
ws:[-1.760390615463257, 6.49167218208313]
memory len:10000
memory used:3166.0
now epsilon is 0.051348417776690095, the reward is 194.33333333333334 with loss [24.242682456970215, 22.640366554260254] in episode 2423
Report: 
rewardSum:194.33333333333334
loss:[24.242682456970215, 22.640366554260254]
policies:[0, 4, 0]
qAverage:[0.0, 70.7237548828125]
ws:[-1.7430990472435952, 6.539806938171386]
memory len:10000
memory used:3167.0
now epsilon is 0.051297088611361, the reward is 194.33333333333334 with loss [17.0056791305542, 22.677430629730225] in episode 2424
Report: 
rewardSum:194.33333333333334
loss:[17.0056791305542, 22.677430629730225]
policies:[0, 4, 0]
qAverage:[0.0, 70.37983856201171]
ws:[-1.604443473368883, 6.966920852661133]
memory len:10000
memory used:3167.0
now epsilon is 0.05124581075595201, the reward is 194.33333333333334 with loss [20.08771777153015, 23.372175931930542] in episode 2425
Report: 
rewardSum:194.33333333333334
loss:[20.08771777153015, 23.372175931930542]
policies:[0, 4, 0]
qAverage:[0.0, 70.57132568359376]
ws:[-0.8884744763374328, 10.728190565109253]
memory len:10000
memory used:3167.0
now epsilon is 0.051194584159172433, the reward is 194.33333333333334 with loss [26.624032020568848, 16.61851453781128] in episode 2426
Report: 
rewardSum:194.33333333333334
loss:[26.624032020568848, 16.61851453781128]
policies:[0, 4, 0]
qAverage:[0.0, 70.84416809082032]
ws:[-1.9488801509141922, 6.28316593170166]
memory len:10000
memory used:3167.0
now epsilon is 0.051143408769782864, the reward is 194.33333333333334 with loss [18.972442984580994, 21.231520175933838] in episode 2427
Report: 
rewardSum:194.33333333333334
loss:[18.972442984580994, 21.231520175933838]
policies:[0, 4, 0]
qAverage:[0.0, 62.903011322021484]
ws:[-0.6232612580060959, 3.2242033183574677]
memory len:10000
memory used:3167.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23*		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.05109228453659511, the reward is 194.33333333333334 with loss [25.884371757507324, 22.800289154052734] in episode 2428
Report: 
rewardSum:194.33333333333334
loss:[25.884371757507324, 22.800289154052734]
policies:[0, 4, 0]
qAverage:[0.0, 71.03601989746093]
ws:[-1.4350334107875824, 10.128743720054626]
memory len:10000
memory used:3167.0
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.051041211408472155, the reward is 194.33333333333334 with loss [24.687411308288574, 21.718586444854736] in episode 2429
Report: 
rewardSum:194.33333333333334
loss:[24.687411308288574, 21.718586444854736]
policies:[0, 4, 0]
qAverage:[0.0, 70.66130218505859]
ws:[-1.7394728899002074, 7.125555801391601]
memory len:10000
memory used:3166.0
now epsilon is 0.050990189334328084, the reward is 194.33333333333334 with loss [26.53614330291748, 12.910260915756226] in episode 2430
Report: 
rewardSum:194.33333333333334
loss:[26.53614330291748, 12.910260915756226]
policies:[0, 4, 0]
qAverage:[0.0, 71.0809112548828]
ws:[-1.177452477812767, 10.9289155960083]
memory len:10000
memory used:3166.0
now epsilon is 0.050939218263128076, the reward is 194.33333333333334 with loss [14.17663025856018, 30.599121570587158] in episode 2431
Report: 
rewardSum:194.33333333333334
loss:[14.17663025856018, 30.599121570587158]
policies:[0, 4, 0]
qAverage:[0.0, 70.30559692382812]
ws:[-1.8463748013600707, 7.7611188888549805]
memory len:10000
memory used:3166.0
now epsilon is 0.050888298143888296, the reward is 194.33333333333334 with loss [22.263827800750732, 22.954397678375244] in episode 2432
Report: 
rewardSum:194.33333333333334
loss:[22.263827800750732, 22.954397678375244]
policies:[0, 4, 0]
qAverage:[0.0, 70.95127563476562]
ws:[-1.444705555215478, 8.461434507369995]
memory len:10000
memory used:3166.0
now epsilon is 0.05083742892567591, the reward is 194.33333333333334 with loss [30.165652751922607, 30.211108207702637] in episode 2433
Report: 
rewardSum:194.33333333333334
loss:[30.165652751922607, 30.211108207702637]
policies:[0, 4, 0]
qAverage:[0.0, 71.10960083007812]
ws:[-0.8641533613204956, 9.044795799255372]
memory len:10000
memory used:3166.0
now epsilon is 0.05078661055760894, the reward is 194.33333333333334 with loss [26.82412576675415, 23.974536418914795] in episode 2434
Report: 
rewardSum:194.33333333333334
loss:[26.82412576675415, 23.974536418914795]
policies:[0, 4, 0]
qAverage:[0.0, 70.60837249755859]
ws:[-0.4525387167930603, 10.28593406677246]
memory len:10000
memory used:3166.0
now epsilon is 0.0507104782383521, the reward is 192.33333333333334 with loss [33.138688802719116, 32.78303337097168] in episode 2435
Report: 
rewardSum:192.33333333333334
loss:[33.138688802719116, 32.78303337097168]
policies:[1, 4, 1]
qAverage:[0.0, 70.96809997558594]
ws:[-0.23588303923606874, 10.123074769973755]
memory len:10000
memory used:3166.0
now epsilon is 0.050659786773373884, the reward is 194.33333333333334 with loss [23.45250415802002, 24.314024448394775] in episode 2436
Report: 
rewardSum:194.33333333333334
loss:[23.45250415802002, 24.314024448394775]
policies:[0, 4, 0]
qAverage:[0.0, 67.48849296569824]
ws:[-0.8639595210552216, 9.689220786094666]
memory len:10000
memory used:3166.0
now epsilon is 0.05060914598085452, the reward is 194.33333333333334 with loss [26.107139110565186, 24.65454912185669] in episode 2437
Report: 
rewardSum:194.33333333333334
loss:[26.107139110565186, 24.65454912185669]
policies:[0, 3, 1]
qAverage:[0.0, 68.8010196685791]
ws:[-0.5754071772098541, 14.772745847702026]
memory len:10000
memory used:3166.0
now epsilon is 0.050558555810140536, the reward is 194.33333333333334 with loss [25.482158660888672, 25.589377880096436] in episode 2438
Report: 
rewardSum:194.33333333333334
loss:[25.482158660888672, 25.589377880096436]
policies:[0, 4, 0]
qAverage:[0.0, 71.35814514160157]
ws:[-1.4373927921056748, 6.917153811454773]
memory len:10000
memory used:3166.0
now epsilon is 0.05050801621062912, the reward is 194.33333333333334 with loss [27.379530429840088, 19.560816287994385] in episode 2439
Report: 
rewardSum:194.33333333333334
loss:[27.379530429840088, 19.560816287994385]
policies:[0, 4, 0]
qAverage:[0.0, 70.344775390625]
ws:[-1.7583649158477783, 6.373738335072995]
memory len:10000
memory used:3166.0
now epsilon is 0.05045752713176802, the reward is 194.33333333333334 with loss [23.910593032836914, 26.54207134246826] in episode 2440
Report: 
rewardSum:194.33333333333334
loss:[23.910593032836914, 26.54207134246826]
policies:[0, 4, 0]
qAverage:[0.0, 71.17743377685547]
ws:[-1.7738520741462707, 6.509048712253571]
memory len:10000
memory used:3166.0
now epsilon is 0.05040708852305554, the reward is 194.33333333333334 with loss [18.089595079421997, 25.829749584197998] in episode 2441
Report: 
rewardSum:194.33333333333334
loss:[18.089595079421997, 25.829749584197998]
policies:[0, 4, 0]
qAverage:[0.0, 70.72125854492188]
ws:[-1.7463058173656463, 6.490023446083069]
memory len:10000
memory used:3166.0
now epsilon is 0.05035670033404044, the reward is 194.33333333333334 with loss [21.283589363098145, 23.598141193389893] in episode 2442
Report: 
rewardSum:194.33333333333334
loss:[21.283589363098145, 23.598141193389893]
policies:[0, 4, 0]
qAverage:[0.0, 70.66701965332031]
ws:[-1.754244899749756, 6.769655632972717]
memory len:10000
memory used:3166.0
now epsilon is 0.050306362514321926, the reward is 194.33333333333334 with loss [31.564101219177246, 24.052672386169434] in episode 2443
Report: 
rewardSum:194.33333333333334
loss:[31.564101219177246, 24.052672386169434]
policies:[1, 3, 0]
qAverage:[0.0, 62.11929512023926]
ws:[-0.13293997943401337, 3.9778100848197937]
memory len:10000
memory used:3167.0
now epsilon is 0.050256075013549596, the reward is 194.33333333333334 with loss [25.880125522613525, 26.298524856567383] in episode 2444
Report: 
rewardSum:194.33333333333334
loss:[25.880125522613525, 26.298524856567383]
policies:[0, 4, 0]
qAverage:[0.0, 70.6056900024414]
ws:[-1.105975615978241, 8.768017721176147]
memory len:10000
memory used:3167.0
now epsilon is 0.050205837781423374, the reward is 194.33333333333334 with loss [28.411094188690186, 22.73677635192871] in episode 2445
Report: 
rewardSum:194.33333333333334
loss:[28.411094188690186, 22.73677635192871]
policies:[1, 3, 0]
qAverage:[0.0, 51.94530487060547]
ws:[0.9833802183469137, 8.567803382873535]
memory len:10000
memory used:3167.0
now epsilon is 0.05013057607703778, the reward is 192.33333333333334 with loss [30.255409479141235, 40.423789978027344] in episode 2446
Report: 
rewardSum:192.33333333333334
loss:[30.255409479141235, 40.423789978027344]
policies:[0, 5, 1]
qAverage:[0.0, 73.01426823933919]
ws:[-1.740046352148056, 8.536156564950943]
memory len:10000
memory used:3167.0
now epsilon is 0.05008046429679381, the reward is 194.33333333333334 with loss [29.336742877960205, 28.832537174224854] in episode 2447
Report: 
rewardSum:194.33333333333334
loss:[29.336742877960205, 28.832537174224854]
policies:[0, 4, 0]
qAverage:[0.0, 71.37769775390625]
ws:[-2.107178473472595, 8.848257458209991]
memory len:10000
memory used:3166.0
now epsilon is 0.0500304026095413, the reward is 194.33333333333334 with loss [22.0149085521698, 20.520660877227783] in episode 2448
Report: 
rewardSum:194.33333333333334
loss:[22.0149085521698, 20.520660877227783]
policies:[0, 4, 0]
qAverage:[0.0, 71.34169464111328]
ws:[-2.4646308183670045, 5.940866357088089]
memory len:10000
memory used:3166.0
now epsilon is 0.04996789586746474, the reward is 193.33333333333334 with loss [32.264498233795166, 28.584028244018555] in episode 2449
Report: 
rewardSum:193.33333333333334
loss:[32.264498233795166, 28.584028244018555]
policies:[0, 4, 1]
qAverage:[0.0, 70.76444854736329]
ws:[-1.574361836910248, 10.411821183562278]
memory len:10000
memory used:3167.0
now epsilon is 0.049917946706435436, the reward is 194.33333333333334 with loss [22.9822998046875, 30.982842922210693] in episode 2450
Report: 
rewardSum:194.33333333333334
loss:[22.9822998046875, 30.982842922210693]
policies:[0, 4, 0]
qAverage:[0.0, 71.854638671875]
ws:[-1.301503735780716, 10.846074295043945]
memory len:10000
memory used:3167.0
now epsilon is 0.049868047475839346, the reward is 194.33333333333334 with loss [24.26429271697998, 24.014642477035522] in episode 2451
Report: 
rewardSum:194.33333333333334
loss:[24.26429271697998, 24.014642477035522]
policies:[1, 3, 0]
qAverage:[0.0, 61.47501564025879]
ws:[0.032142896205186844, 4.479257822036743]
memory len:10000
memory used:3167.0
now epsilon is 0.049818198125764754, the reward is 194.33333333333334 with loss [27.766430854797363, 26.043728351593018] in episode 2452
Report: 
rewardSum:194.33333333333334
loss:[27.766430854797363, 26.043728351593018]
policies:[0, 4, 0]
qAverage:[0.0, 71.7346206665039]
ws:[-0.6826773941516876, 12.301605796813964]
memory len:10000
memory used:3167.0
now epsilon is 0.04976839860634985, the reward is 194.33333333333334 with loss [23.49387264251709, 18.32755756378174] in episode 2453
Report: 
rewardSum:194.33333333333334
loss:[23.49387264251709, 18.32755756378174]
policies:[0, 4, 0]
qAverage:[0.0, 70.66966247558594]
ws:[-0.47745647430419924, 12.46368703842163]
memory len:10000
memory used:3166.0
now epsilon is 0.04971864886778265, the reward is 194.33333333333334 with loss [24.651605129241943, 26.865158081054688] in episode 2454
Report: 
rewardSum:194.33333333333334
loss:[24.651605129241943, 26.865158081054688]
policies:[0, 4, 0]
qAverage:[0.0, 72.24709930419922]
ws:[-0.38616820573806765, 12.15621247291565]
memory len:10000
memory used:3166.0
now epsilon is 0.049668948860300974, the reward is 194.33333333333334 with loss [23.32618808746338, 26.805089950561523] in episode 2455
Report: 
rewardSum:194.33333333333334
loss:[23.32618808746338, 26.805089950561523]
policies:[0, 4, 0]
qAverage:[0.0, 71.90382537841796]
ws:[-0.4704486131668091, 11.528269815444947]
memory len:10000
memory used:3166.0
############# STATE ###############
0-		9-		18-		27-		36-		
1*		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.049619298534192384, the reward is 194.33333333333334 with loss [19.60862398147583, 28.995169162750244] in episode 2456
Report: 
rewardSum:194.33333333333334
loss:[19.60862398147583, 28.995169162750244]
policies:[0, 4, 0]
qAverage:[0.0, 70.54510879516602]
ws:[-0.7594874054193497, 14.339588165283203]
memory len:10000
memory used:3166.0
now epsilon is 0.049569697839794126, the reward is 194.33333333333334 with loss [20.33426570892334, 22.45343852043152] in episode 2457
Report: 
rewardSum:194.33333333333334
loss:[20.33426570892334, 22.45343852043152]
policies:[0, 4, 0]
qAverage:[0.0, 71.47961730957032]
ws:[-1.06456658244133, 9.437721633911133]
memory len:10000
memory used:3166.0
now epsilon is 0.049520146727493115, the reward is 194.33333333333334 with loss [20.710376739501953, 30.333569526672363] in episode 2458
Report: 
rewardSum:194.33333333333334
loss:[20.710376739501953, 30.333569526672363]
policies:[0, 3, 1]
qAverage:[0.0, 70.60860061645508]
ws:[-0.445458322763443, 14.885783672332764]
memory len:10000
memory used:3166.0
now epsilon is 0.04947064514772584, the reward is 194.33333333333334 with loss [24.246498107910156, 22.862791061401367] in episode 2459
Report: 
rewardSum:194.33333333333334
loss:[24.246498107910156, 22.862791061401367]
policies:[0, 4, 0]
qAverage:[0.0, 71.02336273193359]
ws:[-0.6655727505683899, 8.743602681159974]
memory len:10000
memory used:3167.0
now epsilon is 0.049421193050978335, the reward is 194.33333333333334 with loss [24.568665981292725, 20.377117156982422] in episode 2460
Report: 
rewardSum:194.33333333333334
loss:[24.568665981292725, 20.377117156982422]
policies:[0, 4, 0]
qAverage:[0.0, 71.41143493652343]
ws:[0.02624887228012085, 11.10683079957962]
memory len:10000
memory used:3166.0
now epsilon is 0.04937179038778612, the reward is 194.33333333333334 with loss [24.355165004730225, 21.625274181365967] in episode 2461
Report: 
rewardSum:194.33333333333334
loss:[24.355165004730225, 21.625274181365967]
policies:[1, 3, 0]
qAverage:[12.182345581054687, 56.39172515869141]
ws:[-0.016373246908187866, 11.063737165927886]
memory len:10000
memory used:3166.0
now epsilon is 0.04932243710873419, the reward is 194.33333333333334 with loss [23.091546058654785, 30.608078002929688] in episode 2462
Report: 
rewardSum:194.33333333333334
loss:[23.091546058654785, 30.608078002929688]
policies:[0, 4, 0]
qAverage:[0.0, 71.03870697021485]
ws:[0.10027785301208496, 12.19311990737915]
memory len:10000
memory used:3167.0
now epsilon is 0.04927313316445692, the reward is 194.33333333333334 with loss [24.42269277572632, 26.992129802703857] in episode 2463
Report: 
rewardSum:194.33333333333334
loss:[24.42269277572632, 26.992129802703857]
policies:[0, 4, 0]
qAverage:[0.0, 71.11731262207032]
ws:[-0.05815223455429077, 9.572067928314208]
memory len:10000
memory used:3167.0
now epsilon is 0.04922387850563803, the reward is 194.33333333333334 with loss [24.032360076904297, 28.834246158599854] in episode 2464
Report: 
rewardSum:194.33333333333334
loss:[24.032360076904297, 28.834246158599854]
policies:[0, 4, 0]
qAverage:[0.0, 70.63442993164062]
ws:[0.11529378890991211, 9.31923553943634]
memory len:10000
memory used:3166.0
now epsilon is 0.04917467308301054, the reward is 194.33333333333334 with loss [21.239975452423096, 24.940101146697998] in episode 2465
Report: 
rewardSum:194.33333333333334
loss:[21.239975452423096, 24.940101146697998]
policies:[0, 4, 0]
qAverage:[0.0, 70.82015838623047]
ws:[0.7429619073867798, 12.524535965919494]
memory len:10000
memory used:3168.0
now epsilon is 0.04912551684735671, the reward is 194.33333333333334 with loss [29.905635833740234, 25.808947563171387] in episode 2466
Report: 
rewardSum:194.33333333333334
loss:[29.905635833740234, 25.808947563171387]
policies:[0, 4, 0]
qAverage:[0.0, 70.40392456054687]
ws:[0.48100560903549194, 9.146891450881958]
memory len:10000
memory used:3168.0
now epsilon is 0.04907640974950802, the reward is 194.33333333333334 with loss [24.571486473083496, 25.128612518310547] in episode 2467
Report: 
rewardSum:194.33333333333334
loss:[24.571486473083496, 25.128612518310547]
policies:[1, 3, 0]
qAverage:[11.922327423095703, 54.045623779296875]
ws:[0.44499138593673704, 8.271898114681244]
memory len:10000
memory used:3167.0
now epsilon is 0.049027351740345095, the reward is 194.33333333333334 with loss [24.38434362411499, 15.707233428955078] in episode 2468
Report: 
rewardSum:194.33333333333334
loss:[24.38434362411499, 15.707233428955078]
policies:[1, 3, 0]
qAverage:[11.928343963623046, 55.000929260253905]
ws:[0.11667317748069764, 7.524801307916642]
memory len:10000
memory used:3167.0
now epsilon is 0.048978342770797635, the reward is 194.33333333333334 with loss [24.678144931793213, 20.14428949356079] in episode 2469
Report: 
rewardSum:194.33333333333334
loss:[24.678144931793213, 20.14428949356079]
policies:[1, 3, 0]
qAverage:[12.289430236816406, 54.67898406982422]
ws:[0.029691333323717116, 8.060390132665635]
memory len:10000
memory used:3167.0
now epsilon is 0.048929382791844424, the reward is 194.33333333333334 with loss [23.073824405670166, 17.895573139190674] in episode 2470
Report: 
rewardSum:194.33333333333334
loss:[23.073824405670166, 17.895573139190674]
policies:[1, 3, 0]
qAverage:[14.851309776306152, 48.8266658782959]
ws:[-0.011897653341293335, 8.452939920127392]
memory len:10000
memory used:3168.0
now epsilon is 0.04888047175451324, the reward is 194.33333333333334 with loss [31.667535305023193, 12.58677077293396] in episode 2471
Report: 
rewardSum:194.33333333333334
loss:[31.667535305023193, 12.58677077293396]
policies:[0, 4, 0]
qAverage:[0.0, 66.78925895690918]
ws:[-0.27163836918771267, 9.146807253360748]
memory len:10000
memory used:3167.0
now epsilon is 0.0488316096098808, the reward is 194.33333333333334 with loss [26.371777057647705, 29.970130443572998] in episode 2472
Report: 
rewardSum:194.33333333333334
loss:[26.371777057647705, 29.970130443572998]
policies:[2, 2, 0]
qAverage:[0.0, 54.85043080647787]
ws:[0.17695889373620352, 3.268627882003784]
memory len:10000
memory used:3167.0
now epsilon is 0.04878279630907274, the reward is 194.33333333333334 with loss [24.00730323791504, 19.16828155517578] in episode 2473
Report: 
rewardSum:194.33333333333334
loss:[24.00730323791504, 19.16828155517578]
policies:[0, 4, 0]
qAverage:[0.0, 70.36428680419922]
ws:[-0.5534002333879471, 10.733723044395447]
memory len:10000
memory used:3168.0
now epsilon is 0.04873403180326356, the reward is 194.33333333333334 with loss [21.740735054016113, 25.966208457946777] in episode 2474
Report: 
rewardSum:194.33333333333334
loss:[21.740735054016113, 25.966208457946777]
policies:[0, 4, 0]
qAverage:[0.0, 70.87616271972657]
ws:[-1.0872493863105774, 10.482930254936218]
memory len:10000
memory used:3167.0
now epsilon is 0.048660976428486956, the reward is 192.33333333333334 with loss [36.046961307525635, 30.263463020324707] in episode 2475
Report: 
rewardSum:192.33333333333334
loss:[36.046961307525635, 30.263463020324707]
policies:[0, 5, 1]
qAverage:[0.0, 70.5546875]
ws:[-1.9189577934642632, 7.959074099858602]
memory len:10000
memory used:3168.0
now epsilon is 0.04861233369688352, the reward is 194.33333333333334 with loss [23.144543647766113, 25.894031524658203] in episode 2476
Report: 
rewardSum:194.33333333333334
loss:[23.144543647766113, 25.894031524658203]
policies:[0, 4, 0]
qAverage:[0.0, 70.4638900756836]
ws:[-1.7944199204444886, 9.641067218780517]
memory len:10000
memory used:3168.0
now epsilon is 0.04856373958977369, the reward is 194.33333333333334 with loss [21.98139500617981, 22.786259651184082] in episode 2477
Report: 
rewardSum:194.33333333333334
loss:[21.98139500617981, 22.786259651184082]
policies:[0, 4, 0]
qAverage:[0.0, 70.16678161621094]
ws:[-1.309751644730568, 12.28599672317505]
memory len:10000
memory used:3167.0
now epsilon is 0.04851519405855122, the reward is 194.33333333333334 with loss [26.75575542449951, 26.31253719329834] in episode 2478
Report: 
rewardSum:194.33333333333334
loss:[26.75575542449951, 26.31253719329834]
policies:[0, 3, 1]
qAverage:[0.0, 63.61736869812012]
ws:[-0.5301743149757385, 4.405265539884567]
memory len:10000
memory used:3168.0
now epsilon is 0.048466697054658434, the reward is 194.33333333333334 with loss [20.12705135345459, 24.48770570755005] in episode 2479
Report: 
rewardSum:194.33333333333334
loss:[20.12705135345459, 24.48770570755005]
policies:[0, 4, 0]
qAverage:[0.0, 66.87118148803711]
ws:[-2.0851477831602097, 11.074741289019585]
memory len:10000
memory used:3167.0
now epsilon is 0.04841824852958619, the reward is 194.33333333333334 with loss [26.655556678771973, 20.312777996063232] in episode 2480
Report: 
rewardSum:194.33333333333334
loss:[26.655556678771973, 20.312777996063232]
policies:[0, 4, 0]
qAverage:[0.0, 72.55047760009765]
ws:[-2.0155972480773925, 7.019064604490995]
memory len:10000
memory used:3167.0
now epsilon is 0.04835775597276514, the reward is 193.33333333333334 with loss [29.021883010864258, 28.22667169570923] in episode 2481
Report: 
rewardSum:193.33333333333334
loss:[29.021883010864258, 28.22667169570923]
policies:[0, 4, 1]
qAverage:[0.0, 63.32596397399902]
ws:[0.46418567933142185, 6.5304606556892395]
memory len:10000
memory used:3167.0
now epsilon is 0.0483094163479287, the reward is 194.33333333333334 with loss [22.909107208251953, 21.85602331161499] in episode 2482
Report: 
rewardSum:194.33333333333334
loss:[22.909107208251953, 21.85602331161499]
policies:[1, 3, 0]
qAverage:[12.450513458251953, 56.47740020751953]
ws:[-1.3559167644008994, 9.278407251834869]
memory len:10000
memory used:3167.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22*		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.04826112504459276, the reward is 194.33333333333334 with loss [22.922751903533936, 23.377514362335205] in episode 2483
Report: 
rewardSum:194.33333333333334
loss:[22.922751903533936, 23.377514362335205]
policies:[1, 3, 0]
qAverage:[0.0, 66.23755645751953]
ws:[-2.133939675986767, 7.865387342870235]
memory len:10000
memory used:3167.0
now epsilon is 0.04818877858675183, the reward is 192.33333333333334 with loss [34.46107721328735, 30.06110715866089] in episode 2484
Report: 
rewardSum:192.33333333333334
loss:[34.46107721328735, 30.06110715866089]
policies:[0, 5, 1]
qAverage:[0.0, 73.80509821573894]
ws:[-1.2201659765948232, 7.841854413350423]
memory len:10000
memory used:3167.0
now epsilon is 0.04814060787594544, the reward is 194.33333333333334 with loss [21.05303144454956, 24.126573085784912] in episode 2485
Report: 
rewardSum:194.33333333333334
loss:[21.05303144454956, 24.126573085784912]
policies:[0, 4, 0]
qAverage:[0.0, 71.77996826171875]
ws:[-1.1034347534179687, 9.05064754486084]
memory len:10000
memory used:3167.0
now epsilon is 0.048092485317788855, the reward is 194.33333333333334 with loss [16.469173669815063, 16.541853189468384] in episode 2486
Report: 
rewardSum:194.33333333333334
loss:[16.469173669815063, 16.541853189468384]
policies:[0, 4, 0]
qAverage:[0.0, 71.81341400146485]
ws:[-1.0344327628612517, 8.63634750843048]
memory len:10000
memory used:3167.0
now epsilon is 0.04804441086414747, the reward is 194.33333333333334 with loss [26.443857669830322, 26.6633620262146] in episode 2487
Report: 
rewardSum:194.33333333333334
loss:[26.443857669830322, 26.6633620262146]
policies:[0, 4, 0]
qAverage:[0.0, 71.17060699462891]
ws:[-0.598415607213974, 10.579312884807587]
memory len:10000
memory used:3167.0
now epsilon is 0.04799638446693481, the reward is 194.33333333333334 with loss [19.37270474433899, 29.23814058303833] in episode 2488
Report: 
rewardSum:194.33333333333334
loss:[19.37270474433899, 29.23814058303833]
policies:[1, 3, 0]
qAverage:[12.522749328613282, 56.284254455566405]
ws:[-1.3934342503547668, 6.690722803771496]
memory len:10000
memory used:3167.0
now epsilon is 0.04794840607811247, the reward is 194.33333333333334 with loss [27.027186393737793, 23.446261882781982] in episode 2489
Report: 
rewardSum:194.33333333333334
loss:[27.027186393737793, 23.446261882781982]
policies:[1, 3, 0]
qAverage:[12.441574096679688, 56.45736541748047]
ws:[-1.8672330915927886, 5.717103433609009]
memory len:10000
memory used:3167.0
now epsilon is 0.04790047564969006, the reward is 194.33333333333334 with loss [27.228912830352783, 27.185709476470947] in episode 2490
Report: 
rewardSum:194.33333333333334
loss:[27.228912830352783, 27.185709476470947]
policies:[1, 3, 0]
qAverage:[12.634965515136718, 55.83013763427734]
ws:[-1.3227542996406556, 9.087724030017853]
memory len:10000
memory used:3167.0
now epsilon is 0.047852593133725155, the reward is 194.33333333333334 with loss [32.563124656677246, 34.23487186431885] in episode 2491
Report: 
rewardSum:194.33333333333334
loss:[32.563124656677246, 34.23487186431885]
policies:[0, 4, 0]
qAverage:[0.0, 66.57104110717773]
ws:[-1.6310698287561536, 7.6641771495342255]
memory len:10000
memory used:3168.0
now epsilon is 0.047804758482323254, the reward is 194.33333333333334 with loss [19.49191689491272, 25.20937728881836] in episode 2492
Report: 
rewardSum:194.33333333333334
loss:[19.49191689491272, 25.20937728881836]
policies:[0, 4, 0]
qAverage:[0.0, 70.77122039794922]
ws:[-1.1451628550887107, 7.578169369697571]
memory len:10000
memory used:3168.0
now epsilon is 0.04775697164763776, the reward is 194.33333333333334 with loss [26.163268566131592, 28.15729284286499] in episode 2493
Report: 
rewardSum:194.33333333333334
loss:[26.163268566131592, 28.15729284286499]
policies:[1, 3, 0]
qAverage:[0.0, 63.87688064575195]
ws:[0.192058929707855, 4.210993528366089]
memory len:10000
memory used:3168.0
now epsilon is 0.04770923258186987, the reward is 194.33333333333334 with loss [27.42753505706787, 20.866015195846558] in episode 2494
Report: 
rewardSum:194.33333333333334
loss:[27.42753505706787, 20.866015195846558]
policies:[0, 4, 0]
qAverage:[0.0, 70.57525177001953]
ws:[-0.9100140988826751, 11.902372884750367]
memory len:10000
memory used:3168.0
now epsilon is 0.04766154123726858, the reward is 194.33333333333334 with loss [28.307263374328613, 22.833115577697754] in episode 2495
Report: 
rewardSum:194.33333333333334
loss:[28.307263374328613, 22.833115577697754]
policies:[0, 4, 0]
qAverage:[0.0, 71.99395446777343]
ws:[-1.6140683487057685, 9.898924255371094]
memory len:10000
memory used:3168.0
now epsilon is 0.04759009359321616, the reward is 192.33333333333334 with loss [34.37275743484497, 37.622920513153076] in episode 2496
Report: 
rewardSum:192.33333333333334
loss:[34.37275743484497, 37.622920513153076]
policies:[1, 4, 1]
qAverage:[0.0, 71.58782196044922]
ws:[-2.1545126125216485, 8.914791774749755]
memory len:10000
memory used:3168.0
now epsilon is 0.047542521342933845, the reward is 194.33333333333334 with loss [24.407036304473877, 20.574628829956055] in episode 2497
Report: 
rewardSum:194.33333333333334
loss:[24.407036304473877, 20.574628829956055]
policies:[0, 4, 0]
qAverage:[0.0, 71.17475280761718]
ws:[-2.4516494393348696, 7.22559003829956]
memory len:10000
memory used:3168.0
now epsilon is 0.0474949966470652, the reward is 194.33333333333334 with loss [28.594703674316406, 23.61773157119751] in episode 2498
Report: 
rewardSum:194.33333333333334
loss:[28.594703674316406, 23.61773157119751]
policies:[1, 3, 0]
qAverage:[0.0, 63.69099235534668]
ws:[-1.223340094089508, 3.1326670050621033]
memory len:10000
memory used:3168.0
now epsilon is 0.04744751945807363, the reward is 194.33333333333334 with loss [25.902299880981445, 24.315212726593018] in episode 2499
Report: 
rewardSum:194.33333333333334
loss:[25.902299880981445, 24.315212726593018]
policies:[0, 4, 0]
qAverage:[0.0, 71.16016387939453]
ws:[-2.4279123067855837, 5.515784072875976]
memory len:10000
memory used:3167.0
now epsilon is 0.04737639264611144, the reward is 192.33333333333334 with loss [33.113953828811646, 31.256520748138428] in episode 2500
Report: 
rewardSum:192.33333333333334
loss:[33.113953828811646, 31.256520748138428]
policies:[0, 5, 1]
qAverage:[0.0, 71.83537419637044]
ws:[-2.18631175160408, 7.445561945438385]
memory len:10000
memory used:3167.0
now epsilon is 0.04732903401665174, the reward is 194.33333333333334 with loss [21.107269763946533, 23.73188543319702] in episode 2501
Report: 
rewardSum:194.33333333333334
loss:[21.107269763946533, 23.73188543319702]
policies:[0, 3, 1]
qAverage:[0.0, 68.82870292663574]
ws:[-2.201368455775082, 7.163000762462616]
memory len:10000
memory used:3168.0
now epsilon is 0.047281722728064975, the reward is 194.33333333333334 with loss [19.627952814102173, 23.66967225074768] in episode 2502
Report: 
rewardSum:194.33333333333334
loss:[19.627952814102173, 23.66967225074768]
policies:[0, 4, 0]
qAverage:[0.0, 72.06493377685547]
ws:[-1.7120930217206478, 7.229504466056824]
memory len:10000
memory used:3168.0
now epsilon is 0.04723445873302802, the reward is 194.33333333333334 with loss [24.684328079223633, 26.68924856185913] in episode 2503
Report: 
rewardSum:194.33333333333334
loss:[24.684328079223633, 26.68924856185913]
policies:[0, 4, 0]
qAverage:[0.0, 71.90626373291016]
ws:[-0.752729868888855, 10.465743350982667]
memory len:10000
memory used:3168.0
now epsilon is 0.047187241984265056, the reward is 194.33333333333334 with loss [18.83344268798828, 22.777461051940918] in episode 2504
Report: 
rewardSum:194.33333333333334
loss:[18.83344268798828, 22.777461051940918]
policies:[0, 3, 1]
qAverage:[0.0, 66.51290702819824]
ws:[-1.3691791594028473, 8.205429136753082]
memory len:10000
memory used:3168.0
now epsilon is 0.04714007243454752, the reward is 194.33333333333334 with loss [21.46273899078369, 23.79431962966919] in episode 2505
Report: 
rewardSum:194.33333333333334
loss:[21.46273899078369, 23.79431962966919]
policies:[1, 3, 0]
qAverage:[12.597339630126953, 56.67097473144531]
ws:[-0.10369935035705566, 11.104299068450928]
memory len:10000
memory used:3168.0
now epsilon is 0.04709295003669407, the reward is 194.33333333333334 with loss [18.516770362854004, 28.191282272338867] in episode 2506
Report: 
rewardSum:194.33333333333334
loss:[18.516770362854004, 28.191282272338867]
policies:[0, 4, 0]
qAverage:[0.0, 71.60332489013672]
ws:[0.14173355102539062, 11.352408504486084]
memory len:10000
memory used:3167.0
now epsilon is 0.04704587474357052, the reward is 194.33333333333334 with loss [25.98828411102295, 22.399567127227783] in episode 2507
Report: 
rewardSum:194.33333333333334
loss:[25.98828411102295, 22.399567127227783]
policies:[1, 3, 0]
qAverage:[12.587528228759766, 56.73556976318359]
ws:[0.15930500030517578, 11.056847500801087]
memory len:10000
memory used:3167.0
now epsilon is 0.04699884650808979, the reward is 194.33333333333334 with loss [27.7857346534729, 21.08621597290039] in episode 2508
Report: 
rewardSum:194.33333333333334
loss:[27.7857346534729, 21.08621597290039]
policies:[1, 3, 0]
qAverage:[15.839250564575195, 49.45333480834961]
ws:[-0.8461557924747467, 7.104988440871239]
memory len:10000
memory used:3167.0
now epsilon is 0.0469518652832119, the reward is 194.33333333333334 with loss [21.722882747650146, 21.28119659423828] in episode 2509
Report: 
rewardSum:194.33333333333334
loss:[21.722882747650146, 21.28119659423828]
policies:[1, 3, 0]
qAverage:[12.846836853027344, 56.16653137207031]
ws:[-0.8633169442415237, 7.14168164730072]
memory len:10000
memory used:3167.0
now epsilon is 0.04692839228506188, the reward is -1.0 with loss [7.542666912078857, 9.559268474578857] in episode 2510
Report: 
rewardSum:-1.0
loss:[7.542666912078857, 9.559268474578857]
policies:[1, 0, 1]
qAverage:[31.644733428955078, 0.0]
ws:[-0.17901772260665894, -1.3970417976379395]
memory len:10000
memory used:3167.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2*		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.04688148148799109, the reward is 194.33333333333334 with loss [24.7724666595459, 26.297359466552734] in episode 2511
Report: 
rewardSum:194.33333333333334
loss:[24.7724666595459, 26.297359466552734]
policies:[1, 3, 0]
qAverage:[12.935983276367187, 56.222311401367186]
ws:[-2.09073725938797, 6.733817052841187]
memory len:10000
memory used:3167.0
now epsilon is 0.046834617584128754, the reward is 194.33333333333334 with loss [18.16251254081726, 19.050305366516113] in episode 2512
Report: 
rewardSum:194.33333333333334
loss:[18.16251254081726, 19.050305366516113]
policies:[1, 3, 0]
qAverage:[12.559806823730469, 56.91852722167969]
ws:[-2.2805403590202333, 4.5567028522491455]
memory len:10000
memory used:3167.0
now epsilon is 0.04678780052659924, the reward is 194.33333333333334 with loss [24.09604787826538, 21.189594984054565] in episode 2513
Report: 
rewardSum:194.33333333333334
loss:[24.09604787826538, 21.189594984054565]
policies:[1, 3, 0]
qAverage:[12.957046508789062, 56.133018493652344]
ws:[-1.9657083213329316, 7.768039202690124]
memory len:10000
memory used:3167.0
now epsilon is 0.0467410302685738, the reward is 194.33333333333334 with loss [23.867345333099365, 21.989994287490845] in episode 2514
Report: 
rewardSum:194.33333333333334
loss:[23.867345333099365, 21.989994287490845]
policies:[0, 4, 0]
qAverage:[0.0, 72.96005096435547]
ws:[-1.7985655933618545, 8.29671842455864]
memory len:10000
memory used:3168.0
now epsilon is 0.04669430676327044, the reward is 194.33333333333334 with loss [20.29034948348999, 16.124629020690918] in episode 2515
Report: 
rewardSum:194.33333333333334
loss:[20.29034948348999, 16.124629020690918]
policies:[0, 3, 1]
qAverage:[0.0, 71.37568473815918]
ws:[-1.9138815999031067, 11.150505304336548]
memory len:10000
memory used:3167.0
now epsilon is 0.046670962528282976, the reward is -1.0 with loss [9.850228786468506, 12.4831223487854] in episode 2516
Report: 
rewardSum:-1.0
loss:[9.850228786468506, 12.4831223487854]
policies:[0, 1, 1]
qAverage:[0.0, 38.591453552246094]
ws:[0.05328069627285004, 0.7591429948806763]
memory len:10000
memory used:3167.0
now epsilon is 0.04662430906444889, the reward is 194.33333333333334 with loss [20.839970588684082, 24.195727825164795] in episode 2517
Report: 
rewardSum:194.33333333333334
loss:[20.839970588684082, 24.195727825164795]
policies:[0, 4, 0]
qAverage:[0.0, 73.14426116943359]
ws:[-1.4000285148620606, 10.73618812561035]
memory len:10000
memory used:3167.0
now epsilon is 0.04655441629657461, the reward is 44.33333333333334 with loss [28.92289972305298, 33.27142143249512] in episode 2518
Report: 
rewardSum:44.33333333333334
loss:[28.92289972305298, 33.27142143249512]
policies:[0, 4, 2]
qAverage:[0.0, 67.26517944335937]
ws:[0.09070680141448975, 4.833176136016846]
memory len:10000
memory used:3167.0
now epsilon is 0.046507879335274685, the reward is 194.33333333333334 with loss [25.911417961120605, 19.55397081375122] in episode 2519
Report: 
rewardSum:194.33333333333334
loss:[25.911417961120605, 19.55397081375122]
policies:[0, 4, 0]
qAverage:[0.0, 69.71132278442383]
ws:[-1.3786298334598541, 9.096977889537811]
memory len:10000
memory used:3167.0
now epsilon is 0.0464613888934876, the reward is 194.33333333333334 with loss [27.34015989303589, 19.866483449935913] in episode 2520
Report: 
rewardSum:194.33333333333334
loss:[27.34015989303589, 19.866483449935913]
policies:[0, 4, 0]
qAverage:[0.0, 72.90267791748047]
ws:[-0.5204862833023072, 8.349650335311889]
memory len:10000
memory used:3167.0
now epsilon is 0.04641494492471131, the reward is 194.33333333333334 with loss [26.086721420288086, 20.959527015686035] in episode 2521
Report: 
rewardSum:194.33333333333334
loss:[26.086721420288086, 20.959527015686035]
policies:[0, 4, 0]
qAverage:[0.0, 73.19129943847656]
ws:[-0.2102745532989502, 8.412240362167358]
memory len:10000
memory used:3167.0
now epsilon is 0.046368547382490195, the reward is 194.33333333333334 with loss [31.178025722503662, 24.998544692993164] in episode 2522
Report: 
rewardSum:194.33333333333334
loss:[31.178025722503662, 24.998544692993164]
policies:[0, 4, 0]
qAverage:[0.0, 72.26740417480468]
ws:[0.19605164527893065, 8.97593822479248]
memory len:10000
memory used:3167.0
now epsilon is 0.04632219622041513, the reward is 194.33333333333334 with loss [25.11685061454773, 24.31876254081726] in episode 2523
Report: 
rewardSum:194.33333333333334
loss:[25.11685061454773, 24.31876254081726]
policies:[0, 4, 0]
qAverage:[0.0, 72.79000549316406]
ws:[0.24439945220947265, 9.696249008178711]
memory len:10000
memory used:3167.0
now epsilon is 0.04627589139212334, the reward is 194.33333333333334 with loss [23.25334072113037, 24.898441314697266] in episode 2524
Report: 
rewardSum:194.33333333333334
loss:[23.25334072113037, 24.898441314697266]
policies:[0, 4, 0]
qAverage:[0.0, 72.63941802978516]
ws:[0.03775961399078369, 10.26555438041687]
memory len:10000
memory used:3166.0
now epsilon is 0.046229632851298434, the reward is 194.33333333333334 with loss [26.139421463012695, 24.94696617126465] in episode 2525
Report: 
rewardSum:194.33333333333334
loss:[26.139421463012695, 24.94696617126465]
policies:[0, 4, 0]
qAverage:[0.0, 72.60919342041015]
ws:[-0.13753247261047363, 10.34611554145813]
memory len:10000
memory used:3167.0
now epsilon is 0.046183420551670296, the reward is 194.33333333333334 with loss [19.13746452331543, 22.05050754547119] in episode 2526
Report: 
rewardSum:194.33333333333334
loss:[19.13746452331543, 22.05050754547119]
policies:[0, 4, 0]
qAverage:[0.0, 72.27344512939453]
ws:[-0.10159823894500733, 10.632186603546142]
memory len:10000
memory used:3166.0
now epsilon is 0.04613725444701506, the reward is 194.33333333333334 with loss [22.27718210220337, 23.733778476715088] in episode 2527
Report: 
rewardSum:194.33333333333334
loss:[22.27718210220337, 23.733778476715088]
policies:[0, 4, 0]
qAverage:[0.0, 71.41703491210937]
ws:[0.039272212982177736, 10.58967432975769]
memory len:10000
memory used:3166.0
now epsilon is 0.04609113449115506, the reward is 194.33333333333334 with loss [25.30919122695923, 29.193509101867676] in episode 2528
Report: 
rewardSum:194.33333333333334
loss:[25.30919122695923, 29.193509101867676]
policies:[0, 4, 0]
qAverage:[0.0, 70.86519317626953]
ws:[-0.5620131134986878, 8.909987688064575]
memory len:10000
memory used:3167.0
now epsilon is 0.04604506063795883, the reward is 194.33333333333334 with loss [24.301472425460815, 23.21278953552246] in episode 2529
Report: 
rewardSum:194.33333333333334
loss:[24.301472425460815, 23.21278953552246]
policies:[1, 3, 0]
qAverage:[12.949461364746094, 55.67991180419922]
ws:[-1.4450307965278626, 7.28677139878273]
memory len:10000
memory used:3166.0
now epsilon is 0.04599903284134098, the reward is 194.33333333333334 with loss [23.586647987365723, 23.96683692932129] in episode 2530
Report: 
rewardSum:194.33333333333334
loss:[23.586647987365723, 23.96683692932129]
policies:[0, 4, 0]
qAverage:[0.0, 71.13065490722656]
ws:[-1.2828136384487152, 10.24422828257084]
memory len:10000
memory used:3167.0
now epsilon is 0.0459530510552622, the reward is 46.33333333333334 with loss [23.035483360290527, 27.144256591796875] in episode 2531
Report: 
rewardSum:46.33333333333334
loss:[23.035483360290527, 27.144256591796875]
policies:[0, 3, 1]
qAverage:[0.0, 56.094482421875]
ws:[-0.4496948917706807, 2.5397242407004037]
memory len:10000
memory used:3166.0
now epsilon is 0.045907115233729194, the reward is 194.33333333333334 with loss [30.583136558532715, 26.257802724838257] in episode 2532
Report: 
rewardSum:194.33333333333334
loss:[30.583136558532715, 26.257802724838257]
policies:[0, 4, 0]
qAverage:[0.0, 71.25338287353516]
ws:[-1.4862698137760162, 11.717773747444152]
memory len:10000
memory used:3166.0
now epsilon is 0.04586122533079467, the reward is 194.33333333333334 with loss [28.729121208190918, 24.890041828155518] in episode 2533
Report: 
rewardSum:194.33333333333334
loss:[28.729121208190918, 24.890041828155518]
policies:[0, 4, 0]
qAverage:[0.0, 71.39879760742187]
ws:[-1.9509892463684082, 9.742963790893555]
memory len:10000
memory used:3166.0
now epsilon is 0.045815381300557234, the reward is 194.33333333333334 with loss [22.965579509735107, 23.47996187210083] in episode 2534
Report: 
rewardSum:194.33333333333334
loss:[22.965579509735107, 23.47996187210083]
policies:[1, 3, 0]
qAverage:[0.0, 69.42902183532715]
ws:[-1.8192325867712498, 14.94970965385437]
memory len:10000
memory used:3167.0
now epsilon is 0.045769583097161386, the reward is 194.33333333333334 with loss [19.175626754760742, 18.44555377960205] in episode 2535
Report: 
rewardSum:194.33333333333334
loss:[19.175626754760742, 18.44555377960205]
policies:[0, 4, 0]
qAverage:[0.0, 71.83003234863281]
ws:[-2.127130722999573, 9.13361701965332]
memory len:10000
memory used:3168.0
now epsilon is 0.045723830674797475, the reward is 194.33333333333334 with loss [27.038206577301025, 22.54412031173706] in episode 2536
Report: 
rewardSum:194.33333333333334
loss:[27.038206577301025, 22.54412031173706]
policies:[0, 4, 0]
qAverage:[0.0, 71.17927398681641]
ws:[-1.8164995729923248, 11.054643523693084]
memory len:10000
memory used:3167.0
now epsilon is 0.04567812398770162, the reward is 194.33333333333334 with loss [24.67270278930664, 22.983583450317383] in episode 2537
Report: 
rewardSum:194.33333333333334
loss:[24.67270278930664, 22.983583450317383]
policies:[0, 4, 0]
qAverage:[0.0, 71.43994445800782]
ws:[-1.3292670577764512, 12.228796648979188]
memory len:10000
memory used:3168.0
now epsilon is 0.04563246299015572, the reward is 194.33333333333334 with loss [19.263723611831665, 21.982742071151733] in episode 2538
Report: 
rewardSum:194.33333333333334
loss:[19.263723611831665, 21.982742071151733]
policies:[0, 3, 1]
qAverage:[0.0, 71.36312103271484]
ws:[-0.9934754371643066, 14.938832759857178]
memory len:10000
memory used:3168.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22*		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.04558684763648734, the reward is 46.33333333333334 with loss [23.143892526626587, 18.61206042766571] in episode 2539
Report: 
rewardSum:46.33333333333334
loss:[23.143892526626587, 18.61206042766571]
policies:[0, 2, 2]
qAverage:[0.0, 58.50605773925781]
ws:[1.0439843734105427, 9.62209971745809]
memory len:10000
memory used:3167.0
now epsilon is 0.04554127788106972, the reward is 194.33333333333334 with loss [25.20207452774048, 27.005788803100586] in episode 2540
Report: 
rewardSum:194.33333333333334
loss:[25.20207452774048, 27.005788803100586]
policies:[0, 4, 0]
qAverage:[0.0, 65.00952529907227]
ws:[-1.266305334866047, 11.399380818009377]
memory len:10000
memory used:3167.0
now epsilon is 0.04549575367832171, the reward is 194.33333333333334 with loss [24.62173104286194, 25.07194471359253] in episode 2541
Report: 
rewardSum:194.33333333333334
loss:[24.62173104286194, 25.07194471359253]
policies:[1, 3, 0]
qAverage:[12.828408813476562, 56.08012390136719]
ws:[-1.015592859685421, 9.005344808101654]
memory len:10000
memory used:3167.0
now epsilon is 0.04545027498270772, the reward is 194.33333333333334 with loss [26.99760913848877, 24.358481407165527] in episode 2542
Report: 
rewardSum:194.33333333333334
loss:[26.99760913848877, 24.358481407165527]
policies:[1, 3, 0]
qAverage:[12.517592620849609, 55.56885681152344]
ws:[-1.0296710535883904, 7.42358546257019]
memory len:10000
memory used:3167.0
now epsilon is 0.04540484174873767, the reward is 194.33333333333334 with loss [21.811055183410645, 24.913241147994995] in episode 2543
Report: 
rewardSum:194.33333333333334
loss:[21.811055183410645, 24.913241147994995]
policies:[0, 3, 1]
qAverage:[0.0, 69.83764266967773]
ws:[-2.033815398812294, 5.299153506755829]
memory len:10000
memory used:3168.0
now epsilon is 0.04535945393096697, the reward is 194.33333333333334 with loss [22.775827407836914, 20.26064395904541] in episode 2544
Report: 
rewardSum:194.33333333333334
loss:[22.775827407836914, 20.26064395904541]
policies:[2, 2, 0]
qAverage:[15.617339134216309, 49.672136306762695]
ws:[-1.8607025519013405, 4.433665066957474]
memory len:10000
memory used:3168.0
now epsilon is 0.045314111483996444, the reward is 194.33333333333334 with loss [25.526042461395264, 24.398152351379395] in episode 2545
Report: 
rewardSum:194.33333333333334
loss:[25.526042461395264, 24.398152351379395]
policies:[1, 3, 0]
qAverage:[13.010089111328124, 54.90636138916015]
ws:[-1.2700177758932114, 4.401522541046143]
memory len:10000
memory used:3168.0
now epsilon is 0.045268814362472314, the reward is 194.33333333333334 with loss [27.595330238342285, 28.440900325775146] in episode 2546
Report: 
rewardSum:194.33333333333334
loss:[27.595330238342285, 28.440900325775146]
policies:[1, 3, 0]
qAverage:[15.734453201293945, 46.79748725891113]
ws:[-1.42410958558321, 4.227174639701843]
memory len:10000
memory used:3167.0
now epsilon is 0.04522356252108611, the reward is 194.33333333333334 with loss [27.55939292907715, 20.315637826919556] in episode 2547
Report: 
rewardSum:194.33333333333334
loss:[27.55939292907715, 20.315637826919556]
policies:[1, 3, 0]
qAverage:[12.93070068359375, 55.3380126953125]
ws:[-1.0816728472709656, 4.723640823364258]
memory len:10000
memory used:3167.0
now epsilon is 0.045178355914574676, the reward is 194.33333333333334 with loss [20.23938226699829, 16.83913791179657] in episode 2548
Report: 
rewardSum:194.33333333333334
loss:[20.23938226699829, 16.83913791179657]
policies:[1, 3, 0]
qAverage:[12.663333892822266, 56.23089599609375]
ws:[-1.4122661024332046, 4.788801145553589]
memory len:10000
memory used:3167.0
now epsilon is 0.045133194497720096, the reward is 194.33333333333334 with loss [24.0599627494812, 18.83534002304077] in episode 2549
Report: 
rewardSum:194.33333333333334
loss:[24.0599627494812, 18.83534002304077]
policies:[0, 3, 1]
qAverage:[0.0, 69.50362396240234]
ws:[-1.2257193177938461, 9.573374271392822]
memory len:10000
memory used:3167.0
now epsilon is 0.045088078225349666, the reward is 194.33333333333334 with loss [25.93938159942627, 17.102768421173096] in episode 2550
Report: 
rewardSum:194.33333333333334
loss:[25.93938159942627, 17.102768421173096]
policies:[2, 1, 1]
qAverage:[21.326128641764324, 31.239346822102863]
ws:[0.22000300884246826, 0.5498067935307821]
memory len:10000
memory used:3167.0
now epsilon is 0.045043007052335826, the reward is 194.33333333333334 with loss [22.75654935836792, 26.97612428665161] in episode 2551
Report: 
rewardSum:194.33333333333334
loss:[22.75654935836792, 26.97612428665161]
policies:[1, 3, 0]
qAverage:[15.833796501159668, 49.45296669006348]
ws:[-1.8657812885940075, 2.951688528060913]
memory len:10000
memory used:3167.0
now epsilon is 0.044997980933596124, the reward is 194.33333333333334 with loss [24.67401933670044, 28.059808015823364] in episode 2552
Report: 
rewardSum:194.33333333333334
loss:[24.67401933670044, 28.059808015823364]
policies:[1, 3, 0]
qAverage:[12.935360717773438, 58.16940612792969]
ws:[-1.7359325788915156, 3.329593849182129]
memory len:10000
memory used:3167.0
now epsilon is 0.04495299982409318, the reward is 194.33333333333334 with loss [26.297341346740723, 21.963375091552734] in episode 2553
Report: 
rewardSum:194.33333333333334
loss:[26.297341346740723, 21.963375091552734]
policies:[0, 4, 0]
qAverage:[0.0, 70.84761810302734]
ws:[-1.5807132367044687, 8.483163237571716]
memory len:10000
memory used:3167.0
now epsilon is 0.04490806367883464, the reward is 194.33333333333334 with loss [23.44046449661255, 23.862565994262695] in episode 2554
Report: 
rewardSum:194.33333333333334
loss:[23.44046449661255, 23.862565994262695]
policies:[2, 2, 0]
qAverage:[15.968059539794922, 48.427398681640625]
ws:[-1.5398301603272557, 6.633135989308357]
memory len:10000
memory used:3167.0
now epsilon is 0.04484074367059495, the reward is 192.33333333333334 with loss [28.995206832885742, 34.0353798866272] in episode 2555
Report: 
rewardSum:192.33333333333334
loss:[28.995206832885742, 34.0353798866272]
policies:[1, 4, 1]
qAverage:[12.7357177734375, 55.59491119384766]
ws:[-0.4070690035820007, 7.004058611392975]
memory len:10000
memory used:3167.0
now epsilon is 0.04479591973940086, the reward is 194.33333333333334 with loss [25.390454292297363, 24.932551622390747] in episode 2556
Report: 
rewardSum:194.33333333333334
loss:[25.390454292297363, 24.932551622390747]
policies:[1, 3, 0]
qAverage:[0.0, 69.44051361083984]
ws:[-0.9063136875629425, 7.042284548282623]
memory len:10000
memory used:3167.0
now epsilon is 0.0447511406153318, the reward is 194.33333333333334 with loss [17.387200593948364, 22.231691598892212] in episode 2557
Report: 
rewardSum:194.33333333333334
loss:[17.387200593948364, 22.231691598892212]
policies:[1, 3, 0]
qAverage:[0.0, 66.87365531921387]
ws:[-0.9344922006130219, 7.237403750419617]
memory len:10000
memory used:3168.0
now epsilon is 0.04470640625359743, the reward is 194.33333333333334 with loss [21.57804846763611, 21.177412271499634] in episode 2558
Report: 
rewardSum:194.33333333333334
loss:[21.57804846763611, 21.177412271499634]
policies:[0, 4, 0]
qAverage:[0.0, 72.70594635009766]
ws:[-0.306964373588562, 7.9662011623382565]
memory len:10000
memory used:3168.0
now epsilon is 0.04466171660945221, the reward is 194.33333333333334 with loss [21.229251861572266, 30.057730674743652] in episode 2559
Report: 
rewardSum:194.33333333333334
loss:[21.229251861572266, 30.057730674743652]
policies:[0, 4, 0]
qAverage:[0.0, 72.50484619140624]
ws:[0.14632298946380615, 8.430027532577515]
memory len:10000
memory used:3168.0
now epsilon is 0.04461707163819531, the reward is 194.33333333333334 with loss [24.96784210205078, 23.46032190322876] in episode 2560
Report: 
rewardSum:194.33333333333334
loss:[24.96784210205078, 23.46032190322876]
policies:[1, 3, 0]
qAverage:[0.0, 70.17415237426758]
ws:[-0.24145108461380005, 8.110259890556335]
memory len:10000
memory used:3168.0
now epsilon is 0.044572471295170585, the reward is 194.33333333333334 with loss [20.198728799819946, 25.41887331008911] in episode 2561
Report: 
rewardSum:194.33333333333334
loss:[20.198728799819946, 25.41887331008911]
policies:[0, 4, 0]
qAverage:[0.0, 72.18239135742188]
ws:[0.193464994430542, 7.789356136322022]
memory len:10000
memory used:3168.0
now epsilon is 0.04452791553576655, the reward is 194.33333333333334 with loss [26.33593511581421, 20.254929780960083] in episode 2562
Report: 
rewardSum:194.33333333333334
loss:[26.33593511581421, 20.254929780960083]
policies:[0, 4, 0]
qAverage:[0.0, 72.68690338134766]
ws:[0.34512689113616946, 8.614555358886719]
memory len:10000
memory used:3168.0
now epsilon is 0.044483404315416294, the reward is 194.33333333333334 with loss [25.029008865356445, 26.095064163208008] in episode 2563
Report: 
rewardSum:194.33333333333334
loss:[25.029008865356445, 26.095064163208008]
policies:[1, 3, 0]
qAverage:[0.0, 63.48591232299805]
ws:[1.047427624464035, 4.975345611572266]
memory len:10000
memory used:3168.0
now epsilon is 0.044438937589597466, the reward is 194.33333333333334 with loss [26.37226438522339, 21.839735507965088] in episode 2564
Report: 
rewardSum:194.33333333333334
loss:[26.37226438522339, 21.839735507965088]
policies:[0, 4, 0]
qAverage:[0.0, 74.5884994506836]
ws:[-0.9211723312735558, 5.291768717765808]
memory len:10000
memory used:3167.0
now epsilon is 0.04439451531383221, the reward is 46.33333333333334 with loss [29.249526500701904, 18.770982027053833] in episode 2565
Report: 
rewardSum:46.33333333333334
loss:[29.249526500701904, 18.770982027053833]
policies:[1, 2, 1]
qAverage:[0.0, 58.6745859781901]
ws:[0.07737115770578384, 1.7054232756296794]
memory len:10000
memory used:3167.0
now epsilon is 0.04435013744368715, the reward is 194.33333333333334 with loss [19.825088024139404, 24.655938148498535] in episode 2566
Report: 
rewardSum:194.33333333333334
loss:[19.825088024139404, 24.655938148498535]
policies:[0, 4, 0]
qAverage:[0.0, 65.50140571594238]
ws:[0.09971672669053078, 5.307071689516306]
memory len:10000
memory used:3167.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22*		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.04430580393477329, the reward is 194.33333333333334 with loss [24.525983810424805, 25.113561153411865] in episode 2567
Report: 
rewardSum:194.33333333333334
loss:[24.525983810424805, 25.113561153411865]
policies:[0, 4, 0]
qAverage:[0.0, 73.52809295654296]
ws:[-0.7065604522824287, 4.416608452796936]
memory len:10000
memory used:3167.0
now epsilon is 0.044261514742746054, the reward is 194.33333333333334 with loss [20.919142723083496, 21.341333866119385] in episode 2568
Report: 
rewardSum:194.33333333333334
loss:[20.919142723083496, 21.341333866119385]
policies:[0, 4, 0]
qAverage:[0.0, 75.89928894042968]
ws:[-0.440458208322525, 4.526870274543763]
memory len:10000
memory used:3167.0
now epsilon is 0.04421726982330517, the reward is 194.33333333333334 with loss [26.295448303222656, 21.202778100967407] in episode 2569
Report: 
rewardSum:194.33333333333334
loss:[26.295448303222656, 21.202778100967407]
policies:[0, 4, 0]
qAverage:[0.0, 73.61941680908203]
ws:[0.012525081634521484, 7.09052586555481]
memory len:10000
memory used:3167.0
now epsilon is 0.04417306913219465, the reward is 194.33333333333334 with loss [28.337079524993896, 23.802846431732178] in episode 2570
Report: 
rewardSum:194.33333333333334
loss:[28.337079524993896, 23.802846431732178]
policies:[1, 3, 0]
qAverage:[13.323188781738281, 59.88667602539063]
ws:[0.2365421772003174, 4.357699847221374]
memory len:10000
memory used:3167.0
now epsilon is 0.04412891262520274, the reward is 194.33333333333334 with loss [22.713647842407227, 26.658170223236084] in episode 2571
Report: 
rewardSum:194.33333333333334
loss:[22.713647842407227, 26.658170223236084]
policies:[1, 3, 0]
qAverage:[13.117002868652344, 58.61771240234375]
ws:[0.5109272718429565, 3.856665325164795]
memory len:10000
memory used:3167.0
now epsilon is 0.04408480025816189, the reward is 194.33333333333334 with loss [25.41751527786255, 23.863797664642334] in episode 2572
Report: 
rewardSum:194.33333333333334
loss:[25.41751527786255, 23.863797664642334]
policies:[1, 3, 0]
qAverage:[13.352809143066406, 59.44015350341797]
ws:[0.9799545049667359, 5.604442584514618]
memory len:10000
memory used:3167.0
now epsilon is 0.044040731986948696, the reward is 194.33333333333334 with loss [24.709161281585693, 22.57356595993042] in episode 2573
Report: 
rewardSum:194.33333333333334
loss:[24.709161281585693, 22.57356595993042]
policies:[1, 2, 1]
qAverage:[16.35959243774414, 53.162960052490234]
ws:[1.018454521894455, 3.1659775525331497]
memory len:10000
memory used:3167.0
now epsilon is 0.04399670776748388, the reward is 194.33333333333334 with loss [34.60502576828003, 23.260441303253174] in episode 2574
Report: 
rewardSum:194.33333333333334
loss:[34.60502576828003, 23.260441303253174]
policies:[1, 3, 0]
qAverage:[13.364030456542968, 58.905487060546875]
ws:[1.6605878710746764, 5.375052613019943]
memory len:10000
memory used:3167.0
now epsilon is 0.043941739373843255, the reward is 193.33333333333334 with loss [36.313039779663086, 26.44065523147583] in episode 2575
Report: 
rewardSum:193.33333333333334
loss:[36.313039779663086, 26.44065523147583]
policies:[1, 3, 1]
qAverage:[13.153329467773437, 60.21513671875]
ws:[0.9878740072250366, 3.8974796652793886]
memory len:10000
memory used:3168.0
now epsilon is 0.0438978141098755, the reward is 194.33333333333334 with loss [32.48117637634277, 22.325732707977295] in episode 2576
Report: 
rewardSum:194.33333333333334
loss:[32.48117637634277, 22.325732707977295]
policies:[1, 3, 0]
qAverage:[13.178823852539063, 58.50735015869141]
ws:[1.5328626394271851, 6.684211874008179]
memory len:10000
memory used:3167.0
now epsilon is 0.04385393275470247, the reward is 194.33333333333334 with loss [18.36350178718567, 30.32665205001831] in episode 2577
Report: 
rewardSum:194.33333333333334
loss:[18.36350178718567, 30.32665205001831]
policies:[1, 3, 0]
qAverage:[13.318934631347656, 60.25380859375]
ws:[1.8659035861492157, 5.019652223587036]
memory len:10000
memory used:3167.0
now epsilon is 0.04381009526443186, the reward is 194.33333333333334 with loss [27.640543460845947, 29.929490089416504] in episode 2578
Report: 
rewardSum:194.33333333333334
loss:[27.640543460845947, 29.929490089416504]
policies:[1, 3, 0]
qAverage:[13.261735534667968, 58.57173156738281]
ws:[2.0625234156847, 5.609907865524292]
memory len:10000
memory used:3168.0
now epsilon is 0.0437663015952152, the reward is 194.33333333333334 with loss [33.14903783798218, 30.057054042816162] in episode 2579
Report: 
rewardSum:194.33333333333334
loss:[33.14903783798218, 30.057054042816162]
policies:[1, 3, 0]
qAverage:[13.44197998046875, 60.3394287109375]
ws:[1.9952063739299775, 6.396821165084839]
memory len:10000
memory used:3168.0
now epsilon is 0.04372255170324787, the reward is 194.33333333333334 with loss [25.940256595611572, 23.746472358703613] in episode 2580
Report: 
rewardSum:194.33333333333334
loss:[25.940256595611572, 23.746472358703613]
policies:[0, 4, 0]
qAverage:[0.0, 73.03818817138672]
ws:[1.6393386125564575, 7.204727125167847]
memory len:10000
memory used:3168.0
now epsilon is 0.043657008851924486, the reward is 192.33333333333334 with loss [36.71142864227295, 39.57346296310425] in episode 2581
Report: 
rewardSum:192.33333333333334
loss:[36.71142864227295, 39.57346296310425]
policies:[0, 5, 1]
qAverage:[0.0, 76.50498835245769]
ws:[0.9309548338254293, 6.397566835085551]
memory len:10000
memory used:3180.0
now epsilon is 0.04361336821172249, the reward is 194.33333333333334 with loss [24.373274326324463, 22.361820220947266] in episode 2582
Report: 
rewardSum:194.33333333333334
loss:[24.373274326324463, 22.361820220947266]
policies:[0, 4, 0]
qAverage:[0.0, 74.5418701171875]
ws:[0.5779839992523194, 6.297016906738281]
memory len:10000
memory used:3180.0
now epsilon is 0.04356977119579818, the reward is 194.33333333333334 with loss [18.579925537109375, 31.14201784133911] in episode 2583
Report: 
rewardSum:194.33333333333334
loss:[18.579925537109375, 31.14201784133911]
policies:[0, 4, 0]
qAverage:[0.0, 74.5924072265625]
ws:[0.6467091560363769, 6.893960857391358]
memory len:10000
memory used:3180.0
now epsilon is 0.04352621776054365, the reward is 194.33333333333334 with loss [30.802375316619873, 26.626792907714844] in episode 2584
Report: 
rewardSum:194.33333333333334
loss:[30.802375316619873, 26.626792907714844]
policies:[0, 4, 0]
qAverage:[0.0, 74.58904724121093]
ws:[0.7220141887664795, 9.319978666305541]
memory len:10000
memory used:3168.0
now epsilon is 0.04348270786239454, the reward is 194.33333333333334 with loss [23.57963514328003, 24.40714120864868] in episode 2585
Report: 
rewardSum:194.33333333333334
loss:[23.57963514328003, 24.40714120864868]
policies:[0, 4, 0]
qAverage:[0.0, 73.86290588378907]
ws:[0.7563851356506348, 7.1880804061889645]
memory len:10000
memory used:3168.0
now epsilon is 0.0434392414578301, the reward is 194.33333333333334 with loss [32.87183618545532, 16.014647245407104] in episode 2586
Report: 
rewardSum:194.33333333333334
loss:[32.87183618545532, 16.014647245407104]
policies:[0, 4, 0]
qAverage:[0.0, 74.59654693603515]
ws:[1.3624419748783112, 9.75597620010376]
memory len:10000
memory used:3168.0
now epsilon is 0.04339581850337303, the reward is 194.33333333333334 with loss [17.552177667617798, 25.84133529663086] in episode 2587
Report: 
rewardSum:194.33333333333334
loss:[17.552177667617798, 25.84133529663086]
policies:[0, 4, 0]
qAverage:[0.0, 71.1678581237793]
ws:[1.2484373077750206, 7.252370834350586]
memory len:10000
memory used:3168.0
now epsilon is 0.04335243895558953, the reward is 194.33333333333334 with loss [19.384312629699707, 18.569411516189575] in episode 2588
Report: 
rewardSum:194.33333333333334
loss:[19.384312629699707, 18.569411516189575]
policies:[0, 4, 0]
qAverage:[0.0, 74.6996063232422]
ws:[1.3926076531410216, 8.991070461273193]
memory len:10000
memory used:3168.0
now epsilon is 0.043309102771089195, the reward is 194.33333333333334 with loss [27.02099370956421, 23.774553775787354] in episode 2589
Report: 
rewardSum:194.33333333333334
loss:[27.02099370956421, 23.774553775787354]
policies:[0, 4, 0]
qAverage:[0.0, 75.47987365722656]
ws:[0.9867801547050477, 6.210598158836365]
memory len:10000
memory used:3168.0
now epsilon is 0.043265809906524996, the reward is 194.33333333333334 with loss [22.783403873443604, 19.973342418670654] in episode 2590
Report: 
rewardSum:194.33333333333334
loss:[22.783403873443604, 19.973342418670654]
policies:[1, 3, 0]
qAverage:[14.090438842773438, 59.8783935546875]
ws:[1.117205050587654, 5.844050717353821]
memory len:10000
memory used:3168.0
now epsilon is 0.04322256031859325, the reward is 194.33333333333334 with loss [21.081525802612305, 23.343894958496094] in episode 2591
Report: 
rewardSum:194.33333333333334
loss:[21.081525802612305, 23.343894958496094]
policies:[1, 3, 0]
qAverage:[13.736726379394531, 59.63905487060547]
ws:[1.1605778515338898, 5.686614203453064]
memory len:10000
memory used:3168.0
now epsilon is 0.043179353964033544, the reward is 194.33333333333334 with loss [25.92233371734619, 25.4203200340271] in episode 2592
Report: 
rewardSum:194.33333333333334
loss:[25.92233371734619, 25.4203200340271]
policies:[1, 3, 0]
qAverage:[14.082391357421875, 59.63419189453125]
ws:[1.1048349309712648, 5.688422453403473]
memory len:10000
memory used:3168.0
now epsilon is 0.04313619079962871, the reward is 194.33333333333334 with loss [20.00813364982605, 29.61793327331543] in episode 2593
Report: 
rewardSum:194.33333333333334
loss:[20.00813364982605, 29.61793327331543]
policies:[1, 3, 0]
qAverage:[13.674765014648438, 59.58306121826172]
ws:[1.4614636063575746, 6.236121726036072]
memory len:10000
memory used:3168.0
now epsilon is 0.04309307078220479, the reward is 194.33333333333334 with loss [22.931230068206787, 25.26911187171936] in episode 2594
Report: 
rewardSum:194.33333333333334
loss:[22.931230068206787, 25.26911187171936]
policies:[1, 3, 0]
qAverage:[14.03472442626953, 59.717367553710936]
ws:[1.6218921184539794, 6.682302045822143]
memory len:10000
memory used:3168.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20*		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.04304999386863098, the reward is 194.33333333333334 with loss [27.612782955169678, 19.47044038772583] in episode 2595
Report: 
rewardSum:194.33333333333334
loss:[27.612782955169678, 19.47044038772583]
policies:[0, 4, 0]
qAverage:[0.0, 75.31607360839844]
ws:[1.866239070892334, 7.611253118515014]
memory len:10000
memory used:3168.0
now epsilon is 0.0430069600158196, the reward is 194.33333333333334 with loss [24.464684009552002, 19.96445941925049] in episode 2596
Report: 
rewardSum:194.33333333333334
loss:[24.464684009552002, 19.96445941925049]
policies:[0, 3, 1]
qAverage:[0.0, 67.0102767944336]
ws:[2.1224420070648193, 5.383630156517029]
memory len:10000
memory used:3168.0
now epsilon is 0.04296396918072603, the reward is 194.33333333333334 with loss [19.20913338661194, 20.03251004219055] in episode 2597
Report: 
rewardSum:194.33333333333334
loss:[19.20913338661194, 20.03251004219055]
policies:[1, 3, 0]
qAverage:[0.0, 66.47675704956055]
ws:[2.024771511554718, 5.373373627662659]
memory len:10000
memory used:3168.0
now epsilon is 0.042921021320348675, the reward is 194.33333333333334 with loss [26.032296419143677, 20.869788646697998] in episode 2598
Report: 
rewardSum:194.33333333333334
loss:[26.032296419143677, 20.869788646697998]
policies:[0, 4, 0]
qAverage:[0.0, 75.16971435546876]
ws:[1.5091213703155517, 8.247400569915772]
memory len:10000
memory used:3168.0
now epsilon is 0.042878116391728935, the reward is 194.33333333333334 with loss [26.11837339401245, 20.297269344329834] in episode 2599
Report: 
rewardSum:194.33333333333334
loss:[26.11837339401245, 20.297269344329834]
policies:[0, 4, 0]
qAverage:[0.0, 75.57300415039063]
ws:[1.229172645881772, 8.814427757263184]
memory len:10000
memory used:3168.0
now epsilon is 0.04283525435195114, the reward is 194.33333333333334 with loss [25.10010862350464, 23.283076763153076] in episode 2600
Report: 
rewardSum:194.33333333333334
loss:[25.10010862350464, 23.283076763153076]
policies:[0, 4, 0]
qAverage:[0.0, 75.45165405273437]
ws:[0.8992366433143616, 8.612963962554932]
memory len:10000
memory used:3168.0
now epsilon is 0.04279243515814254, the reward is 194.33333333333334 with loss [24.090086460113525, 29.3022780418396] in episode 2601
Report: 
rewardSum:194.33333333333334
loss:[24.090086460113525, 29.3022780418396]
policies:[0, 4, 0]
qAverage:[0.0, 75.91066436767578]
ws:[0.8127048373222351, 11.876444435119629]
memory len:10000
memory used:3167.0
now epsilon is 0.04274965876747322, the reward is 194.33333333333334 with loss [29.10162591934204, 25.412753105163574] in episode 2602
Report: 
rewardSum:194.33333333333334
loss:[29.10162591934204, 25.412753105163574]
policies:[0, 4, 0]
qAverage:[0.0, 75.90637969970703]
ws:[0.6969557285308838, 10.883074760437012]
memory len:10000
memory used:3168.0
now epsilon is 0.0427069251371561, the reward is 194.33333333333334 with loss [24.279417991638184, 22.060392379760742] in episode 2603
Report: 
rewardSum:194.33333333333334
loss:[24.279417991638184, 22.060392379760742]
policies:[1, 3, 0]
qAverage:[0.0, 67.24118423461914]
ws:[1.6264109909534454, 11.8952317237854]
memory len:10000
memory used:3168.0
now epsilon is 0.04266423422444686, the reward is 194.33333333333334 with loss [24.33157968521118, 25.951294422149658] in episode 2604
Report: 
rewardSum:194.33333333333334
loss:[24.33157968521118, 25.951294422149658]
policies:[0, 4, 0]
qAverage:[0.0, 76.4900146484375]
ws:[0.9000696778297425, 11.643187427520752]
memory len:10000
memory used:3168.0
now epsilon is 0.0426215859866439, the reward is 194.33333333333334 with loss [24.250372886657715, 23.115776538848877] in episode 2605
Report: 
rewardSum:194.33333333333334
loss:[24.250372886657715, 23.115776538848877]
policies:[1, 3, 0]
qAverage:[0.0, 62.87287394205729]
ws:[1.1064161856969197, 9.261957486470541]
memory len:10000
memory used:3167.0
now epsilon is 0.04257898038108833, the reward is 194.33333333333334 with loss [17.22279691696167, 20.392300605773926] in episode 2606
Report: 
rewardSum:194.33333333333334
loss:[17.22279691696167, 20.392300605773926]
policies:[0, 4, 0]
qAverage:[0.0, 76.39286193847656]
ws:[0.7747482836246491, 11.968164443969727]
memory len:10000
memory used:3168.0
now epsilon is 0.04253641736516387, the reward is 194.33333333333334 with loss [25.015758991241455, 31.571704864501953] in episode 2607
Report: 
rewardSum:194.33333333333334
loss:[25.015758991241455, 31.571704864501953]
policies:[0, 4, 0]
qAverage:[0.0, 76.3457763671875]
ws:[0.54076868891716, 11.33414192199707]
memory len:10000
memory used:3168.0
now epsilon is 0.04249389689629687, the reward is 194.33333333333334 with loss [15.845763444900513, 25.346639156341553] in episode 2608
Report: 
rewardSum:194.33333333333334
loss:[15.845763444900513, 25.346639156341553]
policies:[0, 4, 0]
qAverage:[0.0, 74.21217155456543]
ws:[0.06261500716209412, 10.459909915924072]
memory len:10000
memory used:3168.0
now epsilon is 0.04245141893195622, the reward is 194.33333333333334 with loss [28.31775140762329, 28.070693969726562] in episode 2609
Report: 
rewardSum:194.33333333333334
loss:[28.31775140762329, 28.070693969726562]
policies:[0, 4, 0]
qAverage:[0.0, 76.81156768798829]
ws:[-0.486901718378067, 7.144122743606568]
memory len:10000
memory used:3168.0
now epsilon is 0.042408983429653324, the reward is 194.33333333333334 with loss [21.198566913604736, 27.47740125656128] in episode 2610
Report: 
rewardSum:194.33333333333334
loss:[21.198566913604736, 27.47740125656128]
policies:[0, 4, 0]
qAverage:[0.0, 75.89929656982422]
ws:[-0.7626500487327575, 6.309004753828049]
memory len:10000
memory used:3168.0
now epsilon is 0.04235599869935534, the reward is 193.33333333333334 with loss [31.728861331939697, 36.51016807556152] in episode 2611
Report: 
rewardSum:193.33333333333334
loss:[31.728861331939697, 36.51016807556152]
policies:[0, 4, 1]
qAverage:[0.0, 75.78387603759765]
ws:[-0.5782097816467285, 8.603590068221092]
memory len:10000
memory used:3168.0
now epsilon is 0.04231365858150843, the reward is 194.33333333333334 with loss [18.77984356880188, 24.16698908805847] in episode 2612
Report: 
rewardSum:194.33333333333334
loss:[18.77984356880188, 24.16698908805847]
policies:[2, 2, 0]
qAverage:[0.0, 63.37439727783203]
ws:[-0.36201876401901245, 4.033368428548177]
memory len:10000
memory used:3167.0
now epsilon is 0.04227136078790445, the reward is 194.33333333333334 with loss [25.43936252593994, 22.612348079681396] in episode 2613
Report: 
rewardSum:194.33333333333334
loss:[25.43936252593994, 22.612348079681396]
policies:[0, 4, 0]
qAverage:[0.0, 76.23796691894532]
ws:[-0.727229879796505, 7.361645030975342]
memory len:10000
memory used:3168.0
now epsilon is 0.042229105276235045, the reward is 194.33333333333334 with loss [18.985921382904053, 25.885167598724365] in episode 2614
Report: 
rewardSum:194.33333333333334
loss:[18.985921382904053, 25.885167598724365]
policies:[0, 4, 0]
qAverage:[0.0, 76.43646087646485]
ws:[-0.2182348072528839, 11.076298046112061]
memory len:10000
memory used:3167.0
now epsilon is 0.04218689200423414, the reward is 194.33333333333334 with loss [23.54171395301819, 21.572784900665283] in episode 2615
Report: 
rewardSum:194.33333333333334
loss:[23.54171395301819, 21.572784900665283]
policies:[0, 4, 0]
qAverage:[0.0, 76.54922485351562]
ws:[-0.33446973860263823, 9.02996859550476]
memory len:10000
memory used:3167.0
now epsilon is 0.0421447209296779, the reward is 194.33333333333334 with loss [21.795083045959473, 26.834746837615967] in episode 2616
Report: 
rewardSum:194.33333333333334
loss:[21.795083045959473, 26.834746837615967]
policies:[0, 4, 0]
qAverage:[0.0, 76.05657043457032]
ws:[0.16051360219717026, 13.247714233398437]
memory len:10000
memory used:3167.0
now epsilon is 0.042102592010384694, the reward is 194.33333333333334 with loss [22.1404447555542, 27.34026002883911] in episode 2617
Report: 
rewardSum:194.33333333333334
loss:[22.1404447555542, 27.34026002883911]
policies:[0, 4, 0]
qAverage:[0.0, 76.3597427368164]
ws:[0.27154417634010314, 13.755689239501953]
memory len:10000
memory used:3167.0
now epsilon is 0.04206050520421507, the reward is 194.33333333333334 with loss [18.984975576400757, 28.826666831970215] in episode 2618
Report: 
rewardSum:194.33333333333334
loss:[18.984975576400757, 28.826666831970215]
policies:[0, 4, 0]
qAverage:[0.0, 76.32030029296875]
ws:[0.3105768352746964, 14.157612609863282]
memory len:10000
memory used:3168.0
now epsilon is 0.0420184604690717, the reward is 194.33333333333334 with loss [26.549548149108887, 18.370092391967773] in episode 2619
Report: 
rewardSum:194.33333333333334
loss:[26.549548149108887, 18.370092391967773]
policies:[0, 4, 0]
qAverage:[0.0, 75.4217025756836]
ws:[0.3958667486906052, 14.273248672485352]
memory len:10000
memory used:3168.0
now epsilon is 0.041976457762899315, the reward is 194.33333333333334 with loss [29.944414377212524, 18.970298290252686] in episode 2620
Report: 
rewardSum:194.33333333333334
loss:[29.944414377212524, 18.970298290252686]
policies:[0, 4, 0]
qAverage:[0.0, 76.55792541503907]
ws:[0.7025397181510925, 13.788150882720947]
memory len:10000
memory used:3168.0
now epsilon is 0.041934497043684724, the reward is 194.33333333333334 with loss [24.431609630584717, 21.13381838798523] in episode 2621
Report: 
rewardSum:194.33333333333334
loss:[24.431609630584717, 21.13381838798523]
policies:[0, 3, 1]
qAverage:[0.0, 62.147682189941406]
ws:[0.8502601385116577, 7.268301963806152]
memory len:10000
memory used:3168.0
now epsilon is 0.04189257826945669, the reward is 194.33333333333334 with loss [23.778846740722656, 23.683141708374023] in episode 2622
Report: 
rewardSum:194.33333333333334
loss:[23.778846740722656, 23.683141708374023]
policies:[0, 4, 0]
qAverage:[0.0, 76.59653778076172]
ws:[1.1673144221305847, 12.376782846450805]
memory len:10000
memory used:3167.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22*		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.04185070139828598, the reward is 194.33333333333334 with loss [24.733352661132812, 21.65873885154724] in episode 2623
Report: 
rewardSum:194.33333333333334
loss:[24.733352661132812, 21.65873885154724]
policies:[1, 3, 0]
qAverage:[13.883447265625, 60.268775939941406]
ws:[0.8368711680173874, 10.283646720647813]
memory len:10000
memory used:3167.0
now epsilon is 0.04180886638828522, the reward is 194.33333333333334 with loss [25.925557613372803, 23.954978227615356] in episode 2624
Report: 
rewardSum:194.33333333333334
loss:[25.925557613372803, 23.954978227615356]
policies:[1, 3, 0]
qAverage:[14.28385009765625, 59.50137176513672]
ws:[0.5278653422370553, 8.929854249954223]
memory len:10000
memory used:3167.0
now epsilon is 0.041746192271452216, the reward is 192.33333333333334 with loss [31.631929397583008, 41.51438093185425] in episode 2625
Report: 
rewardSum:192.33333333333334
loss:[31.631929397583008, 41.51438093185425]
policies:[1, 4, 1]
qAverage:[11.709847768147787, 65.13407135009766]
ws:[0.9040081178148588, 10.255209565162659]
memory len:10000
memory used:3167.0
now epsilon is 0.04170446173139389, the reward is 194.33333333333334 with loss [23.620072841644287, 18.544711112976074] in episode 2626
Report: 
rewardSum:194.33333333333334
loss:[23.620072841644287, 18.544711112976074]
policies:[0, 3, 1]
qAverage:[0.0, 75.555908203125]
ws:[0.770310839638114, 9.857423901557922]
memory len:10000
memory used:3167.0
now epsilon is 0.04166277290622929, the reward is 194.33333333333334 with loss [25.8162260055542, 21.239970207214355] in episode 2627
Report: 
rewardSum:194.33333333333334
loss:[25.8162260055542, 21.239970207214355]
policies:[1, 3, 0]
qAverage:[14.182832336425781, 60.38112182617188]
ws:[0.6012382626533508, 8.291408729553222]
memory len:10000
memory used:3168.0
now epsilon is 0.041621125754259136, the reward is 194.33333333333334 with loss [19.560590505599976, 18.569860219955444] in episode 2628
Report: 
rewardSum:194.33333333333334
loss:[19.560590505599976, 18.569860219955444]
policies:[2, 2, 0]
qAverage:[17.741504669189453, 52.94201469421387]
ws:[-0.010037330910563469, 6.744755432009697]
memory len:10000
memory used:3167.0
now epsilon is 0.04157952023382588, the reward is 194.33333333333334 with loss [20.358922481536865, 26.620173454284668] in episode 2629
Report: 
rewardSum:194.33333333333334
loss:[20.358922481536865, 26.620173454284668]
policies:[1, 3, 0]
qAverage:[14.163461303710937, 60.1300537109375]
ws:[0.36495105624198915, 9.17119358778]
memory len:10000
memory used:3168.0
now epsilon is 0.04153795630331358, the reward is 194.33333333333334 with loss [23.409968376159668, 29.15636110305786] in episode 2630
Report: 
rewardSum:194.33333333333334
loss:[23.409968376159668, 29.15636110305786]
policies:[0, 4, 0]
qAverage:[0.0, 76.4388671875]
ws:[-0.05301152616739273, 7.143150513619185]
memory len:10000
memory used:3168.0
now epsilon is 0.04149643392114793, the reward is 194.33333333333334 with loss [20.44080686569214, 23.97254705429077] in episode 2631
Report: 
rewardSum:194.33333333333334
loss:[20.44080686569214, 23.97254705429077]
policies:[0, 4, 0]
qAverage:[0.0, 75.54738235473633]
ws:[0.46669092774391174, 10.337560892105103]
memory len:10000
memory used:3168.0
now epsilon is 0.04145495304579614, the reward is 194.33333333333334 with loss [35.060372829437256, 23.437291860580444] in episode 2632
Report: 
rewardSum:194.33333333333334
loss:[35.060372829437256, 23.437291860580444]
policies:[0, 4, 0]
qAverage:[0.0, 76.10422210693359]
ws:[0.6330858528614044, 9.306787776947022]
memory len:10000
memory used:3168.0
now epsilon is 0.04141351363576697, the reward is 194.33333333333334 with loss [24.04394292831421, 18.834091424942017] in episode 2633
Report: 
rewardSum:194.33333333333334
loss:[24.04394292831421, 18.834091424942017]
policies:[1, 3, 0]
qAverage:[0.0, 70.76043319702148]
ws:[0.2583593875169754, 8.4444060921669]
memory len:10000
memory used:3168.0
now epsilon is 0.04137211564961064, the reward is 194.33333333333334 with loss [18.549755573272705, 27.755826950073242] in episode 2634
Report: 
rewardSum:194.33333333333334
loss:[18.549755573272705, 27.755826950073242]
policies:[0, 3, 1]
qAverage:[0.0, 74.89836692810059]
ws:[-0.005821550264954567, 9.186373591423035]
memory len:10000
memory used:3168.0
now epsilon is 0.0413307590459188, the reward is 194.33333333333334 with loss [25.53474760055542, 26.440804958343506] in episode 2635
Report: 
rewardSum:194.33333333333334
loss:[25.53474760055542, 26.440804958343506]
policies:[0, 4, 0]
qAverage:[0.0, 76.54033660888672]
ws:[-0.07423223704099655, 6.990153837203979]
memory len:10000
memory used:3168.0
now epsilon is 0.04128944378332452, the reward is 194.33333333333334 with loss [21.033591270446777, 19.01533555984497] in episode 2636
Report: 
rewardSum:194.33333333333334
loss:[21.033591270446777, 19.01533555984497]
policies:[0, 4, 0]
qAverage:[0.0, 76.95338897705078]
ws:[-0.023368847370147706, 6.937727224826813]
memory len:10000
memory used:3168.0
now epsilon is 0.04124816982050219, the reward is 194.33333333333334 with loss [26.749065399169922, 33.761024951934814] in episode 2637
Report: 
rewardSum:194.33333333333334
loss:[26.749065399169922, 33.761024951934814]
policies:[0, 4, 0]
qAverage:[0.0, 75.5590606689453]
ws:[-0.04931006729602814, 6.96937724351883]
memory len:10000
memory used:3168.0
now epsilon is 0.04118633622304302, the reward is 192.33333333333334 with loss [35.743635416030884, 35.329941749572754] in episode 2638
Report: 
rewardSum:192.33333333333334
loss:[35.743635416030884, 35.329941749572754]
policies:[0, 5, 1]
qAverage:[0.0, 77.8693618774414]
ws:[-0.15767807016770044, 6.07980631540219]
memory len:10000
memory used:3168.0
now epsilon is 0.041145165329122074, the reward is 46.33333333333334 with loss [17.997461795806885, 25.39756965637207] in episode 2639
Report: 
rewardSum:46.33333333333334
loss:[17.997461795806885, 25.39756965637207]
policies:[0, 3, 1]
qAverage:[0.0, 65.9656925201416]
ws:[0.6427168196532875, 6.941557019948959]
memory len:10000
memory used:3168.0
now epsilon is 0.04109375958176088, the reward is 193.33333333333334 with loss [25.298940420150757, 32.06381368637085] in episode 2640
Report: 
rewardSum:193.33333333333334
loss:[25.298940420150757, 32.06381368637085]
policies:[0, 4, 1]
qAverage:[0.0, 76.09624786376953]
ws:[0.5204680591821671, 10.691169500350952]
memory len:10000
memory used:3168.0
now epsilon is 0.041052681229770766, the reward is 194.33333333333334 with loss [28.342875480651855, 25.96974277496338] in episode 2641
Report: 
rewardSum:194.33333333333334
loss:[28.342875480651855, 25.96974277496338]
policies:[0, 4, 0]
qAverage:[0.0, 74.11242065429687]
ws:[1.0610260844230652, 12.13258056640625]
memory len:10000
memory used:3168.0
now epsilon is 0.041011643940730834, the reward is 194.33333333333334 with loss [26.129659175872803, 21.978130638599396] in episode 2642
Report: 
rewardSum:194.33333333333334
loss:[26.129659175872803, 21.978130638599396]
policies:[0, 4, 0]
qAverage:[0.0, 72.58447647094727]
ws:[0.8123222589492798, 10.089281558990479]
memory len:10000
memory used:3168.0
now epsilon is 0.04097064767359352, the reward is 194.33333333333334 with loss [29.187912464141846, 19.38375473022461] in episode 2643
Report: 
rewardSum:194.33333333333334
loss:[29.187912464141846, 19.38375473022461]
policies:[0, 4, 0]
qAverage:[0.0, 74.95261077880859]
ws:[1.2194952845573426, 12.155214881896972]
memory len:10000
memory used:3168.0
now epsilon is 0.04095016491042221, the reward is -1.0 with loss [17.355676651000977, 8.121564626693726] in episode 2644
Report: 
rewardSum:-1.0
loss:[17.355676651000977, 8.121564626693726]
policies:[0, 1, 1]
qAverage:[0.0, 41.5008659362793]
ws:[0.4866195321083069, 2.1463468074798584]
memory len:10000
memory used:3168.0
now epsilon is 0.0409092300992644, the reward is 194.33333333333334 with loss [16.25511074066162, 22.726876974105835] in episode 2645
Report: 
rewardSum:194.33333333333334
loss:[16.25511074066162, 22.726876974105835]
policies:[0, 4, 0]
qAverage:[0.0, 74.5391860961914]
ws:[0.7845391273498535, 11.89460735321045]
memory len:10000
memory used:3169.0
now epsilon is 0.040868336207569765, the reward is 194.33333333333334 with loss [28.312374114990234, 17.792744874954224] in episode 2646
Report: 
rewardSum:194.33333333333334
loss:[28.312374114990234, 17.792744874954224]
policies:[0, 4, 0]
qAverage:[0.0, 75.9568603515625]
ws:[0.7776252917945385, 11.67856216430664]
memory len:10000
memory used:3169.0
now epsilon is 0.04082748319443416, the reward is 194.33333333333334 with loss [18.174078345298767, 25.88082218170166] in episode 2647
Report: 
rewardSum:194.33333333333334
loss:[18.174078345298767, 25.88082218170166]
policies:[1, 3, 0]
qAverage:[0.0, 66.12934494018555]
ws:[0.9087565019726753, 9.060627102851868]
memory len:10000
memory used:3169.0
now epsilon is 0.04078667101899437, the reward is 194.33333333333334 with loss [19.699461698532104, 25.31536340713501] in episode 2648
Report: 
rewardSum:194.33333333333334
loss:[19.699461698532104, 25.31536340713501]
policies:[0, 4, 0]
qAverage:[0.0, 75.17048797607421]
ws:[0.9749212622642517, 13.617648792266845]
memory len:10000
memory used:3169.0
now epsilon is 0.04074589964042801, the reward is 194.33333333333334 with loss [24.005367517471313, 24.026047468185425] in episode 2649
Report: 
rewardSum:194.33333333333334
loss:[24.005367517471313, 24.026047468185425]
policies:[1, 3, 0]
qAverage:[0.0, 72.26094245910645]
ws:[0.6620596945285797, 12.114624619483948]
memory len:10000
memory used:3168.0
now epsilon is 0.040725529237226527, the reward is -1.0 with loss [15.943212032318115, 13.897799491882324] in episode 2650
Report: 
rewardSum:-1.0
loss:[15.943212032318115, 13.897799491882324]
policies:[0, 1, 1]
qAverage:[0.0, 40.129417419433594]
ws:[0.5236464142799377, 3.073875665664673]
memory len:10000
memory used:3168.0
now epsilon is 0.040684818977517585, the reward is 194.33333333333334 with loss [22.792949199676514, 23.597777605056763] in episode 2651
Report: 
rewardSum:194.33333333333334
loss:[22.792949199676514, 23.597777605056763]
policies:[0, 4, 0]
qAverage:[0.0, 73.84131240844727]
ws:[1.6884005069732666, 17.145383834838867]
memory len:10000
memory used:3168.0
############# STATE ###############
0-		9-		18-		27-		36-		
1*		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.040644149412804545, the reward is 194.33333333333334 with loss [19.822540998458862, 18.313248872756958] in episode 2652
Report: 
rewardSum:194.33333333333334
loss:[19.822540998458862, 18.313248872756958]
policies:[0, 4, 0]
qAverage:[0.0, 68.81217384338379]
ws:[2.052052766084671, 15.251684427261353]
memory len:10000
memory used:3168.0
now epsilon is 0.040603520502407675, the reward is 194.33333333333334 with loss [27.36463975906372, 22.59358549118042] in episode 2653
Report: 
rewardSum:194.33333333333334
loss:[27.36463975906372, 22.59358549118042]
policies:[0, 4, 0]
qAverage:[0.0, 71.8474178314209]
ws:[1.6205333769321442, 11.587278485298157]
memory len:10000
memory used:3168.0
now epsilon is 0.0405629322056879, the reward is 194.33333333333334 with loss [32.63262128829956, 17.465505838394165] in episode 2654
Report: 
rewardSum:194.33333333333334
loss:[32.63262128829956, 17.465505838394165]
policies:[0, 4, 0]
qAverage:[0.0, 68.12991523742676]
ws:[1.7112618386745453, 10.781148433685303]
memory len:10000
memory used:3168.0
now epsilon is 0.040522384482046765, the reward is 194.33333333333334 with loss [23.288166046142578, 27.578877210617065] in episode 2655
Report: 
rewardSum:194.33333333333334
loss:[23.288166046142578, 27.578877210617065]
policies:[0, 4, 0]
qAverage:[0.0, 74.62221832275391]
ws:[1.7300856113433838, 9.756603050231934]
memory len:10000
memory used:3168.0
now epsilon is 0.04048187729092641, the reward is 194.33333333333334 with loss [25.076995372772217, 17.636982440948486] in episode 2656
Report: 
rewardSum:194.33333333333334
loss:[25.076995372772217, 17.636982440948486]
policies:[1, 3, 0]
qAverage:[0.0, 68.17781066894531]
ws:[2.296254873275757, 13.419061422348022]
memory len:10000
memory used:3168.0
now epsilon is 0.040441410591809515, the reward is 194.33333333333334 with loss [22.731302976608276, 18.77904200553894] in episode 2657
Report: 
rewardSum:194.33333333333334
loss:[22.731302976608276, 18.77904200553894]
policies:[0, 4, 0]
qAverage:[0.0, 74.25402221679687]
ws:[1.4570576667785644, 10.107264423370362]
memory len:10000
memory used:3168.0
now epsilon is 0.04040098434421925, the reward is 194.33333333333334 with loss [26.701791286468506, 18.444119691848755] in episode 2658
Report: 
rewardSum:194.33333333333334
loss:[26.701791286468506, 18.444119691848755]
policies:[0, 3, 1]
qAverage:[0.0, 70.99432182312012]
ws:[1.214714601635933, 12.134039342403412]
memory len:10000
memory used:3168.0
now epsilon is 0.04036059850771926, the reward is 194.33333333333334 with loss [23.089807510375977, 17.323668003082275] in episode 2659
Report: 
rewardSum:194.33333333333334
loss:[23.089807510375977, 17.323668003082275]
policies:[0, 4, 0]
qAverage:[0.0, 74.55543823242188]
ws:[1.3238885283470154, 12.54055118560791]
memory len:10000
memory used:3168.0
now epsilon is 0.040320253041913605, the reward is 194.33333333333334 with loss [27.420437335968018, 28.842605590820312] in episode 2660
Report: 
rewardSum:194.33333333333334
loss:[27.420437335968018, 28.842605590820312]
policies:[0, 4, 0]
qAverage:[0.0, 73.60051879882812]
ws:[1.5505930662155152, 13.233330535888673]
memory len:10000
memory used:3167.0
now epsilon is 0.040279947906446734, the reward is 194.33333333333334 with loss [22.99247932434082, 22.277960777282715] in episode 2661
Report: 
rewardSum:194.33333333333334
loss:[22.99247932434082, 22.277960777282715]
policies:[1, 3, 0]
qAverage:[0.0, 73.54268455505371]
ws:[1.8648834824562073, 14.97968864440918]
memory len:10000
memory used:3168.0
now epsilon is 0.0402195657344531, the reward is 192.33333333333334 with loss [34.43972635269165, 43.09393882751465] in episode 2662
Report: 
rewardSum:192.33333333333334
loss:[34.43972635269165, 43.09393882751465]
policies:[0, 5, 1]
qAverage:[0.0, 74.40845998128255]
ws:[0.8132879883050919, 6.753983199596405]
memory len:10000
memory used:3168.0
now epsilon is 0.04017936124854224, the reward is 194.33333333333334 with loss [26.762775421142578, 26.92731285095215] in episode 2663
Report: 
rewardSum:194.33333333333334
loss:[26.762775421142578, 26.92731285095215]
policies:[1, 3, 0]
qAverage:[13.977996826171875, 54.36865386962891]
ws:[1.253520655632019, 9.548818975687027]
memory len:10000
memory used:3168.0
now epsilon is 0.04013919695204311, the reward is 194.33333333333334 with loss [22.471745252609253, 23.28783416748047] in episode 2664
Report: 
rewardSum:194.33333333333334
loss:[22.471745252609253, 23.28783416748047]
policies:[0, 4, 0]
qAverage:[0.0, 71.94453125]
ws:[1.1605412840843201, 10.127714467048644]
memory len:10000
memory used:3168.0
now epsilon is 0.040099072804781394, the reward is 194.33333333333334 with loss [19.735523462295532, 23.20846939086914] in episode 2665
Report: 
rewardSum:194.33333333333334
loss:[19.735523462295532, 23.20846939086914]
policies:[0, 4, 0]
qAverage:[0.0, 70.8177978515625]
ws:[0.9296251595020294, 8.555205392837525]
memory len:10000
memory used:3168.0
now epsilon is 0.04005898876662288, the reward is 194.33333333333334 with loss [21.55855131149292, 14.47374176979065] in episode 2666
Report: 
rewardSum:194.33333333333334
loss:[21.55855131149292, 14.47374176979065]
policies:[0, 4, 0]
qAverage:[0.0, 66.46158409118652]
ws:[1.2022744864225388, 11.591813296079636]
memory len:10000
memory used:3169.0
now epsilon is 0.04001894479747352, the reward is 194.33333333333334 with loss [25.002338886260986, 24.856361389160156] in episode 2667
Report: 
rewardSum:194.33333333333334
loss:[25.002338886260986, 24.856361389160156]
policies:[0, 4, 0]
qAverage:[0.0, 70.55900115966797]
ws:[0.5465436100959777, 6.8756269931793215]
memory len:10000
memory used:3169.0
now epsilon is 0.03997894085727933, the reward is 194.33333333333334 with loss [22.82151222229004, 22.08027744293213] in episode 2668
Report: 
rewardSum:194.33333333333334
loss:[22.82151222229004, 22.08027744293213]
policies:[1, 3, 0]
qAverage:[0.0, 68.21860313415527]
ws:[0.2476356029510498, 6.639652073383331]
memory len:10000
memory used:3169.0
now epsilon is 0.039938976906026345, the reward is 194.33333333333334 with loss [21.97082281112671, 27.98231315612793] in episode 2669
Report: 
rewardSum:194.33333333333334
loss:[21.97082281112671, 27.98231315612793]
policies:[1, 3, 0]
qAverage:[13.905584716796875, 55.628561401367186]
ws:[0.08651071786880493, 6.563175475597381]
memory len:10000
memory used:3169.0
now epsilon is 0.03987910587097957, the reward is 192.33333333333334 with loss [43.91688013076782, 38.733842611312866] in episode 2670
Report: 
rewardSum:192.33333333333334
loss:[43.91688013076782, 38.733842611312866]
policies:[1, 4, 1]
qAverage:[11.88986841837565, 60.49164581298828]
ws:[-0.25421831011772156, 5.636257201433182]
memory len:10000
memory used:3169.0
now epsilon is 0.03983924171728101, the reward is 194.33333333333334 with loss [26.575167179107666, 21.610085010528564] in episode 2671
Report: 
rewardSum:194.33333333333334
loss:[26.575167179107666, 21.610085010528564]
policies:[1, 3, 0]
qAverage:[13.959701538085938, 56.32510833740234]
ws:[0.0733403742313385, 8.233869469165802]
memory len:10000
memory used:3169.0
now epsilon is 0.03979941741278958, the reward is 194.33333333333334 with loss [24.613115310668945, 30.64226722717285] in episode 2672
Report: 
rewardSum:194.33333333333334
loss:[24.613115310668945, 30.64226722717285]
policies:[1, 3, 0]
qAverage:[13.9628173828125, 55.58618927001953]
ws:[0.1082470715045929, 6.218393850326538]
memory len:10000
memory used:3169.0
now epsilon is 0.039739755586189245, the reward is 192.33333333333334 with loss [42.17149829864502, 43.44221591949463] in episode 2673
Report: 
rewardSum:192.33333333333334
loss:[42.17149829864502, 43.44221591949463]
policies:[1, 4, 1]
qAverage:[11.882498423258463, 59.9536018371582]
ws:[0.08339042651156585, 7.192551975448926]
memory len:10000
memory used:3169.0
now epsilon is 0.03968018319641448, the reward is 192.33333333333334 with loss [37.08803629875183, 28.741133213043213] in episode 2674
Report: 
rewardSum:192.33333333333334
loss:[37.08803629875183, 28.741133213043213]
policies:[1, 4, 1]
qAverage:[17.214157104492188, 43.98272705078125]
ws:[-0.4776001498103142, 3.7618895918130875]
memory len:10000
memory used:3169.0
now epsilon is 0.039640517890806914, the reward is 194.33333333333334 with loss [21.917993545532227, 26.34135866165161] in episode 2675
Report: 
rewardSum:194.33333333333334
loss:[21.917993545532227, 26.34135866165161]
policies:[1, 3, 0]
qAverage:[13.711032104492187, 54.011723327636716]
ws:[-0.7183573484420777, 6.589497327804565]
memory len:10000
memory used:3169.0
now epsilon is 0.03960089223563295, the reward is 194.33333333333334 with loss [21.299859046936035, 23.06168168783188] in episode 2676
Report: 
rewardSum:194.33333333333334
loss:[21.299859046936035, 23.06168168783188]
policies:[1, 3, 0]
qAverage:[13.577870178222657, 54.01658935546875]
ws:[-0.8477302074432373, 6.882259440422058]
memory len:10000
memory used:3169.0
now epsilon is 0.03956130619125701, the reward is 194.33333333333334 with loss [24.057594299316406, 18.369341611862183] in episode 2677
Report: 
rewardSum:194.33333333333334
loss:[24.057594299316406, 18.369341611862183]
policies:[1, 3, 0]
qAverage:[13.754487609863281, 53.81329345703125]
ws:[-0.971829029917717, 6.9790691375732425]
memory len:10000
memory used:3168.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19*		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.039521759718083145, the reward is 194.33333333333334 with loss [20.12248420715332, 31.988250732421875] in episode 2678
Report: 
rewardSum:194.33333333333334
loss:[20.12248420715332, 31.988250732421875]
policies:[0, 3, 1]
qAverage:[0.0, 61.33644676208496]
ws:[-0.6369366198778152, 1.810759775340557]
memory len:10000
memory used:3168.0
now epsilon is 0.039482252776555, the reward is 194.33333333333334 with loss [26.960035800933838, 21.42902183532715] in episode 2679
Report: 
rewardSum:194.33333333333334
loss:[26.960035800933838, 21.42902183532715]
policies:[0, 4, 0]
qAverage:[0.0, 67.23583602905273]
ws:[-0.8121873810887337, 8.752769589424133]
memory len:10000
memory used:3168.0
now epsilon is 0.039442785327155755, the reward is 194.33333333333334 with loss [28.9764084815979, 21.98922348022461] in episode 2680
Report: 
rewardSum:194.33333333333334
loss:[28.9764084815979, 21.98922348022461]
policies:[0, 4, 0]
qAverage:[0.0, 69.67574768066406]
ws:[-0.7616700053215026, 6.3788165807723995]
memory len:10000
memory used:3168.0
now epsilon is 0.039403357330408076, the reward is 194.33333333333334 with loss [28.958454608917236, 25.120965480804443] in episode 2681
Report: 
rewardSum:194.33333333333334
loss:[28.958454608917236, 25.120965480804443]
policies:[0, 4, 0]
qAverage:[0.0, 69.50989074707032]
ws:[-0.6369426190853119, 6.1560064375400545]
memory len:10000
memory used:3168.0
now epsilon is 0.03936396874687411, the reward is 194.33333333333334 with loss [22.45720624923706, 29.660945415496826] in episode 2682
Report: 
rewardSum:194.33333333333334
loss:[22.45720624923706, 29.660945415496826]
policies:[0, 4, 0]
qAverage:[0.0, 69.61844177246094]
ws:[-0.6817440927028656, 6.392970135807991]
memory len:10000
memory used:3168.0
now epsilon is 0.03932461953715542, the reward is 194.33333333333334 with loss [20.937167406082153, 20.52525568008423] in episode 2683
Report: 
rewardSum:194.33333333333334
loss:[20.937167406082153, 20.52525568008423]
policies:[0, 4, 0]
qAverage:[0.0, 69.72020111083984]
ws:[-0.6461255431175232, 6.440182840824127]
memory len:10000
memory used:3168.0
now epsilon is 0.03928530966189296, the reward is 194.33333333333334 with loss [21.921579837799072, 28.75484323501587] in episode 2684
Report: 
rewardSum:194.33333333333334
loss:[21.921579837799072, 28.75484323501587]
policies:[0, 4, 0]
qAverage:[0.0, 69.62815551757812]
ws:[-0.7075240060687065, 6.2199210047721865]
memory len:10000
memory used:3168.0
now epsilon is 0.03924603908176703, the reward is 194.33333333333334 with loss [28.22030258178711, 18.56842589378357] in episode 2685
Report: 
rewardSum:194.33333333333334
loss:[28.22030258178711, 18.56842589378357]
policies:[0, 3, 1]
qAverage:[0.0, 64.63234901428223]
ws:[-0.9627038687467575, 6.705743812024593]
memory len:10000
memory used:3168.0
now epsilon is 0.0392068077574972, the reward is 194.33333333333334 with loss [26.761170625686646, 24.189977169036865] in episode 2686
Report: 
rewardSum:194.33333333333334
loss:[26.761170625686646, 24.189977169036865]
policies:[0, 2, 2]
qAverage:[0.0, 63.45208994547526]
ws:[-1.5352320671081543, 5.37730860710144]
memory len:10000
memory used:3168.0
now epsilon is 0.03916761564984234, the reward is 194.33333333333334 with loss [31.151610374450684, 29.100865364074707] in episode 2687
Report: 
rewardSum:194.33333333333334
loss:[31.151610374450684, 29.100865364074707]
policies:[0, 3, 1]
qAverage:[0.0, 66.37021827697754]
ws:[-1.4737235307693481, 7.865231990814209]
memory len:10000
memory used:3168.0
now epsilon is 0.039128462719600555, the reward is 194.33333333333334 with loss [19.17359471321106, 25.461641311645508] in episode 2688
Report: 
rewardSum:194.33333333333334
loss:[19.17359471321106, 25.461641311645508]
policies:[0, 4, 0]
qAverage:[0.0, 68.5411148071289]
ws:[-1.2203109860420227, 6.8406271696090695]
memory len:10000
memory used:3168.0
now epsilon is 0.03908934892760911, the reward is 194.33333333333334 with loss [25.636584758758545, 22.149489879608154] in episode 2689
Report: 
rewardSum:194.33333333333334
loss:[25.636584758758545, 22.149489879608154]
policies:[0, 4, 0]
qAverage:[0.0, 66.79275054931641]
ws:[-1.0415801227092742, 5.046172070503235]
memory len:10000
memory used:3168.0
now epsilon is 0.03905027423474442, the reward is 194.33333333333334 with loss [22.856260776519775, 26.71452283859253] in episode 2690
Report: 
rewardSum:194.33333333333334
loss:[22.856260776519775, 26.71452283859253]
policies:[0, 4, 0]
qAverage:[0.0, 68.31700286865234]
ws:[-0.604178300499916, 6.782444047927856]
memory len:10000
memory used:3168.0
now epsilon is 0.03901123860192203, the reward is 194.33333333333334 with loss [29.10624647140503, 26.854870796203613] in episode 2691
Report: 
rewardSum:194.33333333333334
loss:[29.10624647140503, 26.854870796203613]
policies:[0, 4, 0]
qAverage:[0.0, 67.71859436035156]
ws:[-0.4266968220472336, 6.548708772659301]
memory len:10000
memory used:3168.0
now epsilon is 0.03897224199009653, the reward is 194.33333333333334 with loss [20.912017345428467, 21.966540336608887] in episode 2692
Report: 
rewardSum:194.33333333333334
loss:[20.912017345428467, 21.966540336608887]
policies:[0, 4, 0]
qAverage:[0.0, 64.95687103271484]
ws:[-0.7622358985245228, 4.683772772550583]
memory len:10000
memory used:3168.0
now epsilon is 0.03893328436026157, the reward is 194.33333333333334 with loss [27.947243213653564, 19.971344709396362] in episode 2693
Report: 
rewardSum:194.33333333333334
loss:[27.947243213653564, 19.971344709396362]
policies:[0, 4, 0]
qAverage:[0.0, 67.9164321899414]
ws:[-0.6672380089759826, 6.278480124473572]
memory len:10000
memory used:3168.0
now epsilon is 0.03889436567344977, the reward is 194.33333333333334 with loss [21.462235927581787, 22.960344791412354] in episode 2694
Report: 
rewardSum:194.33333333333334
loss:[21.462235927581787, 22.960344791412354]
policies:[0, 4, 0]
qAverage:[0.0, 67.63431549072266]
ws:[-0.6832593709230423, 6.418225955963135]
memory len:10000
memory used:3169.0
now epsilon is 0.03885548589073271, the reward is 194.33333333333334 with loss [23.928256511688232, 21.83523154258728] in episode 2695
Report: 
rewardSum:194.33333333333334
loss:[23.928256511688232, 21.83523154258728]
policies:[1, 3, 0]
qAverage:[0.0, 64.8733081817627]
ws:[-0.9207979775965214, 5.132753670215607]
memory len:10000
memory used:3168.0
now epsilon is 0.038816644973220876, the reward is 194.33333333333334 with loss [31.260869026184082, 19.745575666427612] in episode 2696
Report: 
rewardSum:194.33333333333334
loss:[31.260869026184082, 19.745575666427612]
policies:[0, 4, 0]
qAverage:[0.0, 61.72016143798828]
ws:[-0.83012929931283, 5.636009991168976]
memory len:10000
memory used:3168.0
now epsilon is 0.03877784288206364, the reward is 194.33333333333334 with loss [22.33452272415161, 24.223464012145996] in episode 2697
Report: 
rewardSum:194.33333333333334
loss:[22.33452272415161, 24.223464012145996]
policies:[0, 4, 0]
qAverage:[0.0, 67.60615997314453]
ws:[-0.47735044956207273, 7.241136789321899]
memory len:10000
memory used:3168.0
now epsilon is 0.0387390795784492, the reward is 194.33333333333334 with loss [29.54724407196045, 24.93256378173828] in episode 2698
Report: 
rewardSum:194.33333333333334
loss:[29.54724407196045, 24.93256378173828]
policies:[0, 4, 0]
qAverage:[0.0, 66.70960540771485]
ws:[-0.7931051999330521, 4.365033030509949]
memory len:10000
memory used:3168.0
now epsilon is 0.038700355023604555, the reward is 194.33333333333334 with loss [28.814416885375977, 18.295089721679688] in episode 2699
Report: 
rewardSum:194.33333333333334
loss:[28.814416885375977, 18.295089721679688]
policies:[0, 4, 0]
qAverage:[0.0, 66.37239074707031]
ws:[-0.7649781674146652, 4.155021464824676]
memory len:10000
memory used:3169.0
now epsilon is 0.03866166917879547, the reward is 194.33333333333334 with loss [23.37433433532715, 29.13330888748169] in episode 2700
Report: 
rewardSum:194.33333333333334
loss:[23.37433433532715, 29.13330888748169]
policies:[0, 4, 0]
qAverage:[0.0, 66.6634536743164]
ws:[-0.5707457005977631, 5.842474842071534]
memory len:10000
memory used:3168.0
now epsilon is 0.03862302200532641, the reward is 194.33333333333334 with loss [21.749922037124634, 20.322699546813965] in episode 2701
Report: 
rewardSum:194.33333333333334
loss:[21.749922037124634, 20.322699546813965]
policies:[1, 3, 0]
qAverage:[0.0, 65.42139434814453]
ws:[-0.8858809471130371, 6.706285834312439]
memory len:10000
memory used:3168.0
now epsilon is 0.03858441346454055, the reward is 194.33333333333334 with loss [21.756529331207275, 21.299245357513428] in episode 2702
Report: 
rewardSum:194.33333333333334
loss:[21.756529331207275, 21.299245357513428]
policies:[0, 4, 0]
qAverage:[0.0, 66.70470581054687]
ws:[-0.6614721804857254, 5.434951519966125]
memory len:10000
memory used:3168.0
now epsilon is 0.038545843517819686, the reward is 194.33333333333334 with loss [19.885197162628174, 23.356714963912964] in episode 2703
Report: 
rewardSum:194.33333333333334
loss:[19.885197162628174, 23.356714963912964]
policies:[0, 4, 0]
qAverage:[0.0, 66.43859405517578]
ws:[-0.5554527431726456, 6.2793569445610045]
memory len:10000
memory used:3168.0
now epsilon is 0.038507312126584224, the reward is 194.33333333333334 with loss [15.070767164230347, 25.748460292816162] in episode 2704
Report: 
rewardSum:194.33333333333334
loss:[15.070767164230347, 25.748460292816162]
policies:[0, 3, 1]
qAverage:[0.0, 62.71356010437012]
ws:[-0.4972205087542534, 5.668069809675217]
memory len:10000
memory used:3168.0
now epsilon is 0.03846881925229314, the reward is 194.33333333333334 with loss [28.65695571899414, 23.394509077072144] in episode 2705
Report: 
rewardSum:194.33333333333334
loss:[28.65695571899414, 23.394509077072144]
policies:[0, 4, 0]
qAverage:[0.0, 66.42560424804688]
ws:[0.2810888528823853, 8.608731174468994]
memory len:10000
memory used:3168.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23*		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.03843036485644392, the reward is 194.33333333333334 with loss [34.68670415878296, 25.940489292144775] in episode 2706
Report: 
rewardSum:194.33333333333334
loss:[34.68670415878296, 25.940489292144775]
policies:[0, 4, 0]
qAverage:[0.0, 66.06553955078125]
ws:[0.5519713401794434, 8.710946798324585]
memory len:10000
memory used:3168.0
############# STATE ###############
0*		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.038391948900572556, the reward is 194.33333333333334 with loss [26.218212127685547, 21.906226873397827] in episode 2707
Report: 
rewardSum:194.33333333333334
loss:[26.218212127685547, 21.906226873397827]
policies:[0, 4, 0]
qAverage:[0.0, 65.79004516601563]
ws:[0.5878306150436401, 8.615136003494262]
memory len:10000
memory used:3168.0
now epsilon is 0.03835357134625348, the reward is 194.33333333333334 with loss [18.554112911224365, 27.23883867263794] in episode 2708
Report: 
rewardSum:194.33333333333334
loss:[18.554112911224365, 27.23883867263794]
policies:[0, 4, 0]
qAverage:[0.0, 66.29182586669921]
ws:[0.5769510269165039, 8.180338525772095]
memory len:10000
memory used:3169.0
now epsilon is 0.03831523215509954, the reward is 194.33333333333334 with loss [24.481696605682373, 28.326298713684082] in episode 2709
Report: 
rewardSum:194.33333333333334
loss:[24.481696605682373, 28.326298713684082]
policies:[0, 4, 0]
qAverage:[0.0, 65.26532440185547]
ws:[0.257388174533844, 5.554737520217896]
memory len:10000
memory used:3168.0
now epsilon is 0.03827693128876195, the reward is 194.33333333333334 with loss [24.772277355194092, 28.730935096740723] in episode 2710
Report: 
rewardSum:194.33333333333334
loss:[24.772277355194092, 28.730935096740723]
policies:[1, 3, 0]
qAverage:[0.0, 64.08481407165527]
ws:[0.1309133917093277, 9.586098909378052]
memory len:10000
memory used:3168.0
now epsilon is 0.038238668708930265, the reward is 194.33333333333334 with loss [24.65224599838257, 26.86269760131836] in episode 2711
Report: 
rewardSum:194.33333333333334
loss:[24.65224599838257, 26.86269760131836]
policies:[0, 4, 0]
qAverage:[0.0, 62.150489807128906]
ws:[-0.07453107237815856, 5.82177050113678]
memory len:10000
memory used:3169.0
now epsilon is 0.03820044437733234, the reward is 194.33333333333334 with loss [26.445762395858765, 25.07037115097046] in episode 2712
Report: 
rewardSum:194.33333333333334
loss:[26.445762395858765, 25.07037115097046]
policies:[0, 4, 0]
qAverage:[0.0, 63.72299041748047]
ws:[-0.4181369012221694, 5.510526013374329]
memory len:10000
memory used:3169.0
now epsilon is 0.03816225825573428, the reward is 194.33333333333334 with loss [17.12606167793274, 23.67527961730957] in episode 2713
Report: 
rewardSum:194.33333333333334
loss:[17.12606167793274, 23.67527961730957]
policies:[0, 4, 0]
qAverage:[0.0, 63.22954559326172]
ws:[-0.6439709283411503, 5.8510963916778564]
memory len:10000
memory used:3168.0
now epsilon is 0.0381241103059404, the reward is 194.33333333333334 with loss [26.170483112335205, 22.247000217437744] in episode 2714
Report: 
rewardSum:194.33333333333334
loss:[26.170483112335205, 22.247000217437744]
policies:[0, 4, 0]
qAverage:[0.0, 60.846811294555664]
ws:[-0.890531706623733, 6.263656407594681]
memory len:10000
memory used:3168.0
now epsilon is 0.03808600048979323, the reward is 194.33333333333334 with loss [31.800598621368408, 24.600351810455322] in episode 2715
Report: 
rewardSum:194.33333333333334
loss:[31.800598621368408, 24.600351810455322]
policies:[0, 4, 0]
qAverage:[0.0, 63.07276611328125]
ws:[-0.6987123906612396, 8.271803140640259]
memory len:10000
memory used:3168.0
now epsilon is 0.03804792876917339, the reward is 194.33333333333334 with loss [25.416279315948486, 28.418689727783203] in episode 2716
Report: 
rewardSum:194.33333333333334
loss:[25.416279315948486, 28.418689727783203]
policies:[0, 4, 0]
qAverage:[0.0, 63.7935791015625]
ws:[-0.8266833126544952, 8.692793226242065]
memory len:10000
memory used:3168.0
now epsilon is 0.03800989510599967, the reward is 194.33333333333334 with loss [17.028647661209106, 15.530216693878174] in episode 2717
Report: 
rewardSum:194.33333333333334
loss:[17.028647661209106, 15.530216693878174]
policies:[0, 4, 0]
qAverage:[0.0, 63.42561492919922]
ws:[-0.7409669160842896, 9.458566069602966]
memory len:10000
memory used:3168.0
now epsilon is 0.03797189946222887, the reward is 194.33333333333334 with loss [21.240115642547607, 23.5980007648468] in episode 2718
Report: 
rewardSum:194.33333333333334
loss:[21.240115642547607, 23.5980007648468]
policies:[0, 4, 0]
qAverage:[0.0, 62.99874267578125]
ws:[-0.5933966696262359, 9.97406072616577]
memory len:10000
memory used:3168.0
now epsilon is 0.037933941799855854, the reward is 194.33333333333334 with loss [20.127795696258545, 23.569279670715332] in episode 2719
Report: 
rewardSum:194.33333333333334
loss:[20.127795696258545, 23.569279670715332]
policies:[0, 4, 0]
qAverage:[0.0, 63.917422485351565]
ws:[-0.5104992270469666, 10.099872541427612]
memory len:10000
memory used:3168.0
now epsilon is 0.03789602208091345, the reward is 194.33333333333334 with loss [29.703519821166992, 25.168899059295654] in episode 2720
Report: 
rewardSum:194.33333333333334
loss:[29.703519821166992, 25.168899059295654]
policies:[0, 3, 1]
qAverage:[0.0, 56.16679382324219]
ws:[0.10649018734693527, 6.967059314250946]
memory len:10000
memory used:3169.0
now epsilon is 0.03785814026747247, the reward is 194.33333333333334 with loss [29.35000514984131, 25.248887062072754] in episode 2721
Report: 
rewardSum:194.33333333333334
loss:[29.35000514984131, 25.248887062072754]
policies:[1, 3, 0]
qAverage:[0.0, 56.20057169596354]
ws:[-1.0170258283615112, 11.906223932902018]
memory len:10000
memory used:3169.0
now epsilon is 0.037820296321641626, the reward is 194.33333333333334 with loss [26.0072922706604, 26.76963710784912] in episode 2722
Report: 
rewardSum:194.33333333333334
loss:[26.0072922706604, 26.76963710784912]
policies:[0, 4, 0]
qAverage:[0.0, 62.58650360107422]
ws:[-0.9806583046913147, 7.504565091244876]
memory len:10000
memory used:3169.0
now epsilon is 0.03778249020556749, the reward is 194.33333333333334 with loss [25.096351146697998, 29.774490356445312] in episode 2723
Report: 
rewardSum:194.33333333333334
loss:[25.096351146697998, 29.774490356445312]
policies:[1, 3, 0]
qAverage:[12.70408935546875, 50.36068878173828]
ws:[-1.0974090993404388, 6.369425916671753]
memory len:10000
memory used:3169.0
now epsilon is 0.03774472188143449, the reward is 194.33333333333334 with loss [23.2198748588562, 23.62127375602722] in episode 2724
Report: 
rewardSum:194.33333333333334
loss:[23.2198748588562, 23.62127375602722]
policies:[1, 3, 0]
qAverage:[16.149471282958984, 40.46971893310547]
ws:[-0.9919397085905075, 6.725399062037468]
memory len:10000
memory used:3169.0
now epsilon is 0.03770699131146487, the reward is 194.33333333333334 with loss [28.7933349609375, 29.55614423751831] in episode 2725
Report: 
rewardSum:194.33333333333334
loss:[28.7933349609375, 29.55614423751831]
policies:[2, 2, 0]
qAverage:[15.374475479125977, 43.68751525878906]
ws:[-1.1357070729136467, 4.743965201079845]
memory len:10000
memory used:3169.0
now epsilon is 0.03766929845791862, the reward is 194.33333333333334 with loss [24.069076538085938, 23.773262977600098] in episode 2726
Report: 
rewardSum:194.33333333333334
loss:[24.069076538085938, 23.773262977600098]
policies:[0, 4, 0]
qAverage:[0.0, 62.33139495849609]
ws:[-0.6658911257982254, 7.263439929485321]
memory len:10000
memory used:3169.0
now epsilon is 0.03763164328309344, the reward is 194.33333333333334 with loss [29.018436431884766, 24.985246181488037] in episode 2727
Report: 
rewardSum:194.33333333333334
loss:[29.018436431884766, 24.985246181488037]
policies:[0, 4, 0]
qAverage:[0.0, 62.7735595703125]
ws:[-0.9205849647521973, 5.046696579456329]
memory len:10000
memory used:3169.0
now epsilon is 0.03759402574932475, the reward is 194.33333333333334 with loss [32.440109729766846, 23.37039065361023] in episode 2728
Report: 
rewardSum:194.33333333333334
loss:[32.440109729766846, 23.37039065361023]
policies:[0, 4, 0]
qAverage:[0.0, 63.00742034912109]
ws:[-0.9293816328048706, 7.226900732517242]
memory len:10000
memory used:3169.0
now epsilon is 0.03755644581898561, the reward is 194.33333333333334 with loss [24.09653949737549, 21.729925632476807] in episode 2729
Report: 
rewardSum:194.33333333333334
loss:[24.09653949737549, 21.729925632476807]
policies:[0, 4, 0]
qAverage:[0.0, 62.55496673583984]
ws:[-1.5530502080917359, 4.764498656988144]
memory len:10000
memory used:3168.0
now epsilon is 0.03751890345448668, the reward is 194.33333333333334 with loss [20.293039798736572, 23.298150062561035] in episode 2730
Report: 
rewardSum:194.33333333333334
loss:[20.293039798736572, 23.298150062561035]
policies:[1, 3, 0]
qAverage:[0.0, 56.63101768493652]
ws:[-0.8570145517587662, 1.9631368406116962]
memory len:10000
memory used:3169.0
now epsilon is 0.03748139861827621, the reward is 194.33333333333334 with loss [23.365970134735107, 22.12639093399048] in episode 2731
Report: 
rewardSum:194.33333333333334
loss:[23.365970134735107, 22.12639093399048]
policies:[0, 4, 0]
qAverage:[0.0, 60.170257568359375]
ws:[-1.7774767726659775, 5.489425078034401]
memory len:10000
memory used:3169.0
now epsilon is 0.03744393127283997, the reward is 194.33333333333334 with loss [23.850395679473877, 25.0848069190979] in episode 2732
Report: 
rewardSum:194.33333333333334
loss:[23.850395679473877, 25.0848069190979]
policies:[0, 4, 0]
qAverage:[0.0, 62.28361206054687]
ws:[-1.509488618373871, 5.5608080387115475]
memory len:10000
memory used:3169.0
now epsilon is 0.037406501380701264, the reward is 194.33333333333334 with loss [30.5854229927063, 21.85933494567871] in episode 2733
Report: 
rewardSum:194.33333333333334
loss:[30.5854229927063, 21.85933494567871]
policies:[0, 4, 0]
qAverage:[0.0, 62.99354553222656]
ws:[-1.497838032245636, 5.687238025665283]
memory len:10000
memory used:3169.0
now epsilon is 0.03736910890442083, the reward is 194.33333333333334 with loss [23.59599256515503, 24.776676177978516] in episode 2734
Report: 
rewardSum:194.33333333333334
loss:[23.59599256515503, 24.776676177978516]
policies:[0, 4, 0]
qAverage:[0.0, 61.964839172363284]
ws:[-1.5047170042991638, 5.780458545684814]
memory len:10000
memory used:3169.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20*		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.037331753806596826, the reward is 194.33333333333334 with loss [28.987908363342285, 23.403997659683228] in episode 2735
Report: 
rewardSum:194.33333333333334
loss:[28.987908363342285, 23.403997659683228]
policies:[0, 4, 0]
qAverage:[0.0, 62.773170471191406]
ws:[-1.5957442760467528, 5.050212168693543]
memory len:10000
memory used:3168.0
now epsilon is 0.03729443604986482, the reward is 194.33333333333334 with loss [30.175909996032715, 24.54941987991333] in episode 2736
Report: 
rewardSum:194.33333333333334
loss:[30.175909996032715, 24.54941987991333]
policies:[0, 4, 0]
qAverage:[0.0, 59.46790466308594]
ws:[-1.6718942284584046, 4.495203065872192]
memory len:10000
memory used:3168.0
now epsilon is 0.03725715559689772, the reward is 194.33333333333334 with loss [27.865170001983643, 27.29909324645996] in episode 2737
Report: 
rewardSum:194.33333333333334
loss:[27.865170001983643, 27.29909324645996]
policies:[0, 4, 0]
qAverage:[0.0, 59.919247436523435]
ws:[-1.6800641179084779, 4.5763993740081785]
memory len:10000
memory used:3168.0
now epsilon is 0.03721991241040575, the reward is 194.33333333333334 with loss [28.89164924621582, 27.767762660980225] in episode 2738
Report: 
rewardSum:194.33333333333334
loss:[28.89164924621582, 27.767762660980225]
policies:[0, 4, 0]
qAverage:[0.0, 59.935063171386716]
ws:[-1.604219102859497, 5.307636666297912]
memory len:10000
memory used:3168.0
now epsilon is 0.037182706453136406, the reward is 194.33333333333334 with loss [23.408430099487305, 27.241263389587402] in episode 2739
Report: 
rewardSum:194.33333333333334
loss:[23.408430099487305, 27.241263389587402]
policies:[0, 4, 0]
qAverage:[0.0, 59.87454833984375]
ws:[-1.3983608663082123, 5.6665232419967655]
memory len:10000
memory used:3168.0
now epsilon is 0.037145537687874416, the reward is 194.33333333333334 with loss [14.47131633758545, 31.148873805999756] in episode 2740
Report: 
rewardSum:194.33333333333334
loss:[14.47131633758545, 31.148873805999756]
policies:[0, 4, 0]
qAverage:[0.0, 59.495245361328124]
ws:[-1.3734477162361145, 5.520257472991943]
memory len:10000
memory used:3168.0
now epsilon is 0.03710840607744173, the reward is 194.33333333333334 with loss [24.793843269348145, 29.417842864990234] in episode 2741
Report: 
rewardSum:194.33333333333334
loss:[24.793843269348145, 29.417842864990234]
policies:[0, 4, 0]
qAverage:[0.0, 60.38457336425781]
ws:[-1.0801712617278099, 8.141701531410217]
memory len:10000
memory used:3168.0
now epsilon is 0.03707131158469744, the reward is 194.33333333333334 with loss [17.673510313034058, 26.65315842628479] in episode 2742
Report: 
rewardSum:194.33333333333334
loss:[17.673510313034058, 26.65315842628479]
policies:[0, 4, 0]
qAverage:[0.0, 55.47747802734375]
ws:[-1.1042715348303318, 8.590983301401138]
memory len:10000
memory used:3169.0
now epsilon is 0.03703425417253777, the reward is 194.33333333333334 with loss [23.433387517929077, 27.637821197509766] in episode 2743
Report: 
rewardSum:194.33333333333334
loss:[23.433387517929077, 27.637821197509766]
policies:[0, 4, 0]
qAverage:[0.0, 60.688514709472656]
ws:[-1.0103006452322005, 7.626782059669495]
memory len:10000
memory used:3168.0
now epsilon is 0.03699723380389606, the reward is 194.33333333333334 with loss [25.34041690826416, 20.9388325214386] in episode 2744
Report: 
rewardSum:194.33333333333334
loss:[25.34041690826416, 20.9388325214386]
policies:[0, 4, 0]
qAverage:[0.0, 59.3018310546875]
ws:[-1.1563364505767821, 7.325800347328186]
memory len:10000
memory used:3169.0
now epsilon is 0.036960250441742656, the reward is 194.33333333333334 with loss [26.82696533203125, 39.4906702041626] in episode 2745
Report: 
rewardSum:194.33333333333334
loss:[26.82696533203125, 39.4906702041626]
policies:[0, 4, 0]
qAverage:[0.0, 61.18125457763672]
ws:[-1.5037604987621307, 4.346488738059998]
memory len:10000
memory used:3169.0
now epsilon is 0.03692330404908497, the reward is 194.33333333333334 with loss [18.457993984222412, 24.107105255126953] in episode 2746
Report: 
rewardSum:194.33333333333334
loss:[18.457993984222412, 24.107105255126953]
policies:[0, 3, 1]
qAverage:[0.0, 55.276723861694336]
ws:[-1.7782449796795845, 7.0340264067053795]
memory len:10000
memory used:3169.0
now epsilon is 0.03688639458896734, the reward is 194.33333333333334 with loss [31.3214750289917, 19.90808343887329] in episode 2747
Report: 
rewardSum:194.33333333333334
loss:[31.3214750289917, 19.90808343887329]
policies:[1, 3, 0]
qAverage:[11.947915649414062, 47.31883697509765]
ws:[-1.668551152944565, 5.976349139213562]
memory len:10000
memory used:3169.0
now epsilon is 0.0368495220244711, the reward is 194.33333333333334 with loss [38.573190689086914, 28.66609287261963] in episode 2748
Report: 
rewardSum:194.33333333333334
loss:[38.573190689086914, 28.66609287261963]
policies:[1, 3, 0]
qAverage:[12.164765930175781, 45.412332153320314]
ws:[-1.8963958382606507, 2.8865206480026244]
memory len:10000
memory used:3169.0
now epsilon is 0.03681268631871445, the reward is 194.33333333333334 with loss [28.070223331451416, 29.18266010284424] in episode 2749
Report: 
rewardSum:194.33333333333334
loss:[28.070223331451416, 29.18266010284424]
policies:[0, 3, 1]
qAverage:[0.0, 57.255062103271484]
ws:[-2.343596637248993, 3.408182919025421]
memory len:10000
memory used:3169.0
now epsilon is 0.03677588743485245, the reward is 194.33333333333334 with loss [20.46317195892334, 27.13245153427124] in episode 2750
Report: 
rewardSum:194.33333333333334
loss:[20.46317195892334, 27.13245153427124]
policies:[1, 3, 0]
qAverage:[11.755907440185547, 44.613519287109376]
ws:[-1.8459574580192566, 4.955328869819641]
memory len:10000
memory used:3169.0
now epsilon is 0.03673912533607704, the reward is 194.33333333333334 with loss [20.912008047103882, 24.533635139465332] in episode 2751
Report: 
rewardSum:194.33333333333334
loss:[20.912008047103882, 24.533635139465332]
policies:[1, 3, 0]
qAverage:[11.51507339477539, 45.003326416015625]
ws:[-1.8910578727722167, 3.0700127840042115]
memory len:10000
memory used:3169.0
now epsilon is 0.03670239998561692, the reward is 194.33333333333334 with loss [21.741744995117188, 14.776971817016602] in episode 2752
Report: 
rewardSum:194.33333333333334
loss:[21.741744995117188, 14.776971817016602]
policies:[0, 3, 1]
qAverage:[0.0, 55.81186103820801]
ws:[-1.3114646598696709, 8.049908876419067]
memory len:10000
memory used:3169.0
now epsilon is 0.036665711346737545, the reward is 194.33333333333334 with loss [25.26449203491211, 28.449230194091797] in episode 2753
Report: 
rewardSum:194.33333333333334
loss:[25.26449203491211, 28.449230194091797]
policies:[0, 4, 0]
qAverage:[0.0, 57.196170043945315]
ws:[-0.6706852346658707, 7.10535159111023]
memory len:10000
memory used:3169.0
now epsilon is 0.03662905938274111, the reward is 194.33333333333334 with loss [32.63044023513794, 24.584688663482666] in episode 2754
Report: 
rewardSum:194.33333333333334
loss:[32.63044023513794, 24.584688663482666]
policies:[0, 4, 0]
qAverage:[0.0, 57.36751861572266]
ws:[-0.43182170391082764, 4.598156237602234]
memory len:10000
memory used:3169.0
now epsilon is 0.03659244405696647, the reward is 194.33333333333334 with loss [27.15134572982788, 32.90023994445801] in episode 2755
Report: 
rewardSum:194.33333333333334
loss:[27.15134572982788, 32.90023994445801]
policies:[0, 4, 0]
qAverage:[0.0, 57.39429397583008]
ws:[0.06382226943969727, 7.36361403465271]
memory len:10000
memory used:3169.0
now epsilon is 0.03655586533278914, the reward is 194.33333333333334 with loss [28.29393434524536, 20.707808256149292] in episode 2756
Report: 
rewardSum:194.33333333333334
loss:[28.29393434524536, 20.707808256149292]
policies:[0, 4, 0]
qAverage:[0.0, 57.271157836914064]
ws:[0.07768232822418213, 7.451045632362366]
memory len:10000
memory used:3169.0
now epsilon is 0.03651932317362126, the reward is 194.33333333333334 with loss [28.015012741088867, 26.15860652923584] in episode 2757
Report: 
rewardSum:194.33333333333334
loss:[28.015012741088867, 26.15860652923584]
policies:[1, 3, 0]
qAverage:[0.0, 55.92095184326172]
ws:[-0.8955563008785248, 5.189415633678436]
memory len:10000
memory used:3169.0
now epsilon is 0.03648281754291152, the reward is 194.33333333333334 with loss [22.760661125183105, 27.00157594680786] in episode 2758
Report: 
rewardSum:194.33333333333334
loss:[22.760661125183105, 27.00157594680786]
policies:[1, 3, 0]
qAverage:[11.901081848144532, 43.97374725341797]
ws:[-0.8386297225952148, 6.194629879295826]
memory len:10000
memory used:3169.0
now epsilon is 0.036446348404145165, the reward is 194.33333333333334 with loss [16.92565155029297, 21.847751140594482] in episode 2759
Report: 
rewardSum:194.33333333333334
loss:[16.92565155029297, 21.847751140594482]
policies:[1, 3, 0]
qAverage:[11.856695556640625, 44.496908569335936]
ws:[-1.422099083662033, 3.1041698098182677]
memory len:10000
memory used:3169.0
now epsilon is 0.03640991572084392, the reward is 194.33333333333334 with loss [26.027955532073975, 28.423136472702026] in episode 2760
Report: 
rewardSum:194.33333333333334
loss:[26.027955532073975, 28.423136472702026]
policies:[1, 3, 0]
qAverage:[11.846506500244141, 43.638780212402345]
ws:[-1.2041626274585724, 5.881702655553818]
memory len:10000
memory used:3170.0
now epsilon is 0.03637351945656599, the reward is 194.33333333333334 with loss [27.223068237304688, 22.179999828338623] in episode 2761
Report: 
rewardSum:194.33333333333334
loss:[27.223068237304688, 22.179999828338623]
policies:[1, 3, 0]
qAverage:[11.853882598876954, 41.66320648193359]
ws:[-0.9481232076883316, 6.002810823917389]
memory len:10000
memory used:3169.0
now epsilon is 0.036337159574906026, the reward is 194.33333333333334 with loss [25.448538541793823, 24.98453664779663] in episode 2762
Report: 
rewardSum:194.33333333333334
loss:[25.448538541793823, 24.98453664779663]
policies:[1, 3, 0]
qAverage:[11.208795928955078, 40.995541381835935]
ws:[-1.0149227257817983, 3.1124017402529716]
memory len:10000
memory used:3169.0
now epsilon is 0.036300836039495035, the reward is 194.33333333333334 with loss [28.48690128326416, 25.581305742263794] in episode 2763
Report: 
rewardSum:194.33333333333334
loss:[28.48690128326416, 25.581305742263794]
policies:[1, 3, 0]
qAverage:[11.50158462524414, 42.603851318359375]
ws:[-1.5252975940704345, 2.1313945770263674]
memory len:10000
memory used:3170.0
############# STATE ###############
0-		9*		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.036264548814000395, the reward is 194.33333333333334 with loss [21.01890802383423, 28.34993886947632] in episode 2764
Report: 
rewardSum:194.33333333333334
loss:[21.01890802383423, 28.34993886947632]
policies:[1, 2, 1]
qAverage:[14.009249687194824, 32.835039138793945]
ws:[-0.8284685760736465, 0.5915607213973999]
memory len:10000
memory used:3169.0
now epsilon is 0.03622829786212581, the reward is 194.33333333333334 with loss [26.02528476715088, 28.469128608703613] in episode 2765
Report: 
rewardSum:194.33333333333334
loss:[26.02528476715088, 28.469128608703613]
policies:[2, 2, 0]
qAverage:[14.026613235473633, 34.52881622314453]
ws:[-0.8637842386960983, 0.8055254518985748]
memory len:10000
memory used:3170.0
now epsilon is 0.036192083147611256, the reward is 194.33333333333334 with loss [22.18809461593628, 27.292396068572998] in episode 2766
Report: 
rewardSum:194.33333333333334
loss:[22.18809461593628, 27.292396068572998]
policies:[1, 3, 0]
qAverage:[11.331482696533204, 41.21240997314453]
ws:[-1.8671837091445922, 4.38532145023346]
memory len:10000
memory used:3170.0
now epsilon is 0.03615590463423297, the reward is 194.33333333333334 with loss [30.065265655517578, 25.381529808044434] in episode 2767
Report: 
rewardSum:194.33333333333334
loss:[30.065265655517578, 25.381529808044434]
policies:[1, 3, 0]
qAverage:[11.06279296875, 42.542633056640625]
ws:[-2.383166551589966, 1.474364447593689]
memory len:10000
memory used:3170.0
now epsilon is 0.036119762285803374, the reward is 194.33333333333334 with loss [21.85017681121826, 29.32593584060669] in episode 2768
Report: 
rewardSum:194.33333333333334
loss:[21.85017681121826, 29.32593584060669]
policies:[1, 3, 0]
qAverage:[11.366057586669921, 41.11988067626953]
ws:[-2.9896445274353027, 1.1005662322044372]
memory len:10000
memory used:3170.0
now epsilon is 0.03608365606617109, the reward is 194.33333333333334 with loss [28.473496675491333, 28.716712951660156] in episode 2769
Report: 
rewardSum:194.33333333333334
loss:[28.473496675491333, 28.716712951660156]
policies:[1, 3, 0]
qAverage:[10.900654602050782, 42.766363525390624]
ws:[-3.223908853530884, 4.389473390579224]
memory len:10000
memory used:3170.0
now epsilon is 0.03604758593922086, the reward is 194.33333333333334 with loss [24.25437355041504, 26.091145992279053] in episode 2770
Report: 
rewardSum:194.33333333333334
loss:[24.25437355041504, 26.091145992279053]
policies:[0, 3, 1]
qAverage:[0.0, 51.823768615722656]
ws:[-3.584601104259491, 3.32692551612854]
memory len:10000
memory used:3170.0
now epsilon is 0.03601155186887354, the reward is 194.33333333333334 with loss [33.542248249053955, 24.755963802337646] in episode 2771
Report: 
rewardSum:194.33333333333334
loss:[33.542248249053955, 24.755963802337646]
policies:[0, 4, 0]
qAverage:[0.0, 54.74234313964844]
ws:[-3.457283592224121, 2.535167062282562]
memory len:10000
memory used:3170.0
now epsilon is 0.03597555381908604, the reward is 194.33333333333334 with loss [25.577388763427734, 27.504326820373535] in episode 2772
Report: 
rewardSum:194.33333333333334
loss:[25.577388763427734, 27.504326820373535]
policies:[0, 4, 0]
qAverage:[0.0, 54.22913360595703]
ws:[-3.2311548233032226, 6.03405077457428]
memory len:10000
memory used:3170.0
now epsilon is 0.03593959175385131, the reward is 194.33333333333334 with loss [26.16076421737671, 23.97893762588501] in episode 2773
Report: 
rewardSum:194.33333333333334
loss:[26.16076421737671, 23.97893762588501]
policies:[0, 4, 0]
qAverage:[0.0, 54.86660308837891]
ws:[-3.3868807792663573, 4.007403779029846]
memory len:10000
memory used:3170.0
now epsilon is 0.03590366563719828, the reward is 194.33333333333334 with loss [28.948057651519775, 32.020042419433594] in episode 2774
Report: 
rewardSum:194.33333333333334
loss:[28.948057651519775, 32.020042419433594]
policies:[0, 4, 0]
qAverage:[0.0, 52.207026672363284]
ws:[-2.842168813943863, 8.55162718296051]
memory len:10000
memory used:3170.0
now epsilon is 0.03586777543319186, the reward is 194.33333333333334 with loss [24.881049036979675, 31.834227085113525] in episode 2775
Report: 
rewardSum:194.33333333333334
loss:[24.881049036979675, 31.834227085113525]
policies:[0, 4, 0]
qAverage:[0.0, 53.173945617675784]
ws:[-3.2962610960006713, 5.492128515243531]
memory len:10000
memory used:3170.0
now epsilon is 0.03583192110593286, the reward is 194.33333333333334 with loss [32.20197010040283, 20.807156085968018] in episode 2776
Report: 
rewardSum:194.33333333333334
loss:[32.20197010040283, 20.807156085968018]
policies:[0, 4, 0]
qAverage:[0.0, 53.7229621887207]
ws:[-3.483761954307556, 7.8182886123657225]
memory len:10000
memory used:3170.0
now epsilon is 0.03579610261955799, the reward is 194.33333333333334 with loss [21.291717052459717, 23.18220043182373] in episode 2777
Report: 
rewardSum:194.33333333333334
loss:[21.291717052459717, 23.18220043182373]
policies:[0, 4, 0]
qAverage:[0.0, 52.53318862915039]
ws:[-3.89265513420105, 7.279206681251526]
memory len:10000
memory used:3169.0
now epsilon is 0.035760319938239804, the reward is 194.33333333333334 with loss [23.567017555236816, 25.83028292655945] in episode 2778
Report: 
rewardSum:194.33333333333334
loss:[23.567017555236816, 25.83028292655945]
policies:[0, 3, 1]
qAverage:[0.0, 51.90943145751953]
ws:[-4.35456246137619, 4.4753851890563965]
memory len:10000
memory used:3168.0
now epsilon is 0.03572457302618666, the reward is 194.33333333333334 with loss [29.3763747215271, 24.03170680999756] in episode 2779
Report: 
rewardSum:194.33333333333334
loss:[29.3763747215271, 24.03170680999756]
policies:[0, 4, 0]
qAverage:[0.0, 52.96734848022461]
ws:[-3.7049707889556887, 3.617639183998108]
memory len:10000
memory used:3169.0
now epsilon is 0.035688861847642715, the reward is 194.33333333333334 with loss [19.547361373901367, 19.007652282714844] in episode 2780
Report: 
rewardSum:194.33333333333334
loss:[19.547361373901367, 19.007652282714844]
policies:[0, 4, 0]
qAverage:[0.0, 53.04313735961914]
ws:[-2.7488178491592405, 6.83872572183609]
memory len:10000
memory used:3168.0
now epsilon is 0.03565318636688786, the reward is 194.33333333333334 with loss [21.34324359893799, 25.061768054962158] in episode 2781
Report: 
rewardSum:194.33333333333334
loss:[21.34324359893799, 25.061768054962158]
policies:[0, 4, 0]
qAverage:[0.0, 53.17940216064453]
ws:[-2.379729378223419, 4.370121812820434]
memory len:10000
memory used:3167.0
now epsilon is 0.03559974000106022, the reward is 192.33333333333334 with loss [27.472352981567383, 23.85552430152893] in episode 2782
Report: 
rewardSum:192.33333333333334
loss:[27.472352981567383, 23.85552430152893]
policies:[0, 5, 1]
qAverage:[0.0, 54.91001065572103]
ws:[-0.8669734907646974, 7.261496861775716]
memory len:10000
memory used:3168.0
now epsilon is 0.03556415360873682, the reward is 194.33333333333334 with loss [29.9950590133667, 24.56535840034485] in episode 2783
Report: 
rewardSum:194.33333333333334
loss:[29.9950590133667, 24.56535840034485]
policies:[0, 4, 0]
qAverage:[0.0, 53.05727081298828]
ws:[-1.470490427315235, 4.5441090106964115]
memory len:10000
memory used:3167.0
now epsilon is 0.03552860278946307, the reward is 194.33333333333334 with loss [29.654234409332275, 29.289535999298096] in episode 2784
Report: 
rewardSum:194.33333333333334
loss:[29.654234409332275, 29.289535999298096]
policies:[0, 4, 0]
qAverage:[0.0, 52.81965560913086]
ws:[-1.6273475974798202, 7.215237045288086]
memory len:10000
memory used:3168.0
now epsilon is 0.03549308750767926, the reward is 194.33333333333334 with loss [22.378737449645996, 20.01703906059265] in episode 2785
Report: 
rewardSum:194.33333333333334
loss:[22.378737449645996, 20.01703906059265]
policies:[0, 4, 0]
qAverage:[0.0, 52.15762710571289]
ws:[-1.987274158000946, 7.725499939918518]
memory len:10000
memory used:3168.0
now epsilon is 0.035457607727861225, the reward is 194.33333333333334 with loss [26.12758231163025, 30.086379051208496] in episode 2786
Report: 
rewardSum:194.33333333333334
loss:[26.12758231163025, 30.086379051208496]
policies:[1, 3, 0]
qAverage:[0.0, 44.67769527435303]
ws:[-0.8661183416843414, 3.234777420759201]
memory len:10000
memory used:3168.0
now epsilon is 0.0354221634145203, the reward is 194.33333333333334 with loss [23.830679416656494, 24.80768346786499] in episode 2787
Report: 
rewardSum:194.33333333333334
loss:[23.830679416656494, 24.80768346786499]
policies:[0, 4, 0]
qAverage:[0.0, 51.72595062255859]
ws:[-2.45168114900589, 8.555528020858764]
memory len:10000
memory used:3168.0
now epsilon is 0.03538675453220332, the reward is 194.33333333333334 with loss [19.806640148162842, 23.96812105178833] in episode 2788
Report: 
rewardSum:194.33333333333334
loss:[19.806640148162842, 23.96812105178833]
policies:[0, 4, 0]
qAverage:[0.0, 50.142601776123044]
ws:[-2.3562554001808165, 8.984477400779724]
memory len:10000
memory used:3168.0
now epsilon is 0.03535138104549254, the reward is 194.33333333333334 with loss [27.620014190673828, 25.839497804641724] in episode 2789
Report: 
rewardSum:194.33333333333334
loss:[27.620014190673828, 25.839497804641724]
policies:[0, 4, 0]
qAverage:[0.0, 51.13151321411133]
ws:[-2.579352355003357, 5.958314037322998]
memory len:10000
memory used:3168.0
now epsilon is 0.03531604291900562, the reward is 194.33333333333334 with loss [21.337427139282227, 21.33281445503235] in episode 2790
Report: 
rewardSum:194.33333333333334
loss:[21.337427139282227, 21.33281445503235]
policies:[0, 4, 0]
qAverage:[0.0, 50.50849838256836]
ws:[-1.5881212651729584, 9.71264626979828]
memory len:10000
memory used:3168.0
now epsilon is 0.035280740117395605, the reward is 194.33333333333334 with loss [20.405797719955444, 27.39788818359375] in episode 2791
Report: 
rewardSum:194.33333333333334
loss:[20.405797719955444, 27.39788818359375]
policies:[0, 4, 0]
qAverage:[0.0, 51.05417938232422]
ws:[-1.6069631636142732, 8.850524187088013]
memory len:10000
memory used:3169.0
############# STATE ###############
0-		9*		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.03524547260535085, the reward is 194.33333333333334 with loss [21.50203800201416, 23.7176251411438] in episode 2792
Report: 
rewardSum:194.33333333333334
loss:[21.50203800201416, 23.7176251411438]
policies:[0, 4, 0]
qAverage:[0.0, 50.43783264160156]
ws:[-2.582590413093567, 4.105313336849212]
memory len:10000
memory used:3170.0
now epsilon is 0.035210240347595026, the reward is 194.33333333333334 with loss [26.695651054382324, 18.36167335510254] in episode 2793
Report: 
rewardSum:194.33333333333334
loss:[26.695651054382324, 18.36167335510254]
policies:[1, 3, 0]
qAverage:[10.423091888427734, 39.54380111694336]
ws:[-2.1201610803604125, 6.815395927429199]
memory len:10000
memory used:3170.0
now epsilon is 0.035175043308887065, the reward is 194.33333333333334 with loss [29.21437406539917, 31.004135131835938] in episode 2794
Report: 
rewardSum:194.33333333333334
loss:[29.21437406539917, 31.004135131835938]
policies:[1, 3, 0]
qAverage:[10.530693054199219, 39.32072067260742]
ws:[-3.045110273361206, 2.508435916900635]
memory len:10000
memory used:3170.0
now epsilon is 0.035139881454021124, the reward is 194.33333333333334 with loss [33.21743869781494, 22.746663093566895] in episode 2795
Report: 
rewardSum:194.33333333333334
loss:[33.21743869781494, 22.746663093566895]
policies:[1, 3, 0]
qAverage:[10.586223602294922, 39.63720932006836]
ws:[-3.175578498840332, 4.502632308006286]
memory len:10000
memory used:3170.0
now epsilon is 0.03510475474782655, the reward is 194.33333333333334 with loss [22.34505271911621, 26.502233743667603] in episode 2796
Report: 
rewardSum:194.33333333333334
loss:[22.34505271911621, 26.502233743667603]
policies:[0, 4, 0]
qAverage:[0.0, 50.90669326782226]
ws:[-3.9954488277435303, 0.942987585067749]
memory len:10000
memory used:3170.0
now epsilon is 0.03506966315516785, the reward is 194.33333333333334 with loss [24.609277725219727, 16.785163640975952] in episode 2797
Report: 
rewardSum:194.33333333333334
loss:[24.609277725219727, 16.785163640975952]
policies:[0, 4, 0]
qAverage:[0.0, 51.23011474609375]
ws:[-3.870458173751831, 4.024381256103515]
memory len:10000
memory used:3170.0
now epsilon is 0.035034606640944654, the reward is 194.33333333333334 with loss [27.834214210510254, 27.391454696655273] in episode 2798
Report: 
rewardSum:194.33333333333334
loss:[27.834214210510254, 27.391454696655273]
policies:[0, 4, 0]
qAverage:[0.0, 48.75716018676758]
ws:[-3.922262191772461, 1.3306515589356422]
memory len:10000
memory used:3170.0
now epsilon is 0.03499958517009168, the reward is 194.33333333333334 with loss [24.325896739959717, 21.889621019363403] in episode 2799
Report: 
rewardSum:194.33333333333334
loss:[24.325896739959717, 21.889621019363403]
policies:[0, 4, 0]
qAverage:[0.0, 49.46635818481445]
ws:[-3.652274513244629, 4.339823746681214]
memory len:10000
memory used:3170.0
now epsilon is 0.03496459870757869, the reward is 194.33333333333334 with loss [31.228219032287598, 28.019090175628662] in episode 2800
Report: 
rewardSum:194.33333333333334
loss:[31.228219032287598, 28.019090175628662]
policies:[0, 4, 0]
qAverage:[0.0, 50.05380706787109]
ws:[-4.0056725025177, 3.669818902015686]
memory len:10000
memory used:3169.0
now epsilon is 0.03492964721841048, the reward is 194.33333333333334 with loss [19.954983711242676, 25.033153533935547] in episode 2801
Report: 
rewardSum:194.33333333333334
loss:[19.954983711242676, 25.033153533935547]
policies:[0, 4, 0]
qAverage:[0.0, 49.35477828979492]
ws:[-4.452942562103272, 2.3806654512882233]
memory len:10000
memory used:3169.0
now epsilon is 0.03489473066762681, the reward is 194.33333333333334 with loss [18.82404589653015, 22.84456491470337] in episode 2802
Report: 
rewardSum:194.33333333333334
loss:[18.82404589653015, 22.84456491470337]
policies:[0, 4, 0]
qAverage:[0.0, 50.0103157043457]
ws:[-4.256422758102417, 2.1897549986839295]
memory len:10000
memory used:3169.0
now epsilon is 0.034859849020302404, the reward is 194.33333333333334 with loss [23.482778549194336, 28.121840000152588] in episode 2803
Report: 
rewardSum:194.33333333333334
loss:[23.482778549194336, 28.121840000152588]
policies:[0, 3, 1]
qAverage:[0.0, 47.42550086975098]
ws:[-3.898161619901657, -0.018840178847312927]
memory len:10000
memory used:3169.0
now epsilon is 0.034807591916988755, the reward is 192.33333333333334 with loss [37.910600662231445, 32.01484942436218] in episode 2804
Report: 
rewardSum:192.33333333333334
loss:[37.910600662231445, 32.01484942436218]
policies:[0, 5, 1]
qAverage:[0.0, 51.181979497273765]
ws:[-2.307842361430327, 1.959228828549385]
memory len:10000
memory used:3169.0
now epsilon is 0.034772797375743396, the reward is 194.33333333333334 with loss [22.295215606689453, 27.541226387023926] in episode 2805
Report: 
rewardSum:194.33333333333334
loss:[22.295215606689453, 27.541226387023926]
policies:[0, 4, 0]
qAverage:[0.0, 49.845055389404294]
ws:[-1.6952888190746307, 0.5551990926265716]
memory len:10000
memory used:3169.0
now epsilon is 0.03473803761599351, the reward is 194.33333333333334 with loss [24.978724002838135, 23.670127391815186] in episode 2806
Report: 
rewardSum:194.33333333333334
loss:[24.978724002838135, 23.670127391815186]
policies:[0, 4, 0]
qAverage:[0.0, 49.373828125]
ws:[-1.890925770252943, 3.124560073018074]
memory len:10000
memory used:3169.0
now epsilon is 0.03470331260297063, the reward is 194.33333333333334 with loss [23.532321214675903, 20.050373792648315] in episode 2807
Report: 
rewardSum:194.33333333333334
loss:[23.532321214675903, 20.050373792648315]
policies:[0, 4, 0]
qAverage:[0.0, 48.90767288208008]
ws:[-2.838215198367834, 5.190758645534515]
memory len:10000
memory used:3169.0
now epsilon is 0.034668622301941066, the reward is 194.33333333333334 with loss [26.48012924194336, 16.84223198890686] in episode 2808
Report: 
rewardSum:194.33333333333334
loss:[26.48012924194336, 16.84223198890686]
policies:[0, 4, 0]
qAverage:[0.0, 49.32560729980469]
ws:[-2.2665884897112845, 5.268105125427246]
memory len:10000
memory used:3170.0
now epsilon is 0.034633966678205834, the reward is 194.33333333333334 with loss [23.47892951965332, 18.787548542022705] in episode 2809
Report: 
rewardSum:194.33333333333334
loss:[23.47892951965332, 18.787548542022705]
policies:[0, 4, 0]
qAverage:[0.0, 49.84318542480469]
ws:[-2.212817448377609, 2.935163974761963]
memory len:10000
memory used:3169.0
now epsilon is 0.03459934569710064, the reward is 194.33333333333334 with loss [20.052568674087524, 21.544968128204346] in episode 2810
Report: 
rewardSum:194.33333333333334
loss:[20.052568674087524, 21.544968128204346]
policies:[0, 4, 0]
qAverage:[0.0, 49.101453399658205]
ws:[-1.5998432397842408, 5.596635866165161]
memory len:10000
memory used:3169.0
now epsilon is 0.03456475932399585, the reward is 194.33333333333334 with loss [18.865309238433838, 33.58533763885498] in episode 2811
Report: 
rewardSum:194.33333333333334
loss:[18.865309238433838, 33.58533763885498]
policies:[0, 4, 0]
qAverage:[0.0, 49.122782135009764]
ws:[-1.8435960710048676, 4.553413820266724]
memory len:10000
memory used:3170.0
now epsilon is 0.03453020752429645, the reward is 194.33333333333334 with loss [28.655085563659668, 14.19248652458191] in episode 2812
Report: 
rewardSum:194.33333333333334
loss:[28.655085563659668, 14.19248652458191]
policies:[0, 4, 0]
qAverage:[0.0, 49.31311569213867]
ws:[-2.2445348612964153, 4.269060236215592]
memory len:10000
memory used:3170.0
now epsilon is 0.03449569026344197, the reward is 194.33333333333334 with loss [27.691022872924805, 21.046079635620117] in episode 2813
Report: 
rewardSum:194.33333333333334
loss:[27.691022872924805, 21.046079635620117]
policies:[0, 3, 1]
qAverage:[0.0, 48.55286121368408]
ws:[-2.5848348140716553, 5.877262830734253]
memory len:10000
memory used:3170.0
now epsilon is 0.03446120750690654, the reward is 194.33333333333334 with loss [23.467007160186768, 25.90614652633667] in episode 2814
Report: 
rewardSum:194.33333333333334
loss:[23.467007160186768, 25.90614652633667]
policies:[0, 4, 0]
qAverage:[0.0, 49.1430778503418]
ws:[-2.0783615097403527, 4.939017271995544]
memory len:10000
memory used:3169.0
now epsilon is 0.034426759220198756, the reward is 194.33333333333334 with loss [18.58784055709839, 22.31820821762085] in episode 2815
Report: 
rewardSum:194.33333333333334
loss:[18.58784055709839, 22.31820821762085]
policies:[1, 3, 0]
qAverage:[0.0, 46.81288719177246]
ws:[-2.825779086910188, 2.3041209280490875]
memory len:10000
memory used:3169.0
now epsilon is 0.03439234536886174, the reward is 194.33333333333334 with loss [28.67361307144165, 30.28196430206299] in episode 2816
Report: 
rewardSum:194.33333333333334
loss:[28.67361307144165, 30.28196430206299]
policies:[0, 4, 0]
qAverage:[0.0, 48.75552139282227]
ws:[-2.704411691427231, 2.4682580471038817]
memory len:10000
memory used:3169.0
now epsilon is 0.03434078908288664, the reward is 192.33333333333334 with loss [41.97277784347534, 30.567267417907715] in episode 2817
Report: 
rewardSum:192.33333333333334
loss:[41.97277784347534, 30.567267417907715]
policies:[0, 5, 1]
qAverage:[0.0, 50.26952044169108]
ws:[-2.063465898235639, 5.687093873818715]
memory len:10000
memory used:3169.0
now epsilon is 0.0343064611694535, the reward is 194.33333333333334 with loss [22.39912748336792, 30.38902759552002] in episode 2818
Report: 
rewardSum:194.33333333333334
loss:[22.39912748336792, 30.38902759552002]
policies:[0, 4, 0]
qAverage:[0.0, 48.572453308105466]
ws:[-2.817777532339096, 2.8150782585144043]
memory len:10000
memory used:3169.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20*		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.034272167571062974, the reward is 194.33333333333334 with loss [25.142699241638184, 31.882103443145752] in episode 2819
Report: 
rewardSum:194.33333333333334
loss:[25.142699241638184, 31.882103443145752]
policies:[0, 4, 0]
qAverage:[0.0, 49.11495666503906]
ws:[-3.141278105974197, 2.315325653553009]
memory len:10000
memory used:3170.0
now epsilon is 0.03423790825341288, the reward is 194.33333333333334 with loss [21.811452388763428, 24.536268711090088] in episode 2820
Report: 
rewardSum:194.33333333333334
loss:[21.811452388763428, 24.536268711090088]
policies:[0, 4, 0]
qAverage:[0.0, 48.607628631591794]
ws:[-2.3978092789649965, 5.253879106044769]
memory len:10000
memory used:3170.0
now epsilon is 0.03420368318223534, the reward is 194.33333333333334 with loss [26.21260404586792, 27.094711303710938] in episode 2821
Report: 
rewardSum:194.33333333333334
loss:[26.21260404586792, 27.094711303710938]
policies:[0, 4, 0]
qAverage:[0.0, 49.07474594116211]
ws:[-2.8174583435058596, 2.747865390777588]
memory len:10000
memory used:3170.0
now epsilon is 0.0341694923232967, the reward is 194.33333333333334 with loss [24.165809631347656, 25.618535041809082] in episode 2822
Report: 
rewardSum:194.33333333333334
loss:[24.165809631347656, 25.618535041809082]
policies:[0, 4, 0]
qAverage:[0.0, 47.2564193725586]
ws:[-2.105314141511917, 6.048495984077453]
memory len:10000
memory used:3170.0
now epsilon is 0.03413533564239757, the reward is 194.33333333333334 with loss [20.512439727783203, 25.06238317489624] in episode 2823
Report: 
rewardSum:194.33333333333334
loss:[20.512439727783203, 25.06238317489624]
policies:[0, 4, 0]
qAverage:[0.0, 47.580823516845705]
ws:[-2.7652387358248234, 2.854197454452515]
memory len:10000
memory used:3169.0
now epsilon is 0.03410121310537272, the reward is 194.33333333333334 with loss [28.41638469696045, 25.00346279144287] in episode 2824
Report: 
rewardSum:194.33333333333334
loss:[28.41638469696045, 25.00346279144287]
policies:[0, 4, 0]
qAverage:[0.0, 48.09835662841797]
ws:[-3.002093458175659, 3.4588519811630247]
memory len:10000
memory used:3170.0
now epsilon is 0.034067124678091074, the reward is 46.33333333333334 with loss [24.982759952545166, 19.909602642059326] in episode 2825
Report: 
rewardSum:46.33333333333334
loss:[24.982759952545166, 19.909602642059326]
policies:[0, 3, 1]
qAverage:[0.0, 42.49700927734375]
ws:[-0.12864081095904112, 6.1657048761844635]
memory len:10000
memory used:3170.0
now epsilon is 0.034033070326455674, the reward is 194.33333333333334 with loss [21.4460391998291, 26.384448051452637] in episode 2826
Report: 
rewardSum:194.33333333333334
loss:[21.4460391998291, 26.384448051452637]
policies:[0, 4, 0]
qAverage:[0.0, 48.34269485473633]
ws:[-3.976397085189819, 4.7169820964336395]
memory len:10000
memory used:3170.0
now epsilon is 0.03399905001640366, the reward is 194.33333333333334 with loss [29.5276837348938, 24.913430213928223] in episode 2827
Report: 
rewardSum:194.33333333333334
loss:[29.5276837348938, 24.913430213928223]
policies:[0, 4, 0]
qAverage:[0.0, 48.146104431152345]
ws:[-5.177189540863037, 0.6788016557693481]
memory len:10000
memory used:3170.0
now epsilon is 0.0339650637139062, the reward is 194.33333333333334 with loss [22.28663921356201, 28.869835376739502] in episode 2828
Report: 
rewardSum:194.33333333333334
loss:[22.28663921356201, 28.869835376739502]
policies:[0, 4, 0]
qAverage:[0.0, 48.09660110473633]
ws:[-4.1616990804672245, 3.1589319154620172]
memory len:10000
memory used:3170.0
now epsilon is 0.03393111138496851, the reward is 194.33333333333334 with loss [19.314351320266724, 27.748169422149658] in episode 2829
Report: 
rewardSum:194.33333333333334
loss:[19.314351320266724, 27.748169422149658]
policies:[0, 4, 0]
qAverage:[0.0, 48.64241943359375]
ws:[-4.125527262687683, -0.0046032190322875975]
memory len:10000
memory used:3169.0
now epsilon is 0.03389719299562975, the reward is 194.33333333333334 with loss [24.335418701171875, 26.586379528045654] in episode 2830
Report: 
rewardSum:194.33333333333334
loss:[24.335418701171875, 26.586379528045654]
policies:[1, 2, 1]
qAverage:[12.054045677185059, 29.75566291809082]
ws:[-0.6285975947976112, 3.673827886581421]
memory len:10000
memory used:3169.0
now epsilon is 0.03386330851196305, the reward is 194.33333333333334 with loss [21.330272674560547, 24.444716453552246] in episode 2831
Report: 
rewardSum:194.33333333333334
loss:[21.330272674560547, 24.444716453552246]
policies:[0, 3, 1]
qAverage:[0.0, 47.372406005859375]
ws:[-4.165238052606583, -0.17579501867294312]
memory len:10000
memory used:3169.0
now epsilon is 0.03382945790007546, the reward is 194.33333333333334 with loss [33.17464542388916, 22.021145343780518] in episode 2832
Report: 
rewardSum:194.33333333333334
loss:[33.17464542388916, 22.021145343780518]
policies:[1, 3, 0]
qAverage:[9.661090087890624, 38.36469268798828]
ws:[-4.09112628698349, -0.5992323160171509]
memory len:10000
memory used:3169.0
now epsilon is 0.033795641126107896, the reward is 194.33333333333334 with loss [26.528643131256104, 29.44811201095581] in episode 2833
Report: 
rewardSum:194.33333333333334
loss:[26.528643131256104, 29.44811201095581]
policies:[0, 4, 0]
qAverage:[0.0, 48.12512130737305]
ws:[-4.51030125617981, 2.0137283325195314]
memory len:10000
memory used:3169.0
now epsilon is 0.03376185815623512, the reward is 194.33333333333334 with loss [21.53354239463806, 26.736843585968018] in episode 2834
Report: 
rewardSum:194.33333333333334
loss:[21.53354239463806, 26.736843585968018]
policies:[0, 4, 0]
qAverage:[0.0, 48.87194061279297]
ws:[-4.66945104598999, 0.2924701690673828]
memory len:10000
memory used:3169.0
now epsilon is 0.03372810895666571, the reward is 194.33333333333334 with loss [24.862162590026855, 26.81359624862671] in episode 2835
Report: 
rewardSum:194.33333333333334
loss:[24.862162590026855, 26.81359624862671]
policies:[0, 4, 0]
qAverage:[0.0, 48.78429260253906]
ws:[-3.2999020755290984, 4.551441717147827]
memory len:10000
memory used:3170.0
now epsilon is 0.03369439349364204, the reward is 194.33333333333334 with loss [22.876277208328247, 20.956758975982666] in episode 2836
Report: 
rewardSum:194.33333333333334
loss:[22.876277208328247, 20.956758975982666]
policies:[0, 4, 0]
qAverage:[0.0, 48.47024002075195]
ws:[-2.860180786252022, 5.25561808347702]
memory len:10000
memory used:3170.0
now epsilon is 0.0336607117334402, the reward is 194.33333333333334 with loss [25.967644214630127, 21.022099494934082] in episode 2837
Report: 
rewardSum:194.33333333333334
loss:[25.967644214630127, 21.022099494934082]
policies:[0, 4, 0]
qAverage:[0.0, 48.34638290405273]
ws:[-2.228424160182476, 5.657451993227005]
memory len:10000
memory used:3170.0
now epsilon is 0.033627063642369996, the reward is 194.33333333333334 with loss [20.678629159927368, 18.942950963974] in episode 2838
Report: 
rewardSum:194.33333333333334
loss:[20.678629159927368, 18.942950963974]
policies:[0, 4, 0]
qAverage:[0.0, 48.46875686645508]
ws:[-2.937056440114975, 2.775596076250076]
memory len:10000
memory used:3169.0
now epsilon is 0.03359344918677493, the reward is 194.33333333333334 with loss [24.633042573928833, 26.7117862701416] in episode 2839
Report: 
rewardSum:194.33333333333334
loss:[24.633042573928833, 26.7117862701416]
policies:[0, 3, 1]
qAverage:[0.0, 44.0153226852417]
ws:[-4.363662391901016, 1.7954285591840744]
memory len:10000
memory used:3170.0
now epsilon is 0.03355986833303214, the reward is 194.33333333333334 with loss [22.19003438949585, 24.363518238067627] in episode 2840
Report: 
rewardSum:194.33333333333334
loss:[22.19003438949585, 24.363518238067627]
policies:[0, 4, 0]
qAverage:[0.0, 48.835302734375]
ws:[-3.21926172375679, 2.725127625465393]
memory len:10000
memory used:3170.0
now epsilon is 0.033526321047552377, the reward is 194.33333333333334 with loss [30.16778564453125, 31.96593952178955] in episode 2841
Report: 
rewardSum:194.33333333333334
loss:[30.16778564453125, 31.96593952178955]
policies:[0, 4, 0]
qAverage:[0.0, 47.849371337890624]
ws:[-2.61090726852417, 6.104024267196655]
memory len:10000
memory used:3169.0
now epsilon is 0.033492807296779954, the reward is 194.33333333333334 with loss [24.72928762435913, 23.478079319000244] in episode 2842
Report: 
rewardSum:194.33333333333334
loss:[24.72928762435913, 23.478079319000244]
policies:[0, 4, 0]
qAverage:[0.0, 48.6846321105957]
ws:[-3.5084650218486786, 2.667905354499817]
memory len:10000
memory used:3169.0
now epsilon is 0.03345932704719275, the reward is 194.33333333333334 with loss [22.834537506103516, 22.146413326263428] in episode 2843
Report: 
rewardSum:194.33333333333334
loss:[22.834537506103516, 22.146413326263428]
policies:[0, 4, 0]
qAverage:[0.0, 48.28743362426758]
ws:[-3.8387497127056123, 2.0606343269348146]
memory len:10000
memory used:3169.0
now epsilon is 0.03342588026530212, the reward is 194.33333333333334 with loss [23.89616322517395, 20.337550163269043] in episode 2844
Report: 
rewardSum:194.33333333333334
loss:[23.89616322517395, 20.337550163269043]
policies:[0, 4, 0]
qAverage:[0.0, 48.33187484741211]
ws:[-2.500747334957123, 5.632699751853943]
memory len:10000
memory used:3170.0
now epsilon is 0.03337577277122329, the reward is 192.33333333333334 with loss [39.618024587631226, 34.87019395828247] in episode 2845
Report: 
rewardSum:192.33333333333334
loss:[39.618024587631226, 34.87019395828247]
policies:[0, 5, 1]
qAverage:[0.0, 49.10919952392578]
ws:[-1.32265571244061, 7.767321491241455]
memory len:10000
memory used:3170.0
now epsilon is 0.033342409512281006, the reward is 194.33333333333334 with loss [17.418241500854492, 27.55116367340088] in episode 2846
Report: 
rewardSum:194.33333333333334
loss:[17.418241500854492, 27.55116367340088]
policies:[0, 4, 0]
qAverage:[0.0, 48.10298309326172]
ws:[-0.8635182619094849, 6.965660238265992]
memory len:10000
memory used:3170.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20*		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.03330907960408853, the reward is 194.33333333333334 with loss [21.463018655776978, 32.749449729919434] in episode 2847
Report: 
rewardSum:194.33333333333334
loss:[21.463018655776978, 32.749449729919434]
policies:[0, 4, 0]
qAverage:[0.0, 47.344775390625]
ws:[-1.0926460027694702, 7.0277307510375975]
memory len:10000
memory used:3170.0
now epsilon is 0.03327578301330761, the reward is 194.33333333333334 with loss [26.595314025878906, 23.86438536643982] in episode 2848
Report: 
rewardSum:194.33333333333334
loss:[26.595314025878906, 23.86438536643982]
policies:[0, 3, 1]
qAverage:[0.0, 42.37087535858154]
ws:[1.4118138551712036, 6.961663842201233]
memory len:10000
memory used:3170.0
now epsilon is 0.03324251970663333, the reward is 194.33333333333334 with loss [28.59897518157959, 30.794044494628906] in episode 2849
Report: 
rewardSum:194.33333333333334
loss:[28.59897518157959, 30.794044494628906]
policies:[0, 4, 0]
qAverage:[0.0, 47.617626953125]
ws:[-2.209369343519211, 6.8418641805648805]
memory len:10000
memory used:3170.0
now epsilon is 0.03320928965079407, the reward is 194.33333333333334 with loss [20.326518535614014, 21.378888845443726] in episode 2850
Report: 
rewardSum:194.33333333333334
loss:[20.326518535614014, 21.378888845443726]
policies:[0, 4, 0]
qAverage:[0.0, 47.187242126464845]
ws:[-3.046995495259762, 3.87659273147583]
memory len:10000
memory used:3170.0
now epsilon is 0.03317609281255145, the reward is 194.33333333333334 with loss [14.798956394195557, 24.289801120758057] in episode 2851
Report: 
rewardSum:194.33333333333334
loss:[14.798956394195557, 24.289801120758057]
policies:[0, 4, 0]
qAverage:[0.0, 44.048930168151855]
ws:[-3.457702226936817, 6.4171518087387085]
memory len:10000
memory used:3169.0
now epsilon is 0.033142929158700325, the reward is 194.33333333333334 with loss [22.550219535827637, 28.216877460479736] in episode 2852
Report: 
rewardSum:194.33333333333334
loss:[22.550219535827637, 28.216877460479736]
policies:[0, 4, 0]
qAverage:[0.0, 47.841365814208984]
ws:[-2.6999057292938233, 5.455298948287964]
memory len:10000
memory used:3169.0
now epsilon is 0.03310979865606876, the reward is 194.33333333333334 with loss [24.902747631072998, 23.487581729888916] in episode 2853
Report: 
rewardSum:194.33333333333334
loss:[24.902747631072998, 23.487581729888916]
policies:[0, 4, 0]
qAverage:[0.0, 47.85454406738281]
ws:[-2.804833310842514, 5.155842611193657]
memory len:10000
memory used:3169.0
now epsilon is 0.033093245826103145, the reward is -1.0 with loss [12.68145751953125, 15.367072105407715] in episode 2854
Report: 
rewardSum:-1.0
loss:[12.68145751953125, 15.367072105407715]
policies:[0, 1, 1]
qAverage:[0.0, 24.516006469726562]
ws:[-0.2642265856266022, 0.14081911742687225]
memory len:10000
memory used:3169.0
now epsilon is 0.03306016498817604, the reward is 194.33333333333334 with loss [27.201825618743896, 19.49255657196045] in episode 2855
Report: 
rewardSum:194.33333333333334
loss:[27.201825618743896, 19.49255657196045]
policies:[0, 3, 1]
qAverage:[0.0, 47.35506343841553]
ws:[-4.110969789326191, 2.663038492202759]
memory len:10000
memory used:3169.0
now epsilon is 0.03302711721868361, the reward is 194.33333333333334 with loss [28.957425117492676, 27.51814031600952] in episode 2856
Report: 
rewardSum:194.33333333333334
loss:[28.957425117492676, 27.51814031600952]
policies:[0, 4, 0]
qAverage:[0.0, 47.51347961425781]
ws:[-2.9523292005062105, 5.0532070875167845]
memory len:10000
memory used:3169.0
now epsilon is 0.03299410248456982, the reward is 194.33333333333334 with loss [17.228089213371277, 17.732358694076538] in episode 2857
Report: 
rewardSum:194.33333333333334
loss:[17.228089213371277, 17.732358694076538]
policies:[0, 4, 0]
qAverage:[0.0, 48.00514221191406]
ws:[-3.474762602895498, 4.591771531105041]
memory len:10000
memory used:3169.0
now epsilon is 0.032961120752811685, the reward is 194.33333333333334 with loss [23.49272108078003, 31.913599967956543] in episode 2858
Report: 
rewardSum:194.33333333333334
loss:[23.49272108078003, 31.913599967956543]
policies:[0, 4, 0]
qAverage:[0.0, 47.47810592651367]
ws:[-3.758082962036133, 3.8541934847831727]
memory len:10000
memory used:3169.0
now epsilon is 0.03292817199041922, the reward is 194.33333333333334 with loss [25.29185676574707, 28.592569828033447] in episode 2859
Report: 
rewardSum:194.33333333333334
loss:[25.29185676574707, 28.592569828033447]
policies:[0, 4, 0]
qAverage:[0.0, 48.82727584838867]
ws:[-3.9224507212638855, 2.499716264009476]
memory len:10000
memory used:3169.0
now epsilon is 0.03289525616443542, the reward is 194.33333333333334 with loss [18.067018032073975, 24.446473121643066] in episode 2860
Report: 
rewardSum:194.33333333333334
loss:[18.067018032073975, 24.446473121643066]
policies:[2, 2, 0]
qAverage:[11.798995971679688, 32.093461990356445]
ws:[-4.764635115861893, 1.2140508890151978]
memory len:10000
memory used:3169.0
now epsilon is 0.03286237324193623, the reward is 194.33333333333334 with loss [27.212444305419922, 20.174631118774414] in episode 2861
Report: 
rewardSum:194.33333333333334
loss:[27.212444305419922, 20.174631118774414]
policies:[0, 4, 0]
qAverage:[0.0, 48.9363410949707]
ws:[-3.748230993747711, 2.620292615890503]
memory len:10000
memory used:3169.0
now epsilon is 0.03282952319003049, the reward is 194.33333333333334 with loss [25.48801851272583, 36.98032999038696] in episode 2862
Report: 
rewardSum:194.33333333333334
loss:[25.48801851272583, 36.98032999038696]
policies:[0, 4, 0]
qAverage:[0.0, 48.34803924560547]
ws:[-3.249304899573326, 4.02541047334671]
memory len:10000
memory used:3169.0
now epsilon is 0.032796705975859944, the reward is 194.33333333333334 with loss [26.29429864883423, 27.30937623977661] in episode 2863
Report: 
rewardSum:194.33333333333334
loss:[26.29429864883423, 27.30937623977661]
policies:[0, 4, 0]
qAverage:[0.0, 49.15347900390625]
ws:[-2.445546307787299, 5.57212233543396]
memory len:10000
memory used:3169.0
now epsilon is 0.032763921566599165, the reward is 194.33333333333334 with loss [23.082109928131104, 31.204586505889893] in episode 2864
Report: 
rewardSum:194.33333333333334
loss:[23.082109928131104, 31.204586505889893]
policies:[0, 4, 0]
qAverage:[0.0, 48.519194793701175]
ws:[-2.5398822665214538, 5.788950061798095]
memory len:10000
memory used:3169.0
now epsilon is 0.032731169929455545, the reward is 194.33333333333334 with loss [23.395163536071777, 27.70062494277954] in episode 2865
Report: 
rewardSum:194.33333333333334
loss:[23.395163536071777, 27.70062494277954]
policies:[0, 4, 0]
qAverage:[0.0, 49.55814208984375]
ws:[-3.509020473062992, 5.5817660093307495]
memory len:10000
memory used:3169.0
now epsilon is 0.03269845103166925, the reward is 194.33333333333334 with loss [21.85966920852661, 29.012345790863037] in episode 2866
Report: 
rewardSum:194.33333333333334
loss:[21.85966920852661, 29.012345790863037]
policies:[0, 4, 0]
qAverage:[0.0, 48.591895294189456]
ws:[-4.786861610412598, 4.872452080249786]
memory len:10000
memory used:3169.0
now epsilon is 0.032665764840513194, the reward is 194.33333333333334 with loss [33.52476787567139, 20.25313138961792] in episode 2867
Report: 
rewardSum:194.33333333333334
loss:[33.52476787567139, 20.25313138961792]
policies:[0, 4, 0]
qAverage:[0.0, 49.902556610107425]
ws:[-4.9118647813797, 4.617175900936127]
memory len:10000
memory used:3169.0
now epsilon is 0.03263311132329302, the reward is 194.33333333333334 with loss [17.763773441314697, 25.895790576934814] in episode 2868
Report: 
rewardSum:194.33333333333334
loss:[17.763773441314697, 25.895790576934814]
policies:[0, 4, 0]
qAverage:[0.0, 48.89707641601562]
ws:[-4.855214214324951, 4.851755662262439]
memory len:10000
memory used:3169.0
now epsilon is 0.03260049044734704, the reward is 194.33333333333334 with loss [27.3142728805542, 26.654732704162598] in episode 2869
Report: 
rewardSum:194.33333333333334
loss:[27.3142728805542, 26.654732704162598]
policies:[0, 4, 0]
qAverage:[0.0, 49.91143569946289]
ws:[-4.922510123252868, 2.7282024741172792]
memory len:10000
memory used:3169.0
now epsilon is 0.03256790218004621, the reward is 194.33333333333334 with loss [32.088852882385254, 24.9493887424469] in episode 2870
Report: 
rewardSum:194.33333333333334
loss:[32.088852882385254, 24.9493887424469]
policies:[0, 4, 0]
qAverage:[0.0, 49.107855224609374]
ws:[-4.196015805378556, 5.512683439254761]
memory len:10000
memory used:3169.0
now epsilon is 0.03253534648879412, the reward is 194.33333333333334 with loss [16.623599529266357, 28.68986415863037] in episode 2871
Report: 
rewardSum:194.33333333333334
loss:[16.623599529266357, 28.68986415863037]
policies:[0, 4, 0]
qAverage:[0.0, 49.68511047363281]
ws:[-3.7876475512981416, 5.195066118240357]
memory len:10000
memory used:3169.0
now epsilon is 0.032502823341026936, the reward is 194.33333333333334 with loss [24.902560234069824, 20.783446311950684] in episode 2872
Report: 
rewardSum:194.33333333333334
loss:[24.902560234069824, 20.783446311950684]
policies:[0, 4, 0]
qAverage:[0.0, 50.09836654663086]
ws:[-3.616849994659424, 4.770967626571656]
memory len:10000
memory used:3169.0
now epsilon is 0.032470332704213366, the reward is 194.33333333333334 with loss [19.877389907836914, 23.977686405181885] in episode 2873
Report: 
rewardSum:194.33333333333334
loss:[19.877389907836914, 23.977686405181885]
policies:[0, 4, 0]
qAverage:[0.0, 51.07219314575195]
ws:[-3.0207829140126705, 4.329865312576294]
memory len:10000
memory used:3169.0
now epsilon is 0.03242976507721819, the reward is 193.33333333333334 with loss [28.278748989105225, 28.971177577972412] in episode 2874
Report: 
rewardSum:193.33333333333334
loss:[28.278748989105225, 28.971177577972412]
policies:[0, 4, 1]
qAverage:[0.0, 50.91563491821289]
ws:[-2.7491338670253755, 3.7227135419845583]
memory len:10000
memory used:3169.0
now epsilon is 0.03239734747127614, the reward is 194.33333333333334 with loss [28.14531660079956, 24.809487342834473] in episode 2875
Report: 
rewardSum:194.33333333333334
loss:[28.14531660079956, 24.809487342834473]
policies:[1, 3, 0]
qAverage:[9.37103042602539, 40.7127082824707]
ws:[-2.7728327229619025, 0.13587564826011658]
memory len:10000
memory used:3169.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19*		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.03236496227078547, the reward is 194.33333333333334 with loss [24.57438087463379, 30.579396724700928] in episode 2876
Report: 
rewardSum:194.33333333333334
loss:[24.57438087463379, 30.579396724700928]
policies:[1, 3, 0]
qAverage:[9.544188690185546, 40.00302429199219]
ws:[-2.964042750000954, -0.40081036686897276]
memory len:10000
memory used:3169.0
now epsilon is 0.03233260944335286, the reward is 194.33333333333334 with loss [26.134110927581787, 18.40128517150879] in episode 2877
Report: 
rewardSum:194.33333333333334
loss:[26.134110927581787, 18.40128517150879]
policies:[1, 3, 0]
qAverage:[9.452935791015625, 40.814578247070315]
ws:[-2.3833687782287596, 1.471039617061615]
memory len:10000
memory used:3170.0
now epsilon is 0.03230028895661739, the reward is 194.33333333333334 with loss [25.381016731262207, 23.989009380340576] in episode 2878
Report: 
rewardSum:194.33333333333334
loss:[25.381016731262207, 23.989009380340576]
policies:[1, 3, 0]
qAverage:[11.82373046875, 36.65103340148926]
ws:[-3.7721306765452027, -1.5259031429886818]
memory len:10000
memory used:3170.0
now epsilon is 0.0322680007782505, the reward is 194.33333333333334 with loss [22.875052452087402, 26.343850135803223] in episode 2879
Report: 
rewardSum:194.33333333333334
loss:[22.875052452087402, 26.343850135803223]
policies:[1, 3, 0]
qAverage:[9.687161254882813, 40.40928344726562]
ws:[-2.771825632452965, 1.1072352647781372]
memory len:10000
memory used:3169.0
now epsilon is 0.032235744875955916, the reward is 194.33333333333334 with loss [21.848902225494385, 28.685702323913574] in episode 2880
Report: 
rewardSum:194.33333333333334
loss:[21.848902225494385, 28.685702323913574]
policies:[1, 3, 0]
qAverage:[9.335844421386719, 40.568659973144534]
ws:[-2.4032616406679153, 0.9836484909057617]
memory len:10000
memory used:3170.0
now epsilon is 0.03220352121746968, the reward is 194.33333333333334 with loss [25.86701488494873, 31.039997100830078] in episode 2881
Report: 
rewardSum:194.33333333333334
loss:[25.86701488494873, 31.039997100830078]
policies:[0, 4, 0]
qAverage:[0.0, 50.84579925537109]
ws:[-0.4811753749847412, 4.933507823944092]
memory len:10000
memory used:3170.0
now epsilon is 0.03217132977056007, the reward is 194.33333333333334 with loss [15.180307388305664, 27.968063831329346] in episode 2882
Report: 
rewardSum:194.33333333333334
loss:[15.180307388305664, 27.968063831329346]
policies:[0, 4, 0]
qAverage:[0.0, 51.00180206298828]
ws:[0.60078444480896, 6.1151169538497925]
memory len:10000
memory used:3170.0
now epsilon is 0.03213917050302759, the reward is 194.33333333333334 with loss [22.856337070465088, 26.895785331726074] in episode 2883
Report: 
rewardSum:194.33333333333334
loss:[22.856337070465088, 26.895785331726074]
policies:[0, 4, 0]
qAverage:[0.0, 51.35431823730469]
ws:[0.8131376743316651, 6.961002349853516]
memory len:10000
memory used:3170.0
now epsilon is 0.03210704338270494, the reward is 194.33333333333334 with loss [28.601579666137695, 22.957889556884766] in episode 2884
Report: 
rewardSum:194.33333333333334
loss:[28.601579666137695, 22.957889556884766]
policies:[0, 4, 0]
qAverage:[0.0, 50.33870162963867]
ws:[-0.3216038703918457, 4.756432962417603]
memory len:10000
memory used:3170.0
now epsilon is 0.03207494837745694, the reward is 194.33333333333334 with loss [22.466853141784668, 30.362709999084473] in episode 2885
Report: 
rewardSum:194.33333333333334
loss:[22.466853141784668, 30.362709999084473]
policies:[1, 3, 0]
qAverage:[0.0, 51.12093734741211]
ws:[-2.651025965809822, 3.5918461084365845]
memory len:10000
memory used:3170.0
now epsilon is 0.03204288545518057, the reward is 194.33333333333334 with loss [30.262966632843018, 23.39634132385254] in episode 2886
Report: 
rewardSum:194.33333333333334
loss:[30.262966632843018, 23.39634132385254]
policies:[0, 4, 0]
qAverage:[0.0, 51.15066986083984]
ws:[-2.229240870475769, 3.05282883644104]
memory len:10000
memory used:3170.0
now epsilon is 0.032010854583804885, the reward is 194.33333333333334 with loss [26.04487133026123, 23.36169672012329] in episode 2887
Report: 
rewardSum:194.33333333333334
loss:[26.04487133026123, 23.36169672012329]
policies:[0, 4, 0]
qAverage:[0.0, 51.81900634765625]
ws:[-2.344289833307266, 4.389021563529968]
memory len:10000
memory used:3170.0
now epsilon is 0.031978855731291, the reward is 194.33333333333334 with loss [24.490785121917725, 24.974218368530273] in episode 2888
Report: 
rewardSum:194.33333333333334
loss:[24.490785121917725, 24.974218368530273]
policies:[0, 4, 0]
qAverage:[0.0, 51.78995819091797]
ws:[-2.9635838150978087, 3.966648256778717]
memory len:10000
memory used:3170.0
now epsilon is 0.03194688886563206, the reward is 194.33333333333334 with loss [21.542805671691895, 27.547700881958008] in episode 2889
Report: 
rewardSum:194.33333333333334
loss:[21.542805671691895, 27.547700881958008]
policies:[0, 4, 0]
qAverage:[0.0, 51.94518051147461]
ws:[-2.955019789934158, 3.928681230545044]
memory len:10000
memory used:3171.0
now epsilon is 0.0319149539548532, the reward is 194.33333333333334 with loss [25.83201837539673, 24.41101312637329] in episode 2890
Report: 
rewardSum:194.33333333333334
loss:[25.83201837539673, 24.41101312637329]
policies:[0, 4, 0]
qAverage:[0.0, 51.817073822021484]
ws:[-2.2568769246339797, 4.865806698799133]
memory len:10000
memory used:3170.0
now epsilon is 0.03188305096701152, the reward is 194.33333333333334 with loss [26.99631357192993, 23.185826778411865] in episode 2891
Report: 
rewardSum:194.33333333333334
loss:[26.99631357192993, 23.185826778411865]
policies:[1, 3, 0]
qAverage:[0.0, 47.79115295410156]
ws:[-3.00518911331892, 4.604162096977234]
memory len:10000
memory used:3170.0
now epsilon is 0.031851179870196054, the reward is 194.33333333333334 with loss [23.543509483337402, 25.32653284072876] in episode 2892
Report: 
rewardSum:194.33333333333334
loss:[23.543509483337402, 25.32653284072876]
policies:[0, 4, 0]
qAverage:[0.0, 51.71374664306641]
ws:[-2.680071327090263, 4.794708561897278]
memory len:10000
memory used:3170.0
now epsilon is 0.03181934063252774, the reward is 194.33333333333334 with loss [27.637300491333008, 29.858034133911133] in episode 2893
Report: 
rewardSum:194.33333333333334
loss:[27.637300491333008, 29.858034133911133]
policies:[0, 4, 0]
qAverage:[0.0, 51.73448638916015]
ws:[-2.713023316115141, 4.830356526374817]
memory len:10000
memory used:3170.0
now epsilon is 0.031787533222159374, the reward is 194.33333333333334 with loss [21.88392424583435, 32.26770257949829] in episode 2894
Report: 
rewardSum:194.33333333333334
loss:[21.88392424583435, 32.26770257949829]
policies:[0, 4, 0]
qAverage:[0.0, 51.745599365234376]
ws:[-2.376055771112442, 5.4550351858139035]
memory len:10000
memory used:3170.0
now epsilon is 0.03175575760727557, the reward is 194.33333333333334 with loss [20.952609539031982, 25.177050590515137] in episode 2895
Report: 
rewardSum:194.33333333333334
loss:[20.952609539031982, 25.177050590515137]
policies:[0, 4, 0]
qAverage:[0.0, 51.87629852294922]
ws:[-2.0184898376464844, 6.406318521499633]
memory len:10000
memory used:3170.0
now epsilon is 0.031716082752653765, the reward is 193.33333333333334 with loss [30.05823802947998, 26.733853340148926] in episode 2896
Report: 
rewardSum:193.33333333333334
loss:[30.05823802947998, 26.733853340148926]
policies:[1, 3, 1]
qAverage:[0.0, 48.18793487548828]
ws:[-2.613978385925293, 7.322150528430939]
memory len:10000
memory used:3170.0
now epsilon is 0.03168437856145002, the reward is 194.33333333333334 with loss [22.882787227630615, 23.607653617858887] in episode 2897
Report: 
rewardSum:194.33333333333334
loss:[22.882787227630615, 23.607653617858887]
policies:[1, 3, 0]
qAverage:[0.0, 53.21985912322998]
ws:[-2.9872697591781616, 3.7907134294509888]
memory len:10000
memory used:3169.0
now epsilon is 0.03165270606255039, the reward is 194.33333333333334 with loss [20.574620008468628, 24.813544750213623] in episode 2898
Report: 
rewardSum:194.33333333333334
loss:[20.574620008468628, 24.813544750213623]
policies:[0, 4, 0]
qAverage:[0.0, 52.658792877197264]
ws:[-1.9083709359169005, 5.915217685699463]
memory len:10000
memory used:3169.0
now epsilon is 0.03162106522427445, the reward is 194.33333333333334 with loss [24.830928802490234, 28.396583557128906] in episode 2899
Report: 
rewardSum:194.33333333333334
loss:[24.830928802490234, 28.396583557128906]
policies:[1, 3, 0]
qAverage:[0.0, 46.757551193237305]
ws:[0.6743456497788429, 6.092873513698578]
memory len:10000
memory used:3169.0
now epsilon is 0.03158945601497344, the reward is 194.33333333333334 with loss [21.333207368850708, 24.475872039794922] in episode 2900
Report: 
rewardSum:194.33333333333334
loss:[21.333207368850708, 24.475872039794922]
policies:[0, 4, 0]
qAverage:[0.0, 53.051387023925784]
ws:[-2.190730462176725, 4.279984664916992]
memory len:10000
memory used:3169.0
now epsilon is 0.03155787840303026, the reward is 194.33333333333334 with loss [20.287400245666504, 24.025540828704834] in episode 2901
Report: 
rewardSum:194.33333333333334
loss:[20.287400245666504, 24.025540828704834]
policies:[0, 4, 0]
qAverage:[0.0, 54.938925170898436]
ws:[-3.0765711218118668, 1.4594087839126586]
memory len:10000
memory used:3168.0
now epsilon is 0.031526332356859386, the reward is 194.33333333333334 with loss [28.355980396270752, 21.6624755859375] in episode 2902
Report: 
rewardSum:194.33333333333334
loss:[28.355980396270752, 21.6624755859375]
policies:[0, 4, 0]
qAverage:[0.0, 53.28419418334961]
ws:[-2.5101579397916796, 3.9909921169281004]
memory len:10000
memory used:3169.0
now epsilon is 0.03149481784490689, the reward is 194.33333333333334 with loss [25.68824291229248, 21.89609956741333] in episode 2903
Report: 
rewardSum:194.33333333333334
loss:[25.68824291229248, 21.89609956741333]
policies:[0, 4, 0]
qAverage:[0.0, 54.59664306640625]
ws:[-2.7990551725029946, 2.267159730195999]
memory len:10000
memory used:3169.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21*		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.03146333483565038, the reward is 194.33333333333334 with loss [22.089772701263428, 26.432194232940674] in episode 2904
Report: 
rewardSum:194.33333333333334
loss:[22.089772701263428, 26.432194232940674]
policies:[0, 4, 0]
qAverage:[0.0, 53.52198715209961]
ws:[-2.346124392747879, 2.863928258419037]
memory len:10000
memory used:3169.0
now epsilon is 0.03143188329759896, the reward is 194.33333333333334 with loss [25.358596801757812, 34.555357933044434] in episode 2905
Report: 
rewardSum:194.33333333333334
loss:[25.358596801757812, 34.555357933044434]
policies:[0, 4, 0]
qAverage:[0.0, 54.45114440917969]
ws:[-1.4954350590705872, 5.634818649291992]
memory len:10000
memory used:3169.0
now epsilon is 0.031400463199293224, the reward is 194.33333333333334 with loss [25.053569316864014, 19.909573793411255] in episode 2906
Report: 
rewardSum:194.33333333333334
loss:[25.053569316864014, 19.909573793411255]
policies:[0, 4, 0]
qAverage:[0.0, 53.85381164550781]
ws:[-1.9512829702347516, 3.4091914892196655]
memory len:10000
memory used:3170.0
now epsilon is 0.03136907450930523, the reward is 194.33333333333334 with loss [19.399302005767822, 26.004283905029297] in episode 2907
Report: 
rewardSum:194.33333333333334
loss:[19.399302005767822, 26.004283905029297]
policies:[0, 4, 0]
qAverage:[0.0, 54.16388931274414]
ws:[-1.4764970421791077, 5.874152421951294]
memory len:10000
memory used:3170.0
now epsilon is 0.03133771719623842, the reward is 194.33333333333334 with loss [25.367865085601807, 25.84295892715454] in episode 2908
Report: 
rewardSum:194.33333333333334
loss:[25.367865085601807, 25.84295892715454]
policies:[0, 4, 0]
qAverage:[0.0, 54.293602752685544]
ws:[-1.859010273963213, 3.754513716697693]
memory len:10000
memory used:3171.0
now epsilon is 0.03130639122872765, the reward is 194.33333333333334 with loss [24.214375019073486, 28.88942289352417] in episode 2909
Report: 
rewardSum:194.33333333333334
loss:[24.214375019073486, 28.88942289352417]
policies:[0, 4, 0]
qAverage:[0.0, 57.161162567138675]
ws:[-1.4025175631046296, 5.955662679672241]
memory len:10000
memory used:3170.0
now epsilon is 0.031267277801295255, the reward is 193.33333333333334 with loss [30.992233753204346, 30.073599815368652] in episode 2910
Report: 
rewardSum:193.33333333333334
loss:[30.992233753204346, 30.073599815368652]
policies:[0, 4, 1]
qAverage:[0.0, 57.43465995788574]
ws:[-1.9487551748752594, 6.823428571224213]
memory len:10000
memory used:3170.0
now epsilon is 0.03123602224676906, the reward is 194.33333333333334 with loss [23.415870666503906, 20.426705360412598] in episode 2911
Report: 
rewardSum:194.33333333333334
loss:[23.415870666503906, 20.426705360412598]
policies:[0, 4, 0]
qAverage:[0.0, 57.75840835571289]
ws:[-0.7035077333450317, 7.63095121383667]
memory len:10000
memory used:3170.0
now epsilon is 0.031204797936078506, the reward is 194.33333333333334 with loss [33.11321783065796, 22.123344659805298] in episode 2912
Report: 
rewardSum:194.33333333333334
loss:[33.11321783065796, 22.123344659805298]
policies:[1, 3, 0]
qAverage:[0.0, 55.495850563049316]
ws:[-1.1842476427555084, 6.1679805517196655]
memory len:10000
memory used:3170.0
now epsilon is 0.031158019983922786, the reward is 192.33333333333334 with loss [36.06423044204712, 34.423104763031006] in episode 2913
Report: 
rewardSum:192.33333333333334
loss:[36.06423044204712, 34.423104763031006]
policies:[0, 5, 1]
qAverage:[0.0, 57.48369598388672]
ws:[-0.4786385572127377, 8.386711200078329]
memory len:10000
memory used:3169.0
now epsilon is 0.031126873646249106, the reward is 194.33333333333334 with loss [23.79619550704956, 25.408308029174805] in episode 2914
Report: 
rewardSum:194.33333333333334
loss:[23.79619550704956, 25.408308029174805]
policies:[0, 4, 0]
qAverage:[0.0, 57.40845565795898]
ws:[-0.9487870693206787, 8.109833908081054]
memory len:10000
memory used:3169.0
now epsilon is 0.03109575844323517, the reward is 194.33333333333334 with loss [22.40774941444397, 21.539244651794434] in episode 2915
Report: 
rewardSum:194.33333333333334
loss:[22.40774941444397, 21.539244651794434]
policies:[0, 4, 0]
qAverage:[0.0, 57.54064865112305]
ws:[-2.0097337037324907, 4.857832336425782]
memory len:10000
memory used:3169.0
now epsilon is 0.031064674343757988, the reward is 194.33333333333334 with loss [28.394177198410034, 28.041518211364746] in episode 2916
Report: 
rewardSum:194.33333333333334
loss:[28.394177198410034, 28.041518211364746]
policies:[0, 4, 0]
qAverage:[0.0, 57.99781951904297]
ws:[-2.2923529863357546, 4.5755304336547855]
memory len:10000
memory used:3169.0
now epsilon is 0.031033621316725688, the reward is 194.33333333333334 with loss [27.645676136016846, 22.327836513519287] in episode 2917
Report: 
rewardSum:194.33333333333334
loss:[27.645676136016846, 22.327836513519287]
policies:[0, 4, 0]
qAverage:[0.0, 57.575723266601564]
ws:[-2.5098932797089217, 4.5477372169494625]
memory len:10000
memory used:3169.0
now epsilon is 0.03100259933107748, the reward is 194.33333333333334 with loss [25.6688494682312, 24.284998893737793] in episode 2918
Report: 
rewardSum:194.33333333333334
loss:[25.6688494682312, 24.284998893737793]
policies:[0, 4, 0]
qAverage:[0.0, 58.1703483581543]
ws:[-2.077363081648946, 7.226741075515747]
memory len:10000
memory used:3169.0
now epsilon is 0.030971608355783612, the reward is 194.33333333333334 with loss [24.3572735786438, 17.229926705360413] in episode 2919
Report: 
rewardSum:194.33333333333334
loss:[24.3572735786438, 17.229926705360413]
policies:[0, 3, 1]
qAverage:[0.0, 59.24129009246826]
ws:[-3.137853667140007, 5.411293864250183]
memory len:10000
memory used:3170.0
now epsilon is 0.03094064835984536, the reward is 194.33333333333334 with loss [22.631145238876343, 23.454537868499756] in episode 2920
Report: 
rewardSum:194.33333333333334
loss:[22.631145238876343, 23.454537868499756]
policies:[0, 4, 0]
qAverage:[0.0, 57.99585113525391]
ws:[-2.4944061771035195, 5.393309736251831]
memory len:10000
memory used:3169.0
now epsilon is 0.03090971931229498, the reward is 194.33333333333334 with loss [27.944578170776367, 26.366884231567383] in episode 2921
Report: 
rewardSum:194.33333333333334
loss:[27.944578170776367, 26.366884231567383]
policies:[0, 4, 0]
qAverage:[0.0, 61.144143676757814]
ws:[-2.3163353562355042, 6.051571178436279]
memory len:10000
memory used:3169.0
now epsilon is 0.030878821182195697, the reward is 194.33333333333334 with loss [16.200022339820862, 35.353121280670166] in episode 2922
Report: 
rewardSum:194.33333333333334
loss:[16.200022339820862, 35.353121280670166]
policies:[0, 4, 0]
qAverage:[0.0, 63.060142517089844]
ws:[-1.6398070387542247, 9.002234983444215]
memory len:10000
memory used:3170.0
now epsilon is 0.030847953938641644, the reward is 194.33333333333334 with loss [23.55008029937744, 24.75010108947754] in episode 2923
Report: 
rewardSum:194.33333333333334
loss:[23.55008029937744, 24.75010108947754]
policies:[0, 3, 1]
qAverage:[0.0, 61.8065185546875]
ws:[-1.5978666990995407, 10.7158784866333]
memory len:10000
memory used:3170.0
now epsilon is 0.030817117550757854, the reward is 194.33333333333334 with loss [19.347445964813232, 25.321825981140137] in episode 2924
Report: 
rewardSum:194.33333333333334
loss:[19.347445964813232, 25.321825981140137]
policies:[0, 4, 0]
qAverage:[0.0, 62.07610244750977]
ws:[-0.8511546850204468, 9.47629280090332]
memory len:10000
memory used:3170.0
now epsilon is 0.030786311987700234, the reward is 194.33333333333334 with loss [21.24081301689148, 32.25098371505737] in episode 2925
Report: 
rewardSum:194.33333333333334
loss:[21.24081301689148, 32.25098371505737]
policies:[0, 4, 0]
qAverage:[0.0, 61.34176483154297]
ws:[-0.9971789121627808, 7.152090835571289]
memory len:10000
memory used:3170.0
now epsilon is 0.03075553721865551, the reward is 194.33333333333334 with loss [24.86505699157715, 21.545557737350464] in episode 2926
Report: 
rewardSum:194.33333333333334
loss:[24.86505699157715, 21.545557737350464]
policies:[0, 4, 0]
qAverage:[0.0, 61.634056091308594]
ws:[-0.4987975716590881, 9.90493040084839]
memory len:10000
memory used:3169.0
now epsilon is 0.030724793212841214, the reward is 194.33333333333334 with loss [17.53606653213501, 26.022000789642334] in episode 2927
Report: 
rewardSum:194.33333333333334
loss:[17.53606653213501, 26.022000789642334]
policies:[0, 4, 0]
qAverage:[0.0, 61.5044677734375]
ws:[-0.764528501033783, 7.190717506408691]
memory len:10000
memory used:3169.0
now epsilon is 0.030686406419520775, the reward is 193.33333333333334 with loss [39.277979373931885, 26.269962072372437] in episode 2928
Report: 
rewardSum:193.33333333333334
loss:[39.277979373931885, 26.269962072372437]
policies:[0, 4, 1]
qAverage:[0.0, 61.06791687011719]
ws:[-1.0013635829091072, 8.896629476547242]
memory len:10000
memory used:3170.0
now epsilon is 0.030655731518585885, the reward is 194.33333333333334 with loss [25.194673538208008, 26.065271854400635] in episode 2929
Report: 
rewardSum:194.33333333333334
loss:[25.194673538208008, 26.065271854400635]
policies:[0, 4, 0]
qAverage:[0.0, 52.723334312438965]
ws:[0.9099065214395523, 7.168945074081421]
memory len:10000
memory used:3170.0
now epsilon is 0.030625087281050754, the reward is 194.33333333333334 with loss [30.013657569885254, 26.7606782913208] in episode 2930
Report: 
rewardSum:194.33333333333334
loss:[30.013657569885254, 26.7606782913208]
policies:[0, 4, 0]
qAverage:[0.0, 61.05013656616211]
ws:[-0.5234208226203918, 8.224250650405883]
memory len:10000
memory used:3170.0
now epsilon is 0.030594473676263487, the reward is 194.33333333333334 with loss [23.673683166503906, 24.67471933364868] in episode 2931
Report: 
rewardSum:194.33333333333334
loss:[23.673683166503906, 24.67471933364868]
policies:[0, 4, 0]
qAverage:[0.0, 61.29438705444336]
ws:[-0.6950089573860169, 7.570395398139953]
memory len:10000
memory used:3170.0
############# STATE ###############
0-		9-		18-		27-		36-		
1*		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.030563890673602823, the reward is 194.33333333333334 with loss [32.85641145706177, 29.048906803131104] in episode 2932
Report: 
rewardSum:194.33333333333334
loss:[32.85641145706177, 29.048906803131104]
policies:[0, 4, 0]
qAverage:[0.0, 61.7352409362793]
ws:[-0.7744555771350861, 7.652830874919891]
memory len:10000
memory used:3170.0
now epsilon is 0.030533338242478104, the reward is 194.33333333333334 with loss [28.9418625831604, 27.05384874343872] in episode 2933
Report: 
rewardSum:194.33333333333334
loss:[28.9418625831604, 27.05384874343872]
policies:[0, 4, 0]
qAverage:[0.0, 64.54592666625976]
ws:[-0.8292569935321807, 7.968246579170227]
memory len:10000
memory used:3170.0
now epsilon is 0.030502816352329255, the reward is 194.33333333333334 with loss [24.547225952148438, 28.897271156311035] in episode 2934
Report: 
rewardSum:194.33333333333334
loss:[24.547225952148438, 28.897271156311035]
policies:[0, 4, 0]
qAverage:[0.0, 66.47073440551758]
ws:[-0.7818830698728562, 7.708363461494446]
memory len:10000
memory used:3170.0
now epsilon is 0.030472324972626755, the reward is 194.33333333333334 with loss [16.726306915283203, 24.292968273162842] in episode 2935
Report: 
rewardSum:194.33333333333334
loss:[16.726306915283203, 24.292968273162842]
policies:[0, 4, 0]
qAverage:[0.0, 65.1380859375]
ws:[-1.0167267061769962, 4.928815746307373]
memory len:10000
memory used:3170.0
now epsilon is 0.03042664504345166, the reward is 192.33333333333334 with loss [37.19810390472412, 32.09358334541321] in episode 2936
Report: 
rewardSum:192.33333333333334
loss:[37.19810390472412, 32.09358334541321]
policies:[0, 5, 1]
qAverage:[0.0, 66.7826608022054]
ws:[-0.22664992014567056, 8.18781727552414]
memory len:10000
memory used:3170.0
now epsilon is 0.030396229806498556, the reward is 194.33333333333334 with loss [18.38644552230835, 26.329824924468994] in episode 2937
Report: 
rewardSum:194.33333333333334
loss:[18.38644552230835, 26.329824924468994]
policies:[1, 3, 0]
qAverage:[0.0, 60.43332767486572]
ws:[-0.8639232888817787, 4.438891619443893]
memory len:10000
memory used:3170.0
now epsilon is 0.030365844973378592, the reward is 194.33333333333334 with loss [28.21843433380127, 26.645305156707764] in episode 2938
Report: 
rewardSum:194.33333333333334
loss:[28.21843433380127, 26.645305156707764]
policies:[0, 4, 0]
qAverage:[0.0, 65.30496444702149]
ws:[-0.009046578407287597, 7.794401168823242]
memory len:10000
memory used:3170.0
now epsilon is 0.030335490513699336, the reward is 194.33333333333334 with loss [31.725988388061523, 35.10329532623291] in episode 2939
Report: 
rewardSum:194.33333333333334
loss:[31.725988388061523, 35.10329532623291]
policies:[1, 3, 0]
qAverage:[0.0, 66.76487731933594]
ws:[-0.24066677689552307, 7.496386289596558]
memory len:10000
memory used:3169.0
now epsilon is 0.030305166397098734, the reward is 194.33333333333334 with loss [21.161218643188477, 18.11805009841919] in episode 2940
Report: 
rewardSum:194.33333333333334
loss:[21.161218643188477, 18.11805009841919]
policies:[0, 4, 0]
qAverage:[0.0, 65.20108261108399]
ws:[0.0989410400390625, 6.90279631614685]
memory len:10000
memory used:3170.0
now epsilon is 0.030274872593245083, the reward is 194.33333333333334 with loss [23.393898010253906, 29.739724159240723] in episode 2941
Report: 
rewardSum:194.33333333333334
loss:[23.393898010253906, 29.739724159240723]
policies:[0, 4, 0]
qAverage:[0.0, 65.35170288085938]
ws:[0.06596128940582276, 6.62032904624939]
memory len:10000
memory used:3170.0
now epsilon is 0.030244609071837004, the reward is 194.33333333333334 with loss [21.35456681251526, 26.925451040267944] in episode 2942
Report: 
rewardSum:194.33333333333334
loss:[21.35456681251526, 26.925451040267944]
policies:[0, 4, 0]
qAverage:[0.0, 65.67321395874023]
ws:[0.02906264066696167, 7.17166166305542]
memory len:10000
memory used:3170.0
now epsilon is 0.030214375802603406, the reward is 194.33333333333334 with loss [23.729485511779785, 27.337881565093994] in episode 2943
Report: 
rewardSum:194.33333333333334
loss:[23.729485511779785, 27.337881565093994]
policies:[0, 4, 0]
qAverage:[0.0, 65.29777908325195]
ws:[0.20131142139434816, 7.7099522113800045]
memory len:10000
memory used:3170.0
now epsilon is 0.03018417275530345, the reward is 194.33333333333334 with loss [21.39516544342041, 30.308229446411133] in episode 2944
Report: 
rewardSum:194.33333333333334
loss:[21.39516544342041, 30.308229446411133]
policies:[0, 4, 0]
qAverage:[0.0, 65.31819076538086]
ws:[0.3001047372817993, 9.535753774642945]
memory len:10000
memory used:3170.0
now epsilon is 0.030153999899726544, the reward is 194.33333333333334 with loss [24.988767623901367, 24.19903326034546] in episode 2945
Report: 
rewardSum:194.33333333333334
loss:[24.988767623901367, 24.19903326034546]
policies:[0, 4, 0]
qAverage:[0.0, 68.28191604614258]
ws:[-0.38811105489730835, 6.830649042129517]
memory len:10000
memory used:3170.0
now epsilon is 0.030123857205692276, the reward is 194.33333333333334 with loss [22.287979125976562, 27.80217742919922] in episode 2946
Report: 
rewardSum:194.33333333333334
loss:[22.287979125976562, 27.80217742919922]
policies:[1, 3, 0]
qAverage:[0.0, 63.65816116333008]
ws:[-0.6431796327233315, 6.449227750301361]
memory len:10000
memory used:3170.0
now epsilon is 0.030093744643050417, the reward is 194.33333333333334 with loss [25.227569103240967, 21.887168884277344] in episode 2947
Report: 
rewardSum:194.33333333333334
loss:[25.227569103240967, 21.887168884277344]
policies:[1, 3, 0]
qAverage:[0.0, 62.49660110473633]
ws:[-0.4705713838338852, 7.062665820121765]
memory len:10000
memory used:3169.0
now epsilon is 0.03006366218168087, the reward is 194.33333333333334 with loss [21.62792944908142, 30.726844310760498] in episode 2948
Report: 
rewardSum:194.33333333333334
loss:[21.62792944908142, 30.726844310760498]
policies:[0, 4, 0]
qAverage:[0.0, 69.18320770263672]
ws:[0.0343161940574646, 9.160257339477539]
memory len:10000
memory used:3178.0
now epsilon is 0.030033609791493648, the reward is 194.33333333333334 with loss [22.26418972015381, 25.833791255950928] in episode 2949
Report: 
rewardSum:194.33333333333334
loss:[22.26418972015381, 25.833791255950928]
policies:[0, 4, 0]
qAverage:[0.0, 68.52118530273438]
ws:[0.520557451248169, 11.473181343078613]
memory len:10000
memory used:3178.0
now epsilon is 0.030003587442428845, the reward is 194.33333333333334 with loss [23.77683401107788, 21.371076345443726] in episode 2950
Report: 
rewardSum:194.33333333333334
loss:[23.77683401107788, 21.371076345443726]
policies:[0, 4, 0]
qAverage:[0.0, 68.5938934326172]
ws:[0.7088372468948364, 12.10604190826416]
memory len:10000
memory used:3178.0
now epsilon is 0.029973595104456602, the reward is 194.33333333333334 with loss [27.413005352020264, 19.877971172332764] in episode 2951
Report: 
rewardSum:194.33333333333334
loss:[27.413005352020264, 19.877971172332764]
policies:[0, 4, 0]
qAverage:[0.0, 68.69546051025391]
ws:[1.2855756759643555, 14.269768333435058]
memory len:10000
memory used:3178.0
now epsilon is 0.02994363274757708, the reward is 194.33333333333334 with loss [11.414596796035767, 25.878657817840576] in episode 2952
Report: 
rewardSum:194.33333333333334
loss:[11.414596796035767, 25.878657817840576]
policies:[0, 4, 0]
qAverage:[0.0, 68.68477935791016]
ws:[0.7381044864654541, 10.910503768920899]
memory len:10000
memory used:3180.0
now epsilon is 0.02991370034182043, the reward is 194.33333333333334 with loss [30.178444862365723, 24.671728134155273] in episode 2953
Report: 
rewardSum:194.33333333333334
loss:[30.178444862365723, 24.671728134155273]
policies:[0, 4, 0]
qAverage:[0.0, 68.36324920654297]
ws:[1.4541927039623261, 14.59143362045288]
memory len:10000
memory used:3180.0
now epsilon is 0.02988379785724675, the reward is 194.33333333333334 with loss [22.08122444152832, 26.139766216278076] in episode 2954
Report: 
rewardSum:194.33333333333334
loss:[22.08122444152832, 26.139766216278076]
policies:[0, 4, 0]
qAverage:[0.0, 69.87882423400879]
ws:[1.056398093700409, 16.656346321105957]
memory len:10000
memory used:3180.0
now epsilon is 0.029853925263946084, the reward is 194.33333333333334 with loss [23.59048318862915, 24.917128086090088] in episode 2955
Report: 
rewardSum:194.33333333333334
loss:[23.59048318862915, 24.917128086090088]
policies:[0, 4, 0]
qAverage:[0.0, 68.46089172363281]
ws:[0.1530214548110962, 13.792345523834229]
memory len:10000
memory used:3180.0
now epsilon is 0.029824082532038363, the reward is 194.33333333333334 with loss [23.8809552192688, 26.132570266723633] in episode 2956
Report: 
rewardSum:194.33333333333334
loss:[23.8809552192688, 26.132570266723633]
policies:[0, 4, 0]
qAverage:[0.0, 69.58204803466796]
ws:[-0.38675724864006045, 13.912973880767822]
memory len:10000
memory used:3180.0
now epsilon is 0.02979426963167339, the reward is 194.33333333333334 with loss [27.395753860473633, 22.79639720916748] in episode 2957
Report: 
rewardSum:194.33333333333334
loss:[27.395753860473633, 22.79639720916748]
policies:[0, 4, 0]
qAverage:[0.0, 69.19391632080078]
ws:[-0.6056405395269394, 11.301503086090088]
memory len:10000
memory used:3181.0
now epsilon is 0.029764486533030807, the reward is 194.33333333333334 with loss [31.324864387512207, 24.054678440093994] in episode 2958
Report: 
rewardSum:194.33333333333334
loss:[31.324864387512207, 24.054678440093994]
policies:[0, 4, 0]
qAverage:[0.0, 72.03718566894531]
ws:[-0.31704524904489517, 11.052717566490173]
memory len:10000
memory used:3181.0
now epsilon is 0.029734733206320068, the reward is 194.33333333333334 with loss [22.576375484466553, 24.91690683364868] in episode 2959
Report: 
rewardSum:194.33333333333334
loss:[22.576375484466553, 24.91690683364868]
policies:[0, 4, 0]
qAverage:[0.0, 70.73421936035156]
ws:[0.4564288824796677, 12.412596464157104]
memory len:10000
memory used:3181.0
############# STATE ###############
0-		9-		18-		27-		36-		
1*		10-		19-		28-		37-		
2-		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.0297050096217804, the reward is 194.33333333333334 with loss [28.23000144958496, 24.529424905776978] in episode 2960
Report: 
rewardSum:194.33333333333334
loss:[28.23000144958496, 24.529424905776978]
policies:[0, 4, 0]
qAverage:[0.0, 72.70100402832031]
ws:[0.5070018783211708, 12.358505201339721]
memory len:10000
memory used:3181.0
now epsilon is 0.02967531574968078, the reward is 194.33333333333334 with loss [23.527544498443604, 22.836078882217407] in episode 2961
Report: 
rewardSum:194.33333333333334
loss:[23.527544498443604, 22.836078882217407]
policies:[0, 4, 0]
qAverage:[0.0, 71.49334259033203]
ws:[-0.10931218415498734, 8.660458707809449]
memory len:10000
memory used:3200.0
now epsilon is 0.029645651560319914, the reward is 194.33333333333334 with loss [29.5310001373291, 20.225048303604126] in episode 2962
Report: 
rewardSum:194.33333333333334
loss:[29.5310001373291, 20.225048303604126]
policies:[0, 4, 0]
qAverage:[0.0, 72.47244415283203]
ws:[0.33698301315307616, 10.078993535041809]
memory len:10000
memory used:3202.0
now epsilon is 0.029616017024026196, the reward is 194.33333333333334 with loss [21.007014751434326, 24.762016773223877] in episode 2963
Report: 
rewardSum:194.33333333333334
loss:[21.007014751434326, 24.762016773223877]
policies:[0, 4, 0]
qAverage:[0.0, 71.58785400390624]
ws:[0.0069007426500320435, 7.265323451161384]
memory len:10000
memory used:3199.0
now epsilon is 0.029571620754252858, the reward is 192.33333333333334 with loss [42.09216022491455, 39.86965894699097] in episode 2964
Report: 
rewardSum:192.33333333333334
loss:[42.09216022491455, 39.86965894699097]
policies:[1, 4, 1]
qAverage:[10.828364054361979, 59.76567713419596]
ws:[1.0752091258764267, 8.608141845713059]
memory len:10000
memory used:3199.0
now epsilon is 0.029542060221008283, the reward is 194.33333333333334 with loss [26.58268165588379, 28.324317455291748] in episode 2965
Report: 
rewardSum:194.33333333333334
loss:[26.58268165588379, 28.324317455291748]
policies:[0, 4, 0]
qAverage:[0.0, 72.96629791259765]
ws:[1.764845895767212, 8.173699378967285]
memory len:10000
memory used:3200.0
now epsilon is 0.029512529237213595, the reward is 194.33333333333334 with loss [24.83542490005493, 21.787187099456787] in episode 2966
Report: 
rewardSum:194.33333333333334
loss:[24.83542490005493, 21.787187099456787]
policies:[1, 3, 0]
qAverage:[12.676526641845703, 57.76216583251953]
ws:[1.8618780732154847, 5.760515257716179]
memory len:10000
memory used:3200.0
now epsilon is 0.02948302777333043, the reward is 194.33333333333334 with loss [22.948671340942383, 26.012720823287964] in episode 2967
Report: 
rewardSum:194.33333333333334
loss:[22.948671340942383, 26.012720823287964]
policies:[1, 3, 0]
qAverage:[12.906246948242188, 57.692683410644534]
ws:[2.364194446802139, 7.549799680709839]
memory len:10000
memory used:3202.0
now epsilon is 0.029453555799849943, the reward is 194.33333333333334 with loss [24.729822635650635, 24.51173973083496] in episode 2968
Report: 
rewardSum:194.33333333333334
loss:[24.729822635650635, 24.51173973083496]
policies:[1, 3, 0]
qAverage:[12.723512268066406, 58.04662017822265]
ws:[1.583662736415863, 6.347198694944382]
memory len:10000
memory used:3202.0
now epsilon is 0.02942411328729279, the reward is 194.33333333333334 with loss [26.555107593536377, 32.380563735961914] in episode 2969
Report: 
rewardSum:194.33333333333334
loss:[26.555107593536377, 32.380563735961914]
policies:[0, 4, 0]
qAverage:[0.0, 72.54156494140625]
ws:[2.4564754962921143, 9.480296111106872]
memory len:10000
memory used:3082.0
now epsilon is 0.029409403069656222, the reward is -1.0 with loss [15.847692966461182, 12.370916843414307] in episode 2970
Report: 
rewardSum:-1.0
loss:[15.847692966461182, 12.370916843414307]
policies:[0, 1, 1]
qAverage:[0.0, 36.24709701538086]
ws:[0.5290278196334839, 1.4068782329559326]
memory len:10000
memory used:2924.0
now epsilon is 0.029380004693274748, the reward is 194.33333333333334 with loss [30.393877029418945, 26.291884899139404] in episode 2971
Report: 
rewardSum:194.33333333333334
loss:[30.393877029418945, 26.291884899139404]
policies:[0, 4, 0]
qAverage:[0.0, 77.36087799072266]
ws:[1.8336870491504669, 10.006276607513428]
memory len:10000
memory used:2924.0
now epsilon is 0.029350635704247105, the reward is 194.33333333333334 with loss [27.72176218032837, 23.715473651885986] in episode 2972
Report: 
rewardSum:194.33333333333334
loss:[27.72176218032837, 23.715473651885986]
policies:[0, 4, 0]
qAverage:[0.0, 74.5798110961914]
ws:[1.672809910774231, 9.113851165771484]
memory len:10000
memory used:2924.0
now epsilon is 0.02932129607319695, the reward is 194.33333333333334 with loss [20.15523934364319, 29.054214000701904] in episode 2973
Report: 
rewardSum:194.33333333333334
loss:[20.15523934364319, 29.054214000701904]
policies:[0, 4, 0]
qAverage:[0.0, 76.30159301757813]
ws:[1.6647243261337281, 9.492656421661376]
memory len:10000
memory used:2924.0
now epsilon is 0.029291985770777317, the reward is 194.33333333333334 with loss [28.230461597442627, 27.85727310180664] in episode 2974
Report: 
rewardSum:194.33333333333334
loss:[28.230461597442627, 27.85727310180664]
policies:[0, 4, 0]
qAverage:[0.0, 74.91416625976562]
ws:[2.266120195388794, 12.201806831359864]
memory len:10000
memory used:2924.0
now epsilon is 0.029262704767670573, the reward is 194.33333333333334 with loss [27.79990029335022, 23.19832420349121] in episode 2975
Report: 
rewardSum:194.33333333333334
loss:[27.79990029335022, 23.19832420349121]
policies:[0, 4, 0]
qAverage:[0.0, 69.05852699279785]
ws:[1.4939184486865997, 9.783055424690247]
memory len:10000
memory used:2924.0
now epsilon is 0.02923345303458839, the reward is 194.33333333333334 with loss [27.200764179229736, 29.081270217895508] in episode 2976
Report: 
rewardSum:194.33333333333334
loss:[27.200764179229736, 29.081270217895508]
policies:[0, 4, 0]
qAverage:[0.0, 75.22929992675782]
ws:[2.1717151641845702, 12.646478652954102]
memory len:10000
memory used:2924.0
now epsilon is 0.02920423054227172, the reward is 194.33333333333334 with loss [28.03292226791382, 26.408275604248047] in episode 2977
Report: 
rewardSum:194.33333333333334
loss:[28.03292226791382, 26.408275604248047]
policies:[0, 4, 0]
qAverage:[0.0, 75.80211791992187]
ws:[1.7306716680526733, 10.716165351867676]
memory len:10000
memory used:2924.0
now epsilon is 0.02917503726149075, the reward is 194.33333333333334 with loss [28.38759708404541, 25.495588302612305] in episode 2978
Report: 
rewardSum:194.33333333333334
loss:[28.38759708404541, 25.495588302612305]
policies:[0, 4, 0]
qAverage:[0.0, 73.70658302307129]
ws:[1.3203071355819702, 10.134825706481934]
memory len:10000
memory used:2924.0
now epsilon is 0.029145873163044907, the reward is 194.33333333333334 with loss [27.966369152069092, 29.642548084259033] in episode 2979
Report: 
rewardSum:194.33333333333334
loss:[27.966369152069092, 29.642548084259033]
policies:[0, 4, 0]
qAverage:[0.0, 74.8927017211914]
ws:[0.6648876130580902, 9.318185329437256]
memory len:10000
memory used:2924.0
now epsilon is 0.0291167382177628, the reward is 194.33333333333334 with loss [20.436514377593994, 27.927138805389404] in episode 2980
Report: 
rewardSum:194.33333333333334
loss:[20.436514377593994, 27.927138805389404]
policies:[0, 4, 0]
qAverage:[0.0, 75.94766540527344]
ws:[0.7654393956065177, 9.865909099578857]
memory len:10000
memory used:2924.0
now epsilon is 0.02908763239650219, the reward is 194.33333333333334 with loss [25.813465118408203, 18.45162582397461] in episode 2981
Report: 
rewardSum:194.33333333333334
loss:[25.813465118408203, 18.45162582397461]
policies:[0, 4, 0]
qAverage:[0.0, 71.66877937316895]
ws:[1.3757711127400398, 10.668674349784851]
memory len:10000
memory used:2924.0
now epsilon is 0.029058555670149977, the reward is 194.33333333333334 with loss [30.948052406311035, 24.10666584968567] in episode 2982
Report: 
rewardSum:194.33333333333334
loss:[30.948052406311035, 24.10666584968567]
policies:[0, 4, 0]
qAverage:[0.0, 77.55073852539063]
ws:[1.8463008522987365, 10.938892555236816]
memory len:10000
memory used:2924.0
now epsilon is 0.02902950800962216, the reward is 194.33333333333334 with loss [33.40258502960205, 30.670701026916504] in episode 2983
Report: 
rewardSum:194.33333333333334
loss:[33.40258502960205, 30.670701026916504]
policies:[0, 4, 0]
qAverage:[0.0, 81.52572326660156]
ws:[1.9115892887115478, 10.33492670059204]
memory len:10000
memory used:2923.0
now epsilon is 0.02900048938586381, the reward is 194.33333333333334 with loss [19.170501708984375, 28.47549819946289] in episode 2984
Report: 
rewardSum:194.33333333333334
loss:[19.170501708984375, 28.47549819946289]
policies:[0, 4, 0]
qAverage:[0.0, 78.8887725830078]
ws:[2.116909551620483, 10.742985057830811]
memory len:10000
memory used:2924.0
now epsilon is 0.02897149976984905, the reward is 194.33333333333334 with loss [27.47757577896118, 17.60590434074402] in episode 2985
Report: 
rewardSum:194.33333333333334
loss:[27.47757577896118, 17.60590434074402]
policies:[0, 4, 0]
qAverage:[0.0, 80.23641052246094]
ws:[2.746003007888794, 13.921531391143798]
memory len:10000
memory used:2923.0
now epsilon is 0.028942539132581015, the reward is 194.33333333333334 with loss [27.778292655944824, 28.455270051956177] in episode 2986
Report: 
rewardSum:194.33333333333334
loss:[27.778292655944824, 28.455270051956177]
policies:[1, 3, 0]
qAverage:[0.0, 79.55938911437988]
ws:[3.2110357880592346, 16.101874589920044]
memory len:10000
memory used:2923.0
now epsilon is 0.02891360744509182, the reward is 194.33333333333334 with loss [30.33486557006836, 29.404139518737793] in episode 2987
Report: 
rewardSum:194.33333333333334
loss:[30.33486557006836, 29.404139518737793]
policies:[0, 4, 0]
qAverage:[0.0, 79.57071228027344]
ws:[2.728897285461426, 14.033277034759521]
memory len:10000
memory used:2923.0
now epsilon is 0.02889915244846974, the reward is -1.0 with loss [15.427744388580322, 14.592670440673828] in episode 2988
Report: 
rewardSum:-1.0
loss:[15.427744388580322, 14.592670440673828]
policies:[0, 1, 1]
qAverage:[0.0, 39.25902557373047]
ws:[0.45694929361343384, 3.0431625843048096]
memory len:10000
memory used:2923.0
############# STATE ###############
0-		9-		18-		27-		36-		
1-		10-		19-		28-		37-		
2*		11-		20-		29-		38-		
3x		12-		21-		30-		39-		
4x		13x		22-		31-		40-		
5x		14x		23x		32-		41-		
6x		15x		24x		33x		42-		
7x		16x		25x		34x		43-		
8x		17x		26x		35x		44x		
-----------------------------------
1		50		198		445		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[50.33333333333334, -5]
[198.33333333333334, -7]
[445.0, -9]
[1000, -12]
now epsilon is 0.028870264131397358, the reward is 194.33333333333334 with loss [27.219335556030273, 24.452887058258057] in episode 2989
Report: 
rewardSum:194.33333333333334
loss:[27.219335556030273, 24.452887058258057]
policies:[0, 4, 0]
qAverage:[0.0, 79.86107330322265]
ws:[2.785279929637909, 14.016397285461426]
memory len:10000
memory used:2923.0
now epsilon is 0.02884140469181073, the reward is 194.33333333333334 with loss [20.640233039855957, 20.886348724365234] in episode 2990
Report: 
rewardSum:194.33333333333334
loss:[20.640233039855957, 20.886348724365234]
policies:[0, 4, 0]
qAverage:[0.0, 80.2718521118164]
ws:[3.7804611682891847, 14.618920135498048]
memory len:10000
memory used:2923.0
now epsilon is 0.028812574100843206, the reward is 194.33333333333334 with loss [27.90442132949829, 20.091205596923828] in episode 2991
Report: 
rewardSum:194.33333333333334
loss:[27.90442132949829, 20.091205596923828]
policies:[0, 4, 0]
qAverage:[0.0, 79.06873168945313]
ws:[4.181040024757385, 14.768931770324707]
memory len:10000
memory used:2923.0
now epsilon is 0.02878377232965698, the reward is 194.33333333333334 with loss [20.189908027648926, 21.06345224380493] in episode 2992
Report: 
rewardSum:194.33333333333334
loss:[20.189908027648926, 21.06345224380493]
policies:[0, 4, 0]
qAverage:[0.0, 80.41667938232422]
ws:[4.328278827667236, 15.305642223358154]
memory len:10000
memory used:2924.0
now epsilon is 0.02875499934944308, the reward is 194.33333333333334 with loss [26.574246406555176, 27.366342067718506] in episode 2993
Report: 
rewardSum:194.33333333333334
loss:[26.574246406555176, 27.366342067718506]
policies:[0, 4, 0]
qAverage:[0.0, 79.07990264892578]
ws:[4.860128402709961, 17.38334846496582]
memory len:10000
memory used:2924.0
now epsilon is 0.02872625513142132, the reward is 194.33333333333334 with loss [26.748610734939575, 23.502574920654297] in episode 2994
Report: 
rewardSum:194.33333333333334
loss:[26.748610734939575, 23.502574920654297]
policies:[0, 4, 0]
qAverage:[0.0, 79.85324401855469]
ws:[5.011054468154907, 17.937748908996582]
memory len:10000
memory used:2924.0
now epsilon is 0.028697539646840298, the reward is 194.33333333333334 with loss [24.467575788497925, 28.844865322113037] in episode 2995
Report: 
rewardSum:194.33333333333334
loss:[24.467575788497925, 28.844865322113037]
policies:[0, 4, 0]
qAverage:[0.0, 81.48841705322266]
ws:[4.1811541557312015, 17.31074504852295]
memory len:10000
memory used:2924.0
now epsilon is 0.028668852866977346, the reward is 194.33333333333334 with loss [19.874183177947998, 23.703240394592285] in episode 2996
Report: 
rewardSum:194.33333333333334
loss:[19.874183177947998, 23.703240394592285]
policies:[0, 4, 0]
qAverage:[0.0, 84.03635215759277]
ws:[4.221906840801239, 19.038761615753174]
memory len:10000
memory used:2924.0
now epsilon is 0.028640194763138505, the reward is 194.33333333333334 with loss [18.601516246795654, 27.04378890991211] in episode 2997
Report: 
rewardSum:194.33333333333334
loss:[18.601516246795654, 27.04378890991211]
policies:[0, 3, 1]
qAverage:[0.0, 79.35192680358887]
ws:[2.315235332585871, 13.191005945205688]
memory len:10000
memory used:2924.0
now epsilon is 0.02861156530665851, the reward is 194.33333333333334 with loss [26.8936824798584, 30.112149715423584] in episode 2998
Report: 
rewardSum:194.33333333333334
loss:[26.8936824798584, 30.112149715423584]
policies:[0, 4, 0]
qAverage:[0.0, 83.75186462402344]
ws:[3.8843941673636437, 18.90217342376709]
memory len:10000
memory used:2924.0
now epsilon is 0.028582964468900733, the reward is 194.33333333333334 with loss [17.811601638793945, 29.063965797424316] in episode 2999
Report: 
rewardSum:194.33333333333334
loss:[17.811601638793945, 29.063965797424316]
policies:[0, 4, 0]
qAverage:[0.0, 83.09749450683594]
ws:[2.5578856825828553, 15.15580244064331]
memory len:10000
memory used:2924.0
now epsilon is 0.028554392221257188, the reward is 194.33333333333334 with loss [25.041950225830078, 22.142649173736572] in episode 3000
Report: 
rewardSum:194.33333333333334
loss:[25.041950225830078, 22.142649173736572]
policies:[0, 4, 0]
qAverage:[0.0, 82.25698852539062]
ws:[1.997752273082733, 14.435494804382325]
memory len:10000
memory used:2924.0
Traceback (most recent call last):
  File "agentRunner.py", line 57, in <module>
    agent.train()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 234, in train
    self.env.close()
AttributeError: 'DeepSeaTreasure' object has no attribute 'close'
pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)
Hello from the pygame community. https://www.pygame.org/contribute.html
2021-03-25 14:58:53.521765: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
############# STATE ###############
0*		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
2021-03-25 14:58:55.311919: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 14:58:55.312751: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-25 14:58:56.273751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0001:00:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 149.31GiB/s
2021-03-25 14:58:56.273830: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 14:58:56.279793: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 14:58:56.279868: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 14:58:56.281016: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-25 14:58:56.281811: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-25 14:58:56.282855: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-25 14:58:56.283726: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-25 14:58:56.283898: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 14:58:56.285118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-25 14:58:56.285774: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 14:58:56.286402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0001:00:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 149.31GiB/s
2021-03-25 14:58:56.286443: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 14:58:56.286476: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 14:58:56.286508: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 14:58:56.286539: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-25 14:58:56.286569: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-25 14:58:56.286600: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-25 14:58:56.286630: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-25 14:58:56.286661: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 14:58:56.287788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-25 14:58:56.287849: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 14:58:56.866624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-25 14:58:56.866699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-25 14:58:56.866712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-25 14:58:56.868637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7468 MB memory) -> physical GPU (device: 0, name: Tesla M60, pci bus id: 0001:00:00.0, compute capability: 5.2)
############# STATE ###############
0*		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.9990003749375039, the reward is 37.15999999999997 with loss [0, 0] in episode 0
No handles with labels found to put in legend.
Report: 
rewardSum:37.15999999999997
loss:[0, 0]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:8
memory used:1993.0
now epsilon is 0.9982513119532618, the reward is -2.0 with loss [0, 0] in episode 1
Report: 
rewardSum:-2.0
loss:[0, 0]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:14
memory used:1992.0
now epsilon is 0.9977522486879922, the reward is -1.0 with loss [0, 0] in episode 2
Report: 
rewardSum:-1.0
loss:[0, 0]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:18
memory used:1992.0
2021-03-25 14:58:57.790669: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 14:58:58.563535: W tensorflow/stream_executor/gpu/asm_compiler.cc:98] *** WARNING *** You are using ptxas 9.1.108, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You do not need to update to CUDA 9.2.88; cherry-picking the ptxas binary is sufficient.
2021-03-25 14:58:58.614520: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 14:58:58.845129: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 14:58:59.278091: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-03-25 14:58:59.280588: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2596990000 Hz
now epsilon is 0.9955095497619452, the reward is 631.1600000000001 with loss [10.748249381780624, 5.749132573604584] in episode 3
Report: 
rewardSum:631.1600000000001
loss:[10.748249381780624, 5.749132573604584]
policies:[0, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:36
memory used:2684.0
now epsilon is 0.9947631042421095, the reward is -2.0 with loss [11.41930428147316, 15.767388820648193] in episode 4
Report: 
rewardSum:-2.0
loss:[11.41930428147316, 15.767388820648193]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:42
memory used:2690.0
now epsilon is 0.9917829149096129, the reward is 29.159999999999968 with loss [49.665352404117584, 82.7849140688777] in episode 5
Report: 
rewardSum:29.159999999999968
loss:[49.665352404117584, 82.7849140688777]
policies:[0, 0, 12]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:66
memory used:2699.0
now epsilon is 0.9885644509486061, the reward is 28.159999999999968 with loss [70.59803679585457, 60.260907150805] in episode 6
Report: 
rewardSum:28.159999999999968
loss:[70.59803679585457, 60.260907150805]
policies:[0, 0, 13]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:92
memory used:2712.0
now epsilon is 0.9873293630832584, the reward is -4.0 with loss [12.748940914869308, 27.139214601367712] in episode 7
Report: 
rewardSum:-4.0
loss:[12.748940914869308, 27.139214601367712]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:102
memory used:2712.0
now epsilon is 0.9851100922120042, the reward is 32.15999999999997 with loss [60.127466797828674, 45.467282865196466] in episode 8
Report: 
rewardSum:32.15999999999997
loss:[60.127466797828674, 45.467282865196466]
policies:[0, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:120
memory used:2712.0
now epsilon is 0.9828958097097626, the reward is 242.25 with loss [42.25973904132843, 35.735622853040695] in episode 9
Report: 
rewardSum:242.25
loss:[42.25973904132843, 35.735622853040695]
policies:[0, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:138
memory used:2718.0
now epsilon is 0.9824044232358959, the reward is -1.0 with loss [9.014404535293579, 9.63365888595581] in episode 10
Report: 
rewardSum:-1.0
loss:[9.014404535293579, 9.63365888595581]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:142
memory used:2718.0
now epsilon is 0.9809317372982452, the reward is 35.15999999999997 with loss [16.732680168002844, 28.974882006645203] in episode 11
Report: 
rewardSum:35.15999999999997
loss:[16.732680168002844, 28.974882006645203]
policies:[0, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:154
memory used:2718.0
now epsilon is 0.979461259009318, the reward is 245.25 with loss [36.12098240852356, 18.003854900598526] in episode 12
Report: 
rewardSum:245.25
loss:[36.12098240852356, 18.003854900598526]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:166
memory used:2725.0
now epsilon is 0.976771104921209, the reward is 989.0 with loss [52.69603621959686, 53.2693287730217] in episode 13
Report: 
rewardSum:989.0
loss:[52.69603621959686, 53.2693287730217]
policies:[0, 0, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:188
memory used:2725.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44*		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.9731146163991041, the reward is 625.1600000000001 with loss [113.35514736175537, 84.61660830676556] in episode 14
Report: 
rewardSum:625.1600000000001
loss:[113.35514736175537, 84.61660830676556]
policies:[0, 0, 15]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:218
memory used:2725.0
now epsilon is 0.970199283304499, the reward is -11.0 with loss [64.97748183831573, 79.54629439115524] in episode 15
Report: 
rewardSum:-11.0
loss:[64.97748183831573, 79.54629439115524]
policies:[1, 0, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:242
memory used:2726.0
now epsilon is 0.9680185165925426, the reward is 242.25 with loss [67.42755031585693, 55.29448366165161] in episode 16
Report: 
rewardSum:242.25
loss:[67.42755031585693, 55.29448366165161]
policies:[0, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:260
memory used:2726.0
now epsilon is 0.9675345678354036, the reward is -1.0 with loss [16.13304042816162, 15.165449619293213] in episode 17
Report: 
rewardSum:-1.0
loss:[16.13304042816162, 15.165449619293213]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:264
memory used:2726.0
now epsilon is 0.9670508610223963, the reward is -1.0 with loss [3.006665736436844, 21.80208921432495] in episode 18
Report: 
rewardSum:-1.0
loss:[3.006665736436844, 21.80208921432495]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:268
memory used:2726.0
now epsilon is 0.9653597907411388, the reward is -6.0 with loss [37.2194098867476, 47.45823931694031] in episode 19
Report: 
rewardSum:-6.0
loss:[37.2194098867476, 47.45823931694031]
policies:[0, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:282
memory used:2728.0
now epsilon is 0.964153694201763, the reward is -4.0 with loss [25.16797936707735, 17.76643943414092] in episode 20
Report: 
rewardSum:-4.0
loss:[25.16797936707735, 17.76643943414092]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:292
memory used:2728.0
now epsilon is 0.9627083672533073, the reward is -5.0 with loss [40.19707054644823, 38.1499879360199] in episode 21
Report: 
rewardSum:-5.0
loss:[40.19707054644823, 38.1499879360199]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:304
memory used:2727.0
now epsilon is 0.9607846344163371, the reward is 243.25 with loss [59.08290076255798, 51.55687713623047] in episode 22
Report: 
rewardSum:243.25
loss:[59.08290076255798, 51.55687713623047]
policies:[0, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:320
memory used:2727.0
now epsilon is 0.9583853732363975, the reward is 31.159999999999968 with loss [71.4615982323885, 63.76834423094988] in episode 23
Report: 
rewardSum:31.159999999999968
loss:[71.4615982323885, 63.76834423094988]
policies:[0, 1, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:340
memory used:2727.0
now epsilon is 0.9567094561900504, the reward is 34.15999999999997 with loss [76.12177765369415, 50.42124104872346] in episode 24
Report: 
rewardSum:34.15999999999997
loss:[76.12177765369415, 50.42124104872346]
policies:[1, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:354
memory used:2727.0
now epsilon is 0.9528897853218017, the reward is -15.0 with loss [76.39053547382355, 103.818139154464] in episode 25
Report: 
rewardSum:-15.0
loss:[76.39053547382355, 103.818139154464]
policies:[0, 0, 16]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:386
memory used:2727.0
now epsilon is 0.9524133999847524, the reward is -1.0 with loss [17.132282733917236, 4.994213104248047] in episode 26
Report: 
rewardSum:-1.0
loss:[17.132282733917236, 4.994213104248047]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:390
memory used:2727.0
now epsilon is 0.9507479260566456, the reward is 244.25 with loss [18.789735540747643, 38.518434680998325] in episode 27
Report: 
rewardSum:244.25
loss:[18.789735540747643, 38.518434680998325]
policies:[0, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:404
memory used:2727.0
now epsilon is 0.9490853645228893, the reward is 244.25 with loss [54.04576587677002, 36.82490296289325] in episode 28
Report: 
rewardSum:244.25
loss:[54.04576587677002, 36.82490296289325]
policies:[0, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:418
memory used:2727.0
############# STATE ###############
0-		10-		20-		30-		40*		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.9445873380358527, the reward is 232.25 with loss [114.80478897318244, 166.1388560384512] in episode 29
Report: 
rewardSum:232.25
loss:[114.80478897318244, 166.1388560384512]
policies:[0, 0, 19]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:456
memory used:2726.0
now epsilon is 0.9441151034035435, the reward is -1.0 with loss [7.390918135643005, 9.994222164154053] in episode 30
Report: 
rewardSum:-1.0
loss:[7.390918135643005, 9.994222164154053]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:460
memory used:2727.0
now epsilon is 0.9415220298320217, the reward is 240.25 with loss [38.0229882709682, 61.46056818962097] in episode 31
Report: 
rewardSum:240.25
loss:[38.0229882709682, 61.46056818962097]
policies:[0, 1, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:482
memory used:2728.0
now epsilon is 0.9408160648303171, the reward is -2.0 with loss [24.482344388961792, 20.570484220981598] in episode 32
Report: 
rewardSum:-2.0
loss:[24.482344388961792, 20.570484220981598]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:488
memory used:2728.0
now epsilon is 0.940345715598906, the reward is -1.0 with loss [11.807733297348022, 11.318763732910156] in episode 33
Report: 
rewardSum:-1.0
loss:[11.807733297348022, 11.318763732910156]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:492
memory used:2728.0
now epsilon is 0.9394057224541827, the reward is 247.25 with loss [32.57566845417023, 33.14863324910402] in episode 34
Report: 
rewardSum:247.25
loss:[32.57566845417023, 33.14863324910402]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:500
memory used:2728.0
now epsilon is 0.9382320522829278, the reward is -4.0 with loss [14.92974042892456, 30.502303957939148] in episode 35
Report: 
rewardSum:-4.0
loss:[14.92974042892456, 30.502303957939148]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:510
memory used:2728.0
now epsilon is 0.9356551368941594, the reward is 30.159999999999968 with loss [36.57262500375509, 43.574933513998985] in episode 36
Report: 
rewardSum:30.159999999999968
loss:[36.57262500375509, 43.574933513998985]
policies:[1, 0, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:532
memory used:2728.0
now epsilon is 0.9351873678041583, the reward is -1.0 with loss [4.640327371656895, 21.369829557836056] in episode 37
Report: 
rewardSum:-1.0
loss:[4.640327371656895, 21.369829557836056]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:536
memory used:2728.0
now epsilon is 0.9333186288284107, the reward is -7.0 with loss [40.08198297023773, 35.8282645046711] in episode 38
Report: 
rewardSum:-7.0
loss:[40.08198297023773, 35.8282645046711]
policies:[0, 1, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:552
memory used:2728.0
now epsilon is 0.9300573165726109, the reward is 237.25 with loss [70.0153283663094, 81.92624519765377] in episode 39
Report: 
rewardSum:237.25
loss:[70.0153283663094, 81.92624519765377]
policies:[0, 0, 14]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:580
memory used:2728.0
now epsilon is 0.9275028536274567, the reward is 240.25 with loss [60.326736360788345, 31.475994933396578] in episode 40
Report: 
rewardSum:240.25
loss:[60.326736360788345, 31.475994933396578]
policies:[0, 0, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:602
memory used:2728.0
now epsilon is 0.9254180578713245, the reward is -8.0 with loss [59.99615780264139, 72.84689638018608] in episode 41
Report: 
rewardSum:-8.0
loss:[59.99615780264139, 72.84689638018608]
policies:[0, 1, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:620
memory used:2728.0
now epsilon is 0.9242618635406932, the reward is 246.25 with loss [34.33399170637131, 28.349637523293495] in episode 42
Report: 
rewardSum:246.25
loss:[34.33399170637131, 28.349637523293495]
policies:[1, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:630
memory used:2728.0
############# STATE ###############
0*		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.9226456178678637, the reward is -6.0 with loss [35.67085933685303, 44.48252022638917] in episode 43
Report: 
rewardSum:-6.0
loss:[35.67085933685303, 44.48252022638917]
policies:[1, 1, 5]
qAverage:[0.0, -0.36335253715515137]
ws:[-0.2499251514673233, -0.1820138692855835]
memory len:644
memory used:2734.0
now epsilon is 0.91919174510821, the reward is 625.1600000000001 with loss [82.99062536656857, 74.20135974511504] in episode 44
Report: 
rewardSum:625.1600000000001
loss:[82.99062536656857, 74.20135974511504]
policies:[0, 1, 14]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:674
memory used:2734.0
now epsilon is 0.9180433297780599, the reward is -4.0 with loss [22.345244899392128, 10.421910181641579] in episode 45
Report: 
rewardSum:-4.0
loss:[22.345244899392128, 10.421910181641579]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:684
memory used:2734.0
now epsilon is 0.9155218640294692, the reward is -10.0 with loss [71.82563376426697, 67.21265298128128] in episode 46
Report: 
rewardSum:-10.0
loss:[71.82563376426697, 67.21265298128128]
policies:[1, 1, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:706
memory used:2734.0
now epsilon is 0.9141494392491258, the reward is 245.25 with loss [33.549749583005905, 36.64687430858612] in episode 47
Report: 
rewardSum:245.25
loss:[33.549749583005905, 36.64687430858612]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:718
memory used:2735.0
now epsilon is 0.9136924216638413, the reward is -1.0 with loss [14.910507678985596, 13.247735500335693] in episode 48
Report: 
rewardSum:-1.0
loss:[14.910507678985596, 13.247735500335693]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:722
memory used:2735.0
now epsilon is 0.9114107586571939, the reward is 241.25 with loss [60.03170419111848, 69.42689728736877] in episode 49
Report: 
rewardSum:241.25
loss:[60.03170419111848, 69.42689728736877]
policies:[0, 1, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:742
memory used:2735.0
now epsilon is 0.9109551102410377, the reward is -1.0 with loss [4.625856876373291, 9.166320323944092] in episode 50
Report: 
rewardSum:-1.0
loss:[4.625856876373291, 9.166320323944092]
policies:[1, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:746
memory used:2735.0
now epsilon is 0.910044496682032, the reward is -3.0 with loss [19.994576424360275, 26.836065113544464] in episode 51
Report: 
rewardSum:-3.0
loss:[19.994576424360275, 26.836065113544464]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:754
memory used:2735.0
now epsilon is 0.9075450002490781, the reward is 30.159999999999968 with loss [78.19319808483124, 77.95049864053726] in episode 52
Report: 
rewardSum:30.159999999999968
loss:[78.19319808483124, 77.95049864053726]
policies:[1, 0, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:776
memory used:2735.0
now epsilon is 0.9064111360726058, the reward is 36.15999999999997 with loss [31.402857065200806, 32.35358591377735] in episode 53
Report: 
rewardSum:36.15999999999997
loss:[31.402857065200806, 32.35358591377735]
policies:[0, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:786
memory used:2735.0
now epsilon is 0.9052786885178662, the reward is 36.15999999999997 with loss [32.615601539611816, 40.88801598548889] in episode 54
Report: 
rewardSum:36.15999999999997
loss:[32.615601539611816, 40.88801598548889]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:796
memory used:2735.0
now epsilon is 0.9036956384962882, the reward is 34.15999999999997 with loss [21.776135981082916, 41.34138348698616] in episode 55
Report: 
rewardSum:34.15999999999997
loss:[21.776135981082916, 41.34138348698616]
policies:[0, 2, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:810
memory used:2735.0
now epsilon is 0.9005378383942583, the reward is 237.25 with loss [90.69863115251064, 68.41836236044765] in episode 56
Report: 
rewardSum:237.25
loss:[90.69863115251064, 68.41836236044765]
policies:[0, 0, 14]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:838
memory used:2735.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22*		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.897615476519675, the reward is 238.25 with loss [66.0129863396287, 80.87131705880165] in episode 57
Report: 
rewardSum:238.25
loss:[66.0129863396287, 80.87131705880165]
policies:[0, 0, 13]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:864
memory used:2735.0
now epsilon is 0.8958218156085515, the reward is 33.15999999999997 with loss [24.134055502712727, 55.318143010139465] in episode 58
Report: 
rewardSum:33.15999999999997
loss:[24.134055502712727, 55.318143010139465]
policies:[1, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:880
memory used:2735.0
now epsilon is 0.8951501171994384, the reward is -2.0 with loss [15.617405086755753, 15.55715274810791] in episode 59
Report: 
rewardSum:-2.0
loss:[15.617405086755753, 15.55715274810791]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:886
memory used:2735.0
now epsilon is 0.8917991723233974, the reward is 236.25 with loss [72.68459558486938, 43.98385915160179] in episode 60
Report: 
rewardSum:236.25
loss:[72.68459558486938, 43.98385915160179]
policies:[2, 0, 13]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:916
memory used:2736.0
now epsilon is 0.8911304901425654, the reward is -2.0 with loss [13.52005523443222, 20.080639123916626] in episode 61
Report: 
rewardSum:-2.0
loss:[13.52005523443222, 20.080639123916626]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:922
memory used:2736.0
now epsilon is 0.8900171338472219, the reward is -4.0 with loss [29.366884469985962, 28.54535609483719] in episode 62
Report: 
rewardSum:-4.0
loss:[29.366884469985962, 28.54535609483719]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:932
memory used:2736.0
now epsilon is 0.8886829422594359, the reward is 35.15999999999997 with loss [22.42391597479582, 33.39058429002762] in episode 63
Report: 
rewardSum:35.15999999999997
loss:[22.42391597479582, 33.39058429002762]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:944
memory used:2736.0
now epsilon is 0.8875726438696112, the reward is -4.0 with loss [15.630769968032837, 15.122743248939514] in episode 64
Report: 
rewardSum:-4.0
loss:[15.630769968032837, 15.122743248939514]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:954
memory used:2736.0
now epsilon is 0.8844711826408793, the reward is 237.25 with loss [50.45451533794403, 64.53588628023863] in episode 65
Report: 
rewardSum:237.25
loss:[50.45451533794403, 64.53588628023863]
policies:[1, 0, 13]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:982
memory used:2737.0
now epsilon is 0.8827037873264969, the reward is -7.0 with loss [39.14363217353821, 13.896129935979843] in episode 66
Report: 
rewardSum:-7.0
loss:[39.14363217353821, 13.896129935979843]
policies:[0, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:998
memory used:2737.0
now epsilon is 0.8809399237113474, the reward is -7.0 with loss [40.49977123737335, 51.694640159606934] in episode 67
Report: 
rewardSum:-7.0
loss:[40.49977123737335, 51.694640159606934]
policies:[1, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1014
memory used:2737.0
now epsilon is 0.8796193394317169, the reward is 245.25 with loss [29.764677584171295, 44.45386481285095] in episode 68
Report: 
rewardSum:245.25
loss:[29.764677584171295, 44.45386481285095]
policies:[0, 2, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1026
memory used:2737.0
now epsilon is 0.8769841068212059, the reward is 988.0 with loss [56.100036561489105, 67.60144484043121] in episode 69
Report: 
rewardSum:988.0
loss:[56.100036561489105, 67.60144484043121]
policies:[1, 0, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1050
memory used:2737.0
############# STATE ###############
0-		10-		20-		30-		40-		
1*		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.8761074515286167, the reward is 247.25 with loss [14.728450745344162, 24.358478546142578] in episode 70
Report: 
rewardSum:247.25
loss:[14.728450745344162, 24.358478546142578]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1058
memory used:2737.0
now epsilon is 0.8747941114283275, the reward is -5.0 with loss [27.815322175621986, 34.04938793182373] in episode 71
Report: 
rewardSum:-5.0
loss:[27.815322175621986, 34.04938793182373]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1070
memory used:2743.0
now epsilon is 0.8730460533299603, the reward is 243.25 with loss [37.23433303833008, 59.714485466480255] in episode 72
Report: 
rewardSum:243.25
loss:[37.23433303833008, 59.714485466480255]
policies:[0, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1086
memory used:2755.0
now epsilon is 0.8704305134855324, the reward is 239.25 with loss [51.50046008825302, 59.5667279958725] in episode 73
Report: 
rewardSum:239.25
loss:[51.50046008825302, 59.5667279958725]
policies:[1, 1, 10]
qAverage:[0.0, 11.915359497070312]
ws:[-0.5569887161254883, 0.2544720768928528]
memory len:1110
memory used:2755.0
now epsilon is 0.8699953526306968, the reward is -1.0 with loss [3.7881076335906982, 6.0941314697265625] in episode 74
Report: 
rewardSum:-1.0
loss:[3.7881076335906982, 6.0941314697265625]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1114
memory used:2755.0
now epsilon is 0.8684740021568338, the reward is 34.15999999999997 with loss [27.39404332637787, 30.099477529525757] in episode 75
Report: 
rewardSum:34.15999999999997
loss:[27.39404332637787, 30.099477529525757]
policies:[0, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1128
memory used:2755.0
now epsilon is 0.8658721596219217, the reward is 239.25 with loss [54.05844807624817, 75.10237318277359] in episode 76
Report: 
rewardSum:239.25
loss:[54.05844807624817, 75.10237318277359]
policies:[0, 2, 10]
qAverage:[0.0, 41.856422424316406]
ws:[2.323887586593628, 3.0957605838775635]
memory len:1152
memory used:2768.0
now epsilon is 0.8654392776591207, the reward is -1.0 with loss [3.336974322795868, 4.036551475524902] in episode 77
Report: 
rewardSum:-1.0
loss:[3.336974322795868, 4.036551475524902]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1156
memory used:2768.0
now epsilon is 0.8639258943391006, the reward is 244.25 with loss [20.472968339920044, 16.243871450424194] in episode 78
Report: 
rewardSum:244.25
loss:[20.472968339920044, 16.243871450424194]
policies:[1, 1, 5]
qAverage:[4.890218734741211, 1.7017618815104167]
ws:[-2.1967854499816895, -2.20310910542806]
memory len:1170
memory used:2768.0
now epsilon is 0.8630622923629802, the reward is 247.25 with loss [22.908373713493347, 20.859342098236084] in episode 79
Report: 
rewardSum:247.25
loss:[22.908373713493347, 20.859342098236084]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1178
memory used:2768.0
now epsilon is 0.8613376773823224, the reward is 243.25 with loss [21.521297246217728, 39.24356299638748] in episode 80
Report: 
rewardSum:243.25
loss:[21.521297246217728, 39.24356299638748]
policies:[1, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1194
memory used:2769.0
now epsilon is 0.860046478101204, the reward is -5.0 with loss [25.602150082588196, 30.492817163467407] in episode 81
Report: 
rewardSum:-5.0
loss:[25.602150082588196, 30.492817163467407]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1206
memory used:2769.0
now epsilon is 0.8596165086150583, the reward is -1.0 with loss [17.957311630249023, 12.722041130065918] in episode 82
Report: 
rewardSum:-1.0
loss:[17.957311630249023, 12.722041130065918]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1210
memory used:2769.0
now epsilon is 0.8585425251053093, the reward is -4.0 with loss [16.02373230457306, 36.49143886566162] in episode 83
Report: 
rewardSum:-4.0
loss:[16.02373230457306, 36.49143886566162]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1220
memory used:2769.0
now epsilon is 0.8572555159330245, the reward is 245.25 with loss [24.302400827407837, 17.689761996269226] in episode 84
Report: 
rewardSum:245.25
loss:[24.302400827407837, 17.689761996269226]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1232
memory used:2769.0
now epsilon is 0.8559704360683291, the reward is 245.25 with loss [24.00618213415146, 32.25163960456848] in episode 85
Report: 
rewardSum:245.25
loss:[24.00618213415146, 32.25163960456848]
policies:[0, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1244
memory used:2769.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2*		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.8538329157907675, the reward is -9.0 with loss [35.59286451339722, 35.55471348762512] in episode 86
Report: 
rewardSum:-9.0
loss:[35.59286451339722, 35.55471348762512]
policies:[0, 1, 9]
qAverage:[0.0, 19.03094482421875]
ws:[4.08552885055542, 4.338170051574707]
memory len:1264
memory used:2769.0
now epsilon is 0.8512749361707725, the reward is 988.0 with loss [57.47668719291687, 68.04101192951202] in episode 87
Report: 
rewardSum:988.0
loss:[57.47668719291687, 68.04101192951202]
policies:[0, 2, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1288
memory used:2769.0
now epsilon is 0.8502113744143993, the reward is 246.25 with loss [6.570528596639633, 22.486716389656067] in episode 88
Report: 
rewardSum:246.25
loss:[6.570528596639633, 22.486716389656067]
policies:[0, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1298
memory used:2769.0
now epsilon is 0.8493614818161155, the reward is -3.0 with loss [48.138023853302, 26.81212282180786] in episode 89
Report: 
rewardSum:-3.0
loss:[48.138023853302, 26.81212282180786]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1306
memory used:2768.0
now epsilon is 0.8487246199467601, the reward is -2.0 with loss [11.830552339553833, 27.425009727478027] in episode 90
Report: 
rewardSum:-2.0
loss:[11.830552339553833, 27.425009727478027]
policies:[0, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1312
memory used:2768.0
now epsilon is 0.8478762135455039, the reward is 37.15999999999997 with loss [16.27191722393036, 36.973206520080566] in episode 91
Report: 
rewardSum:37.15999999999997
loss:[16.27191722393036, 36.973206520080566]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1320
memory used:2768.0
now epsilon is 0.8472404653488868, the reward is -2.0 with loss [18.07157588005066, 19.983521223068237] in episode 92
Report: 
rewardSum:-2.0
loss:[18.07157588005066, 19.983521223068237]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1326
memory used:2769.0
now epsilon is 0.8455474663478997, the reward is 632.1600000000001 with loss [41.27483427524567, 46.501054763793945] in episode 93
Report: 
rewardSum:632.1600000000001
loss:[41.27483427524567, 46.501054763793945]
policies:[0, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1342
memory used:2769.0
now epsilon is 0.8447022359090083, the reward is -3.0 with loss [25.194652318954468, 9.265283942222595] in episode 94
Report: 
rewardSum:-3.0
loss:[25.194652318954468, 9.265283942222595]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1350
memory used:2769.0
now epsilon is 0.8434359741995711, the reward is 245.25 with loss [38.2908576130867, 41.61318254470825] in episode 95
Report: 
rewardSum:245.25
loss:[38.2908576130867, 41.61318254470825]
policies:[1, 1, 4]
qAverage:[0.0, 32.11005783081055]
ws:[7.164221286773682, 8.516862869262695]
memory len:1362
memory used:2769.0
now epsilon is 0.8430143089272197, the reward is -1.0 with loss [5.655110597610474, 9.51695990562439] in episode 96
Report: 
rewardSum:-1.0
loss:[5.655110597610474, 9.51695990562439]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1366
memory used:2769.0
now epsilon is 0.8421716106959733, the reward is 247.25 with loss [11.386865735054016, 28.05570685863495] in episode 97
Report: 
rewardSum:247.25
loss:[11.386865735054016, 28.05570685863495]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1374
memory used:2769.0
now epsilon is 0.8404887405382303, the reward is 243.25 with loss [20.42476725578308, 36.81558656692505] in episode 98
Report: 
rewardSum:243.25
loss:[20.42476725578308, 36.81558656692505]
policies:[0, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1390
memory used:2769.0
now epsilon is 0.8388092331772521, the reward is 632.1600000000001 with loss [83.80716049671173, 60.38665294647217] in episode 99
Report: 
rewardSum:632.1600000000001
loss:[83.80716049671173, 60.38665294647217]
policies:[0, 1, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1406
memory used:2769.0
now epsilon is 0.836296262684022, the reward is 29.159999999999968 with loss [66.3807487487793, 65.347904920578] in episode 100
Report: 
rewardSum:29.159999999999968
loss:[66.3807487487793, 65.347904920578]
policies:[1, 2, 9]
qAverage:[0.0, 39.738929748535156]
ws:[10.569758415222168, 15.63058090209961]
memory len:1430
memory used:2769.0
now epsilon is 0.8331656339520941, the reward is 985.0 with loss [82.13493609428406, 90.22558033466339] in episode 101
Report: 
rewardSum:985.0
loss:[82.13493609428406, 90.22558033466339]
policies:[0, 1, 14]
qAverage:[0.0, 62.21478271484375]
ws:[8.537543296813965, 9.190871238708496]
memory len:1460
memory used:2769.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44*		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.8317086871670492, the reward is 244.25 with loss [24.827522039413452, 29.857823848724365] in episode 102
Report: 
rewardSum:244.25
loss:[24.827522039413452, 29.857823848724365]
policies:[0, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1474
memory used:2769.0
now epsilon is 0.8304619036033327, the reward is 245.25 with loss [31.897013664245605, 28.522250652313232] in episode 103
Report: 
rewardSum:245.25
loss:[31.897013664245605, 28.522250652313232]
policies:[2, 1, 3]
qAverage:[0.0, 52.618812561035156]
ws:[6.172314167022705, 7.160068511962891]
memory len:1486
memory used:2769.0
now epsilon is 0.8288024323780303, the reward is 243.25 with loss [67.07212018966675, 80.13560485839844] in episode 104
Report: 
rewardSum:243.25
loss:[67.07212018966675, 80.13560485839844]
policies:[0, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1502
memory used:2769.0
now epsilon is 0.8279739406947675, the reward is 37.15999999999997 with loss [36.01855134963989, 30.686048984527588] in episode 105
Report: 
rewardSum:37.15999999999997
loss:[36.01855134963989, 30.686048984527588]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1510
memory used:2769.0
now epsilon is 0.8275600054727914, the reward is -1.0 with loss [22.75000762939453, 12.3147611618042] in episode 106
Report: 
rewardSum:-1.0
loss:[22.75000762939453, 12.3147611618042]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1514
memory used:2769.0
now epsilon is 0.8271462771925554, the reward is -1.0 with loss [4.7295355796813965, 11.583242416381836] in episode 107
Report: 
rewardSum:-1.0
loss:[4.7295355796813965, 11.583242416381836]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1518
memory used:2769.0
now epsilon is 0.8263194410435236, the reward is 247.25 with loss [17.243632078170776, 27.72048568725586] in episode 108
Report: 
rewardSum:247.25
loss:[17.243632078170776, 27.72048568725586]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1526
memory used:2769.0
now epsilon is 0.8252870580627737, the reward is -4.0 with loss [19.436763167381287, 30.236019134521484] in episode 109
Report: 
rewardSum:-4.0
loss:[19.436763167381287, 30.236019134521484]
policies:[0, 1, 4]
qAverage:[0.0, 51.81552505493164]
ws:[8.547218322753906, 9.234171867370605]
memory len:1536
memory used:2769.0
now epsilon is 0.822814598362371, the reward is 628.1600000000001 with loss [73.14823400974274, 69.99623930454254] in episode 110
Report: 
rewardSum:628.1600000000001
loss:[73.14823400974274, 69.99623930454254]
policies:[2, 1, 9]
qAverage:[0.0, 77.10639953613281]
ws:[8.012606620788574, 10.73945426940918]
memory len:1560
memory used:2769.0
now epsilon is 0.8219920922680604, the reward is 37.15999999999997 with loss [24.096354484558105, 21.214088916778564] in episode 111
Report: 
rewardSum:37.15999999999997
loss:[24.096354484558105, 21.214088916778564]
policies:[2, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1568
memory used:2769.0
now epsilon is 0.8203495458506674, the reward is 243.25 with loss [33.5966362953186, 33.812209606170654] in episode 112
Report: 
rewardSum:243.25
loss:[33.5966362953186, 33.812209606170654]
policies:[0, 1, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1584
memory used:2769.0
now epsilon is 0.8191197903532796, the reward is 245.25 with loss [24.116914749145508, 26.996103167533875] in episode 113
Report: 
rewardSum:245.25
loss:[24.116914749145508, 26.996103167533875]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1596
memory used:2769.0
now epsilon is 0.8174829835157005, the reward is 33.15999999999997 with loss [49.171850085258484, 68.55850124359131] in episode 114
Report: 
rewardSum:33.15999999999997
loss:[49.171850085258484, 68.55850124359131]
policies:[0, 2, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1612
memory used:2769.0
now epsilon is 0.8156454850669591, the reward is 32.15999999999997 with loss [54.31248331069946, 81.56528854370117] in episode 115
Report: 
rewardSum:32.15999999999997
loss:[54.31248331069946, 81.56528854370117]
policies:[1, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1630
memory used:2769.0
now epsilon is 0.8150339038739429, the reward is -2.0 with loss [18.665886163711548, 18.252188444137573] in episode 116
Report: 
rewardSum:-2.0
loss:[18.665886163711548, 18.252188444137573]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1636
memory used:2770.0
now epsilon is 0.814626437861625, the reward is -1.0 with loss [12.883471250534058, 7.288971424102783] in episode 117
Report: 
rewardSum:-1.0
loss:[12.883471250534058, 7.288971424102783]
policies:[1, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1640
memory used:2770.0
now epsilon is 0.8132019103471795, the reward is 34.15999999999997 with loss [32.639981746673584, 25.95657777786255] in episode 118
Report: 
rewardSum:34.15999999999997
loss:[32.639981746673584, 25.95657777786255]
policies:[1, 2, 4]
qAverage:[0.0, 117.07369995117188]
ws:[7.949927806854248, 9.51817798614502]
memory len:1654
memory used:2770.0
now epsilon is 0.8123890133367268, the reward is 247.25 with loss [25.195623874664307, 27.01861333847046] in episode 119
Report: 
rewardSum:247.25
loss:[25.195623874664307, 27.01861333847046]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1662
memory used:2770.0
now epsilon is 0.8119828696043718, the reward is -1.0 with loss [2.5312137603759766, 6.216078519821167] in episode 120
Report: 
rewardSum:-1.0
loss:[2.5312137603759766, 6.216078519821167]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1666
memory used:2770.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48*		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.8101577340438908, the reward is 991.0 with loss [24.366267681121826, 58.31602120399475] in episode 121
Report: 
rewardSum:991.0
loss:[24.366267681121826, 58.31602120399475]
policies:[0, 2, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1684
memory used:2770.0
now epsilon is 0.808134616759027, the reward is 990.0 with loss [53.04552102088928, 70.7045704126358] in episode 122
Report: 
rewardSum:990.0
loss:[53.04552102088928, 70.7045704126358]
policies:[2, 3, 5]
qAverage:[0.0, 68.3603515625]
ws:[5.571824073791504, 5.719551086425781]
memory len:1704
memory used:2769.0
now epsilon is 0.8069231722075971, the reward is 245.25 with loss [49.61240482330322, 62.866098403930664] in episode 123
Report: 
rewardSum:245.25
loss:[49.61240482330322, 62.866098403930664]
policies:[2, 1, 3]
qAverage:[0.0, 83.79032135009766]
ws:[6.147531032562256, 6.752407550811768]
memory len:1716
memory used:2769.0
now epsilon is 0.8059150224432544, the reward is -4.0 with loss [38.09771728515625, 34.532421350479126] in episode 124
Report: 
rewardSum:-4.0
loss:[38.09771728515625, 34.532421350479126]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1726
memory used:2769.0
now epsilon is 0.8053107372728964, the reward is -2.0 with loss [19.226194858551025, 11.366488933563232] in episode 125
Report: 
rewardSum:-2.0
loss:[19.226194858551025, 11.366488933563232]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1732
memory used:2769.0
now epsilon is 0.8041035258941909, the reward is 245.25 with loss [41.58136701583862, 37.06619644165039] in episode 126
Report: 
rewardSum:245.25
loss:[41.58136701583862, 37.06619644165039]
policies:[1, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1744
memory used:2769.0
now epsilon is 0.8037015243877141, the reward is -1.0 with loss [21.456482887268066, 8.412957668304443] in episode 127
Report: 
rewardSum:-1.0
loss:[21.456482887268066, 8.412957668304443]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1748
memory used:2769.0
now epsilon is 0.8028981242011699, the reward is 37.15999999999997 with loss [23.6779625415802, 17.909046173095703] in episode 128
Report: 
rewardSum:37.15999999999997
loss:[23.6779625415802, 17.909046173095703]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1756
memory used:2769.0
now epsilon is 0.8010934088890881, the reward is 242.25 with loss [54.1481317281723, 62.51700735092163] in episode 129
Report: 
rewardSum:242.25
loss:[54.1481317281723, 62.51700735092163]
policies:[2, 1, 6]
qAverage:[0.0, 100.35687255859375]
ws:[7.546160697937012, 8.210179328918457]
memory len:1774
memory used:2769.0
now epsilon is 0.7986934304205224, the reward is 29.159999999999968 with loss [78.8119626045227, 68.76421594619751] in episode 130
Report: 
rewardSum:29.159999999999968
loss:[78.8119626045227, 68.76421594619751]
policies:[0, 1, 11]
qAverage:[0.0, 80.08887481689453]
ws:[5.478845596313477, 6.158228397369385]
memory len:1798
memory used:2768.0
now epsilon is 0.7966989416728492, the reward is 241.25 with loss [51.97620916366577, 87.55027890205383] in episode 131
Report: 
rewardSum:241.25
loss:[51.97620916366577, 87.55027890205383]
policies:[1, 3, 6]
qAverage:[0.0, 113.9338607788086]
ws:[4.808711051940918, 5.915018081665039]
memory len:1818
memory used:2768.0
now epsilon is 0.7961015668351977, the reward is -2.0 with loss [16.794026374816895, 12.819307565689087] in episode 132
Report: 
rewardSum:-2.0
loss:[16.794026374816895, 12.819307565689087]
policies:[1, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1824
memory used:2769.0
now epsilon is 0.7937165433185953, the reward is 239.25 with loss [55.901124477386475, 56.98962426185608] in episode 133
Report: 
rewardSum:239.25
loss:[55.901124477386475, 56.98962426185608]
policies:[0, 2, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1848
memory used:2775.0
now epsilon is 0.7919324659169887, the reward is -8.0 with loss [38.337958097457886, 40.55311703681946] in episode 134
Report: 
rewardSum:-8.0
loss:[38.337958097457886, 40.55311703681946]
policies:[0, 1, 8]
qAverage:[0.0, 95.74020385742188]
ws:[3.8543858528137207, 4.445180892944336]
memory len:1866
memory used:2774.0
now epsilon is 0.789954860578033, the reward is 241.25 with loss [42.989245653152466, 74.87861490249634] in episode 135
Report: 
rewardSum:241.25
loss:[42.989245653152466, 74.87861490249634]
policies:[1, 1, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1886
memory used:2774.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23*		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.7891652019011587, the reward is 247.25 with loss [24.70881414413452, 29.77922248840332] in episode 136
Report: 
rewardSum:247.25
loss:[24.70881414413452, 29.77922248840332]
policies:[1, 1, 2]
qAverage:[0.0, 98.67557525634766]
ws:[4.998880863189697, 5.480983734130859]
memory len:1894
memory used:2774.0
now epsilon is 0.7879821936941159, the reward is -5.0 with loss [27.417311668395996, 31.477853775024414] in episode 137
Report: 
rewardSum:-5.0
loss:[27.417311668395996, 31.477853775024414]
policies:[1, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1906
memory used:2774.0
now epsilon is 0.7860144529329803, the reward is 241.25 with loss [57.00923204421997, 63.474496603012085] in episode 138
Report: 
rewardSum:241.25
loss:[57.00923204421997, 63.474496603012085]
policies:[1, 3, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1926
memory used:2774.0
now epsilon is 0.7828762833576038, the reward is -15.0 with loss [103.6846113204956, 105.78951811790466] in episode 139
Report: 
rewardSum:-15.0
loss:[103.6846113204956, 105.78951811790466]
policies:[1, 5, 10]
qAverage:[36.94395065307617, 101.35881042480469]
ws:[8.853543043136597, 9.494239807128906]
memory len:1958
memory used:2775.0
now epsilon is 0.7817027026344802, the reward is -5.0 with loss [37.01606464385986, 20.310521841049194] in episode 140
Report: 
rewardSum:-5.0
loss:[37.01606464385986, 20.310521841049194]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1970
memory used:2775.0
now epsilon is 0.781311900139582, the reward is -1.0 with loss [3.6171048879623413, 7.745677471160889] in episode 141
Report: 
rewardSum:-1.0
loss:[3.6171048879623413, 7.745677471160889]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1974
memory used:2775.0
now epsilon is 0.780921293021506, the reward is -1.0 with loss [6.399114608764648, 6.239278316497803] in episode 142
Report: 
rewardSum:-1.0
loss:[6.399114608764648, 6.239278316497803]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1978
memory used:2775.0
now epsilon is 0.7774146048153556, the reward is 622.1600000000001 with loss [85.20188462734222, 105.15294927358627] in episode 143
Report: 
rewardSum:622.1600000000001
loss:[85.20188462734222, 105.15294927358627]
policies:[1, 2, 15]
qAverage:[0.0, 95.51702880859375]
ws:[5.440537929534912, 5.554806232452393]
memory len:2014
memory used:2775.0
now epsilon is 0.7764433223220087, the reward is 246.25 with loss [20.67628812789917, 33.20051908493042] in episode 144
Report: 
rewardSum:246.25
loss:[20.67628812789917, 33.20051908493042]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2024
memory used:2775.0
now epsilon is 0.7756671701174079, the reward is -3.0 with loss [18.8274142742157, 13.423161387443542] in episode 145
Report: 
rewardSum:-3.0
loss:[18.8274142742157, 13.423161387443542]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2032
memory used:2775.0
now epsilon is 0.7746980708255597, the reward is 246.25 with loss [9.562197804450989, 19.467243194580078] in episode 146
Report: 
rewardSum:246.25
loss:[9.562197804450989, 19.467243194580078]
policies:[2, 1, 2]
qAverage:[0.0, 113.35942077636719]
ws:[3.0693247318267822, 3.6271793842315674]
memory len:2042
memory used:2775.0
now epsilon is 0.7706410577848074, the reward is -20.0 with loss [97.43923807144165, 95.2936339378357] in episode 147
Report: 
rewardSum:-20.0
loss:[97.43923807144165, 95.2936339378357]
policies:[1, 3, 17]
qAverage:[0.0, 131.5264129638672]
ws:[5.389989852905273, 6.41270383199056]
memory len:2084
memory used:2776.0
now epsilon is 0.7692934469787335, the reward is 244.25 with loss [40.69489598274231, 36.354289054870605] in episode 148
Report: 
rewardSum:244.25
loss:[40.69489598274231, 36.354289054870605]
policies:[0, 1, 6]
qAverage:[0.0, 102.39315032958984]
ws:[12.786126136779785, 14.365117073059082]
memory len:2098
memory used:2776.0
############# STATE ###############
0-		10*		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.7681402277805127, the reward is -5.0 with loss [22.1265549659729, 32.01047670841217] in episode 149
Report: 
rewardSum:-5.0
loss:[22.1265549659729, 32.01047670841217]
policies:[1, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2110
memory used:2776.0
now epsilon is 0.7671805324634227, the reward is 246.25 with loss [18.383469462394714, 19.85514998435974] in episode 150
Report: 
rewardSum:246.25
loss:[18.383469462394714, 19.85514998435974]
policies:[0, 3, 2]
qAverage:[0.0, 115.8844985961914]
ws:[22.29503631591797, 25.890607833862305]
memory len:2120
memory used:2777.0
now epsilon is 0.7664136395757132, the reward is 247.25 with loss [16.64342451095581, 12.556376338005066] in episode 151
Report: 
rewardSum:247.25
loss:[16.64342451095581, 12.556376338005066]
policies:[1, 1, 2]
qAverage:[0.0, 96.87867736816406]
ws:[12.268462181091309, 13.52507209777832]
memory len:2128
memory used:2792.0
now epsilon is 0.7648821528500287, the reward is 33.15999999999997 with loss [36.98746967315674, 53.68738651275635] in episode 152
Report: 
rewardSum:33.15999999999997
loss:[36.98746967315674, 53.68738651275635]
policies:[0, 3, 5]
qAverage:[0.0, 140.48777770996094]
ws:[3.505630095799764, 4.542659600575765]
memory len:2144
memory used:2802.0
now epsilon is 0.7641175574801838, the reward is -3.0 with loss [16.191975593566895, 9.908530235290527] in episode 153
Report: 
rewardSum:-3.0
loss:[16.191975593566895, 9.908530235290527]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2152
memory used:2801.0
now epsilon is 0.7620188588822114, the reward is 240.25 with loss [49.91741716861725, 58.11718821525574] in episode 154
Report: 
rewardSum:240.25
loss:[49.91741716861725, 58.11718821525574]
policies:[1, 1, 9]
qAverage:[0.0, 98.24878692626953]
ws:[5.663559913635254, 7.393187046051025]
memory len:2174
memory used:2789.0
now epsilon is 0.7614474876046794, the reward is -2.0 with loss [12.48793363571167, 16.610969305038452] in episode 155
Report: 
rewardSum:-2.0
loss:[12.48793363571167, 16.610969305038452]
policies:[1, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2180
memory used:2789.0
now epsilon is 0.7606863256122952, the reward is 37.15999999999997 with loss [16.214528143405914, 26.91386890411377] in episode 156
Report: 
rewardSum:37.15999999999997
loss:[16.214528143405914, 26.91386890411377]
policies:[2, 1, 1]
qAverage:[0.0, 96.88468170166016]
ws:[8.922765731811523, 9.713556289672852]
memory len:2188
memory used:2789.0
now epsilon is 0.7599259244965153, the reward is 247.25 with loss [15.972984313964844, 19.150409698486328] in episode 157
Report: 
rewardSum:247.25
loss:[15.972984313964844, 19.150409698486328]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2196
memory used:2789.0
now epsilon is 0.7591662834967481, the reward is 247.25 with loss [8.3075031042099, 24.596126317977905] in episode 158
Report: 
rewardSum:247.25
loss:[8.3075031042099, 24.596126317977905]
policies:[3, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2204
memory used:2789.0
now epsilon is 0.7582178000026996, the reward is 246.25 with loss [28.239482879638672, 27.822903990745544] in episode 159
Report: 
rewardSum:246.25
loss:[28.239482879638672, 27.822903990745544]
policies:[0, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2214
memory used:2790.0
now epsilon is 0.7572705015203646, the reward is -4.0 with loss [13.817042350769043, 33.995236992836] in episode 160
Report: 
rewardSum:-4.0
loss:[13.817042350769043, 33.995236992836]
policies:[1, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2224
memory used:2790.0
now epsilon is 0.7551906088071706, the reward is 240.25 with loss [49.88755643367767, 61.12504732608795] in episode 161
Report: 
rewardSum:240.25
loss:[49.88755643367767, 61.12504732608795]
policies:[0, 1, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2246
memory used:2790.0
now epsilon is 0.7534931281254084, the reward is 242.25 with loss [37.14872121810913, 23.047687709331512] in episode 162
Report: 
rewardSum:242.25
loss:[37.14872121810913, 23.047687709331512]
policies:[1, 1, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2264
memory used:2791.0
now epsilon is 0.7517994629580756, the reward is 242.25 with loss [61.608723163604736, 44.72973418235779] in episode 163
Report: 
rewardSum:242.25
loss:[61.608723163604736, 44.72973418235779]
policies:[2, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2282
memory used:2791.0
now epsilon is 0.750109604728845, the reward is 242.25 with loss [48.21334904432297, 38.6533944606781] in episode 164
Report: 
rewardSum:242.25
loss:[48.21334904432297, 38.6533944606781]
policies:[1, 1, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2300
memory used:2794.0
############# STATE ###############
0-		10-		20-		30-		40*		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.7476754019478413, the reward is 238.25 with loss [48.756468772888184, 76.53244495391846] in episode 165
Report: 
rewardSum:238.25
loss:[48.756468772888184, 76.53244495391846]
policies:[2, 1, 10]
qAverage:[0.0, 99.3180160522461]
ws:[14.983311653137207, 16.07090950012207]
memory len:2326
memory used:2800.0
now epsilon is 0.7467412748757232, the reward is -4.0 with loss [30.768927693367004, 19.521726846694946] in episode 166
Report: 
rewardSum:-4.0
loss:[30.768927693367004, 19.521726846694946]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2336
memory used:2800.0
now epsilon is 0.7454354573343419, the reward is -6.0 with loss [30.422120690345764, 29.236379384994507] in episode 167
Report: 
rewardSum:-6.0
loss:[30.422120690345764, 29.236379384994507]
policies:[0, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2350
memory used:2800.0
now epsilon is 0.7443180027611768, the reward is 245.25 with loss [14.443944036960602, 50.714229583740234] in episode 168
Report: 
rewardSum:245.25
loss:[14.443944036960602, 50.714229583740234]
policies:[1, 3, 2]
qAverage:[0.0, 121.08752950032552]
ws:[11.247062047322592, 12.384429931640625]
memory len:2362
memory used:2800.0
now epsilon is 0.743202223322607, the reward is -5.0 with loss [36.71738886833191, 35.699336528778076] in episode 169
Report: 
rewardSum:-5.0
loss:[36.71738886833191, 35.699336528778076]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2374
memory used:2800.0
now epsilon is 0.7406052382795654, the reward is 237.25 with loss [70.6753853559494, 66.27127587795258] in episode 170
Report: 
rewardSum:237.25
loss:[70.6753853559494, 66.27127587795258]
policies:[0, 4, 10]
qAverage:[0.0, 86.72759246826172]
ws:[12.201980590820312, 14.169120788574219]
memory len:2402
memory used:2801.0
now epsilon is 0.7398649107219653, the reward is 247.25 with loss [26.565096616744995, 14.016799211502075] in episode 171
Report: 
rewardSum:247.25
loss:[26.565096616744995, 14.016799211502075]
policies:[2, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2410
memory used:2807.0
now epsilon is 0.7383864750169357, the reward is 243.25 with loss [32.8251633644104, 23.941678047180176] in episode 172
Report: 
rewardSum:243.25
loss:[32.8251633644104, 23.941678047180176]
policies:[1, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2426
memory used:2812.0
now epsilon is 0.737279587311028, the reward is 245.25 with loss [35.010005474090576, 19.175796031951904] in episode 173
Report: 
rewardSum:245.25
loss:[35.010005474090576, 19.175796031951904]
policies:[1, 1, 4]
qAverage:[0.0, 91.36592102050781]
ws:[1.139657735824585, 1.7019844055175781]
memory len:2438
memory used:2812.0
now epsilon is 0.7356223661513334, the reward is 242.25 with loss [26.29694640636444, 41.0369678735733] in episode 174
Report: 
rewardSum:242.25
loss:[26.29694640636444, 41.0369678735733]
policies:[0, 4, 5]
qAverage:[0.0, 113.22705586751302]
ws:[2.836983799934387, 3.317218542098999]
memory len:2456
memory used:2812.0
now epsilon is 0.734519622018236, the reward is 245.25 with loss [30.898849725723267, 23.6754390001297] in episode 175
Report: 
rewardSum:245.25
loss:[30.898849725723267, 23.6754390001297]
policies:[1, 1, 4]
qAverage:[0.0, 86.86187744140625]
ws:[9.026348114013672, 11.007905960083008]
memory len:2468
memory used:2819.0
now epsilon is 0.7341524081147032, the reward is -1.0 with loss [18.118340492248535, 9.195237159729004] in episode 176
Report: 
rewardSum:-1.0
loss:[18.118340492248535, 9.195237159729004]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2472
memory used:2819.0
now epsilon is 0.7328686045741489, the reward is 34.15999999999997 with loss [36.01284623146057, 27.861920595169067] in episode 177
Report: 
rewardSum:34.15999999999997
loss:[36.01284623146057, 27.861920595169067]
policies:[1, 2, 4]
qAverage:[0.0, 113.45756530761719]
ws:[2.9550673166910806, 3.2332968711853027]
memory len:2486
memory used:2819.0
now epsilon is 0.7321360107495001, the reward is 247.25 with loss [23.756032705307007, 13.599676966667175] in episode 178
Report: 
rewardSum:247.25
loss:[23.756032705307007, 13.599676966667175]
policies:[0, 1, 3]
qAverage:[0.0, 95.16551208496094]
ws:[15.129341125488281, 15.184800148010254]
memory len:2494
memory used:2819.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46*		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.7304903510707698, the reward is 242.25 with loss [37.327292919158936, 33.773620545864105] in episode 179
Report: 
rewardSum:242.25
loss:[37.327292919158936, 33.773620545864105]
policies:[3, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2512
memory used:2819.0
now epsilon is 0.7301251515508813, the reward is -1.0 with loss [3.8722978830337524, 13.331128358840942] in episode 180
Report: 
rewardSum:-1.0
loss:[3.8722978830337524, 13.331128358840942]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2516
memory used:2819.0
now epsilon is 0.7297601346079279, the reward is -1.0 with loss [6.9596076011657715, 10.67311716079712] in episode 181
Report: 
rewardSum:-1.0
loss:[6.9596076011657715, 10.67311716079712]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2520
memory used:2819.0
now epsilon is 0.7284840117835529, the reward is 244.25 with loss [27.447468280792236, 25.96219539642334] in episode 182
Report: 
rewardSum:244.25
loss:[27.447468280792236, 25.96219539642334]
policies:[1, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2534
memory used:2819.0
now epsilon is 0.7259384568547219, the reward is 237.25 with loss [57.02254796028137, 67.80123031139374] in episode 183
Report: 
rewardSum:237.25
loss:[57.02254796028137, 67.80123031139374]
policies:[2, 2, 10]
qAverage:[0.0, 100.32157897949219]
ws:[12.381321907043457, 13.119590759277344]
memory len:2562
memory used:2820.0
now epsilon is 0.7243067277358898, the reward is -8.0 with loss [41.50707280635834, 28.33922243118286] in episode 184
Report: 
rewardSum:-8.0
loss:[41.50707280635834, 28.33922243118286]
policies:[0, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2580
memory used:2820.0
now epsilon is 0.7212345730611684, the reward is 24.159999999999968 with loss [75.03405499458313, 83.81109577417374] in episode 185
Report: 
rewardSum:24.159999999999968
loss:[75.03405499458313, 83.81109577417374]
policies:[2, 1, 14]
qAverage:[0.0, 88.18671417236328]
ws:[1.3872621059417725, 1.5658347606658936]
memory len:2614
memory used:2820.0
now epsilon is 0.7199733587843623, the reward is 244.25 with loss [31.854797035455704, 29.138181924819946] in episode 186
Report: 
rewardSum:244.25
loss:[31.854797035455704, 29.138181924819946]
policies:[2, 2, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2628
memory used:2819.0
now epsilon is 0.7190738419567494, the reward is -4.0 with loss [20.712642431259155, 29.403571248054504] in episode 187
Report: 
rewardSum:-4.0
loss:[20.712642431259155, 29.403571248054504]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2638
memory used:2819.0
now epsilon is 0.7183550377225442, the reward is 247.25 with loss [8.887153148651123, 13.640967011451721] in episode 188
Report: 
rewardSum:247.25
loss:[8.887153148651123, 13.640967011451721]
policies:[1, 2, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2646
memory used:2819.0
now epsilon is 0.7169195841400511, the reward is 243.25 with loss [50.112876415252686, 36.54752457141876] in episode 189
Report: 
rewardSum:243.25
loss:[50.112876415252686, 36.54752457141876]
policies:[0, 1, 7]
qAverage:[0.0, 73.60366821289062]
ws:[2.5097055435180664, 4.078499794006348]
memory len:2662
memory used:2820.0
now epsilon is 0.7158448766519558, the reward is 245.25 with loss [32.75949037075043, 35.853862285614014] in episode 190
Report: 
rewardSum:245.25
loss:[32.75949037075043, 35.853862285614014]
policies:[0, 2, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2674
memory used:2820.0
now epsilon is 0.7144144390010175, the reward is 33.15999999999997 with loss [30.423794984817505, 37.19177174568176] in episode 191
Report: 
rewardSum:33.15999999999997
loss:[30.423794984817505, 37.19177174568176]
policies:[1, 2, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2690
memory used:2821.0
now epsilon is 0.7129868597233667, the reward is 243.25 with loss [26.664718985557556, 42.06627798080444] in episode 192
Report: 
rewardSum:243.25
loss:[26.664718985557556, 42.06627798080444]
policies:[0, 2, 6]
qAverage:[0.0, 82.55186462402344]
ws:[1.5279546976089478, 2.0785653591156006]
memory len:2706
memory used:2821.0
now epsilon is 0.7126304108551839, the reward is -1.0 with loss [5.896644115447998, 5.825207471847534] in episode 193
Report: 
rewardSum:-1.0
loss:[5.896644115447998, 5.825207471847534]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2710
memory used:2821.0
############# STATE ###############
0-		10-		20-		30*		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.7101402534523524, the reward is 237.25 with loss [38.24588453769684, 53.10020899772644] in episode 194
Report: 
rewardSum:237.25
loss:[38.24588453769684, 53.10020899772644]
policies:[3, 2, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2738
memory used:2821.0
now epsilon is 0.7081898070325647, the reward is 30.159999999999968 with loss [57.52594554424286, 40.53523117303848] in episode 195
Report: 
rewardSum:30.159999999999968
loss:[57.52594554424286, 40.53523117303848]
policies:[0, 1, 10]
qAverage:[0.0, 77.1969223022461]
ws:[8.927667617797852, 10.28721809387207]
memory len:2760
memory used:2821.0
now epsilon is 0.7069514039821849, the reward is 244.25 with loss [28.592435359954834, 36.83814811706543] in episode 196
Report: 
rewardSum:244.25
loss:[28.592435359954834, 36.83814811706543]
policies:[4, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2774
memory used:2821.0
now epsilon is 0.7062447176407978, the reward is 247.25 with loss [20.901861906051636, 19.712266445159912] in episode 197
Report: 
rewardSum:247.25
loss:[20.901861906051636, 19.712266445159912]
policies:[2, 0, 2]
qAverage:[83.54682159423828, 0.0]
ws:[3.27502179145813, 3.21437406539917]
memory len:2782
memory used:2821.0
now epsilon is 0.7044810908363346, the reward is 241.25 with loss [28.034465670585632, 55.62181758880615] in episode 198
Report: 
rewardSum:241.25
loss:[28.034465670585632, 55.62181758880615]
policies:[2, 2, 6]
qAverage:[0.0, 79.30724334716797]
ws:[11.708096504211426, 13.727387428283691]
memory len:2802
memory used:2821.0
now epsilon is 0.70377687388188, the reward is 247.25 with loss [19.736290097236633, 13.813871264457703] in episode 199
Report: 
rewardSum:247.25
loss:[19.736290097236633, 13.813871264457703]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2810
memory used:2820.0
now epsilon is 0.7028975925401225, the reward is -4.0 with loss [19.26166981458664, 29.61778211593628] in episode 200
Report: 
rewardSum:-4.0
loss:[19.26166981458664, 29.61778211593628]
policies:[0, 2, 3]
qAverage:[0.0, 69.59903717041016]
ws:[0.6802260875701904, 0.8877318501472473]
memory len:2820
memory used:2820.0
now epsilon is 0.7023705511280333, the reward is -2.0 with loss [10.035605430603027, 19.28739094734192] in episode 201
Report: 
rewardSum:-2.0
loss:[10.035605430603027, 19.28739094734192]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2826
memory used:2820.0
now epsilon is 0.701843904898191, the reward is -2.0 with loss [25.041263818740845, 14.157328009605408] in episode 202
Report: 
rewardSum:-2.0
loss:[25.041263818740845, 14.157328009605408]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2832
memory used:2820.0
now epsilon is 0.7009670385598595, the reward is -4.0 with loss [16.71943712234497, 18.47583317756653] in episode 203
Report: 
rewardSum:-4.0
loss:[16.71943712234497, 18.47583317756653]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2842
memory used:2820.0
now epsilon is 0.6993914389792624, the reward is -8.0 with loss [25.916013479232788, 61.86042618751526] in episode 204
Report: 
rewardSum:-8.0
loss:[25.916013479232788, 61.86042618751526]
policies:[1, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2860
memory used:2820.0
now epsilon is 0.6986923097683636, the reward is 37.15999999999997 with loss [6.723317980766296, 25.723701000213623] in episode 205
Report: 
rewardSum:37.15999999999997
loss:[6.723317980766296, 25.723701000213623]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2868
memory used:2820.0
now epsilon is 0.6976449261094512, the reward is 245.25 with loss [32.517879128456116, 17.155250310897827] in episode 206
Report: 
rewardSum:245.25
loss:[32.517879128456116, 17.155250310897827]
policies:[1, 3, 2]
qAverage:[0.0, 74.07324981689453]
ws:[1.2865872383117676, 1.6882985830307007]
memory len:2880
memory used:2820.0
now epsilon is 0.6965991125444321, the reward is -5.0 with loss [22.6952805519104, 12.607066333293915] in episode 207
Report: 
rewardSum:-5.0
loss:[22.6952805519104, 12.607066333293915]
policies:[0, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2892
memory used:2821.0
now epsilon is 0.6952071327584566, the reward is 243.25 with loss [17.399763196706772, 25.996841073036194] in episode 208
Report: 
rewardSum:243.25
loss:[17.399763196706772, 25.996841073036194]
policies:[2, 2, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2908
memory used:2822.0
now epsilon is 0.6939914323553948, the reward is 244.25 with loss [33.020753145217896, 36.71262347698212] in episode 209
Report: 
rewardSum:244.25
loss:[33.020753145217896, 36.71262347698212]
policies:[4, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2922
memory used:2822.0
############# STATE ###############
0-		10-		20-		30*		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.692604663368638, the reward is 243.25 with loss [39.09072935581207, 35.9424684047699] in episode 210
Report: 
rewardSum:243.25
loss:[39.09072935581207, 35.9424684047699]
policies:[3, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2938
memory used:2821.0
now epsilon is 0.6915664054740587, the reward is 35.15999999999997 with loss [26.52756369113922, 30.223248183727264] in episode 211
Report: 
rewardSum:35.15999999999997
loss:[26.52756369113922, 30.223248183727264]
policies:[2, 1, 3]
qAverage:[75.05921936035156, 0.0]
ws:[1.282268762588501, 0.9434130787849426]
memory len:2950
memory used:2820.0
now epsilon is 0.690011936178814, the reward is -8.0 with loss [40.38803243637085, 27.658184826374054] in episode 212
Report: 
rewardSum:-8.0
loss:[40.38803243637085, 27.658184826374054]
policies:[2, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2968
memory used:2820.0
now epsilon is 0.6889775649451476, the reward is 245.25 with loss [34.59726643562317, 23.891446113586426] in episode 213
Report: 
rewardSum:245.25
loss:[34.59726643562317, 23.891446113586426]
policies:[2, 2, 2]
qAverage:[49.31331888834635, 46.956390380859375]
ws:[1.8796896934509277, 1.7641839186350505]
memory len:2980
memory used:2820.0
now epsilon is 0.6874289147195983, the reward is 32.15999999999997 with loss [38.07114523649216, 34.572038531303406] in episode 214
Report: 
rewardSum:32.15999999999997
loss:[38.07114523649216, 34.572038531303406]
policies:[2, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2998
memory used:2820.0
now epsilon is 0.6857122745382566, the reward is 241.25 with loss [36.737249314785004, 40.66569244861603] in episode 215
Report: 
rewardSum:241.25
loss:[36.737249314785004, 40.66569244861603]
policies:[1, 3, 6]
qAverage:[0.0, 63.22722244262695]
ws:[2.5491268634796143, 2.9790704250335693]
memory len:3018
memory used:2819.0
now epsilon is 0.6845131776802698, the reward is 244.25 with loss [46.63188576698303, 38.537750244140625] in episode 216
Report: 
rewardSum:244.25
loss:[46.63188576698303, 38.537750244140625]
policies:[2, 2, 3]
qAverage:[0.0, 64.95316314697266]
ws:[1.0178196430206299, 1.2063814401626587]
memory len:3032
memory used:2819.0
now epsilon is 0.6838289211522518, the reward is 247.25 with loss [21.537479400634766, 23.14967107772827] in episode 217
Report: 
rewardSum:247.25
loss:[21.537479400634766, 23.14967107772827]
policies:[2, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3040
memory used:2819.0
now epsilon is 0.6828038186464805, the reward is 35.15999999999997 with loss [30.588196277618408, 27.201964855194092] in episode 218
Report: 
rewardSum:35.15999999999997
loss:[30.588196277618408, 27.201964855194092]
policies:[0, 2, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3052
memory used:2819.0
now epsilon is 0.6814394053186036, the reward is -7.0 with loss [24.76664650440216, 19.44120967388153] in episode 219
Report: 
rewardSum:-7.0
loss:[24.76664650440216, 19.44120967388153]
policies:[1, 1, 6]
qAverage:[0.0, 60.69586181640625]
ws:[8.493258476257324, 9.159501075744629]
memory len:3068
memory used:2819.0
now epsilon is 0.6804178848471583, the reward is 245.25 with loss [18.932923793792725, 31.86375105381012] in episode 220
Report: 
rewardSum:245.25
loss:[18.932923793792725, 31.86375105381012]
policies:[1, 0, 5]
qAverage:[76.90201568603516, 0.0]
ws:[8.059732437133789, 7.622641563415527]
memory len:3080
memory used:2819.0
now epsilon is 0.6788884746537796, the reward is 242.25 with loss [25.57432746887207, 51.3071905374527] in episode 221
Report: 
rewardSum:242.25
loss:[25.57432746887207, 51.3071905374527]
policies:[2, 0, 7]
qAverage:[83.91799672444661, 0.0]
ws:[1.1685328880945842, 1.1015851497650146]
memory len:3098
memory used:2819.0
now epsilon is 0.6777013104930844, the reward is 244.25 with loss [39.9170446395874, 31.773045301437378] in episode 222
Report: 
rewardSum:244.25
loss:[39.9170446395874, 31.773045301437378]
policies:[1, 0, 6]
qAverage:[59.87028503417969, 0.0]
ws:[1.3500661849975586, 1.2258213758468628]
memory len:3112
memory used:2820.0
now epsilon is 0.6770238632782292, the reward is 37.15999999999997 with loss [11.986413359642029, 17.0243558883667] in episode 223
Report: 
rewardSum:37.15999999999997
loss:[11.986413359642029, 17.0243558883667]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3120
memory used:2820.0
now epsilon is 0.6763470932565885, the reward is 247.25 with loss [17.46518635749817, 9.380574703216553] in episode 224
Report: 
rewardSum:247.25
loss:[17.46518635749817, 9.380574703216553]
policies:[2, 2, 0]
qAverage:[0.0, 59.8339958190918]
ws:[0.875304639339447, 0.889937162399292]
memory len:3128
memory used:2819.0
############# STATE ###############
0-		10*		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.6741522593750465, the reward is 238.25 with loss [58.62632292509079, 56.57577919960022] in episode 225
Report: 
rewardSum:238.25
loss:[58.62632292509079, 56.57577919960022]
policies:[2, 1, 10]
qAverage:[63.476213455200195, 28.45927619934082]
ws:[3.8198274075984955, 3.5124784745275974]
memory len:3154
memory used:2819.0
now epsilon is 0.6731416627930941, the reward is -5.0 with loss [35.29479265213013, 20.961228132247925] in episode 226
Report: 
rewardSum:-5.0
loss:[35.29479265213013, 20.961228132247925]
policies:[1, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3166
memory used:2819.0
now epsilon is 0.671628607737384, the reward is 32.15999999999997 with loss [53.15121412277222, 28.919248431921005] in episode 227
Report: 
rewardSum:32.15999999999997
loss:[53.15121412277222, 28.919248431921005]
policies:[0, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3184
memory used:2819.0
now epsilon is 0.6696164900747539, the reward is 29.159999999999968 with loss [36.278175950050354, 54.95448410511017] in episode 228
Report: 
rewardSum:29.159999999999968
loss:[36.278175950050354, 54.95448410511017]
policies:[0, 2, 10]
qAverage:[0.0, 56.59865188598633]
ws:[1.0381321907043457, 1.2685703039169312]
memory len:3208
memory used:2819.0
now epsilon is 0.6679443308909636, the reward is 241.25 with loss [41.97154772281647, 39.40116631984711] in episode 229
Report: 
rewardSum:241.25
loss:[41.97154772281647, 39.40116631984711]
policies:[1, 1, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3228
memory used:2819.0
now epsilon is 0.6669430403837441, the reward is 245.25 with loss [17.555158257484436, 14.869036257266998] in episode 230
Report: 
rewardSum:245.25
loss:[17.555158257484436, 14.869036257266998]
policies:[3, 0, 3]
qAverage:[56.968868255615234, 0.0]
ws:[-0.0349867008626461, -0.48556947708129883]
memory len:3240
memory used:2819.0
now epsilon is 0.6664429581448554, the reward is -2.0 with loss [22.111576318740845, 17.548441648483276] in episode 231
Report: 
rewardSum:-2.0
loss:[22.111576318740845, 17.548441648483276]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3246
memory used:2819.0
now epsilon is 0.6656103208699046, the reward is 246.25 with loss [19.2577805519104, 24.39381194114685] in episode 232
Report: 
rewardSum:246.25
loss:[19.2577805519104, 24.39381194114685]
policies:[0, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3256
memory used:2819.0
now epsilon is 0.6649449601113069, the reward is 247.25 with loss [21.11014747619629, 17.538676738739014] in episode 233
Report: 
rewardSum:247.25
loss:[21.11014747619629, 17.538676738739014]
policies:[1, 1, 2]
qAverage:[68.86105346679688, 0.0]
ws:[4.023697376251221, 3.996896505355835]
memory len:3264
memory used:2819.0
now epsilon is 0.6639481658492838, the reward is 245.25 with loss [14.121465414762497, 22.790253281593323] in episode 234
Report: 
rewardSum:245.25
loss:[14.121465414762497, 22.790253281593323]
policies:[0, 2, 4]
qAverage:[0.0, 52.05278396606445]
ws:[7.531747341156006, 8.458995819091797]
memory len:3276
memory used:2819.0
now epsilon is 0.6629528658444706, the reward is 245.25 with loss [18.777592539787292, 33.43104553222656] in episode 235
Report: 
rewardSum:245.25
loss:[18.777592539787292, 33.43104553222656]
policies:[2, 0, 4]
qAverage:[83.6697260538737, 0.0]
ws:[4.062600165605545, 3.758234073718389]
memory len:3288
memory used:2819.0
now epsilon is 0.6609667396498898, the reward is 239.25 with loss [48.268349170684814, 52.67419937252998] in episode 236
Report: 
rewardSum:239.25
loss:[48.268349170684814, 52.67419937252998]
policies:[2, 2, 8]
qAverage:[0.0, 51.78577423095703]
ws:[8.276498794555664, 8.436674118041992]
memory len:3312
memory used:2820.0
now epsilon is 0.6601409442262767, the reward is -4.0 with loss [12.980286955833435, 24.422133207321167] in episode 237
Report: 
rewardSum:-4.0
loss:[12.980286955833435, 24.422133207321167]
policies:[0, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3322
memory used:2820.0
now epsilon is 0.6596459622842192, the reward is -2.0 with loss [22.79249119758606, 10.356948614120483] in episode 238
Report: 
rewardSum:-2.0
loss:[22.79249119758606, 10.356948614120483]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3328
memory used:2832.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43*		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.6584924472748938, the reward is 244.25 with loss [24.85893476009369, 23.315960437059402] in episode 239
Report: 
rewardSum:244.25
loss:[24.85893476009369, 23.315960437059402]
policies:[0, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3342
memory used:2832.0
now epsilon is 0.6575053257349105, the reward is 245.25 with loss [17.14333701133728, 27.560941696166992] in episode 240
Report: 
rewardSum:245.25
loss:[17.14333701133728, 27.560941696166992]
policies:[4, 0, 2]
qAverage:[52.491371154785156, 0.0]
ws:[-0.2053787112236023, -0.5005585551261902]
memory len:3354
memory used:2832.0
now epsilon is 0.6563555540311313, the reward is -6.0 with loss [23.852839946746826, 31.781684160232544] in episode 241
Report: 
rewardSum:-6.0
loss:[23.852839946746826, 31.781684160232544]
policies:[0, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3368
memory used:2832.0
now epsilon is 0.6540620388837792, the reward is 237.25 with loss [63.46523869037628, 61.605573281645775] in episode 242
Report: 
rewardSum:237.25
loss:[63.46523869037628, 61.605573281645775]
policies:[0, 3, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3396
memory used:2832.0
now epsilon is 0.6532448700217645, the reward is 246.25 with loss [20.61789608001709, 27.484325408935547] in episode 243
Report: 
rewardSum:246.25
loss:[20.61789608001709, 27.484325408935547]
policies:[0, 2, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3406
memory used:2832.0
now epsilon is 0.6529182884145579, the reward is -1.0 with loss [5.726189136505127, 16.39349365234375] in episode 244
Report: 
rewardSum:-1.0
loss:[5.726189136505127, 16.39349365234375]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3410
memory used:2832.0
now epsilon is 0.6514506904751404, the reward is 242.25 with loss [32.0257922410965, 43.476444125175476] in episode 245
Report: 
rewardSum:242.25
loss:[32.0257922410965, 43.476444125175476]
policies:[2, 1, 6]
qAverage:[58.9405632019043, 0.0]
ws:[-0.2614079713821411, -0.875251293182373]
memory len:3428
memory used:2831.0
now epsilon is 0.6499863913309165, the reward is 242.25 with loss [35.87819051742554, 30.683229446411133] in episode 246
Report: 
rewardSum:242.25
loss:[35.87819051742554, 30.683229446411133]
policies:[0, 4, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3446
memory used:2831.0
now epsilon is 0.6485253835670153, the reward is 242.25 with loss [37.54605281352997, 37.94096124172211] in episode 247
Report: 
rewardSum:242.25
loss:[37.54605281352997, 37.94096124172211]
policies:[1, 2, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3464
memory used:2831.0
now epsilon is 0.646905892870286, the reward is 241.25 with loss [38.64922285079956, 25.523734480142593] in episode 248
Report: 
rewardSum:241.25
loss:[38.64922285079956, 25.523734480142593]
policies:[2, 1, 7]
qAverage:[84.87965393066406, 0.0]
ws:[7.156917095184326, 5.097859541575114]
memory len:3484
memory used:2831.0
now epsilon is 0.6459361403031352, the reward is 35.15999999999997 with loss [12.96776333451271, 13.681660622358322] in episode 249
Report: 
rewardSum:35.15999999999997
loss:[12.96776333451271, 13.681660622358322]
policies:[2, 0, 4]
qAverage:[55.92794418334961, 0.0]
ws:[0.9821755886077881, 0.10021618753671646]
memory len:3496
memory used:2831.0
now epsilon is 0.6440009941496478, the reward is 239.25 with loss [42.08266282081604, 30.4098120033741] in episode 250
Report: 
rewardSum:239.25
loss:[42.08266282081604, 30.4098120033741]
policies:[4, 1, 7]
qAverage:[73.38566589355469, 0.0]
ws:[2.092711885770162, 1.281659225622813]
memory len:3520
memory used:2833.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26*		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.6423928017100963, the reward is 241.25 with loss [36.157895147800446, 28.242420613765717] in episode 251
Report: 
rewardSum:241.25
loss:[36.157895147800446, 28.242420613765717]
policies:[4, 1, 5]
qAverage:[70.0157470703125, 0.0]
ws:[1.1498375336329143, 0.13056192795435587]
memory len:3540
memory used:2833.0
############# STATE ###############
0*		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.6412694570964351, the reward is 244.25 with loss [44.19372022151947, 40.299487233161926] in episode 252
Report: 
rewardSum:244.25
loss:[44.19372022151947, 40.299487233161926]
policies:[2, 1, 4]
qAverage:[53.74903106689453, 0.0]
ws:[1.1434069871902466, 0.26690739393234253]
memory len:3554
memory used:2833.0
now epsilon is 0.6403081539005474, the reward is 245.25 with loss [22.209041714668274, 31.212734818458557] in episode 253
Report: 
rewardSum:245.25
loss:[22.209041714668274, 31.212734818458557]
policies:[3, 1, 2]
qAverage:[80.70827229817708, 0.0]
ws:[9.181081453959147, 7.537340799967448]
memory len:3566
memory used:2838.0
now epsilon is 0.6396680858221878, the reward is 247.25 with loss [19.328519582748413, 7.88454270362854] in episode 254
Report: 
rewardSum:247.25
loss:[19.328519582748413, 7.88454270362854]
policies:[1, 3, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3574
memory used:2838.0
now epsilon is 0.6387091831824261, the reward is 35.15999999999997 with loss [22.540115237236023, 29.878202438354492] in episode 255
Report: 
rewardSum:35.15999999999997
loss:[22.540115237236023, 29.878202438354492]
policies:[1, 0, 5]
qAverage:[55.56399917602539, 0.0]
ws:[2.308591842651367, 1.316117525100708]
memory len:3586
memory used:2839.0
now epsilon is 0.6377517179979526, the reward is 245.25 with loss [16.960289120674133, 19.917492985725403] in episode 256
Report: 
rewardSum:245.25
loss:[16.960289120674133, 19.917492985725403]
policies:[2, 0, 4]
qAverage:[71.90209452311198, 0.0]
ws:[1.5629771550496419, 0.4574550787607829]
memory len:3598
memory used:2839.0
now epsilon is 0.6371142053969919, the reward is 247.25 with loss [19.182772874832153, 12.601110696792603] in episode 257
Report: 
rewardSum:247.25
loss:[19.182772874832153, 12.601110696792603]
policies:[3, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3606
memory used:2839.0
now epsilon is 0.6348879256440491, the reward is 237.25 with loss [56.1104382276535, 40.550768584012985] in episode 258
Report: 
rewardSum:237.25
loss:[56.1104382276535, 40.550768584012985]
policies:[2, 2, 10]
qAverage:[51.998870849609375, 0.0]
ws:[-0.4961284399032593, -1.004501223564148]
memory len:3634
memory used:2844.0
now epsilon is 0.633460855476205, the reward is 242.25 with loss [31.315393030643463, 42.49700903892517] in episode 259
Report: 
rewardSum:242.25
loss:[31.315393030643463, 42.49700903892517]
policies:[3, 0, 6]
qAverage:[64.3386001586914, 0.0]
ws:[1.4465365012486775, 0.6742551426092783]
memory len:3652
memory used:2844.0
now epsilon is 0.6321950417676446, the reward is 243.25 with loss [27.998063325881958, 43.07633864879608] in episode 260
Report: 
rewardSum:243.25
loss:[27.998063325881958, 43.07633864879608]
policies:[2, 0, 6]
qAverage:[65.28646087646484, 0.0]
ws:[3.532069444656372, 2.066606044769287]
memory len:3668
memory used:2844.0
now epsilon is 0.6307740245330667, the reward is 32.15999999999997 with loss [27.534765243530273, 33.96307098865509] in episode 261
Report: 
rewardSum:32.15999999999997
loss:[27.534765243530273, 33.96307098865509]
policies:[1, 1, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3686
memory used:2844.0
now epsilon is 0.6304586769441768, the reward is -1.0 with loss [8.720054626464844, 5.238399982452393] in episode 262
Report: 
rewardSum:-1.0
loss:[8.720054626464844, 5.238399982452393]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3690
memory used:2844.0
now epsilon is 0.6290415626259088, the reward is 242.25 with loss [41.05165082216263, 27.744868457317352] in episode 263
Report: 
rewardSum:242.25
loss:[41.05165082216263, 27.744868457317352]
policies:[5, 0, 4]
qAverage:[89.11044921875, 0.0]
ws:[3.798965835571289, 2.5566611528396606]
memory len:3708
memory used:2844.0
now epsilon is 0.6279415651644435, the reward is 244.25 with loss [30.05667471885681, 18.413115918636322] in episode 264
Report: 
rewardSum:244.25
loss:[30.05667471885681, 18.413115918636322]
policies:[1, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3722
memory used:2850.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23*		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.6260603285705713, the reward is 239.25 with loss [48.75218451023102, 41.55476075410843] in episode 265
Report: 
rewardSum:239.25
loss:[48.75218451023102, 41.55476075410843]
policies:[3, 0, 9]
qAverage:[71.73935445149739, 0.0]
ws:[2.1024245421091714, 1.2079124848047893]
memory len:3746
memory used:2852.0
now epsilon is 0.6241847279328426, the reward is 239.25 with loss [29.88310480117798, 51.74054718017578] in episode 266
Report: 
rewardSum:239.25
loss:[29.88310480117798, 51.74054718017578]
policies:[2, 2, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3770
memory used:2851.0
now epsilon is 0.6235607772351738, the reward is 247.25 with loss [13.774328470230103, 18.775044858455658] in episode 267
Report: 
rewardSum:247.25
loss:[13.774328470230103, 18.775044858455658]
policies:[2, 2, 0]
qAverage:[58.74345779418945, 0.0]
ws:[2.601426601409912, 1.602944254875183]
memory len:3778
memory used:2851.0
now epsilon is 0.6229374502542601, the reward is 247.25 with loss [9.65235698223114, 21.55118429660797] in episode 268
Report: 
rewardSum:247.25
loss:[9.65235698223114, 21.55118429660797]
policies:[0, 2, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3786
memory used:2851.0
now epsilon is 0.6213818574727067, the reward is 241.25 with loss [26.594892024993896, 37.063035905361176] in episode 269
Report: 
rewardSum:241.25
loss:[26.594892024993896, 37.063035905361176]
policies:[5, 2, 3]
qAverage:[83.05579833984375, 0.0]
ws:[2.8738815009593965, 1.883653163909912]
memory len:3806
memory used:2851.0
now epsilon is 0.6204503670378437, the reward is 35.15999999999997 with loss [22.54356813430786, 39.532944202423096] in episode 270
Report: 
rewardSum:35.15999999999997
loss:[22.54356813430786, 39.532944202423096]
policies:[2, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3818
memory used:2851.0
now epsilon is 0.6196751917635926, the reward is 246.25 with loss [28.899396419525146, 43.75774884223938] in episode 271
Report: 
rewardSum:246.25
loss:[28.899396419525146, 43.75774884223938]
policies:[1, 2, 2]
qAverage:[63.345455169677734, 0.0]
ws:[4.20804500579834, 1.4134280681610107]
memory len:3828
memory used:2851.0
now epsilon is 0.6185915731628957, the reward is 244.25 with loss [30.46877110004425, 26.20908808708191] in episode 272
Report: 
rewardSum:244.25
loss:[30.46877110004425, 26.20908808708191]
policies:[2, 2, 3]
qAverage:[76.97657267252605, 0.0]
ws:[2.2876216570536294, 0.8205633461475372]
memory len:3842
memory used:2851.0
now epsilon is 0.6179732135229132, the reward is 37.15999999999997 with loss [17.430176973342896, 22.481469184160233] in episode 273
Report: 
rewardSum:37.15999999999997
loss:[17.430176973342896, 22.481469184160233]
policies:[1, 0, 3]
qAverage:[52.66432189941406, 0.0]
ws:[1.2229083776474, 0.21203821897506714]
memory len:3850
memory used:2851.0
now epsilon is 0.6173554720107246, the reward is 37.15999999999997 with loss [17.851806163787842, 10.901589393615723] in episode 274
Report: 
rewardSum:37.15999999999997
loss:[17.851806163787842, 10.901589393615723]
policies:[1, 1, 2]
qAverage:[52.45116424560547, 0.0]
ws:[0.44731128215789795, -1.3029965162277222]
memory len:3858
memory used:2851.0
now epsilon is 0.6156598650313063, the reward is 240.25 with loss [54.51740998029709, 58.76767164468765] in episode 275
Report: 
rewardSum:240.25
loss:[54.51740998029709, 58.76767164468765]
policies:[4, 1, 6]
qAverage:[72.81503295898438, 0.0]
ws:[1.924347698688507, 1.5042318999767303]
memory len:3880
memory used:2851.0
now epsilon is 0.6150444360002482, the reward is -3.0 with loss [13.193550109863281, 18.31388020515442] in episode 276
Report: 
rewardSum:-3.0
loss:[13.193550109863281, 18.31388020515442]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3888
memory used:2852.0
now epsilon is 0.6141224457582414, the reward is 245.25 with loss [36.30028337240219, 27.468561351299286] in episode 277
Report: 
rewardSum:245.25
loss:[36.30028337240219, 27.468561351299286]
policies:[3, 2, 1]
qAverage:[85.80198097229004, 0.0]
ws:[4.447064846754074, 2.609503038227558]
memory len:3900
memory used:2852.0
now epsilon is 0.612742051225055, the reward is 242.25 with loss [50.13893222808838, 37.594420194625854] in episode 278
Report: 
rewardSum:242.25
loss:[50.13893222808838, 37.594420194625854]
policies:[2, 2, 5]
qAverage:[60.13408660888672, 20.120899200439453]
ws:[5.912600338459015, 4.193347811698914]
memory len:3918
memory used:2853.0
now epsilon is 0.6119765065290768, the reward is -4.0 with loss [25.542853355407715, 14.730308532714844] in episode 279
Report: 
rewardSum:-4.0
loss:[25.542853355407715, 14.730308532714844]
policies:[0, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3928
memory used:2852.0
now epsilon is 0.6115176388852128, the reward is -2.0 with loss [12.42201542854309, 16.356923818588257] in episode 280
Report: 
rewardSum:-2.0
loss:[12.42201542854309, 16.356923818588257]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3934
memory used:2852.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34*		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.6099905635352653, the reward is 31.159999999999968 with loss [33.10421359539032, 43.43258519470692] in episode 281
Report: 
rewardSum:31.159999999999968
loss:[33.10421359539032, 43.43258519470692]
policies:[3, 3, 4]
qAverage:[71.09806060791016, 0.0]
ws:[1.9359676043192546, 0.901815781990687]
memory len:3954
memory used:2852.0
now epsilon is 0.6090761493655297, the reward is 245.25 with loss [19.296189546585083, 22.71227192878723] in episode 282
Report: 
rewardSum:245.25
loss:[19.296189546585083, 22.71227192878723]
policies:[1, 3, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3966
memory used:2852.0
now epsilon is 0.607859062417285, the reward is 243.25 with loss [33.1276898086071, 24.930124640464783] in episode 283
Report: 
rewardSum:243.25
loss:[33.1276898086071, 24.930124640464783]
policies:[1, 2, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3982
memory used:2852.0
now epsilon is 0.6070996184062114, the reward is 246.25 with loss [18.249760150909424, 27.732437133789062] in episode 284
Report: 
rewardSum:246.25
loss:[18.249760150909424, 27.732437133789062]
policies:[1, 0, 4]
qAverage:[59.2440071105957, 0.0]
ws:[4.629001617431641, 3.6456568241119385]
memory len:3992
memory used:2862.0
now epsilon is 0.6060379905603254, the reward is 244.25 with loss [33.024080872535706, 21.254074931144714] in episode 285
Report: 
rewardSum:244.25
loss:[33.024080872535706, 21.254074931144714]
policies:[1, 1, 5]
qAverage:[68.15432739257812, 0.0]
ws:[6.268476963043213, 3.005060911178589]
memory len:4006
memory used:2862.0
now epsilon is 0.6045245989299489, the reward is 241.25 with loss [33.42777378857136, 43.4361669421196] in episode 286
Report: 
rewardSum:241.25
loss:[33.42777378857136, 43.4361669421196]
policies:[2, 2, 6]
qAverage:[52.2650146484375, 0.0]
ws:[1.7006425857543945, 0.8233748078346252]
memory len:4026
memory used:2862.0
now epsilon is 0.6033166071213436, the reward is 243.25 with loss [32.38199806213379, 21.408463418483734] in episode 287
Report: 
rewardSum:243.25
loss:[32.38199806213379, 21.408463418483734]
policies:[5, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4042
memory used:2862.0
now epsilon is 0.6024121976314798, the reward is 245.25 with loss [25.798654794692993, 19.534260153770447] in episode 288
Report: 
rewardSum:245.25
loss:[25.798654794692993, 19.534260153770447]
policies:[1, 2, 3]
qAverage:[61.202659606933594, 0.0]
ws:[2.9737935066223145, 2.1699378490448]
memory len:4054
memory used:2863.0
now epsilon is 0.6016595587979489, the reward is 246.25 with loss [27.53581440448761, 14.654964447021484] in episode 289
Report: 
rewardSum:246.25
loss:[27.53581440448761, 14.654964447021484]
policies:[1, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4064
memory used:2863.0
now epsilon is 0.6007576333276051, the reward is 245.25 with loss [16.68198537826538, 11.001633062958717] in episode 290
Report: 
rewardSum:245.25
loss:[16.68198537826538, 11.001633062958717]
policies:[2, 0, 4]
qAverage:[70.42921447753906, 0.0]
ws:[7.845058441162109, 5.349974632263184]
memory len:4076
memory used:2863.0
now epsilon is 0.5998570599001936, the reward is 245.25 with loss [20.793278217315674, 20.339324414730072] in episode 291
Report: 
rewardSum:245.25
loss:[20.793278217315674, 20.339324414730072]
policies:[2, 1, 3]
qAverage:[68.38211059570312, 0.0]
ws:[8.048872947692871, 5.9139299392700195]
memory len:4088
memory used:2863.0
now epsilon is 0.5983591032241842, the reward is 241.25 with loss [35.836991138756275, 48.007323265075684] in episode 292
Report: 
rewardSum:241.25
loss:[35.836991138756275, 48.007323265075684]
policies:[3, 1, 6]
qAverage:[64.21674346923828, 0.0]
ws:[2.7516307830810547, 1.689598798751831]
memory len:4108
memory used:2863.0
now epsilon is 0.5968648872296688, the reward is 241.25 with loss [42.73111605644226, 31.379496335983276] in episode 293
Report: 
rewardSum:241.25
loss:[42.73111605644226, 31.379496335983276]
policies:[2, 0, 8]
qAverage:[56.089744567871094, 0.0]
ws:[6.9824700355529785, 6.585320472717285]
memory len:4128
memory used:2864.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46*		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.5949279833969243, the reward is 627.1600000000001 with loss [53.44692814350128, 64.43853735923767] in episode 294
Report: 
rewardSum:627.1600000000001
loss:[53.44692814350128, 64.43853735923767]
policies:[3, 1, 9]
qAverage:[73.7031478881836, 0.0]
ws:[1.409424622853597, 0.1163023014863332]
memory len:4154
memory used:2864.0
now epsilon is 0.5935907332416938, the reward is 242.25 with loss [38.07389569282532, 34.75583100318909] in episode 295
Report: 
rewardSum:242.25
loss:[38.07389569282532, 34.75583100318909]
policies:[4, 1, 4]
qAverage:[92.82950134277344, 0.0]
ws:[6.285510921478272, 5.8483827114105225]
memory len:4172
memory used:2863.0
now epsilon is 0.5927009034476813, the reward is 35.15999999999997 with loss [14.583058595657349, 20.44320398569107] in episode 296
Report: 
rewardSum:35.15999999999997
loss:[14.583058595657349, 20.44320398569107]
policies:[2, 0, 4]
qAverage:[72.80484771728516, 0.0]
ws:[2.320971449216207, 1.3217867116133373]
memory len:4184
memory used:2857.0
now epsilon is 0.5921084247700311, the reward is -3.0 with loss [18.854713201522827, 19.54946517944336] in episode 297
Report: 
rewardSum:-3.0
loss:[18.854713201522827, 19.54946517944336]
policies:[1, 0, 3]
qAverage:[55.265872955322266, 0.0]
ws:[7.4725751876831055, 7.104804515838623]
memory len:4192
memory used:2857.0
now epsilon is 0.5918124075644228, the reward is -1.0 with loss [4.908447027206421, 2.445826679468155] in episode 298
Report: 
rewardSum:-1.0
loss:[4.908447027206421, 2.445826679468155]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4196
memory used:2857.0
now epsilon is 0.5906298179033331, the reward is 243.25 with loss [34.02891552448273, 19.026886582374573] in episode 299
Report: 
rewardSum:243.25
loss:[34.02891552448273, 19.026886582374573]
policies:[2, 0, 6]
qAverage:[85.1155776977539, 0.0]
ws:[6.605862855911255, 3.8826745748519897]
memory len:4212
memory used:2857.0
now epsilon is 0.5901869562737679, the reward is -2.0 with loss [4.953024208545685, 9.55777883529663] in episode 300
Report: 
rewardSum:-2.0
loss:[4.953024208545685, 9.55777883529663]
policies:[0, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4218
memory used:2857.0
now epsilon is 0.5894495913530683, the reward is 246.25 with loss [29.418859004974365, 27.219480991363525] in episode 301
Report: 
rewardSum:246.25
loss:[29.418859004974365, 27.219480991363525]
policies:[2, 1, 2]
qAverage:[79.7442143758138, 0.0]
ws:[6.953324635823567, 6.5220866203308105]
memory len:4228
memory used:2858.0
now epsilon is 0.5890076146721417, the reward is -2.0 with loss [19.231142103672028, 16.244411945343018] in episode 302
Report: 
rewardSum:-2.0
loss:[19.231142103672028, 16.244411945343018]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4234
memory used:2858.0
now epsilon is 0.5881246552607418, the reward is 35.15999999999997 with loss [15.480303347110748, 30.915653228759766] in episode 303
Report: 
rewardSum:35.15999999999997
loss:[15.480303347110748, 30.915653228759766]
policies:[1, 2, 3]
qAverage:[49.62842559814453, 0.0]
ws:[0.968652069568634, 0.5314991474151611]
memory len:4246
memory used:2857.0
now epsilon is 0.5875367511154713, the reward is 247.25 with loss [16.107659459114075, 22.59626007080078] in episode 304
Report: 
rewardSum:247.25
loss:[16.107659459114075, 22.59626007080078]
policies:[1, 2, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4254
memory used:2858.0
now epsilon is 0.5869494346539188, the reward is 247.25 with loss [12.066116452217102, 13.330600023269653] in episode 305
Report: 
rewardSum:247.25
loss:[12.066116452217102, 13.330600023269653]
policies:[1, 2, 1]
qAverage:[73.35871124267578, 0.0]
ws:[9.359827995300293, 6.266811847686768]
memory len:4262
memory used:2858.0
now epsilon is 0.5866559966209316, the reward is -1.0 with loss [8.468149185180664, 5.449025630950928] in episode 306
Report: 
rewardSum:-1.0
loss:[8.468149185180664, 5.449025630950928]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4266
memory used:2858.0
now epsilon is 0.5854837107625204, the reward is 243.25 with loss [32.63917565345764, 49.012048959732056] in episode 307
Report: 
rewardSum:243.25
loss:[32.63917565345764, 49.012048959732056]
policies:[2, 1, 5]
qAverage:[74.13339233398438, 0.0]
ws:[8.371030807495117, 4.410916328430176]
memory len:4282
memory used:2858.0
now epsilon is 0.5847522219599162, the reward is 36.15999999999997 with loss [19.72066593170166, 24.197704315185547] in episode 308
Report: 
rewardSum:36.15999999999997
loss:[19.72066593170166, 24.197704315185547]
policies:[2, 1, 2]
qAverage:[62.579490661621094, 0.0]
ws:[4.160059928894043, 3.599264144897461]
memory len:4292
memory used:2858.0
now epsilon is 0.5837296727390712, the reward is 244.25 with loss [26.98425269126892, 19.419721126556396] in episode 309
Report: 
rewardSum:244.25
loss:[26.98425269126892, 19.419721126556396]
policies:[0, 3, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4306
memory used:2859.0
now epsilon is 0.582708911637826, the reward is 993.0 with loss [28.62781310081482, 18.001115322113037] in episode 310
Report: 
rewardSum:993.0
loss:[28.62781310081482, 18.001115322113037]
policies:[3, 1, 3]
qAverage:[57.76582717895508, 0.0]
ws:[1.5611824989318848, 1.0486472845077515]
memory len:4320
memory used:2859.0
now epsilon is 0.5822719892029138, the reward is -2.0 with loss [12.225349187850952, 16.63565707206726] in episode 311
Report: 
rewardSum:-2.0
loss:[12.225349187850952, 16.63565707206726]
policies:[1, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4326
memory used:2859.0
now epsilon is 0.5813991269171735, the reward is 245.25 with loss [25.14789170026779, 45.34190332889557] in episode 312
Report: 
rewardSum:245.25
loss:[25.14789170026779, 45.34190332889557]
policies:[4, 0, 2]
qAverage:[94.10867309570312, 0.0]
ws:[9.743589758872986, 6.944033175706863]
memory len:4338
memory used:2859.0
now epsilon is 0.5808179457785938, the reward is 37.15999999999997 with loss [19.84736394882202, 7.867189288139343] in episode 313
Report: 
rewardSum:37.15999999999997
loss:[19.84736394882202, 7.867189288139343]
policies:[2, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4346
memory used:2870.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26*		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.579657325810385, the reward is 243.25 with loss [27.39838707447052, 33.2432816028595] in episode 314
Report: 
rewardSum:243.25
loss:[27.39838707447052, 33.2432816028595]
policies:[2, 1, 5]
qAverage:[74.58924865722656, 0.0]
ws:[10.002083778381348, 5.888101577758789]
memory len:4362
memory used:2872.0
############# STATE ###############
0*		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.5790778858198457, the reward is 247.25 with loss [19.522939205169678, 9.248534798622131] in episode 315
Report: 
rewardSum:247.25
loss:[19.522939205169678, 9.248534798622131]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4370
memory used:2872.0
now epsilon is 0.5786436859735363, the reward is -2.0 with loss [16.608238220214844, 16.572728157043457] in episode 316
Report: 
rewardSum:-2.0
loss:[16.608238220214844, 16.572728157043457]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4376
memory used:2872.0
now epsilon is 0.5777762627422395, the reward is 245.25 with loss [28.352303504943848, 34.53656792640686] in episode 317
Report: 
rewardSum:245.25
loss:[28.352303504943848, 34.53656792640686]
policies:[3, 1, 2]
qAverage:[90.29773712158203, 0.0]
ws:[1.6732749193906784, -0.08068221807479858]
memory len:4388
memory used:2871.0
now epsilon is 0.5761893626367668, the reward is 240.25 with loss [31.6589093208313, 57.07058596611023] in episode 318
Report: 
rewardSum:240.25
loss:[31.6589093208313, 57.07058596611023]
policies:[3, 2, 6]
qAverage:[96.87482198079427, 0.0]
ws:[7.914058844248454, 3.711954196294149]
memory len:4410
memory used:2871.0
now epsilon is 0.5756133893091315, the reward is 247.25 with loss [21.68843388557434, 10.830604434013367] in episode 319
Report: 
rewardSum:247.25
loss:[21.68843388557434, 10.830604434013367]
policies:[1, 0, 3]
qAverage:[68.89537811279297, 0.0]
ws:[3.4658565521240234, 1.99955153465271]
memory len:4418
memory used:2872.0
now epsilon is 0.5744631693504404, the reward is 33.15999999999997 with loss [22.893521308898926, 29.526004195213318] in episode 320
Report: 
rewardSum:33.15999999999997
loss:[22.893521308898926, 29.526004195213318]
policies:[0, 1, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4434
memory used:2872.0
now epsilon is 0.573458612472906, the reward is 244.25 with loss [27.722349643707275, 24.169732093811035] in episode 321
Report: 
rewardSum:244.25
loss:[27.722349643707275, 24.169732093811035]
policies:[2, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4448
memory used:2872.0
now epsilon is 0.5728853688715739, the reward is 247.25 with loss [12.98097249865532, 7.626957058906555] in episode 322
Report: 
rewardSum:247.25
loss:[12.98097249865532, 7.626957058906555]
policies:[1, 1, 2]
qAverage:[61.442317962646484, 0.0]
ws:[7.657299995422363, 7.006795883178711]
memory len:4456
memory used:2872.0
now epsilon is 0.5723126982989126, the reward is 247.25 with loss [14.967091798782349, 19.453920602798462] in episode 323
Report: 
rewardSum:247.25
loss:[14.967091798782349, 19.453920602798462]
policies:[2, 1, 1]
qAverage:[69.64816284179688, 0.0]
ws:[4.211831569671631, 3.5756959915161133]
memory len:4464
memory used:2872.0
now epsilon is 0.5714547656158049, the reward is 35.15999999999997 with loss [33.77641677856445, 34.33907151222229] in episode 324
Report: 
rewardSum:35.15999999999997
loss:[33.77641677856445, 34.33907151222229]
policies:[2, 0, 4]
qAverage:[74.70790608723958, 0.0]
ws:[4.084352334340413, 2.8520453770955405]
memory len:4476
memory used:2872.0
now epsilon is 0.5704554694979209, the reward is 244.25 with loss [27.802428364753723, 23.545258045196533] in episode 325
Report: 
rewardSum:244.25
loss:[27.802428364753723, 23.545258045196533]
policies:[3, 1, 3]
qAverage:[91.22341537475586, 0.0]
ws:[3.1288616061210632, 1.2960565686225891]
memory len:4490
memory used:2872.0
now epsilon is 0.5696003209174428, the reward is 35.15999999999997 with loss [28.685078382492065, 24.26875129342079] in episode 326
Report: 
rewardSum:35.15999999999997
loss:[28.685078382492065, 24.26875129342079]
policies:[3, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4502
memory used:2871.0
now epsilon is 0.5687464542584008, the reward is 245.25 with loss [20.208211958408356, 21.260692596435547] in episode 327
Report: 
rewardSum:245.25
loss:[20.208211958408356, 21.260692596435547]
policies:[1, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4514
memory used:2871.0
now epsilon is 0.5676099561586814, the reward is 243.25 with loss [37.317296266555786, 32.43824291229248] in episode 328
Report: 
rewardSum:243.25
loss:[37.317296266555786, 32.43824291229248]
policies:[4, 0, 4]
qAverage:[91.66521835327148, 0.0]
ws:[6.0270973443984985, 4.0637252032756805]
memory len:4530
memory used:2871.0
now epsilon is 0.5656265463676887, the reward is 237.25 with loss [55.21946233510971, 52.3821804523468] in episode 329
Report: 
rewardSum:237.25
loss:[55.21946233510971, 52.3821804523468]
policies:[6, 1, 7]
qAverage:[77.74202219645183, 0.0]
ws:[2.5521366596221924, 0.6432206531365713]
memory len:4558
memory used:2871.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22*		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.5647786366462993, the reward is 245.25 with loss [31.427966356277466, 37.31423807144165] in episode 330
Report: 
rewardSum:245.25
loss:[31.427966356277466, 37.31423807144165]
policies:[3, 1, 2]
qAverage:[62.02637481689453, 0.0]
ws:[3.3337457180023193, 1.8367305994033813]
memory len:4570
memory used:2871.0
now epsilon is 0.5635091547247837, the reward is 242.25 with loss [24.375091910362244, 38.761016845703125] in episode 331
Report: 
rewardSum:242.25
loss:[24.375091910362244, 38.761016845703125]
policies:[1, 3, 5]
qAverage:[59.859046936035156, 0.0]
ws:[7.421842575073242, 6.261148452758789]
memory len:4588
memory used:2871.0
now epsilon is 0.5629458568507748, the reward is 37.15999999999997 with loss [9.442988872528076, 16.48150110244751] in episode 332
Report: 
rewardSum:37.15999999999997
loss:[9.442988872528076, 16.48150110244751]
policies:[2, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4596
memory used:2871.0
now epsilon is 0.5615400744388086, the reward is 241.25 with loss [46.58814525604248, 32.69812035560608] in episode 333
Report: 
rewardSum:241.25
loss:[46.58814525604248, 32.69812035560608]
policies:[4, 0, 6]
qAverage:[54.71207046508789, 0.0]
ws:[2.3307175636291504, 1.4808580875396729]
memory len:4616
memory used:2871.0
now epsilon is 0.560277871999744, the reward is 32.15999999999997 with loss [44.25863432884216, 25.841436505317688] in episode 334
Report: 
rewardSum:32.15999999999997
loss:[44.25863432884216, 25.841436505317688]
policies:[3, 1, 5]
qAverage:[73.71853129069011, 0.0]
ws:[1.5685631434122722, 0.8193197051684061]
memory len:4634
memory used:2871.0
now epsilon is 0.5597178041969311, the reward is 247.25 with loss [16.603317379951477, 4.918588876724243] in episode 335
Report: 
rewardSum:247.25
loss:[16.603317379951477, 4.918588876724243]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4642
memory used:2872.0
now epsilon is 0.558878752051198, the reward is 245.25 with loss [31.423885226249695, 28.124332681298256] in episode 336
Report: 
rewardSum:245.25
loss:[31.423885226249695, 28.124332681298256]
policies:[2, 0, 4]
qAverage:[63.43293380737305, 0.0]
ws:[9.218426704406738, 7.815242767333984]
memory len:4654
memory used:2871.0
now epsilon is 0.5580409576973344, the reward is 245.25 with loss [29.99812614917755, 21.12655532360077] in episode 337
Report: 
rewardSum:245.25
loss:[29.99812614917755, 21.12655532360077]
policies:[3, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4666
memory used:2872.0
now epsilon is 0.5572044192498314, the reward is 35.15999999999997 with loss [21.25794780254364, 43.42210674285889] in episode 338
Report: 
rewardSum:35.15999999999997
loss:[21.25794780254364, 43.42210674285889]
policies:[2, 1, 3]
qAverage:[63.969886779785156, 0.0]
ws:[0.7324876189231873, -0.1370670348405838]
memory len:4678
memory used:2877.0
now epsilon is 0.5562300425422997, the reward is 993.0 with loss [25.4990736246109, 18.73625248670578] in episode 339
Report: 
rewardSum:993.0
loss:[25.4990736246109, 18.73625248670578]
policies:[1, 1, 5]
qAverage:[53.93447494506836, 0.0]
ws:[1.0799667835235596, 0.15502677857875824]
memory len:4692
memory used:2877.0
now epsilon is 0.5555351025459984, the reward is 246.25 with loss [31.279710292816162, 15.60580575466156] in episode 340
Report: 
rewardSum:246.25
loss:[31.279710292816162, 15.60580575466156]
policies:[2, 1, 2]
qAverage:[86.64554341634114, 0.0]
ws:[9.403413931528727, 4.054913759231567]
memory len:4702
memory used:2878.0
now epsilon is 0.5551185553732404, the reward is -2.0 with loss [10.32526695728302, 12.436639070510864] in episode 341
Report: 
rewardSum:-2.0
loss:[10.32526695728302, 12.436639070510864]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4708
memory used:2878.0
now epsilon is 0.5537323192153522, the reward is 241.25 with loss [69.1237735748291, 47.66726076602936] in episode 342
Report: 
rewardSum:241.25
loss:[69.1237735748291, 47.66726076602936]
policies:[3, 0, 7]
qAverage:[65.38690948486328, 0.0]
ws:[9.341054916381836, 6.608975410461426]
memory len:4728
memory used:2878.0
now epsilon is 0.5531787945111505, the reward is 37.15999999999997 with loss [20.483762502670288, 29.469481945037842] in episode 343
Report: 
rewardSum:37.15999999999997
loss:[20.483762502670288, 29.469481945037842]
policies:[2, 1, 1]
qAverage:[69.52071380615234, 0.0]
ws:[3.696058750152588, 2.9194419384002686]
memory len:4736
memory used:2878.0
now epsilon is 0.5526258231241159, the reward is 247.25 with loss [15.52727735042572, 19.017130613327026] in episode 344
Report: 
rewardSum:247.25
loss:[15.52727735042572, 19.017130613327026]
policies:[2, 0, 2]
qAverage:[88.50431569417317, 0.0]
ws:[6.6488620440165205, 3.2174700101216636]
memory len:4744
memory used:2878.0
now epsilon is 0.550970223337681, the reward is 29.159999999999968 with loss [56.11128234863281, 85.79801821708679] in episode 345
Report: 
rewardSum:29.159999999999968
loss:[56.11128234863281, 85.79801821708679]
policies:[1, 1, 10]
qAverage:[63.800819396972656, 0.0]
ws:[8.185431480407715, 7.0191330909729]
memory len:4768
memory used:2878.0
############# STATE ###############
0-		10-		20-		30*		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.5500067482940217, the reward is 244.25 with loss [33.21422839164734, 34.223347306251526] in episode 346
Report: 
rewardSum:244.25
loss:[33.21422839164734, 34.223347306251526]
policies:[1, 2, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4782
memory used:2884.0
now epsilon is 0.5490449580676546, the reward is 244.25 with loss [41.51295280456543, 45.38498663902283] in episode 347
Report: 
rewardSum:244.25
loss:[41.51295280456543, 45.38498663902283]
policies:[2, 1, 4]
qAverage:[77.11920166015625, 0.0]
ws:[16.10362434387207, 9.156521797180176]
memory len:4796
memory used:2884.0
now epsilon is 0.5478108415428068, the reward is 242.25 with loss [38.86885154247284, 42.231649696826935] in episode 348
Report: 
rewardSum:242.25
loss:[38.86885154247284, 42.231649696826935]
policies:[3, 1, 5]
qAverage:[62.1751594543457, 0.0]
ws:[0.2581697106361389, -0.6442598700523376]
memory len:4814
memory used:2884.0
now epsilon is 0.5469896386819977, the reward is 245.25 with loss [20.402936160564423, 20.085086584091187] in episode 349
Report: 
rewardSum:245.25
loss:[20.402936160564423, 20.085086584091187]
policies:[2, 1, 3]
qAverage:[85.68659973144531, 0.0]
ws:[3.604053338368734, 2.2926236391067505]
memory len:4826
memory used:2884.0
now epsilon is 0.5461696668558589, the reward is 245.25 with loss [14.631347209215164, 18.48824429512024] in episode 350
Report: 
rewardSum:245.25
loss:[14.631347209215164, 18.48824429512024]
policies:[1, 2, 3]
qAverage:[49.15642293294271, 29.40411631266276]
ws:[13.373657862345377, 10.406123161315918]
memory len:4838
memory used:2884.0
now epsilon is 0.5454872960430026, the reward is 36.15999999999997 with loss [12.4904625415802, 21.985819697380066] in episode 351
Report: 
rewardSum:36.15999999999997
loss:[12.4904625415802, 21.985819697380066]
policies:[1, 2, 2]
qAverage:[37.08229319254557, 33.019927978515625]
ws:[0.23418490091959634, 0.6034835577011108]
memory len:4848
memory used:2884.0
now epsilon is 0.5443972755765325, the reward is 33.15999999999997 with loss [31.5030996799469, 28.348944664001465] in episode 352
Report: 
rewardSum:33.15999999999997
loss:[31.5030996799469, 28.348944664001465]
policies:[2, 1, 5]
qAverage:[60.15668869018555, 0.0]
ws:[7.405045032501221, 7.15243673324585]
memory len:4864
memory used:2890.0
now epsilon is 0.543173605886102, the reward is 242.25 with loss [37.87685462832451, 36.36587929725647] in episode 353
Report: 
rewardSum:242.25
loss:[37.87685462832451, 36.36587929725647]
policies:[3, 2, 4]
qAverage:[44.82221221923828, 38.11867980957031]
ws:[1.6945262908935548, 1.6068381547927857]
memory len:4882
memory used:2891.0
now epsilon is 0.5426306359363718, the reward is 37.15999999999997 with loss [28.595321655273438, 18.578045465052128] in episode 354
Report: 
rewardSum:37.15999999999997
loss:[28.595321655273438, 18.578045465052128]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4890
memory used:2891.0
now epsilon is 0.5412755844782072, the reward is 31.159999999999968 with loss [31.98429012298584, 65.21336913108826] in episode 355
Report: 
rewardSum:31.159999999999968
loss:[31.98429012298584, 65.21336913108826]
policies:[2, 1, 7]
qAverage:[77.55905151367188, 0.0]
ws:[0.6601141914725304, 0.19464224576950073]
memory len:4910
memory used:2891.0
now epsilon is 0.5407345118382457, the reward is 247.25 with loss [27.824727296829224, 16.172280311584473] in episode 356
Report: 
rewardSum:247.25
loss:[27.824727296829224, 16.172280311584473]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4918
memory used:2891.0
now epsilon is 0.5396539886269702, the reward is 243.25 with loss [48.118138551712036, 41.25480365753174] in episode 357
Report: 
rewardSum:243.25
loss:[48.118138551712036, 41.25480365753174]
policies:[2, 2, 4]
qAverage:[58.223541259765625, 25.79718780517578]
ws:[3.337737739086151, 3.029204308986664]
memory len:4934
memory used:2891.0
now epsilon is 0.5384409806660037, the reward is 242.25 with loss [44.271576166152954, 37.96738076210022] in episode 358
Report: 
rewardSum:242.25
loss:[44.271576166152954, 37.96738076210022]
policies:[2, 3, 4]
qAverage:[29.33162498474121, 43.927181243896484]
ws:[3.2939637899398804, 3.535901129245758]
memory len:4952
memory used:2891.0
now epsilon is 0.5376338238151929, the reward is 245.25 with loss [21.320226907730103, 23.19753873348236] in episode 359
Report: 
rewardSum:245.25
loss:[21.320226907730103, 23.19753873348236]
policies:[1, 2, 3]
qAverage:[0.0, 44.809532165527344]
ws:[2.748828411102295, 2.7976908683776855]
memory len:4964
memory used:2892.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23*		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.5362912503431619, the reward is 241.25 with loss [60.841349601745605, 37.51824641227722] in episode 360
Report: 
rewardSum:241.25
loss:[60.841349601745605, 37.51824641227722]
policies:[4, 0, 6]
qAverage:[60.57990264892578, 0.0]
ws:[8.26860523223877, 7.233476638793945]
memory len:4984
memory used:2892.0
now epsilon is 0.5358891324516345, the reward is -2.0 with loss [12.033003330230713, 25.38712787628174] in episode 361
Report: 
rewardSum:-2.0
loss:[12.033003330230713, 25.38712787628174]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4990
memory used:2892.0
now epsilon is 0.534283673755868, the reward is 239.25 with loss [44.646424770355225, 62.63809025287628] in episode 362
Report: 
rewardSum:239.25
loss:[44.646424770355225, 62.63809025287628]
policies:[2, 2, 8]
qAverage:[48.033447265625, 32.33354695638021]
ws:[8.929582754770914, 5.952721118927002]
memory len:5014
memory used:2892.0
now epsilon is 0.5337495904050993, the reward is 247.25 with loss [15.411784827709198, 15.860677003860474] in episode 363
Report: 
rewardSum:247.25
loss:[15.411784827709198, 15.860677003860474]
policies:[2, 1, 1]
qAverage:[88.41482289632161, 0.0]
ws:[8.059937953948975, 3.4825719197591147]
memory len:5022
memory used:2892.0
now epsilon is 0.5334827489692461, the reward is -1.0 with loss [10.381654262542725, 6.495686769485474] in episode 364
Report: 
rewardSum:-1.0
loss:[10.381654262542725, 6.495686769485474]
policies:[1, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5026
memory used:2892.0
now epsilon is 0.5328162288764066, the reward is -4.0 with loss [6.2333725690841675, 17.92061185836792] in episode 365
Report: 
rewardSum:-4.0
loss:[6.2333725690841675, 17.92061185836792]
policies:[1, 0, 4]
qAverage:[53.44374465942383, 0.0]
ws:[-0.08450133353471756, -0.7901695370674133]
memory len:5036
memory used:2892.0
now epsilon is 0.5322836124203172, the reward is -3.0 with loss [19.389503240585327, 15.443213939666748] in episode 366
Report: 
rewardSum:-3.0
loss:[19.389503240585327, 15.443213939666748]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5044
memory used:2892.0
now epsilon is 0.5317515283809859, the reward is 247.25 with loss [10.997480392456055, 26.594640016555786] in episode 367
Report: 
rewardSum:247.25
loss:[10.997480392456055, 26.594640016555786]
policies:[3, 0, 1]
qAverage:[83.76943461100261, 0.0]
ws:[6.552406469980876, 5.66464360555013]
memory len:5052
memory used:2892.0
now epsilon is 0.5304236441146092, the reward is 630.1600000000001 with loss [37.416038274765015, 39.30782341957092] in episode 368
Report: 
rewardSum:630.1600000000001
loss:[37.416038274765015, 39.30782341957092]
policies:[3, 1, 6]
qAverage:[59.98054122924805, 0.0]
ws:[8.595767974853516, 7.378950119018555]
memory len:5072
memory used:2892.0
now epsilon is 0.5290990758267127, the reward is 241.25 with loss [45.35950469970703, 39.30393326282501] in episode 369
Report: 
rewardSum:241.25
loss:[45.35950469970703, 39.30393326282501]
policies:[1, 3, 6]
qAverage:[0.0, 61.495068868001304]
ws:[6.697191874186198, 7.375432014465332]
memory len:5092
memory used:2892.0
now epsilon is 0.52777781523667, the reward is 241.25 with loss [38.91245698928833, 43.031321942806244] in episode 370
Report: 
rewardSum:241.25
loss:[38.91245698928833, 43.031321942806244]
policies:[1, 3, 6]
qAverage:[36.776668548583984, 42.30068397521973]
ws:[5.78876705467701, 3.9421620666980743]
memory len:5112
memory used:2897.0
now epsilon is 0.52725023530513, the reward is 37.15999999999997 with loss [20.087320268154144, 23.76521921157837] in episode 371
Report: 
rewardSum:37.15999999999997
loss:[20.087320268154144, 23.76521921157837]
policies:[0, 2, 2]
qAverage:[0.0, 63.20300801595052]
ws:[4.411130428314209, 4.632997989654541]
memory len:5120
memory used:2897.0
now epsilon is 0.5267231827557122, the reward is 247.25 with loss [14.009244084358215, 39.45735216140747] in episode 372
Report: 
rewardSum:247.25
loss:[14.009244084358215, 39.45735216140747]
policies:[1, 0, 3]
qAverage:[60.501930236816406, 0.0]
ws:[1.279890537261963, 0.35903820395469666]
memory len:5128
memory used:2898.0
now epsilon is 0.5259335916199923, the reward is 35.15999999999997 with loss [22.23399829864502, 38.441221952438354] in episode 373
Report: 
rewardSum:35.15999999999997
loss:[22.23399829864502, 38.441221952438354]
policies:[1, 2, 3]
qAverage:[26.582807540893555, 48.665313720703125]
ws:[0.7213484206004068, 1.0810199677944183]
memory len:5140
memory used:2898.0
now epsilon is 0.5251451841309811, the reward is 245.25 with loss [29.160450339317322, 26.631124019622803] in episode 374
Report: 
rewardSum:245.25
loss:[29.160450339317322, 26.631124019622803]
policies:[4, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5152
memory used:2898.0
now epsilon is 0.5248826443604896, the reward is -1.0 with loss [8.442630290985107, 16.049800872802734] in episode 375
Report: 
rewardSum:-1.0
loss:[8.442630290985107, 16.049800872802734]
policies:[1, 0, 1]
qAverage:[54.18263626098633, 0.0]
ws:[-0.4753115177154541, -1.2164011001586914]
memory len:5156
memory used:2897.0
now epsilon is 0.5238337971572676, the reward is -7.0 with loss [38.21628451347351, 38.21458315849304] in episode 376
Report: 
rewardSum:-7.0
loss:[38.21628451347351, 38.21458315849304]
policies:[2, 0, 6]
qAverage:[53.41678237915039, 0.0]
ws:[-0.7724366188049316, -1.7467600107192993]
memory len:5172
memory used:2898.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22*		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.5226563490524334, the reward is 242.25 with loss [41.010995864868164, 29.498488187789917] in episode 377
Report: 
rewardSum:242.25
loss:[41.010995864868164, 29.498488187789917]
policies:[3, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5190
memory used:2899.0
now epsilon is 0.522133888666848, the reward is 247.25 with loss [11.923999339342117, 12.597617030143738] in episode 378
Report: 
rewardSum:247.25
loss:[11.923999339342117, 12.597617030143738]
policies:[3, 0, 1]
qAverage:[76.94917297363281, 0.0]
ws:[11.451172828674316, 7.341777801513672]
memory len:5198
memory used:2898.0
now epsilon is 0.521611950545758, the reward is 247.25 with loss [34.4781756401062, 16.616182327270508] in episode 379
Report: 
rewardSum:247.25
loss:[34.4781756401062, 16.616182327270508]
policies:[2, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5206
memory used:2898.0
now epsilon is 0.5210905341670949, the reward is 247.25 with loss [16.852031469345093, 11.315809965133667] in episode 380
Report: 
rewardSum:247.25
loss:[16.852031469345093, 11.315809965133667]
policies:[2, 0, 2]
qAverage:[86.42479451497395, 0.0]
ws:[7.714941342671712, 6.815934181213379]
memory len:5214
memory used:2898.0
now epsilon is 0.519919252235246, the reward is 242.25 with loss [35.10310649871826, 65.96331250667572] in episode 381
Report: 
rewardSum:242.25
loss:[35.10310649871826, 65.96331250667572]
policies:[4, 0, 5]
qAverage:[60.18242645263672, 0.0]
ws:[0.8851520419120789, 0.21039362251758575]
memory len:5232
memory used:2899.0
now epsilon is 0.5192696780382574, the reward is 36.15999999999997 with loss [27.121553659439087, 14.745092555880547] in episode 382
Report: 
rewardSum:36.15999999999997
loss:[27.121553659439087, 14.745092555880547]
policies:[2, 1, 2]
qAverage:[36.583229064941406, 34.93407185872396]
ws:[1.5156513055165608, 1.340907911459605]
memory len:5242
memory used:2899.0
now epsilon is 0.5190100756535931, the reward is -1.0 with loss [4.765395879745483, 16.638596534729004] in episode 383
Report: 
rewardSum:-1.0
loss:[4.765395879745483, 16.638596534729004]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5246
memory used:2899.0
now epsilon is 0.5184912601742819, the reward is 247.25 with loss [13.854067325592041, 16.695662140846252] in episode 384
Report: 
rewardSum:247.25
loss:[13.854067325592041, 16.695662140846252]
policies:[2, 2, 0]
qAverage:[89.97894541422527, 0.0]
ws:[16.507909774780273, 12.285397211710611]
memory len:5254
memory used:2899.0
now epsilon is 0.5179729633159266, the reward is 247.25 with loss [14.167448997497559, 9.458538517355919] in episode 385
Report: 
rewardSum:247.25
loss:[14.167448997497559, 9.458538517355919]
policies:[3, 1, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5262
memory used:2899.0
now epsilon is 0.5174551845601006, the reward is 247.25 with loss [7.434431076049805, 13.328918814659119] in episode 386
Report: 
rewardSum:247.25
loss:[7.434431076049805, 13.328918814659119]
policies:[2, 0, 2]
qAverage:[53.626678466796875, 0.0]
ws:[1.3168401718139648, 0.2711906433105469]
memory len:5270
memory used:2899.0
now epsilon is 0.5170671901864424, the reward is -2.0 with loss [12.850548267364502, 13.778907537460327] in episode 387
Report: 
rewardSum:-2.0
loss:[12.850548267364502, 13.778907537460327]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5276
memory used:2899.0
now epsilon is 0.5168086889080487, the reward is -1.0 with loss [3.8383195400238037, 7.618555307388306] in episode 388
Report: 
rewardSum:-1.0
loss:[3.8383195400238037, 7.618555307388306]
policies:[1, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5280
memory used:2899.0
now epsilon is 0.5157759754933717, the reward is 243.25 with loss [34.283740520477295, 35.001251339912415] in episode 389
Report: 
rewardSum:243.25
loss:[34.283740520477295, 35.001251339912415]
policies:[2, 2, 4]
qAverage:[81.44601440429688, 0.0]
ws:[6.2235995928446455, 5.365448236465454]
memory len:5296
memory used:2899.0
now epsilon is 0.5151315778034099, the reward is 246.25 with loss [32.09085249900818, 17.544885635375977] in episode 390
Report: 
rewardSum:246.25
loss:[32.09085249900818, 17.544885635375977]
policies:[2, 2, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5306
memory used:2899.0
now epsilon is 0.5141022156774651, the reward is 243.25 with loss [26.293723940849304, 39.37551665306091] in episode 391
Report: 
rewardSum:243.25
loss:[26.293723940849304, 39.37551665306091]
policies:[3, 0, 5]
qAverage:[67.66068267822266, 0.0]
ws:[3.3133864402770996, 2.5929882526397705]
memory len:5322
memory used:2899.0
now epsilon is 0.5128184050872331, the reward is 241.25 with loss [43.112125873565674, 48.988933086395264] in episode 392
Report: 
rewardSum:241.25
loss:[43.112125873565674, 48.988933086395264]
policies:[4, 0, 6]
qAverage:[86.95325724283855, 0.0]
ws:[7.709978739420573, 6.772143840789795]
memory len:5342
memory used:2917.0
now epsilon is 0.5123057789569987, the reward is 37.15999999999997 with loss [20.556567668914795, 7.637069582939148] in episode 393
Report: 
rewardSum:37.15999999999997
loss:[20.556567668914795, 7.637069582939148]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5350
memory used:2924.0
now epsilon is 0.511154242970199, the reward is 242.25 with loss [38.9607892036438, 27.15267825126648] in episode 394
Report: 
rewardSum:242.25
loss:[38.9607892036438, 27.15267825126648]
policies:[2, 3, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5368
memory used:2924.0
now epsilon is 0.5105156195580303, the reward is 246.25 with loss [33.36911225318909, 25.105618119239807] in episode 395
Report: 
rewardSum:246.25
loss:[33.36911225318909, 25.105618119239807]
policies:[3, 0, 2]
qAverage:[94.51529947916667, 0.0]
ws:[8.7889191309611, 5.76845105489095]
memory len:5378
memory used:2930.0
now epsilon is 0.509877794026087, the reward is 246.25 with loss [23.208160161972046, 18.088789105415344] in episode 396
Report: 
rewardSum:246.25
loss:[23.208160161972046, 18.088789105415344]
policies:[1, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5388
memory used:2937.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12*		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.5093681074043683, the reward is 247.25 with loss [16.291678428649902, 13.128649115562439] in episode 397
Report: 
rewardSum:247.25
loss:[16.291678428649902, 13.128649115562439]
policies:[2, 0, 2]
qAverage:[62.88713455200195, 0.0]
ws:[9.516239166259766, 9.506022453308105]
memory len:5396
memory used:2936.0
now epsilon is 0.5086045326167149, the reward is 245.25 with loss [23.56293547153473, 28.170982837677002] in episode 398
Report: 
rewardSum:245.25
loss:[23.56293547153473, 28.170982837677002]
policies:[2, 2, 2]
qAverage:[71.26142883300781, 0.0]
ws:[9.288122177124023, 5.866876125335693]
memory len:5408
memory used:2936.0
now epsilon is 0.5079690947493174, the reward is -4.0 with loss [22.8446524143219, 24.123951584100723] in episode 399
Report: 
rewardSum:-4.0
loss:[22.8446524143219, 24.123951584100723]
policies:[0, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5418
memory used:2936.0
now epsilon is 0.5072076171695092, the reward is 245.25 with loss [32.09623432159424, 25.55245852470398] in episode 400
Report: 
rewardSum:245.25
loss:[32.09623432159424, 25.55245852470398]
policies:[1, 1, 4]
qAverage:[62.337257385253906, 0.0]
ws:[2.751901149749756, 2.7010128498077393]
memory len:5430
memory used:2937.0
now epsilon is 0.5069540450614006, the reward is -1.0 with loss [13.31466919183731, 15.378956317901611] in episode 401
Report: 
rewardSum:-1.0
loss:[13.31466919183731, 15.378956317901611]
policies:[1, 0, 1]
qAverage:[57.24886703491211, 0.0]
ws:[-0.1792462170124054, -0.5778726935386658]
memory len:5434
memory used:2936.0
now epsilon is 0.5061940891048325, the reward is 245.25 with loss [28.576839327812195, 26.073415756225586] in episode 402
Report: 
rewardSum:245.25
loss:[28.576839327812195, 26.073415756225586]
policies:[1, 0, 5]
qAverage:[56.294986724853516, 0.0]
ws:[-0.026095017790794373, -0.396005243062973]
memory len:5446
memory used:2936.0
now epsilon is 0.5045514237507847, the reward is 627.1600000000001 with loss [49.18432092666626, 54.16710877418518] in episode 403
Report: 
rewardSum:627.1600000000001
loss:[49.18432092666626, 54.16710877418518]
policies:[2, 2, 9]
qAverage:[54.8441047668457, 0.0]
ws:[-10.9359769821167, -14.226551055908203]
memory len:5472
memory used:2936.0
now epsilon is 0.5040470615022853, the reward is 247.25 with loss [10.255631536245346, 21.319593906402588] in episode 404
Report: 
rewardSum:247.25
loss:[10.255631536245346, 21.319593906402588]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5480
memory used:2936.0
now epsilon is 0.5034173176260734, the reward is 246.25 with loss [16.682897806167603, 17.4458144903183] in episode 405
Report: 
rewardSum:246.25
loss:[16.682897806167603, 17.4458144903183]
policies:[2, 1, 2]
qAverage:[57.42617416381836, 0.0]
ws:[0.5011367797851562, 0.06562626361846924]
memory len:5490
memory used:2936.0
now epsilon is 0.5029140890584799, the reward is 247.25 with loss [23.74862766265869, 15.583858489990234] in episode 406
Report: 
rewardSum:247.25
loss:[23.74862766265869, 15.583858489990234]
policies:[3, 1, 0]
qAverage:[37.374114990234375, 33.98150634765625]
ws:[2.9813078244527182, 2.7488145430882773]
memory len:5498
memory used:2936.0
now epsilon is 0.5017836632549717, the reward is 242.25 with loss [54.64179229736328, 29.941093802452087] in episode 407
Report: 
rewardSum:242.25
loss:[54.64179229736328, 29.941093802452087]
policies:[3, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5516
memory used:2936.0
now epsilon is 0.5009062001609892, the reward is 244.25 with loss [34.10874557495117, 35.46200704574585] in episode 408
Report: 
rewardSum:244.25
loss:[34.10874557495117, 35.46200704574585]
policies:[2, 2, 3]
qAverage:[48.52485961914063, 34.3083251953125]
ws:[3.3647394239902497, 2.324248421192169]
memory len:5530
memory used:2910.0
now epsilon is 0.49990526390836165, the reward is 243.25 with loss [29.34406042098999, 36.97471225261688] in episode 409
Report: 
rewardSum:243.25
loss:[29.34406042098999, 36.97471225261688]
policies:[3, 2, 3]
qAverage:[59.04975128173828, 20.88222885131836]
ws:[0.6078330185264349, 0.2722683874890208]
memory len:5546
memory used:2910.0
now epsilon is 0.49940554607768517, the reward is 247.25 with loss [13.32771584391594, 22.395299673080444] in episode 410
Report: 
rewardSum:247.25
loss:[13.32771584391594, 22.395299673080444]
policies:[3, 1, 0]
qAverage:[78.85504150390625, 0.0]
ws:[3.9018352031707764, 1.3252428770065308]
memory len:5554
memory used:2910.0
now epsilon is 0.4989063277774764, the reward is 37.15999999999997 with loss [20.491896629333496, 26.899017333984375] in episode 411
Report: 
rewardSum:37.15999999999997
loss:[20.491896629333496, 26.899017333984375]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5562
memory used:2910.0
now epsilon is 0.4984076085083922, the reward is 37.15999999999997 with loss [17.163034200668335, 24.58488702774048] in episode 412
Report: 
rewardSum:37.15999999999997
loss:[17.163034200668335, 24.58488702774048]
policies:[3, 0, 1]
qAverage:[79.61591339111328, 0.0]
ws:[3.34661865234375, 2.3205857276916504]
memory len:5570
memory used:2912.0
now epsilon is 0.4971629903244147, the reward is 241.25 with loss [49.37814545631409, 40.26048743724823] in episode 413
Report: 
rewardSum:241.25
loss:[49.37814545631409, 40.26048743724823]
policies:[2, 1, 7]
qAverage:[56.332584381103516, 0.0]
ws:[1.690868616104126, 0.8301634192466736]
memory len:5590
memory used:2911.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24*		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.49666601373914093, the reward is 247.25 with loss [14.633094429969788, 17.483396768569946] in episode 414
Report: 
rewardSum:247.25
loss:[14.633094429969788, 17.483396768569946]
policies:[3, 0, 1]
qAverage:[81.98715209960938, 0.0]
ws:[5.268673658370972, 1.846091777086258]
memory len:5598
memory used:2911.0
now epsilon is 0.49616953394411734, the reward is 247.25 with loss [24.24321937561035, 38.58580827713013] in episode 415
Report: 
rewardSum:247.25
loss:[24.24321937561035, 38.58580827713013]
policies:[2, 0, 2]
qAverage:[74.10548400878906, 0.0]
ws:[6.849038124084473, 5.662836869557698]
memory len:5606
memory used:2911.0
now epsilon is 0.4954257446471154, the reward is 245.25 with loss [28.256597995758057, 27.2217538356781] in episode 416
Report: 
rewardSum:245.25
loss:[28.256597995758057, 27.2217538356781]
policies:[3, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5618
memory used:2911.0
now epsilon is 0.4946830703369889, the reward is 245.25 with loss [18.883587956428528, 22.856410026550293] in episode 417
Report: 
rewardSum:245.25
loss:[18.883587956428528, 22.856410026550293]
policies:[4, 0, 2]
qAverage:[72.64474232991536, 0.0]
ws:[5.914034287134807, 4.882759968439738]
memory len:5630
memory used:2911.0
now epsilon is 0.4944357597195123, the reward is -1.0 with loss [7.880615472793579, 5.2290158867836] in episode 418
Report: 
rewardSum:-1.0
loss:[7.880615472793579, 5.2290158867836]
policies:[1, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5634
memory used:2911.0
now epsilon is 0.4934477530301568, the reward is 243.25 with loss [25.647873759269714, 26.504092812538147] in episode 419
Report: 
rewardSum:243.25
loss:[25.647873759269714, 26.504092812538147]
policies:[4, 0, 4]
qAverage:[73.76106770833333, 0.0]
ws:[1.3456251472234726, 0.31919678052266437]
memory len:5650
memory used:2911.0
now epsilon is 0.4929544902891955, the reward is 247.25 with loss [16.52350425720215, 10.767147779464722] in episode 420
Report: 
rewardSum:247.25
loss:[16.52350425720215, 10.767147779464722]
policies:[2, 1, 1]
qAverage:[52.59278869628906, 0.0]
ws:[-0.3743528723716736, -1.2250473499298096]
memory len:5658
memory used:2911.0
now epsilon is 0.4924617206260325, the reward is 247.25 with loss [24.004777908325195, 19.797150194644928] in episode 421
Report: 
rewardSum:247.25
loss:[24.004777908325195, 19.797150194644928]
policies:[2, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5666
memory used:2911.0
now epsilon is 0.4917234895740912, the reward is 35.15999999999997 with loss [19.35103189945221, 47.630908250808716] in episode 422
Report: 
rewardSum:35.15999999999997
loss:[19.35103189945221, 47.630908250808716]
policies:[2, 2, 2]
qAverage:[39.13577779134115, 25.748738606770832]
ws:[2.423988421758016, 2.373100996017456]
memory len:5678
memory used:2911.0
now epsilon is 0.4912319504500949, the reward is 247.25 with loss [18.12673330307007, 17.641382575035095] in episode 423
Report: 
rewardSum:247.25
loss:[18.12673330307007, 17.641382575035095]
policies:[3, 1, 0]
qAverage:[83.85597610473633, 0.0]
ws:[4.744043350219727, 2.587811052799225]
memory len:5686
memory used:2911.0
now epsilon is 0.49049556290089225, the reward is 245.25 with loss [13.773745596408844, 22.584083914756775] in episode 424
Report: 
rewardSum:245.25
loss:[13.773745596408844, 22.584083914756775]
policies:[3, 2, 1]
qAverage:[53.14306640625, 0.0]
ws:[1.7692142724990845, 1.1144932508468628]
memory len:5698
memory used:2911.0
now epsilon is 0.4898827499303626, the reward is -4.0 with loss [17.31288981437683, 18.114331424236298] in episode 425
Report: 
rewardSum:-4.0
loss:[17.31288981437683, 18.114331424236298]
policies:[1, 1, 3]
qAverage:[51.753231048583984, 0.0]
ws:[-0.4894089698791504, -0.8590841293334961]
memory len:5708
memory used:2911.0
now epsilon is 0.4893930508558478, the reward is 247.25 with loss [25.021583914756775, 25.772445917129517] in episode 426
Report: 
rewardSum:247.25
loss:[25.021583914756775, 25.772445917129517]
policies:[2, 0, 2]
qAverage:[67.7449239095052, 0.0]
ws:[0.2646874189376831, 0.23664544026056925]
memory len:5716
memory used:2911.0
now epsilon is 0.4886594199326426, the reward is 245.25 with loss [24.96854031085968, 43.97085642814636] in episode 427
Report: 
rewardSum:245.25
loss:[24.96854031085968, 43.97085642814636]
policies:[4, 0, 2]
qAverage:[72.50035603841145, 0.0]
ws:[4.163273473580678, 3.660586933294932]
memory len:5728
memory used:2911.0
now epsilon is 0.48817094372945324, the reward is 247.25 with loss [13.55344557762146, 14.938140034675598] in episode 428
Report: 
rewardSum:247.25
loss:[13.55344557762146, 14.938140034675598]
policies:[1, 0, 3]
qAverage:[49.17167282104492, 0.0]
ws:[1.0771796703338623, 0.6498051881790161]
memory len:5736
memory used:2911.0
now epsilon is 0.48707365685020126, the reward is 242.25 with loss [47.447163820266724, 39.05507302284241] in episode 429
Report: 
rewardSum:242.25
loss:[47.447163820266724, 39.05507302284241]
policies:[1, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5754
memory used:2911.0
now epsilon is 0.48610036148934416, the reward is 33.15999999999997 with loss [36.846691608428955, 48.68085169792175] in episode 430
Report: 
rewardSum:33.15999999999997
loss:[36.846691608428955, 48.68085169792175]
policies:[1, 2, 5]
qAverage:[0.0, 51.38697306315104]
ws:[0.7665755152702332, 1.2293145954608917]
memory len:5770
memory used:2918.0
now epsilon is 0.4854930397742648, the reward is 246.25 with loss [21.358098030090332, 25.07117748260498] in episode 431
Report: 
rewardSum:246.25
loss:[21.358098030090332, 25.07117748260498]
policies:[1, 2, 2]
qAverage:[30.332483291625977, 40.446590423583984]
ws:[3.559894025325775, 2.2509527802467346]
memory len:5780
memory used:2919.0
now epsilon is 0.485007728764039, the reward is 247.25 with loss [12.21400773525238, 25.643823623657227] in episode 432
Report: 
rewardSum:247.25
loss:[12.21400773525238, 25.643823623657227]
policies:[2, 1, 1]
qAverage:[33.05052185058594, 23.914749145507812]
ws:[4.569436510403951, 4.6322395006815595]
memory len:5788
memory used:2919.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44*		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.48403856164578724, the reward is 33.15999999999997 with loss [41.371259808540344, 50.015949726104736] in episode 433
Report: 
rewardSum:33.15999999999997
loss:[41.371259808540344, 50.015949726104736]
policies:[2, 1, 5]
qAverage:[0.0, 36.144004821777344]
ws:[-0.4609205722808838, 0.08618935197591782]
memory len:5804
memory used:2919.0
now epsilon is 0.48246779384673066, the reward is 238.25 with loss [58.354423969984055, 59.09557294845581] in episode 434
Report: 
rewardSum:238.25
loss:[58.354423969984055, 59.09557294845581]
policies:[3, 0, 10]
qAverage:[47.56951904296875, 0.0]
ws:[5.520397663116455, 5.416780471801758]
memory len:5830
memory used:2919.0
now epsilon is 0.4816241081826948, the reward is 244.25 with loss [27.881656169891357, 61.36886143684387] in episode 435
Report: 
rewardSum:244.25
loss:[27.881656169891357, 61.36886143684387]
policies:[3, 0, 4]
qAverage:[71.55242156982422, 0.0]
ws:[2.6188199718793235, 0.26442063848177594]
memory len:5844
memory used:2919.0
now epsilon is 0.48090212339254296, the reward is 245.25 with loss [26.20576286315918, 24.88443386554718] in episode 436
Report: 
rewardSum:245.25
loss:[26.20576286315918, 24.88443386554718]
policies:[3, 0, 3]
qAverage:[63.23898569742838, 0.0]
ws:[-0.20714270571867624, -0.5326412121454874]
memory len:5856
memory used:2919.0
now epsilon is 0.48042140157739227, the reward is 247.25 with loss [26.14622449874878, 18.414878606796265] in episode 437
Report: 
rewardSum:247.25
loss:[26.14622449874878, 18.414878606796265]
policies:[2, 0, 2]
qAverage:[49.0775146484375, 0.0]
ws:[6.648276329040527, 5.651780128479004]
memory len:5864
memory used:2918.0
now epsilon is 0.47958129441505665, the reward is 244.25 with loss [32.715585470199585, 35.51127648353577] in episode 438
Report: 
rewardSum:244.25
loss:[32.715585470199585, 35.51127648353577]
policies:[4, 0, 3]
qAverage:[84.77300453186035, 0.0]
ws:[4.616243898868561, 1.950547695159912]
memory len:5878
memory used:2918.0
now epsilon is 0.47910189293365507, the reward is 247.25 with loss [18.212553024291992, 24.28974688053131] in episode 439
Report: 
rewardSum:247.25
loss:[18.212553024291992, 24.28974688053131]
policies:[3, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5886
memory used:2919.0
now epsilon is 0.47862297067398935, the reward is 247.25 with loss [20.888224840164185, 12.743253350257874] in episode 440
Report: 
rewardSum:247.25
loss:[20.888224840164185, 12.743253350257874]
policies:[2, 1, 1]
qAverage:[78.75290934244792, 0.0]
ws:[5.851589838663737, 3.0734665393829346]
memory len:5894
memory used:2919.0
now epsilon is 0.4781445271570174, the reward is 247.25 with loss [17.248326778411865, 21.048391103744507] in episode 441
Report: 
rewardSum:247.25
loss:[17.248326778411865, 21.048391103744507]
policies:[2, 0, 2]
qAverage:[71.28112030029297, 0.0]
ws:[2.5864869753519693, 2.2774871985117593]
memory len:5902
memory used:2919.0
now epsilon is 0.4775471452636999, the reward is 246.25 with loss [18.086468994617462, 12.26219293475151] in episode 442
Report: 
rewardSum:246.25
loss:[18.086468994617462, 12.26219293475151]
policies:[2, 1, 2]
qAverage:[75.102294921875, 0.0]
ws:[4.6003168026606245, 1.7149623831113179]
memory len:5912
memory used:2918.0
now epsilon is 0.4768312720970476, the reward is 245.25 with loss [18.02530074119568, 23.16219425201416] in episode 443
Report: 
rewardSum:245.25
loss:[18.02530074119568, 23.16219425201416]
policies:[1, 3, 2]
qAverage:[33.46734873453776, 23.232767740885418]
ws:[5.210785706837972, 5.093648314476013]
memory len:5924
memory used:2918.0
now epsilon is 0.4763546196068775, the reward is 247.25 with loss [17.297386169433594, 16.54088169336319] in episode 444
Report: 
rewardSum:247.25
loss:[17.297386169433594, 16.54088169336319]
policies:[2, 2, 0]
qAverage:[63.6993153889974, 0.0]
ws:[3.1187996864318848, 2.8452322483062744]
memory len:5932
memory used:2918.0
now epsilon is 0.47540274357156803, the reward is 243.25 with loss [42.970643639564514, 30.630006432533264] in episode 445
Report: 
rewardSum:243.25
loss:[42.970643639564514, 30.630006432533264]
policies:[4, 1, 3]
qAverage:[64.0445327758789, 0.0]
ws:[2.7120649814605713, 2.5005411307017007]
memory len:5948
memory used:2918.0
now epsilon is 0.47409701899864226, the reward is 240.25 with loss [43.00014936923981, 44.547892570495605] in episode 446
Report: 
rewardSum:240.25
loss:[43.00014936923981, 44.547892570495605]
policies:[5, 1, 5]
qAverage:[87.71825981140137, 0.0]
ws:[5.168816268444061, 2.9498956203460693]
memory len:5970
memory used:2919.0
now epsilon is 0.47314965421572297, the reward is 243.25 with loss [35.54311776161194, 31.637075781822205] in episode 447
Report: 
rewardSum:243.25
loss:[35.54311776161194, 31.637075781822205]
policies:[2, 0, 6]
qAverage:[84.77331034342448, 0.0]
ws:[7.046945571899414, 4.087180932362874]
memory len:5986
memory used:2922.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25*		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.47220418250530993, the reward is 243.25 with loss [36.39694905281067, 29.510825514793396] in episode 448
Report: 
rewardSum:243.25
loss:[36.39694905281067, 29.510825514793396]
policies:[4, 0, 4]
qAverage:[91.32689666748047, 0.0]
ws:[5.935757994651794, 3.4698278307914734]
memory len:6002
memory used:2922.0
now epsilon is 0.4712606000845693, the reward is 243.25 with loss [27.196861147880554, 32.90331196784973] in episode 449
Report: 
rewardSum:243.25
loss:[27.196861147880554, 32.90331196784973]
policies:[3, 1, 4]
qAverage:[64.50044708251953, 17.707742309570314]
ws:[4.013696479797363, 2.488851523399353]
memory len:6018
memory used:2934.0
now epsilon is 0.4707895161777579, the reward is 247.25 with loss [21.182804107666016, 20.094588041305542] in episode 450
Report: 
rewardSum:247.25
loss:[21.182804107666016, 20.094588041305542]
policies:[1, 2, 1]
qAverage:[51.42011642456055, 0.0]
ws:[7.420034408569336, 6.929070949554443]
memory len:6026
memory used:2934.0
now epsilon is 0.4698487606152436, the reward is 243.25 with loss [28.937304854393005, 29.390472650527954] in episode 451
Report: 
rewardSum:243.25
loss:[28.937304854393005, 29.390472650527954]
policies:[2, 1, 5]
qAverage:[74.07447814941406, 0.0]
ws:[6.304295380910237, 5.845001379648845]
memory len:6042
memory used:2935.0
now epsilon is 0.4687926574471255, the reward is 242.25 with loss [36.59262442588806, 34.71242642402649] in episode 452
Report: 
rewardSum:242.25
loss:[36.59262442588806, 34.71242642402649]
policies:[2, 3, 4]
qAverage:[45.9363759358724, 30.749493916829426]
ws:[5.087128162384033, 3.102698802947998]
memory len:6060
memory used:2923.0
now epsilon is 0.4683240405576273, the reward is 247.25 with loss [12.626358985900879, 22.577412128448486] in episode 453
Report: 
rewardSum:247.25
loss:[12.626358985900879, 22.577412128448486]
policies:[4, 0, 0]
qAverage:[60.31332015991211, 0.0]
ws:[1.8773201704025269, 1.8437304496765137]
memory len:6068
memory used:2923.0
now epsilon is 0.468089907807601, the reward is -1.0 with loss [11.098329544067383, 4.7508403062820435] in episode 454
Report: 
rewardSum:-1.0
loss:[11.098329544067383, 4.7508403062820435]
policies:[1, 0, 1]
qAverage:[45.9159049987793, 0.0]
ws:[0.5802081823348999, 0.48431214690208435]
memory len:6072
memory used:2923.0
now epsilon is 0.46762199340425514, the reward is 37.15999999999997 with loss [16.185351848602295, 14.064180612564087] in episode 455
Report: 
rewardSum:37.15999999999997
loss:[16.185351848602295, 14.064180612564087]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6080
memory used:2923.0
now epsilon is 0.4671545467398739, the reward is 247.25 with loss [18.5759117603302, 8.506618320941925] in episode 456
Report: 
rewardSum:247.25
loss:[18.5759117603302, 8.506618320941925]
policies:[0, 2, 2]
qAverage:[0.0, 51.19526672363281]
ws:[2.851585308710734, 2.8894472122192383]
memory len:6088
memory used:2923.0
now epsilon is 0.4662210547582185, the reward is 243.25 with loss [26.881447076797485, 26.297338724136353] in episode 457
Report: 
rewardSum:243.25
loss:[26.881447076797485, 26.297338724136353]
policies:[1, 3, 4]
qAverage:[47.26086934407552, 23.170461018880207]
ws:[5.884962717692058, 4.421271324157715]
memory len:6104
memory used:2923.0
now epsilon is 0.46552216011265335, the reward is 245.25 with loss [25.24670398235321, 27.323819041252136] in episode 458
Report: 
rewardSum:245.25
loss:[25.24670398235321, 27.323819041252136]
policies:[1, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6116
memory used:2923.0
now epsilon is 0.4650568124942575, the reward is -3.0 with loss [16.69892144203186, 13.725293397903442] in episode 459
Report: 
rewardSum:-3.0
loss:[16.69892144203186, 13.725293397903442]
policies:[2, 0, 2]
qAverage:[46.265316009521484, 0.0]
ws:[1.4909615516662598, 1.404995322227478]
memory len:6124
memory used:2940.0
now epsilon is 0.46435966312097493, the reward is 245.25 with loss [17.722831189632416, 20.465429306030273] in episode 460
Report: 
rewardSum:245.25
loss:[17.722831189632416, 20.465429306030273]
policies:[3, 0, 3]
qAverage:[67.31160481770833, 0.0]
ws:[3.766878286997477, 3.528728167215983]
memory len:6136
memory used:2948.0
now epsilon is 0.4637795036943161, the reward is 246.25 with loss [22.899699211120605, 18.303457498550415] in episode 461
Report: 
rewardSum:246.25
loss:[22.899699211120605, 18.303457498550415]
policies:[4, 0, 1]
qAverage:[64.0046157836914, 0.0]
ws:[3.2470943927764893, 2.8402349948883057]
memory len:6146
memory used:2948.0
now epsilon is 0.46308426908715544, the reward is 35.15999999999997 with loss [19.12367081642151, 30.366350889205933] in episode 462
Report: 
rewardSum:35.15999999999997
loss:[19.12367081642151, 30.366350889205933]
policies:[2, 0, 4]
qAverage:[74.48629252115886, 0.0]
ws:[2.960099935531616, 2.744777202606201]
memory len:6158
memory used:2948.0
now epsilon is 0.46273704270640487, the reward is -2.0 with loss [18.56720209121704, 4.398150324821472] in episode 463
Report: 
rewardSum:-2.0
loss:[18.56720209121704, 4.398150324821472]
policies:[1, 0, 2]
qAverage:[46.64965057373047, 0.0]
ws:[1.7324843406677246, 1.6767792701721191]
memory len:6164
memory used:2948.0
now epsilon is 0.4619278599710412, the reward is 993.0 with loss [37.42350149154663, 28.550321578979492] in episode 464
Report: 
rewardSum:993.0
loss:[37.42350149154663, 28.550321578979492]
policies:[3, 0, 4]
qAverage:[81.94671630859375, 0.0]
ws:[0.5155080556869507, -0.3247568607330322]
memory len:6178
memory used:2948.0
now epsilon is 0.461466105305149, the reward is 247.25 with loss [18.882826328277588, 11.551990032196045] in episode 465
Report: 
rewardSum:247.25
loss:[18.882826328277588, 11.551990032196045]
policies:[1, 0, 3]
qAverage:[73.51763153076172, 0.0]
ws:[8.491735458374023, 6.388782501220703]
memory len:6186
memory used:2950.0
now epsilon is 0.460774338627484, the reward is 245.25 with loss [24.987884283065796, 19.99108874797821] in episode 466
Report: 
rewardSum:245.25
loss:[24.987884283065796, 19.99108874797821]
policies:[2, 3, 1]
qAverage:[0.0, 59.45676231384277]
ws:[4.984174489974976, 5.068523406982422]
memory len:6198
memory used:2950.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32*		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.45904945541586634, the reward is 625.1600000000001 with loss [72.80082750320435, 71.21718239784241] in episode 467
Report: 
rewardSum:625.1600000000001
loss:[72.80082750320435, 71.21718239784241]
policies:[3, 2, 10]
qAverage:[47.557294845581055, 24.444358825683594]
ws:[-2.331237494945526, -3.3385547399520874]
memory len:6228
memory used:2987.0
now epsilon is 0.4584759304307887, the reward is 246.25 with loss [20.364171147346497, 28.920000553131104] in episode 468
Report: 
rewardSum:246.25
loss:[20.364171147346497, 28.920000553131104]
policies:[1, 1, 3]
qAverage:[75.67669677734375, 0.0]
ws:[9.514979362487793, 7.07012939453125]
memory len:6238
memory used:2952.0
now epsilon is 0.45813215944003893, the reward is -2.0 with loss [5.203240633010864, 21.347742557525635] in episode 469
Report: 
rewardSum:-2.0
loss:[5.203240633010864, 21.347742557525635]
policies:[1, 0, 2]
qAverage:[49.14109420776367, 0.0]
ws:[1.9596912860870361, 1.7608528137207031]
memory len:6244
memory used:2952.0
now epsilon is 0.4576741990515273, the reward is 247.25 with loss [21.702986240386963, 41.7287392616272] in episode 470
Report: 
rewardSum:247.25
loss:[21.702986240386963, 41.7287392616272]
policies:[0, 2, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6252
memory used:2952.0
now epsilon is 0.45721669645169766, the reward is 247.25 with loss [13.06536316871643, 23.57594859600067] in episode 471
Report: 
rewardSum:247.25
loss:[13.06536316871643, 23.57594859600067]
policies:[3, 0, 1]
qAverage:[81.80661265055339, 0.0]
ws:[2.422191540400187, 1.5129154125849407]
memory len:6260
memory used:2952.0
now epsilon is 0.4564171670798436, the reward is 244.25 with loss [23.632906079292297, 30.816981077194214] in episode 472
Report: 
rewardSum:244.25
loss:[23.632906079292297, 30.816981077194214]
policies:[4, 0, 3]
qAverage:[100.25359725952148, 0.0]
ws:[9.376335859298706, 6.22789341211319]
memory len:6274
memory used:2952.0
now epsilon is 0.4550497967313666, the reward is 239.25 with loss [67.08043074607849, 52.31384074687958] in episode 473
Report: 
rewardSum:239.25
loss:[67.08043074607849, 52.31384074687958]
policies:[4, 0, 8]
qAverage:[87.0268783569336, 0.0]
ws:[2.475313150882721, 1.4406021177768706]
memory len:6298
memory used:2952.0
now epsilon is 0.45436764850327765, the reward is 245.25 with loss [30.396106481552124, 29.85379594564438] in episode 474
Report: 
rewardSum:245.25
loss:[30.396106481552124, 29.85379594564438]
policies:[4, 1, 1]
qAverage:[97.00128555297852, 0.0]
ws:[6.432552561163902, 4.099239498376846]
memory len:6310
memory used:2951.0
now epsilon is 0.4539134512142465, the reward is 247.25 with loss [11.410789251327515, 24.25855255126953] in episode 475
Report: 
rewardSum:247.25
loss:[11.410789251327515, 24.25855255126953]
policies:[3, 0, 1]
qAverage:[83.05940500895183, 0.0]
ws:[4.060972849527995, 3.832630236943563]
memory len:6318
memory used:2951.0
now epsilon is 0.4534597079522087, the reward is 247.25 with loss [17.788621425628662, 8.830992251634598] in episode 476
Report: 
rewardSum:247.25
loss:[17.788621425628662, 8.830992251634598]
policies:[1, 0, 3]
qAverage:[82.13877868652344, 0.0]
ws:[8.113043785095215, 3.4724647998809814]
memory len:6326
memory used:2952.0
now epsilon is 0.452779943367077, the reward is 245.25 with loss [23.500579953193665, 29.47152543067932] in episode 477
Report: 
rewardSum:245.25
loss:[23.500579953193665, 29.47152543067932]
policies:[1, 2, 3]
qAverage:[58.162967681884766, 0.0]
ws:[8.66593074798584, 8.464639663696289]
memory len:6338
memory used:2951.0
now epsilon is 0.4523273331878917, the reward is 247.25 with loss [16.946053981781006, 30.6565580368042] in episode 478
Report: 
rewardSum:247.25
loss:[16.946053981781006, 30.6565580368042]
policies:[3, 0, 1]
qAverage:[103.11422348022461, 0.0]
ws:[9.649586200714111, 5.748917937278748]
memory len:6346
memory used:2951.0
now epsilon is 0.45187517544918515, the reward is 247.25 with loss [14.471053123474121, 18.48792803287506] in episode 479
Report: 
rewardSum:247.25
loss:[14.471053123474121, 18.48792803287506]
policies:[3, 0, 1]
qAverage:[84.49981943766277, 0.0]
ws:[8.58263874053955, 6.729780197143555]
memory len:6354
memory used:2951.0
now epsilon is 0.45142346969868635, the reward is 37.15999999999997 with loss [23.344918727874756, 15.395252704620361] in episode 480
Report: 
rewardSum:37.15999999999997
loss:[23.344918727874756, 15.395252704620361]
policies:[3, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6362
memory used:2957.0
now epsilon is 0.4509722154845766, the reward is 247.25 with loss [12.24087655544281, 9.152344703674316] in episode 481
Report: 
rewardSum:247.25
loss:[12.24087655544281, 9.152344703674316]
policies:[1, 2, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6370
memory used:2957.0
now epsilon is 0.45052141235548887, the reward is 247.25 with loss [17.73901343345642, 21.978132247924805] in episode 482
Report: 
rewardSum:247.25
loss:[17.73901343345642, 21.978132247924805]
policies:[2, 0, 2]
qAverage:[49.11929702758789, 0.0]
ws:[1.7201331853866577, 0.891228973865509]
memory len:6378
memory used:2963.0
now epsilon is 0.45007105986050727, the reward is 247.25 with loss [16.1978976726532, 12.449319005012512] in episode 483
Report: 
rewardSum:247.25
loss:[16.1978976726532, 12.449319005012512]
policies:[2, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6386
memory used:2964.0
now epsilon is 0.44939637507171437, the reward is 245.25 with loss [29.612095952033997, 29.46280527114868] in episode 484
Report: 
rewardSum:245.25
loss:[29.612095952033997, 29.46280527114868]
policies:[2, 1, 3]
qAverage:[73.99943542480469, 0.0]
ws:[13.072500228881836, 8.105302810668945]
memory len:6398
memory used:2964.0
now epsilon is 0.4489471471921978, the reward is 247.25 with loss [17.30952286720276, 24.341272234916687] in episode 485
Report: 
rewardSum:247.25
loss:[17.30952286720276, 24.341272234916687]
policies:[2, 2, 0]
qAverage:[69.6159896850586, 0.0]
ws:[2.5451817512512207, 1.5831968784332275]
memory len:6406
memory used:2964.0
############# STATE ###############
0-		10-		20*		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.448050038162615, the reward is 243.25 with loss [23.918546199798584, 29.36626386642456] in episode 486
Report: 
rewardSum:243.25
loss:[23.918546199798584, 29.36626386642456]
policies:[1, 1, 6]
qAverage:[52.24956512451172, 0.0]
ws:[-0.09374131262302399, -0.5060239434242249]
memory len:6422
memory used:2964.0
now epsilon is 0.4474902555761866, the reward is 246.25 with loss [33.380940198898315, 27.4848210811615] in episode 487
Report: 
rewardSum:246.25
loss:[33.380940198898315, 27.4848210811615]
policies:[3, 1, 1]
qAverage:[98.49123954772949, 0.0]
ws:[12.814955711364746, 9.771843791007996]
memory len:6432
memory used:2964.0
now epsilon is 0.44681943957512243, the reward is 245.25 with loss [27.413695335388184, 32.40449643135071] in episode 488
Report: 
rewardSum:245.25
loss:[27.413695335388184, 32.40449643135071]
policies:[4, 0, 2]
qAverage:[107.1684555053711, 0.0]
ws:[8.331966972351074, 5.064386463165283]
memory len:6444
memory used:2964.0
now epsilon is 0.44637278766491273, the reward is 37.15999999999997 with loss [23.69963836669922, 14.671940237283707] in episode 489
Report: 
rewardSum:37.15999999999997
loss:[23.69963836669922, 14.671940237283707]
policies:[3, 0, 1]
qAverage:[57.935821533203125, 0.0]
ws:[1.4784317016601562, 0.5434268116950989]
memory len:6452
memory used:2964.0
now epsilon is 0.44592658223914666, the reward is 247.25 with loss [30.19209051132202, 16.43008828163147] in episode 490
Report: 
rewardSum:247.25
loss:[30.19209051132202, 16.43008828163147]
policies:[2, 1, 1]
qAverage:[93.78585815429688, 0.0]
ws:[6.560586611429851, 3.61249041557312]
memory len:6460
memory used:2964.0
now epsilon is 0.4450355090561235, the reward is 243.25 with loss [48.406243562698364, 24.80241894721985] in episode 491
Report: 
rewardSum:243.25
loss:[48.406243562698364, 24.80241894721985]
policies:[4, 1, 3]
qAverage:[101.20020294189453, 0.0]
ws:[9.742478787899017, 7.543402588367462]
memory len:6476
memory used:2964.0
now epsilon is 0.44436837287428155, the reward is 245.25 with loss [28.433861255645752, 19.749377369880676] in episode 492
Report: 
rewardSum:245.25
loss:[28.433861255645752, 19.749377369880676]
policies:[2, 1, 3]
qAverage:[66.6770248413086, 0.0]
ws:[3.778074026107788, 3.4244558811187744]
memory len:6488
memory used:2964.0
now epsilon is 0.44392417111177584, the reward is 247.25 with loss [15.44012975692749, 9.507694721221924] in episode 493
Report: 
rewardSum:247.25
loss:[15.44012975692749, 9.507694721221924]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6496
memory used:2964.0
now epsilon is 0.44281560838873396, the reward is 241.25 with loss [70.03401160240173, 61.91204476356506] in episode 494
Report: 
rewardSum:241.25
loss:[70.03401160240173, 61.91204476356506]
policies:[1, 3, 6]
qAverage:[0.0, 52.029083251953125]
ws:[2.0375730991363525, 2.656064748764038]
memory len:6516
memory used:2964.0
now epsilon is 0.44226236556882204, the reward is 36.15999999999997 with loss [20.83362352848053, 11.49093697220087] in episode 495
Report: 
rewardSum:36.15999999999997
loss:[20.83362352848053, 11.49093697220087]
policies:[1, 2, 2]
qAverage:[0.0, 63.62118021647135]
ws:[3.2346129417419434, 3.884836514790853]
memory len:6526
memory used:2964.0
now epsilon is 0.44159938650325553, the reward is 245.25 with loss [37.731006145477295, 34.031771183013916] in episode 496
Report: 
rewardSum:245.25
loss:[37.731006145477295, 34.031771183013916]
policies:[2, 2, 2]
qAverage:[78.33375549316406, 0.0]
ws:[12.017511367797852, 7.983034133911133]
memory len:6538
memory used:2964.0
now epsilon is 0.441157952688924, the reward is 247.25 with loss [18.00886368751526, 17.282474040985107] in episode 497
Report: 
rewardSum:247.25
loss:[18.00886368751526, 17.282474040985107]
policies:[3, 1, 0]
qAverage:[90.61968231201172, 0.0]
ws:[7.204680263996124, 6.660424649715424]
memory len:6546
memory used:2964.0
now epsilon is 0.4407169601428968, the reward is 247.25 with loss [5.683943390846252, 12.69173812866211] in episode 498
Report: 
rewardSum:247.25
loss:[5.683943390846252, 12.69173812866211]
policies:[2, 1, 1]
qAverage:[69.23776245117188, 0.0]
ws:[3.091593027114868, 2.611250400543213]
memory len:6554
memory used:2964.0
now epsilon is 0.44027640842407095, the reward is 247.25 with loss [17.12190079689026, 17.418628692626953] in episode 499
Report: 
rewardSum:247.25
loss:[17.12190079689026, 17.418628692626953]
policies:[4, 0, 0]
qAverage:[95.16803932189941, 0.0]
ws:[10.627880960702896, 8.431368723511696]
memory len:6562
memory used:2964.0
now epsilon is 0.4395065023313991, the reward is 633.1600000000001 with loss [26.590865924954414, 19.46131980419159] in episode 500
Report: 
rewardSum:633.1600000000001
loss:[26.590865924954414, 19.46131980419159]
policies:[2, 2, 3]
qAverage:[25.035585403442383, 44.23895835876465]
ws:[-7.747987054288387, -9.160145843401551]
memory len:6576
memory used:2964.0
now epsilon is 0.4390671606165387, the reward is 247.25 with loss [9.515777885913849, 16.822330951690674] in episode 501
Report: 
rewardSum:247.25
loss:[9.515777885913849, 16.822330951690674]
policies:[2, 2, 0]
qAverage:[38.589454650878906, 43.91324996948242]
ws:[7.935623794794083, 6.518864333629608]
memory len:6584
memory used:2964.0
now epsilon is 0.43862825807866745, the reward is 247.25 with loss [24.205705404281616, 15.801834404468536] in episode 502
Report: 
rewardSum:247.25
loss:[24.205705404281616, 15.801834404468536]
policies:[2, 0, 2]
qAverage:[68.78433990478516, 0.0]
ws:[4.6576151847839355, 4.640478610992432]
memory len:6592
memory used:2964.0
now epsilon is 0.4381897942787731, the reward is 37.15999999999997 with loss [12.12234652042389, 14.893779516220093] in episode 503
Report: 
rewardSum:37.15999999999997
loss:[12.12234652042389, 14.893779516220093]
policies:[0, 2, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6600
memory used:2964.0
now epsilon is 0.43775176877828204, the reward is 247.25 with loss [32.59397077560425, 13.756089925765991] in episode 504
Report: 
rewardSum:247.25
loss:[32.59397077560425, 13.756089925765991]
policies:[2, 0, 2]
qAverage:[69.99970245361328, 0.0]
ws:[2.3750622272491455, 2.182272434234619]
memory len:6608
memory used:2964.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24*		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.43731418113905934, the reward is 247.25 with loss [18.815088272094727, 20.98188352584839] in episode 505
Report: 
rewardSum:247.25
loss:[18.815088272094727, 20.98188352584839]
policies:[2, 1, 1]
qAverage:[62.778343200683594, 20.612668991088867]
ws:[3.0671078860759735, 2.7731615602970123]
memory len:6616
memory used:2964.0
now epsilon is 0.4366586197127606, the reward is 245.25 with loss [20.09459936618805, 33.03285312652588] in episode 506
Report: 
rewardSum:245.25
loss:[20.09459936618805, 33.03285312652588]
policies:[5, 0, 1]
qAverage:[98.85379600524902, 0.0]
ws:[6.89154976606369, 4.971821665763855]
memory len:6628
memory used:2970.0
now epsilon is 0.4361130692815377, the reward is 36.15999999999997 with loss [19.69621193408966, 32.43373394012451] in episode 507
Report: 
rewardSum:36.15999999999997
loss:[19.69621193408966, 32.43373394012451]
policies:[3, 0, 2]
qAverage:[82.09426371256511, 0.0]
ws:[3.5236790974934897, 3.0634538332621255]
memory len:6638
memory used:2970.0
now epsilon is 0.4354593083973582, the reward is 245.25 with loss [23.125298976898193, 34.555580496788025] in episode 508
Report: 
rewardSum:245.25
loss:[23.125298976898193, 34.555580496788025]
policies:[4, 0, 2]
qAverage:[93.39215278625488, 0.0]
ws:[7.1491923332214355, 5.553895056247711]
memory len:6650
memory used:2970.0
now epsilon is 0.4349152563558973, the reward is 36.15999999999997 with loss [41.14260816574097, 30.309767246246338] in episode 509
Report: 
rewardSum:36.15999999999997
loss:[41.14260816574097, 30.309767246246338]
policies:[1, 1, 3]
qAverage:[60.28921127319336, 0.0]
ws:[7.836338520050049, 7.362245082855225]
memory len:6660
memory used:2970.0
now epsilon is 0.43426329106853084, the reward is 245.25 with loss [22.589683890342712, 34.939810276031494] in episode 510
Report: 
rewardSum:245.25
loss:[22.589683890342712, 34.939810276031494]
policies:[0, 2, 4]
qAverage:[0.0, 46.8108024597168]
ws:[0.8786025643348694, 0.8867113590240479]
memory len:6672
memory used:2970.0
now epsilon is 0.43361230311808163, the reward is 245.25 with loss [47.475807428359985, 30.912447690963745] in episode 511
Report: 
rewardSum:245.25
loss:[47.475807428359985, 30.912447690963745]
policies:[3, 1, 2]
qAverage:[74.45887756347656, 0.0]
ws:[10.228882789611816, 6.6252665519714355]
memory len:6684
memory used:2970.0
now epsilon is 0.4329622910394603, the reward is 245.25 with loss [28.176984906196594, 21.835183024406433] in episode 512
Report: 
rewardSum:245.25
loss:[28.176984906196594, 21.835183024406433]
policies:[4, 1, 1]
qAverage:[108.2458282470703, 0.0]
ws:[6.624682331085205, 5.3141378402709964]
memory len:6696
memory used:2970.0
now epsilon is 0.4323132533697737, the reward is 245.25 with loss [27.485103875398636, 43.635759353637695] in episode 513
Report: 
rewardSum:245.25
loss:[27.485103875398636, 43.635759353637695]
policies:[2, 1, 3]
qAverage:[62.727699279785156, 0.0]
ws:[2.2057647705078125, 1.8759410381317139]
memory len:6708
memory used:2970.0
now epsilon is 0.4318811022068561, the reward is 247.25 with loss [23.675907373428345, 24.164672374725342] in episode 514
Report: 
rewardSum:247.25
loss:[23.675907373428345, 24.164672374725342]
policies:[2, 2, 0]
qAverage:[72.44586944580078, 21.68372917175293]
ws:[6.660346180200577, 4.764096811413765]
memory len:6716
memory used:2971.0
now epsilon is 0.4314493830330718, the reward is 247.25 with loss [14.481228590011597, 32.81354761123657] in episode 515
Report: 
rewardSum:247.25
loss:[14.481228590011597, 32.81354761123657]
policies:[1, 2, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6724
memory used:2970.0
now epsilon is 0.4309103408927393, the reward is 246.25 with loss [22.342883586883545, 22.48090386390686] in episode 516
Report: 
rewardSum:246.25
loss:[22.342883586883545, 22.48090386390686]
policies:[3, 1, 1]
qAverage:[98.04046058654785, 0.0]
ws:[7.102227568626404, 4.842488408088684]
memory len:6734
memory used:2970.0
now epsilon is 0.43026437922521055, the reward is 245.25 with loss [24.88535165786743, 20.01908838748932] in episode 517
Report: 
rewardSum:245.25
loss:[24.88535165786743, 20.01908838748932]
policies:[2, 1, 3]
qAverage:[76.7935562133789, 0.0]
ws:[10.229228019714355, 6.250490665435791]
memory len:6746
memory used:2970.0
now epsilon is 0.42918992758932134, the reward is 241.25 with loss [47.72381556034088, 64.29013419151306] in episode 518
Report: 
rewardSum:241.25
loss:[47.72381556034088, 64.29013419151306]
policies:[5, 2, 3]
qAverage:[100.95617980957032, 0.0]
ws:[7.204409193992615, 5.337956178188324]
memory len:6766
memory used:2970.0
now epsilon is 0.42876089858113225, the reward is 247.25 with loss [12.94105577468872, 17.329535961151123] in episode 519
Report: 
rewardSum:247.25
loss:[12.94105577468872, 17.329535961151123]
policies:[1, 2, 1]
qAverage:[0.0, 52.8466796875]
ws:[2.8229897022247314, 2.9458720684051514]
memory len:6774
memory used:2970.0
now epsilon is 0.427904126740494, the reward is 243.25 with loss [51.30404019355774, 66.50355577468872] in episode 520
Report: 
rewardSum:243.25
loss:[51.30404019355774, 66.50355577468872]
policies:[3, 3, 2]
qAverage:[74.87866973876953, 17.510302734375]
ws:[5.834970092773437, 4.732864809036255]
memory len:6790
memory used:2970.0
now epsilon is 0.4268355691020301, the reward is 31.159999999999968 with loss [41.24491572380066, 39.099109411239624] in episode 521
Report: 
rewardSum:31.159999999999968
loss:[41.24491572380066, 39.099109411239624]
policies:[1, 2, 7]
qAverage:[0.0, 54.9595832824707]
ws:[2.8053979873657227, 3.2841553688049316]
memory len:6810
memory used:2971.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34*		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.42608916684441867, the reward is 244.25 with loss [29.86999225616455, 25.04635190963745] in episode 522
Report: 
rewardSum:244.25
loss:[29.86999225616455, 25.04635190963745]
policies:[0, 1, 6]
qAverage:[0.0, 54.307979583740234]
ws:[3.554723024368286, 3.722421646118164]
memory len:6824
memory used:2976.0
now epsilon is 0.42566323743438295, the reward is 247.25 with loss [14.257018327713013, 14.003741949796677] in episode 523
Report: 
rewardSum:247.25
loss:[14.257018327713013, 14.003741949796677]
policies:[1, 2, 1]
qAverage:[0.0, 45.27400207519531]
ws:[-0.582467794418335, 0.19307631254196167]
memory len:6832
memory used:2977.0
now epsilon is 0.4252377337940604, the reward is 247.25 with loss [20.230384349822998, 25.225388526916504] in episode 524
Report: 
rewardSum:247.25
loss:[20.230384349822998, 25.225388526916504]
policies:[1, 2, 1]
qAverage:[40.51231002807617, 50.755714416503906]
ws:[10.888729274272919, 9.940748572349548]
memory len:6840
memory used:2976.0
now epsilon is 0.4248126554978408, the reward is 247.25 with loss [18.01163351535797, 16.025105237960815] in episode 525
Report: 
rewardSum:247.25
loss:[18.01163351535797, 16.025105237960815]
policies:[0, 1, 3]
qAverage:[0.0, 44.377166748046875]
ws:[1.7888388633728027, 1.9460433721542358]
memory len:6848
memory used:2977.0
now epsilon is 0.4243880021205397, the reward is 247.25 with loss [19.38183569908142, 22.92861533164978] in episode 526
Report: 
rewardSum:247.25
loss:[19.38183569908142, 22.92861533164978]
policies:[2, 1, 1]
qAverage:[58.21564865112305, 0.0]
ws:[-0.23618946969509125, -0.39369261264801025]
memory len:6856
memory used:2977.0
now epsilon is 0.4239637732373974, the reward is 247.25 with loss [10.77452290058136, 21.429633259773254] in episode 527
Report: 
rewardSum:247.25
loss:[10.77452290058136, 21.429633259773254]
policies:[1, 2, 1]
qAverage:[57.49516296386719, 0.0]
ws:[-0.040334880352020264, -0.29734620451927185]
memory len:6864
memory used:2977.0
now epsilon is 0.42311658725667345, the reward is 243.25 with loss [25.84398341178894, 33.62154698371887] in episode 528
Report: 
rewardSum:243.25
loss:[25.84398341178894, 33.62154698371887]
policies:[5, 0, 3]
qAverage:[106.55490112304688, 0.0]
ws:[10.997223281860352, 9.229565459489823]
memory len:6880
memory used:2982.0
now epsilon is 0.4224823089153899, the reward is 245.25 with loss [47.134427309036255, 41.629436016082764] in episode 529
Report: 
rewardSum:245.25
loss:[47.134427309036255, 41.629436016082764]
policies:[4, 0, 2]
qAverage:[92.88224283854167, 0.0]
ws:[7.3356923659642534, 5.090038865804672]
memory len:6892
memory used:2982.0
now epsilon is 0.4220599850109369, the reward is 247.25 with loss [23.59251356124878, 18.492255210876465] in episode 530
Report: 
rewardSum:247.25
loss:[23.59251356124878, 18.492255210876465]
policies:[3, 1, 0]
qAverage:[58.486412048339844, 0.0]
ws:[0.21214382350444794, -0.08676913380622864]
memory len:6900
memory used:2982.0
now epsilon is 0.4214272905827875, the reward is 245.25 with loss [24.410988807678223, 31.454638957977295] in episode 531
Report: 
rewardSum:245.25
loss:[24.410988807678223, 31.454638957977295]
policies:[3, 0, 3]
qAverage:[92.57714462280273, 0.0]
ws:[2.345745176076889, 1.1295960694551468]
memory len:6912
memory used:2982.0
now epsilon is 0.4205851731307469, the reward is 243.25 with loss [38.16256111860275, 52.05947542190552] in episode 532
Report: 
rewardSum:243.25
loss:[38.16256111860275, 52.05947542190552]
policies:[3, 2, 3]
qAverage:[100.63458061218262, 0.0]
ws:[10.531876683235168, 7.197118699550629]
memory len:6928
memory used:2982.0
now epsilon is 0.4199546895382424, the reward is 245.25 with loss [21.00859248638153, 29.001563549041748] in episode 533
Report: 
rewardSum:245.25
loss:[21.00859248638153, 29.001563549041748]
policies:[4, 0, 2]
qAverage:[102.2376880645752, 0.0]
ws:[11.473448395729065, 8.149468302726746]
memory len:6940
memory used:2982.0
now epsilon is 0.4195348923054672, the reward is 37.15999999999997 with loss [12.251217722892761, 15.551302552223206] in episode 534
Report: 
rewardSum:37.15999999999997
loss:[12.251217722892761, 15.551302552223206]
policies:[1, 1, 2]
qAverage:[66.149169921875, 0.0]
ws:[4.463510513305664, 4.288217067718506]
memory len:6948
memory used:2982.0
now epsilon is 0.4189059831498906, the reward is 245.25 with loss [17.454064548015594, 23.82013165950775] in episode 535
Report: 
rewardSum:245.25
loss:[17.454064548015594, 23.82013165950775]
policies:[4, 1, 1]
qAverage:[56.042625427246094, 0.0]
ws:[2.0150954723358154, 1.6856874227523804]
memory len:6960
memory used:2983.0
now epsilon is 0.4180689039026331, the reward is 33.15999999999997 with loss [31.793519496917725, 50.4006507396698] in episode 536
Report: 
rewardSum:33.15999999999997
loss:[31.793519496917725, 50.4006507396698]
policies:[1, 2, 5]
qAverage:[61.28496170043945, 0.0]
ws:[5.206641674041748, 4.430179119110107]
memory len:6976
memory used:2984.0
now epsilon is 0.4178598955799883, the reward is -1.0 with loss [6.973477125167847, 10.914039611816406] in episode 537
Report: 
rewardSum:-1.0
loss:[6.973477125167847, 10.914039611816406]
policies:[1, 0, 1]
qAverage:[55.11738586425781, 0.0]
ws:[1.154938817024231, 0.24101755023002625]
memory len:6980
memory used:2984.0
now epsilon is 0.4173378318076657, the reward is -4.0 with loss [16.08008186519146, 20.800912857055664] in episode 538
Report: 
rewardSum:-4.0
loss:[16.08008186519146, 20.800912857055664]
policies:[1, 1, 3]
qAverage:[63.710445404052734, 0.0]
ws:[5.348484516143799, 4.966073513031006]
memory len:6990
memory used:2984.0
now epsilon is 0.4160875383970069, the reward is 239.25 with loss [53.90313336253166, 62.443663597106934] in episode 539
Report: 
rewardSum:239.25
loss:[53.90313336253166, 62.443663597106934]
policies:[3, 1, 8]
qAverage:[87.53248087565105, 0.0]
ws:[5.968141078948975, 4.8409139315287275]
memory len:7014
memory used:2995.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43*		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.4154637970414758, the reward is 245.25 with loss [32.49222469329834, 17.3085515499115] in episode 540
Report: 
rewardSum:245.25
loss:[32.49222469329834, 17.3085515499115]
policies:[4, 0, 2]
qAverage:[97.06830406188965, 0.0]
ws:[8.76386296749115, 6.604219526052475]
memory len:7026
memory used:2995.0
now epsilon is 0.4150484890173934, the reward is 247.25 with loss [20.00016188621521, 14.811979293823242] in episode 541
Report: 
rewardSum:247.25
loss:[20.00016188621521, 14.811979293823242]
policies:[3, 1, 0]
qAverage:[67.85711669921875, 0.0]
ws:[1.9392191171646118, 1.662693738937378]
memory len:7034
memory used:2995.0
now epsilon is 0.4144263052621475, the reward is -5.0 with loss [20.73786300420761, 24.17188262939453] in episode 542
Report: 
rewardSum:-5.0
loss:[20.73786300420761, 24.17188262939453]
policies:[2, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7046
memory used:2995.0
now epsilon is 0.4137016029358818, the reward is 244.25 with loss [23.072919607162476, 20.936465814709663] in episode 543
Report: 
rewardSum:244.25
loss:[23.072919607162476, 20.936465814709663]
policies:[0, 3, 4]
qAverage:[0.0, 52.23069763183594]
ws:[2.9888129234313965, 3.108149528503418]
memory len:7060
memory used:2995.0
now epsilon is 0.4130814382474733, the reward is 245.25 with loss [28.862062454223633, 22.85586988925934] in episode 544
Report: 
rewardSum:245.25
loss:[28.862062454223633, 22.85586988925934]
policies:[1, 0, 5]
qAverage:[77.95923614501953, 0.0]
ws:[9.316075325012207, 6.780733108520508]
memory len:7072
memory used:2995.0
now epsilon is 0.41266851168894925, the reward is 247.25 with loss [10.956754922866821, 13.594839572906494] in episode 545
Report: 
rewardSum:247.25
loss:[10.956754922866821, 13.594839572906494]
policies:[1, 1, 2]
qAverage:[77.79810333251953, 0.0]
ws:[8.975617408752441, 6.532217502593994]
memory len:7080
memory used:2995.0
now epsilon is 0.412255997902162, the reward is 247.25 with loss [28.78964674472809, 17.4236478805542] in episode 546
Report: 
rewardSum:247.25
loss:[28.78964674472809, 17.4236478805542]
policies:[4, 0, 0]
qAverage:[105.96542053222656, 0.0]
ws:[7.065417915582657, 5.913178342580795]
memory len:7088
memory used:2995.0
now epsilon is 0.41184389647449465, the reward is 247.25 with loss [21.57687270641327, 9.718258678913116] in episode 547
Report: 
rewardSum:247.25
loss:[21.57687270641327, 9.718258678913116]
policies:[0, 3, 1]
qAverage:[0.0, 43.77379608154297]
ws:[-0.33024632930755615, -0.22556743025779724]
memory len:7096
memory used:2995.0
now epsilon is 0.41153509076643435, the reward is -2.0 with loss [15.181859731674194, 22.568315505981445] in episode 548
Report: 
rewardSum:-2.0
loss:[15.181859731674194, 22.568315505981445]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7102
memory used:2996.0
now epsilon is 0.41112370997560765, the reward is 37.15999999999997 with loss [18.242183446884155, 19.989386558532715] in episode 549
Report: 
rewardSum:37.15999999999997
loss:[18.242183446884155, 19.989386558532715]
policies:[2, 0, 2]
qAverage:[84.54024759928386, 0.0]
ws:[2.0220600366592407, 1.7879156470298767]
memory len:7110
memory used:2996.0
now epsilon is 0.4099945319515294, the reward is 240.25 with loss [49.25951385498047, 74.17611837387085] in episode 550
Report: 
rewardSum:240.25
loss:[49.25951385498047, 74.17611837387085]
policies:[3, 2, 6]
qAverage:[75.4148661295573, 0.0]
ws:[5.719013651212056, 5.095220645268758]
memory len:7132
memory used:2996.0
now epsilon is 0.4095846911419044, the reward is 247.25 with loss [14.250630378723145, 22.912423372268677] in episode 551
Report: 
rewardSum:247.25
loss:[14.250630378723145, 22.912423372268677]
policies:[3, 0, 1]
qAverage:[103.00254440307617, 0.0]
ws:[4.8923399746418, 3.0748026967048645]
memory len:7140
memory used:2997.0
now epsilon is 0.4091752600194242, the reward is 247.25 with loss [17.532755613327026, 13.3211270570755] in episode 552
Report: 
rewardSum:247.25
loss:[17.532755613327026, 13.3211270570755]
policies:[2, 1, 1]
qAverage:[101.10737609863281, 0.0]
ws:[8.468130588531494, 6.346054871877034]
memory len:7148
memory used:2997.0
now epsilon is 0.4089706979628683, the reward is -1.0 with loss [7.944659233093262, 13.974558115005493] in episode 553
Report: 
rewardSum:-1.0
loss:[7.944659233093262, 13.974558115005493]
policies:[1, 0, 1]
qAverage:[56.04301452636719, 0.0]
ws:[1.2653634548187256, 0.7964535355567932]
memory len:7152
memory used:2997.0
now epsilon is 0.4087662381745555, the reward is -1.0 with loss [8.400989055633545, 4.8723567724227905] in episode 554
Report: 
rewardSum:-1.0
loss:[8.400989055633545, 4.8723567724227905]
policies:[1, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7156
memory used:2996.0
now epsilon is 0.408357625198174, the reward is 247.25 with loss [10.560678482055664, 11.311087489128113] in episode 555
Report: 
rewardSum:247.25
loss:[10.560678482055664, 11.311087489128113]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7164
memory used:2996.0
now epsilon is 0.4079494206815646, the reward is 247.25 with loss [20.348251342773438, 14.158233523368835] in episode 556
Report: 
rewardSum:247.25
loss:[20.348251342773438, 14.158233523368835]
policies:[1, 1, 2]
qAverage:[82.04966735839844, 0.0]
ws:[9.293343544006348, 7.072797775268555]
memory len:7172
memory used:2996.0
now epsilon is 0.4074397388103665, the reward is 246.25 with loss [45.37384033203125, 15.125566601753235] in episode 557
Report: 
rewardSum:246.25
loss:[45.37384033203125, 15.125566601753235]
policies:[2, 1, 2]
qAverage:[90.39115905761719, 0.0]
ws:[7.643376429875691, 5.6199266115824384]
memory len:7182
memory used:2997.0
now epsilon is 0.40703245183599485, the reward is 247.25 with loss [17.779020309448242, 28.0921688079834] in episode 558
Report: 
rewardSum:247.25
loss:[17.779020309448242, 28.0921688079834]
policies:[1, 2, 1]
qAverage:[55.70648193359375, 0.0]
ws:[1.0862106084823608, 0.5704011917114258]
memory len:7190
memory used:2997.0
now epsilon is 0.40682896104960514, the reward is -1.0 with loss [11.563251495361328, 10.434106826782227] in episode 559
Report: 
rewardSum:-1.0
loss:[11.563251495361328, 10.434106826782227]
policies:[1, 0, 1]
qAverage:[53.37332534790039, 0.0]
ws:[0.004868221469223499, -0.3367835283279419]
memory len:7194
memory used:2997.0
now epsilon is 0.40642228462399077, the reward is 247.25 with loss [14.33730936050415, 11.895331501960754] in episode 560
Report: 
rewardSum:247.25
loss:[14.33730936050415, 11.895331501960754]
policies:[0, 2, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7202
memory used:2997.0
now epsilon is 0.40581303209096353, the reward is 245.25 with loss [24.267171382904053, 36.8428258895874] in episode 561
Report: 
rewardSum:245.25
loss:[24.267171382904053, 36.8428258895874]
policies:[3, 0, 3]
qAverage:[103.65793037414551, 0.0]
ws:[7.106408268213272, 3.919275561347604]
memory len:7214
memory used:2997.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23*		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.4050021158446123, the reward is 243.25 with loss [44.073612451553345, 37.75021147727966] in episode 562
Report: 
rewardSum:243.25
loss:[44.073612451553345, 37.75021147727966]
policies:[6, 0, 2]
qAverage:[114.04641596476237, 0.0]
ws:[11.368854999542236, 7.764736175537109]
memory len:7230
memory used:2997.0
now epsilon is 0.40459726557925013, the reward is 247.25 with loss [25.545954704284668, 20.811829090118408] in episode 563
Report: 
rewardSum:247.25
loss:[25.545954704284668, 20.811829090118408]
policies:[3, 1, 0]
qAverage:[63.68646240234375, 0.0]
ws:[8.952376365661621, 7.210231781005859]
memory len:7238
memory used:2997.0
now epsilon is 0.4041928200123598, the reward is -3.0 with loss [22.290541172027588, 15.465936422348022] in episode 564
Report: 
rewardSum:-3.0
loss:[22.290541172027588, 15.465936422348022]
policies:[1, 1, 2]
qAverage:[55.797271728515625, 0.0]
ws:[1.14790940284729, -0.122867651283741]
memory len:7246
memory used:2997.0
now epsilon is 0.4035869095868235, the reward is 245.25 with loss [23.772914171218872, 19.712934970855713] in episode 565
Report: 
rewardSum:245.25
loss:[23.772914171218872, 19.712934970855713]
policies:[4, 0, 2]
qAverage:[107.73444557189941, 0.0]
ws:[10.098798632621765, 6.82946503162384]
memory len:7258
memory used:2997.0
now epsilon is 0.4031834739971052, the reward is 247.25 with loss [22.17592191696167, 24.147292852401733] in episode 566
Report: 
rewardSum:247.25
loss:[22.17592191696167, 24.147292852401733]
policies:[4, 0, 0]
qAverage:[109.07514038085938, 0.0]
ws:[8.135602474212646, 5.059730744361877]
memory len:7266
memory used:2997.0
now epsilon is 0.40257907664464526, the reward is 245.25 with loss [28.87878680229187, 21.76170313358307] in episode 567
Report: 
rewardSum:245.25
loss:[28.87878680229187, 21.76170313358307]
policies:[4, 0, 2]
qAverage:[112.278125, 0.0]
ws:[12.70229787826538, 9.293830013275146]
memory len:7278
memory used:2997.0
now epsilon is 0.4020761043478672, the reward is 246.25 with loss [29.94541835784912, 23.716696739196777] in episode 568
Report: 
rewardSum:246.25
loss:[29.94541835784912, 23.716696739196777]
policies:[2, 1, 2]
qAverage:[83.20724487304688, 0.0]
ws:[11.520600318908691, 7.165619850158691]
memory len:7288
memory used:2997.0
now epsilon is 0.4015737604521811, the reward is 36.15999999999997 with loss [20.564129114151, 23.064423769712448] in episode 569
Report: 
rewardSum:36.15999999999997
loss:[20.564129114151, 23.064423769712448]
policies:[2, 1, 2]
qAverage:[68.1020622253418, 21.542728424072266]
ws:[7.48208874464035, 6.929560363292694]
memory len:7298
memory used:2997.0
now epsilon is 0.40117233725679236, the reward is 247.25 with loss [22.19814705848694, 16.515453457832336] in episode 570
Report: 
rewardSum:247.25
loss:[22.19814705848694, 16.515453457832336]
policies:[2, 1, 1]
qAverage:[41.954376220703125, 28.02355702718099]
ws:[2.724390188852946, 2.708060105641683]
memory len:7306
memory used:2997.0
now epsilon is 0.40097177616143503, the reward is -1.0 with loss [9.329182386398315, 15.658774852752686] in episode 571
Report: 
rewardSum:-1.0
loss:[9.329182386398315, 15.658774852752686]
policies:[0, 1, 1]
qAverage:[0.0, 43.15879440307617]
ws:[0.05321941152215004, 0.5066272020339966]
memory len:7310
memory used:2997.0
now epsilon is 0.4005709547246305, the reward is 247.25 with loss [20.133012294769287, 16.505924224853516] in episode 572
Report: 
rewardSum:247.25
loss:[20.133012294769287, 16.505924224853516]
policies:[1, 2, 1]
qAverage:[0.0, 42.369346618652344]
ws:[0.9069584608078003, 1.200972080230713]
memory len:7318
memory used:2997.0
now epsilon is 0.3998704810842331, the reward is 244.25 with loss [25.082873702049255, 19.69796061515808] in episode 573
Report: 
rewardSum:244.25
loss:[25.082873702049255, 19.69796061515808]
policies:[1, 2, 4]
qAverage:[0.0, 42.60847091674805]
ws:[2.304236888885498, 2.3300058841705322]
memory len:7332
memory used:2998.0
now epsilon is 0.39937089283945665, the reward is 36.15999999999997 with loss [20.228445768356323, 25.340581968426704] in episode 574
Report: 
rewardSum:36.15999999999997
loss:[20.228445768356323, 25.340581968426704]
policies:[3, 0, 2]
qAverage:[89.65147654215495, 0.0]
ws:[3.594139893849691, 2.6813499132792153]
memory len:7342
memory used:2997.0
now epsilon is 0.39897167168574293, the reward is 247.25 with loss [11.054753303527832, 10.777242064476013] in episode 575
Report: 
rewardSum:247.25
loss:[11.054753303527832, 10.777242064476013]
policies:[1, 2, 1]
qAverage:[84.17835998535156, 0.0]
ws:[12.587702751159668, 8.68088150024414]
memory len:7350
memory used:2997.0
now epsilon is 0.3985728496034999, the reward is 247.25 with loss [18.923000812530518, 17.93925905227661] in episode 576
Report: 
rewardSum:247.25
loss:[18.923000812530518, 17.93925905227661]
policies:[4, 0, 0]
qAverage:[109.63501739501953, 0.0]
ws:[9.016084766387939, 6.576130533218384]
memory len:7358
memory used:2998.0
now epsilon is 0.39817442619380583, the reward is 247.25 with loss [15.71449065208435, 25.402012825012207] in episode 577
Report: 
rewardSum:247.25
loss:[15.71449065208435, 25.402012825012207]
policies:[3, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7366
memory used:2998.0
now epsilon is 0.39777640105813755, the reward is 247.25 with loss [23.448501586914062, 17.696643590927124] in episode 578
Report: 
rewardSum:247.25
loss:[23.448501586914062, 17.696643590927124]
policies:[1, 2, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7374
memory used:2998.0
now epsilon is 0.3973787737983704, the reward is 247.25 with loss [23.805508613586426, 12.081421375274658] in episode 579
Report: 
rewardSum:247.25
loss:[23.805508613586426, 12.081421375274658]
policies:[3, 1, 0]
qAverage:[95.63046264648438, 0.0]
ws:[14.880860646565756, 11.84941291809082]
memory len:7382
memory used:2998.0
now epsilon is 0.39698154401677765, the reward is 247.25 with loss [31.837594032287598, 20.033125162124634] in episode 580
Report: 
rewardSum:247.25
loss:[31.837594032287598, 20.033125162124634]
policies:[4, 0, 0]
qAverage:[102.14839935302734, 0.0]
ws:[8.43289801478386, 5.643516838550568]
memory len:7390
memory used:2977.0
now epsilon is 0.3967830780561158, the reward is -1.0 with loss [9.704089641571045, 9.007795333862305] in episode 581
Report: 
rewardSum:-1.0
loss:[9.704089641571045, 9.007795333862305]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7394
memory used:2977.0
now epsilon is 0.39638644374691656, the reward is 247.25 with loss [17.35394310951233, 12.677039504051208] in episode 582
Report: 
rewardSum:247.25
loss:[17.35394310951233, 12.677039504051208]
policies:[2, 1, 1]
qAverage:[54.2964973449707, 0.0]
ws:[1.1342709064483643, 0.3684592843055725]
memory len:7402
memory used:2977.0
now epsilon is 0.39589120837183267, the reward is 246.25 with loss [34.12971782684326, 22.06663191318512] in episode 583
Report: 
rewardSum:246.25
loss:[34.12971782684326, 22.06663191318512]
policies:[3, 1, 1]
qAverage:[89.63327534993489, 0.0]
ws:[2.2167645059525967, 1.7458216150601704]
memory len:7412
memory used:2977.0
now epsilon is 0.3952977425835901, the reward is 245.25 with loss [47.97059917449951, 31.092272758483887] in episode 584
Report: 
rewardSum:245.25
loss:[47.97059917449951, 31.092272758483887]
policies:[3, 1, 2]
qAverage:[111.54090690612793, 0.0]
ws:[10.816184997558594, 8.463029980659485]
memory len:7424
memory used:2960.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44*		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.39470516643784104, the reward is 245.25 with loss [26.744677543640137, 28.485984683036804] in episode 585
Report: 
rewardSum:245.25
loss:[26.744677543640137, 28.485984683036804]
policies:[5, 0, 1]
qAverage:[101.53730583190918, 0.0]
ws:[4.385992705821991, 3.4516846239566803]
memory len:7436
memory used:2752.0
now epsilon is 0.39391644649374763, the reward is 243.25 with loss [47.06944668292999, 40.66241693496704] in episode 586
Report: 
rewardSum:243.25
loss:[47.06944668292999, 40.66241693496704]
policies:[2, 1, 5]
qAverage:[64.4717025756836, 0.0]
ws:[1.278106451034546, 0.43074652552604675]
memory len:7452
memory used:2752.0
now epsilon is 0.3935226777413031, the reward is 247.25 with loss [20.472618609666824, 24.981369256973267] in episode 587
Report: 
rewardSum:247.25
loss:[20.472618609666824, 24.981369256973267]
policies:[2, 1, 1]
qAverage:[82.95672607421875, 0.0]
ws:[15.376763343811035, 8.597285270690918]
memory len:7460
memory used:2752.0
now epsilon is 0.3931293026099724, the reward is 37.15999999999997 with loss [17.956154584884644, 21.912259340286255] in episode 588
Report: 
rewardSum:37.15999999999997
loss:[17.956154584884644, 21.912259340286255]
policies:[0, 3, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7468
memory used:2756.0
now epsilon is 0.39263813662610547, the reward is -4.0 with loss [18.238595008850098, 18.736247062683105] in episode 589
Report: 
rewardSum:-4.0
loss:[18.238595008850098, 18.736247062683105]
policies:[0, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7478
memory used:2756.0
now epsilon is 0.3922456457042423, the reward is 247.25 with loss [15.257442235946655, 25.871371269226074] in episode 590
Report: 
rewardSum:247.25
loss:[15.257442235946655, 25.871371269226074]
policies:[2, 1, 1]
qAverage:[103.81512959798177, 0.0]
ws:[11.53878911336263, 6.273956457773845]
memory len:7486
memory used:2757.0
now epsilon is 0.3918535471261414, the reward is 247.25 with loss [17.31196641921997, 20.12376093864441] in episode 591
Report: 
rewardSum:247.25
loss:[17.31196641921997, 20.12376093864441]
policies:[1, 2, 1]
qAverage:[76.41252136230469, 0.0]
ws:[3.7545483112335205, 2.402740716934204]
memory len:7494
memory used:2757.0
now epsilon is 0.39126613404572136, the reward is 245.25 with loss [40.240792989730835, 34.699866771698] in episode 592
Report: 
rewardSum:245.25
loss:[40.240792989730835, 34.699866771698]
policies:[2, 1, 3]
qAverage:[94.13843536376953, 0.0]
ws:[8.72752571105957, 4.324950019518535]
memory len:7506
memory used:2757.0
now epsilon is 0.3907772958583703, the reward is -4.0 with loss [41.16552495956421, 28.077763557434082] in episode 593
Report: 
rewardSum:-4.0
loss:[41.16552495956421, 28.077763557434082]
policies:[1, 0, 4]
qAverage:[65.19820404052734, 0.0]
ws:[9.677080154418945, 8.304953575134277]
memory len:7516
memory used:2757.0
now epsilon is 0.38989892567890183, the reward is 242.25 with loss [38.499454736709595, 34.89646053314209] in episode 594
Report: 
rewardSum:242.25
loss:[38.499454736709595, 34.89646053314209]
policies:[4, 1, 4]
qAverage:[111.48916320800781, 0.0]
ws:[8.415401649475097, 6.484969806671143]
memory len:7534
memory used:2757.0
now epsilon is 0.38931444269880583, the reward is 245.25 with loss [23.630725353956223, 24.460907220840454] in episode 595
Report: 
rewardSum:245.25
loss:[23.630725353956223, 24.460907220840454]
policies:[2, 2, 2]
qAverage:[91.91742451985677, 0.0]
ws:[0.4041493485371272, 0.2869338591893514]
memory len:7546
memory used:2757.0
now epsilon is 0.3889252742246925, the reward is 247.25 with loss [17.97829532623291, 21.861527681350708] in episode 596
Report: 
rewardSum:247.25
loss:[17.97829532623291, 21.861527681350708]
policies:[2, 2, 0]
qAverage:[80.41093444824219, 0.0]
ws:[12.998549461364746, 8.361494064331055]
memory len:7554
memory used:2757.0
now epsilon is 0.38853649477313934, the reward is 247.25 with loss [27.03657054901123, 20.073806881904602] in episode 597
Report: 
rewardSum:247.25
loss:[27.03657054901123, 20.073806881904602]
policies:[3, 1, 0]
qAverage:[75.24258422851562, 0.0]
ws:[4.643937110900879, 3.7864232063293457]
memory len:7562
memory used:2757.0
now epsilon is 0.3881481039552697, the reward is 37.15999999999997 with loss [25.752567291259766, 10.500321924686432] in episode 598
Report: 
rewardSum:37.15999999999997
loss:[25.752567291259766, 10.500321924686432]
policies:[2, 1, 1]
qAverage:[80.93150838216145, 0.0]
ws:[7.300309340159099, 5.843535145123799]
memory len:7570
memory used:2758.0
now epsilon is 0.38776010138259576, the reward is 247.25 with loss [22.928618907928467, 21.81688094139099] in episode 599
Report: 
rewardSum:247.25
loss:[22.928618907928467, 21.81688094139099]
policies:[3, 1, 0]
qAverage:[91.89389292399089, 0.0]
ws:[9.535031954447428, 7.148158073425293]
memory len:7578
memory used:2758.0
now epsilon is 0.38708202992830604, the reward is 244.25 with loss [25.75473153591156, 46.00905990600586] in episode 600
Report: 
rewardSum:244.25
loss:[25.75473153591156, 46.00905990600586]
policies:[2, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7592
memory used:2758.0
now epsilon is 0.38650176965187627, the reward is 245.25 with loss [26.417492985725403, 25.40903639793396] in episode 601
Report: 
rewardSum:245.25
loss:[26.417492985725403, 25.40903639793396]
policies:[4, 1, 1]
qAverage:[88.37760670979817, 0.0]
ws:[3.8558784325917563, 3.448729932308197]
memory len:7604
memory used:2758.0
now epsilon is 0.38582589862724287, the reward is 244.25 with loss [27.95886504650116, 31.806114435195923] in episode 602
Report: 
rewardSum:244.25
loss:[27.95886504650116, 31.806114435195923]
policies:[2, 0, 5]
qAverage:[90.7643814086914, 0.0]
ws:[0.5229223966598511, -0.2970474561055501]
memory len:7618
memory used:2758.0
now epsilon is 0.3853438573348678, the reward is 246.25 with loss [31.67739200592041, 18.92307484149933] in episode 603
Report: 
rewardSum:246.25
loss:[31.67739200592041, 18.92307484149933]
policies:[3, 1, 1]
qAverage:[66.03248596191406, 0.0]
ws:[9.720651626586914, 8.432945251464844]
memory len:7628
memory used:2758.0
now epsilon is 0.38515120949019144, the reward is -1.0 with loss [9.287243366241455, 5.857544839382172] in episode 604
Report: 
rewardSum:-1.0
loss:[9.287243366241455, 5.857544839382172]
policies:[1, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7632
memory used:2760.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12*		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.3844777001739692, the reward is 244.25 with loss [25.895311951637268, 37.133583068847656] in episode 605
Report: 
rewardSum:244.25
loss:[25.895311951637268, 37.133583068847656]
policies:[1, 3, 3]
qAverage:[57.13409423828125, 0.0]
ws:[1.9966425895690918, 1.667104959487915]
memory len:7646
memory used:2759.0
now epsilon is 0.3839013439514254, the reward is 245.25 with loss [28.952526330947876, 34.50330829620361] in episode 606
Report: 
rewardSum:245.25
loss:[28.952526330947876, 34.50330829620361]
policies:[3, 0, 3]
qAverage:[73.88849639892578, 0.0]
ws:[9.433951377868652, 5.825984477996826]
memory len:7658
memory used:2759.0
now epsilon is 0.3833258517230616, the reward is 245.25 with loss [23.926909923553467, 31.068270683288574] in episode 607
Report: 
rewardSum:245.25
loss:[23.926909923553467, 31.068270683288574]
policies:[4, 1, 1]
qAverage:[104.40933532714844, 0.0]
ws:[8.302688455581665, 5.745973134040833]
memory len:7670
memory used:2759.0
now epsilon is 0.3829426695945766, the reward is 247.25 with loss [12.209317028522491, 22.705185651779175] in episode 608
Report: 
rewardSum:247.25
loss:[12.209317028522491, 22.705185651779175]
policies:[2, 1, 1]
qAverage:[84.66717529296875, 0.0]
ws:[2.5344966650009155, 1.869232705173393]
memory len:7678
memory used:2759.0
now epsilon is 0.38246423053692463, the reward is 246.25 with loss [15.35555899143219, 22.243100106716156] in episode 609
Report: 
rewardSum:246.25
loss:[15.35555899143219, 22.243100106716156]
policies:[2, 0, 3]
qAverage:[86.39764404296875, 0.0]
ws:[7.8378885587056475, 4.6715086698532104]
memory len:7688
memory used:2759.0
now epsilon is 0.38208190970657174, the reward is 247.25 with loss [12.383522748947144, 27.21747064590454] in episode 610
Report: 
rewardSum:247.25
loss:[12.383522748947144, 27.21747064590454]
policies:[4, 0, 0]
qAverage:[96.7037410736084, 0.0]
ws:[2.3028242886066437, 1.2166548389941454]
memory len:7696
memory used:2759.0
now epsilon is 0.38189089263183784, the reward is -1.0 with loss [8.583833575248718, 6.939413070678711] in episode 611
Report: 
rewardSum:-1.0
loss:[8.583833575248718, 6.939413070678711]
policies:[1, 0, 1]
qAverage:[56.512939453125, 0.0]
ws:[0.174740731716156, -0.14460839331150055]
memory len:7700
memory used:2759.0
now epsilon is 0.38112777882158616, the reward is 243.25 with loss [35.27386999130249, 32.863749265670776] in episode 612
Report: 
rewardSum:243.25
loss:[35.27386999130249, 32.863749265670776]
policies:[3, 1, 4]
qAverage:[92.47493362426758, 0.0]
ws:[1.909622460603714, 1.4945481047034264]
memory len:7716
memory used:2759.0
now epsilon is 0.380461305230481, the reward is 244.25 with loss [41.11441349983215, 33.16791534423828] in episode 613
Report: 
rewardSum:244.25
loss:[41.11441349983215, 33.16791534423828]
policies:[2, 2, 3]
qAverage:[64.41640281677246, 20.80584716796875]
ws:[5.127639174461365, 2.873484969139099]
memory len:7730
memory used:2759.0
now epsilon is 0.3800809865744626, the reward is 247.25 with loss [12.016838908195496, 12.557136058807373] in episode 614
Report: 
rewardSum:247.25
loss:[12.016838908195496, 12.557136058807373]
policies:[3, 1, 0]
qAverage:[94.74159812927246, 0.0]
ws:[5.40919703245163, 3.3490880876779556]
memory len:7738
memory used:2759.0
now epsilon is 0.37970104809450456, the reward is 247.25 with loss [17.47617244720459, 22.242574453353882] in episode 615
Report: 
rewardSum:247.25
loss:[17.47617244720459, 22.242574453353882]
policies:[4, 0, 0]
qAverage:[86.26139577229817, 0.0]
ws:[6.490204220016797, 3.498273551464081]
memory len:7746
memory used:2759.0
now epsilon is 0.37951122130177284, the reward is -1.0 with loss [8.796602964401245, 11.160224914550781] in episode 616
Report: 
rewardSum:-1.0
loss:[8.796602964401245, 11.160224914550781]
policies:[1, 0, 1]
qAverage:[56.34725570678711, 0.0]
ws:[0.7817233204841614, 0.1606094390153885]
memory len:7750
memory used:2759.0
now epsilon is 0.3789423101430152, the reward is 245.25 with loss [30.728161811828613, 29.4642915725708] in episode 617
Report: 
rewardSum:245.25
loss:[30.728161811828613, 29.4642915725708]
policies:[3, 2, 1]
qAverage:[82.0167744954427, 0.0]
ws:[9.091704448064169, 5.394000848134358]
memory len:7762
memory used:2759.0
now epsilon is 0.3782796582548648, the reward is 244.25 with loss [24.578290045261383, 34.658358573913574] in episode 618
Report: 
rewardSum:244.25
loss:[24.578290045261383, 34.658358573913574]
policies:[3, 1, 3]
qAverage:[63.95222473144531, 0.0]
ws:[0.2764265537261963, 0.2644655704498291]
memory len:7776
memory used:2760.0
now epsilon is 0.3774293796567167, the reward is 32.15999999999997 with loss [46.038278579711914, 45.182503432035446] in episode 619
Report: 
rewardSum:32.15999999999997
loss:[46.038278579711914, 45.182503432035446]
policies:[1, 3, 5]
qAverage:[64.68125915527344, 0.0]
ws:[1.968170404434204, 1.7520511150360107]
memory len:7794
memory used:2760.0
now epsilon is 0.3767693734120232, the reward is 244.25 with loss [38.01245304942131, 30.60906720161438] in episode 620
Report: 
rewardSum:244.25
loss:[38.01245304942131, 30.60906720161438]
policies:[3, 2, 2]
qAverage:[62.45986557006836, 22.68674659729004]
ws:[4.937221378087997, 2.514791637659073]
memory len:7808
memory used:2760.0
now epsilon is 0.3762045724554745, the reward is 245.25 with loss [39.18036699295044, 29.075452089309692] in episode 621
Report: 
rewardSum:245.25
loss:[39.18036699295044, 29.075452089309692]
policies:[4, 0, 2]
qAverage:[94.29681396484375, 0.0]
ws:[7.250463575124741, 4.810708582401276]
memory len:7820
memory used:2760.0
now epsilon is 0.3756406181710362, the reward is 245.25 with loss [30.01359987258911, 32.77723217010498] in episode 622
Report: 
rewardSum:245.25
loss:[30.01359987258911, 32.77723217010498]
policies:[4, 1, 1]
qAverage:[63.50605392456055, 23.263662338256836]
ws:[6.222038626670837, 4.321226716041565]
memory len:7832
memory used:2760.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42*		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.37479627147869893, the reward is 242.25 with loss [47.1417892575264, 36.9064736366272] in episode 623
Report: 
rewardSum:242.25
loss:[47.1417892575264, 36.9064736366272]
policies:[5, 1, 3]
qAverage:[99.37435455322266, 0.0]
ws:[5.309180355072021, 3.2100189924240112]
memory len:7850
memory used:2759.0
now epsilon is 0.3738603342120802, the reward is 31.159999999999968 with loss [43.80568170547485, 43.72230100631714] in episode 624
Report: 
rewardSum:31.159999999999968
loss:[43.80568170547485, 43.72230100631714]
policies:[4, 3, 3]
qAverage:[39.215216318766274, 45.52734375]
ws:[2.3976428707440696, 2.393363669514656]
memory len:7870
memory used:2759.0
now epsilon is 0.37311326747221546, the reward is 243.25 with loss [32.66628456115723, 40.11290502548218] in episode 625
Report: 
rewardSum:243.25
loss:[32.66628456115723, 40.11290502548218]
policies:[3, 2, 3]
qAverage:[91.27604484558105, 0.0]
ws:[5.526899337768555, 3.3232016563415527]
memory len:7886
memory used:2771.0
now epsilon is 0.3727402940989004, the reward is 37.15999999999997 with loss [16.194719791412354, 13.140792965888977] in episode 626
Report: 
rewardSum:37.15999999999997
loss:[16.194719791412354, 13.140792965888977]
policies:[2, 0, 2]
qAverage:[59.81922149658203, 0.0]
ws:[4.841221809387207, 3.5591769218444824]
memory len:7894
memory used:2771.0
now epsilon is 0.37218153298531836, the reward is 245.25 with loss [37.01636838912964, 22.597017526626587] in episode 627
Report: 
rewardSum:245.25
loss:[37.01636838912964, 22.597017526626587]
policies:[4, 0, 2]
qAverage:[92.35725021362305, 0.0]
ws:[4.795344054698944, 2.5910094678401947]
memory len:7906
memory used:2771.0
now epsilon is 0.37180949099714805, the reward is 247.25 with loss [20.84819793701172, 27.64900827407837] in episode 628
Report: 
rewardSum:247.25
loss:[20.84819793701172, 27.64900827407837]
policies:[4, 0, 0]
qAverage:[85.27806663513184, 0.0]
ws:[5.480219662189484, 3.216471493244171]
memory len:7914
memory used:2771.0
now epsilon is 0.3712521252158815, the reward is 245.25 with loss [27.938112497329712, 31.280738353729248] in episode 629
Report: 
rewardSum:245.25
loss:[27.938112497329712, 31.280738353729248]
policies:[4, 1, 1]
qAverage:[96.01690216064453, 0.0]
ws:[2.218760669231415, 0.427569717168808]
memory len:7926
memory used:2771.0
now epsilon is 0.37088101228701087, the reward is 247.25 with loss [15.55448305606842, 24.28787112236023] in episode 630
Report: 
rewardSum:247.25
loss:[15.55448305606842, 24.28787112236023]
policies:[2, 2, 0]
qAverage:[80.46409606933594, 0.0]
ws:[2.4623745679855347, 0.11584742863972981]
memory len:7934
memory used:2771.0
now epsilon is 0.3703250383536508, the reward is 35.15999999999997 with loss [35.53810787200928, 41.83241152763367] in episode 631
Report: 
rewardSum:35.15999999999997
loss:[35.53810787200928, 41.83241152763367]
policies:[1, 3, 2]
qAverage:[36.60106404622396, 27.802513122558594]
ws:[0.11836964885393779, 0.16684230168660483]
memory len:7946
memory used:2771.0
now epsilon is 0.36986236345100176, the reward is 36.15999999999997 with loss [33.2843918800354, 22.844502687454224] in episode 632
Report: 
rewardSum:36.15999999999997
loss:[33.2843918800354, 22.844502687454224]
policies:[2, 1, 2]
qAverage:[73.8180669148763, 0.0]
ws:[5.866858005523682, 5.1310875415802]
memory len:7956
memory used:2771.0
now epsilon is 0.36930791653623074, the reward is 245.25 with loss [29.801841616630554, 39.61527681350708] in episode 633
Report: 
rewardSum:245.25
loss:[29.801841616630554, 39.61527681350708]
policies:[3, 1, 2]
qAverage:[72.62255350748698, 0.0]
ws:[3.9682122468948364, 2.803580125172933]
memory len:7968
memory used:2773.0
now epsilon is 0.36856994666896875, the reward is 243.25 with loss [37.16595983505249, 33.97405678033829] in episode 634
Report: 
rewardSum:243.25
loss:[37.16595983505249, 33.97405678033829]
policies:[5, 2, 1]
qAverage:[90.73444213867188, 0.0]
ws:[3.346380978822708, 2.2769578456878663]
memory len:7984
memory used:2773.0
now epsilon is 0.3678334514506396, the reward is 243.25 with loss [39.83257222175598, 55.35248160362244] in episode 635
Report: 
rewardSum:243.25
loss:[39.83257222175598, 55.35248160362244]
policies:[3, 0, 5]
qAverage:[83.96600532531738, 0.0]
ws:[5.78215716779232, 4.166311603039503]
memory len:8000
memory used:2773.0
now epsilon is 0.36728204600239805, the reward is 245.25 with loss [28.523115396499634, 29.06851291656494] in episode 636
Report: 
rewardSum:245.25
loss:[28.523115396499634, 29.06851291656494]
policies:[2, 1, 3]
qAverage:[0.0, 37.58930587768555]
ws:[3.1244070529937744, 3.719203233718872]
memory len:8012
memory used:2773.0
now epsilon is 0.36673146714555854, the reward is 245.25 with loss [17.020020961761475, 26.682072401046753] in episode 637
Report: 
rewardSum:245.25
loss:[17.020020961761475, 26.682072401046753]
policies:[1, 3, 2]
qAverage:[31.610660552978516, 38.560903549194336]
ws:[2.9546015858650208, 1.8136388063430786]
memory len:8024
memory used:2773.0
now epsilon is 0.3663648731797939, the reward is 247.25 with loss [28.50365662574768, 17.770960330963135] in episode 638
Report: 
rewardSum:247.25
loss:[28.50365662574768, 17.770960330963135]
policies:[3, 0, 1]
qAverage:[78.57160949707031, 0.0]
ws:[4.322103341420491, 2.308348019917806]
memory len:8032
memory used:2773.0
now epsilon is 0.3658156692226253, the reward is 35.15999999999997 with loss [27.44385576248169, 24.15137028694153] in episode 639
Report: 
rewardSum:35.15999999999997
loss:[27.44385576248169, 24.15137028694153]
policies:[2, 2, 2]
qAverage:[26.3579158782959, 38.31263542175293]
ws:[0.7538856267929077, 0.8602544665336609]
memory len:8044
memory used:2773.0
############# STATE ###############
0-		10-		20-		30-		40-		
1*		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.3652672885566854, the reward is 245.25 with loss [28.179418802261353, 25.75496196746826] in episode 640
Report: 
rewardSum:245.25
loss:[28.179418802261353, 25.75496196746826]
policies:[1, 1, 4]
qAverage:[52.54920959472656, 0.0]
ws:[4.788094520568848, 4.283550262451172]
memory len:8056
memory used:2773.0
now epsilon is 0.3649021582205342, the reward is 247.25 with loss [28.00736141204834, 24.4813511967659] in episode 641
Report: 
rewardSum:247.25
loss:[28.00736141204834, 24.4813511967659]
policies:[2, 1, 1]
qAverage:[60.41645431518555, 0.0]
ws:[4.042076110839844, 1.7309926748275757]
memory len:8064
memory used:2773.0
now epsilon is 0.36471972994780877, the reward is -1.0 with loss [18.384129524230957, 7.41421103477478] in episode 642
Report: 
rewardSum:-1.0
loss:[18.384129524230957, 7.41421103477478]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:8068
memory used:2773.0
now epsilon is 0.36435514696496624, the reward is 247.25 with loss [31.619877338409424, 12.613431990146637] in episode 643
Report: 
rewardSum:247.25
loss:[31.619877338409424, 12.613431990146637]
policies:[4, 0, 0]
qAverage:[84.10315895080566, 0.0]
ws:[5.519878268241882, 3.6266262531280518]
memory len:8076
memory used:2773.0
now epsilon is 0.3639909284284106, the reward is 247.25 with loss [11.049052953720093, 21.320993185043335] in episode 644
Report: 
rewardSum:247.25
loss:[11.049052953720093, 21.320993185043335]
policies:[4, 0, 0]
qAverage:[80.73221969604492, 0.0]
ws:[3.121307097375393, 2.301204040646553]
memory len:8084
memory used:2772.0
now epsilon is 0.36362707397383237, the reward is 37.15999999999997 with loss [22.742236137390137, 13.758734703063965] in episode 645
Report: 
rewardSum:37.15999999999997
loss:[22.742236137390137, 13.758734703063965]
policies:[2, 0, 2]
qAverage:[51.78364562988281, 0.0]
ws:[1.0090529918670654, 0.27640077471733093]
memory len:8092
memory used:2772.0
now epsilon is 0.3630819741496414, the reward is 245.25 with loss [39.46171855926514, 29.022500693798065] in episode 646
Report: 
rewardSum:245.25
loss:[39.46171855926514, 29.022500693798065]
policies:[2, 1, 3]
qAverage:[74.4131596883138, 0.0]
ws:[2.879474719365438, 1.1982866525650024]
memory len:8104
memory used:2772.0
now epsilon is 0.3627190283085409, the reward is 37.15999999999997 with loss [14.246976971626282, 27.84552574157715] in episode 647
Report: 
rewardSum:37.15999999999997
loss:[14.246976971626282, 27.84552574157715]
policies:[1, 1, 2]
qAverage:[47.219757080078125, 0.0]
ws:[1.8361873626708984, 1.6453239917755127]
memory len:8112
memory used:2773.0
now epsilon is 0.36235644527719957, the reward is 247.25 with loss [17.8711941242218, 30.265372276306152] in episode 648
Report: 
rewardSum:247.25
loss:[17.8711941242218, 30.265372276306152]
policies:[2, 0, 2]
qAverage:[70.79691823323567, 0.0]
ws:[5.077403386433919, 4.669135411580403]
memory len:8120
memory used:2773.0
now epsilon is 0.3618132502052361, the reward is 245.25 with loss [31.21662563085556, 21.4997878074646] in episode 649
Report: 
rewardSum:245.25
loss:[31.21662563085556, 21.4997878074646]
policies:[3, 0, 3]
qAverage:[76.55764579772949, 0.0]
ws:[1.71220201253891, 0.15372182428836823]
memory len:8132
memory used:2773.0
now epsilon is 0.3614515726123878, the reward is 247.25 with loss [17.62818056344986, 15.189435720443726] in episode 650
Report: 
rewardSum:247.25
loss:[17.62818056344986, 15.189435720443726]
policies:[1, 3, 0]
qAverage:[29.132204055786133, 37.02849769592285]
ws:[5.6542388796806335, 4.648626208305359]
memory len:8140
memory used:2774.0
now epsilon is 0.36099998399738553, the reward is 246.25 with loss [21.667543649673462, 43.05591058731079] in episode 651
Report: 
rewardSum:246.25
loss:[21.667543649673462, 43.05591058731079]
policies:[3, 1, 1]
qAverage:[38.8562978108724, 22.01922098795573]
ws:[3.038799206415812, 1.5437302589416504]
memory len:8150
memory used:2774.0
now epsilon is 0.3606391193658211, the reward is 247.25 with loss [20.131375551223755, 17.041706085205078] in episode 652
Report: 
rewardSum:247.25
loss:[20.131375551223755, 17.041706085205078]
policies:[4, 0, 0]
qAverage:[67.88107299804688, 0.0]
ws:[0.8563173015912374, 0.4175105591615041]
memory len:8158
memory used:2774.0
now epsilon is 0.3602786154635865, the reward is 247.25 with loss [18.323298931121826, 28.950170278549194] in episode 653
Report: 
rewardSum:247.25
loss:[18.323298931121826, 28.950170278549194]
policies:[3, 1, 0]
qAverage:[66.6269302368164, 0.0]
ws:[0.6289806663990021, 0.20635169744491577]
memory len:8166
memory used:2774.0
now epsilon is 0.3598284923121053, the reward is 246.25 with loss [29.791062355041504, 23.718770265579224] in episode 654
Report: 
rewardSum:246.25
loss:[29.791062355041504, 23.718770265579224]
policies:[3, 1, 1]
qAverage:[78.45821189880371, 0.0]
ws:[4.45255970954895, 2.2967565581202507]
memory len:8176
memory used:2774.0
now epsilon is 0.3591992645287233, the reward is 244.25 with loss [41.82237160205841, 34.2311475276947] in episode 655
Report: 
rewardSum:244.25
loss:[41.82237160205841, 34.2311475276947]
policies:[4, 0, 3]
qAverage:[80.5531723022461, 0.0]
ws:[3.1245981812477113, 1.3278822302818298]
memory len:8190
memory used:2774.0
now epsilon is 0.35866080226901204, the reward is 245.25 with loss [37.536559104919434, 22.639230012893677] in episode 656
Report: 
rewardSum:245.25
loss:[37.536559104919434, 22.639230012893677]
policies:[4, 1, 1]
qAverage:[71.07973734537761, 0.0]
ws:[1.6205629110336304, 0.8447491526603699]
memory len:8202
memory used:2780.0
now epsilon is 0.3581231471980502, the reward is 245.25 with loss [22.214592933654785, 26.11935567855835] in episode 657
Report: 
rewardSum:245.25
loss:[22.214592933654785, 26.11935567855835]
policies:[3, 0, 3]
qAverage:[74.31151390075684, 0.0]
ws:[3.778055652976036, 2.321647211909294]
memory len:8214
memory used:2780.0
now epsilon is 0.35767571703506995, the reward is 246.25 with loss [25.650768756866455, 18.611128330230713] in episode 658
Report: 
rewardSum:246.25
loss:[25.650768756866455, 18.611128330230713]
policies:[2, 2, 1]
qAverage:[40.84727478027344, 28.009007263183594]
ws:[2.999710810184479, 2.256755840778351]
memory len:8224
memory used:2780.0
now epsilon is 0.35678253303508795, the reward is 241.25 with loss [43.467082381248474, 40.03575134277344] in episode 659
Report: 
rewardSum:241.25
loss:[43.467082381248474, 40.03575134277344]
policies:[2, 2, 6]
qAverage:[51.82456398010254, 15.595316886901855]
ws:[1.3778158873319626, 0.5570519790053368]
memory len:8244
memory used:2781.0
now epsilon is 0.35660416406747875, the reward is -1.0 with loss [9.237485885620117, 12.931766033172607] in episode 660
Report: 
rewardSum:-1.0
loss:[9.237485885620117, 12.931766033172607]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:8248
memory used:2781.0
############# STATE ###############
0-		10*		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.35624769360768643, the reward is 247.25 with loss [15.561923861503601, 22.646740198135376] in episode 661
Report: 
rewardSum:247.25
loss:[15.561923861503601, 22.646740198135376]
policies:[3, 1, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:8256
memory used:2781.0
now epsilon is 0.35606959202636346, the reward is -1.0 with loss [9.203423500061035, 7.797185897827148] in episode 662
Report: 
rewardSum:-1.0
loss:[9.203423500061035, 7.797185897827148]
policies:[1, 0, 1]
qAverage:[44.66996765136719, 0.0]
ws:[-0.42595937848091125, -0.7147394418716431]
memory len:8260
memory used:2781.0
now epsilon is 0.3555358213423156, the reward is 245.25 with loss [31.22638177871704, 22.808258056640625] in episode 663
Report: 
rewardSum:245.25
loss:[31.22638177871704, 22.808258056640625]
policies:[4, 0, 2]
qAverage:[74.17914581298828, 0.0]
ws:[3.19850767031312, 1.4304747879505157]
memory len:8272
memory used:2781.0
now epsilon is 0.3551804188246867, the reward is 247.25 with loss [23.28589653968811, 11.417489111423492] in episode 664
Report: 
rewardSum:247.25
loss:[23.28589653968811, 11.417489111423492]
policies:[2, 1, 1]
qAverage:[72.93940734863281, 0.0]
ws:[1.630953053633372, -0.5198572476704916]
memory len:8280
memory used:2781.0
now epsilon is 0.35464798106711926, the reward is 245.25 with loss [21.558196306228638, 24.136950254440308] in episode 665
Report: 
rewardSum:245.25
loss:[21.558196306228638, 24.136950254440308]
policies:[3, 1, 2]
qAverage:[46.399497985839844, 17.913719177246094]
ws:[5.4105958342552185, 5.3521528840065]
memory len:8292
memory used:2781.0
now epsilon is 0.35438206157227403, the reward is -2.0 with loss [7.954025119543076, 10.230765342712402] in episode 666
Report: 
rewardSum:-2.0
loss:[7.954025119543076, 10.230765342712402]
policies:[1, 0, 2]
qAverage:[45.787010192871094, 0.0]
ws:[0.4032607674598694, 0.2514645755290985]
memory len:8298
memory used:2781.0
now epsilon is 0.3538508206023748, the reward is 245.25 with loss [20.267479181289673, 26.832916975021362] in episode 667
Report: 
rewardSum:245.25
loss:[20.267479181289673, 26.832916975021362]
policies:[2, 0, 4]
qAverage:[44.050506591796875, 0.0]
ws:[5.866775035858154, 5.601179599761963]
memory len:8310
memory used:2781.0
now epsilon is 0.35332037599605803, the reward is 35.15999999999997 with loss [29.081490516662598, 21.274048030376434] in episode 668
Report: 
rewardSum:35.15999999999997
loss:[29.081490516662598, 21.274048030376434]
policies:[2, 2, 2]
qAverage:[64.29563649495442, 0.0]
ws:[1.2376314004262288, 0.47559064626693726]
memory len:8322
memory used:2781.0
now epsilon is 0.3527907265595246, the reward is 245.25 with loss [23.657944202423096, 31.675868272781372] in episode 669
Report: 
rewardSum:245.25
loss:[23.657944202423096, 31.675868272781372]
policies:[3, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:8334
memory used:2781.0
now epsilon is 0.35226187110076507, the reward is 245.25 with loss [23.623040556907654, 21.84951615333557] in episode 670
Report: 
rewardSum:245.25
loss:[23.623040556907654, 21.84951615333557]
policies:[4, 0, 2]
qAverage:[77.35648193359376, 0.0]
ws:[4.090091836452484, 1.769686222076416]
memory len:8346
memory used:2781.0
now epsilon is 0.35208576218158166, the reward is -1.0 with loss [7.5974016189575195, 8.992760300636292] in episode 671
Report: 
rewardSum:-1.0
loss:[7.5974016189575195, 8.992760300636292]
policies:[1, 0, 1]
qAverage:[41.59663391113281, 0.0]
ws:[1.3422452211380005, 0.7263231873512268]
memory len:8350
memory used:2780.0
now epsilon is 0.3516458749774496, the reward is 36.15999999999997 with loss [19.777843475341797, 29.283800840377808] in episode 672
Report: 
rewardSum:36.15999999999997
loss:[19.777843475341797, 29.283800840377808]
policies:[1, 1, 3]
qAverage:[51.11125183105469, 0.0]
ws:[1.9636908769607544, 1.106192708015442]
memory len:8360
memory used:2780.0
now epsilon is 0.35103095603919177, the reward is 244.25 with loss [27.57768738269806, 36.336950063705444] in episode 673
Report: 
rewardSum:244.25
loss:[27.57768738269806, 36.336950063705444]
policies:[3, 1, 3]
qAverage:[61.89390309651693, 0.0]
ws:[2.593388875325521, 1.5571020444234211]
memory len:8374
memory used:2780.0
now epsilon is 0.3506800566978231, the reward is 247.25 with loss [33.58643198013306, 16.854512691497803] in episode 674
Report: 
rewardSum:247.25
loss:[33.58643198013306, 16.854512691497803]
policies:[3, 0, 1]
qAverage:[50.81502914428711, 0.0]
ws:[1.9111452102661133, 0.7277461290359497]
memory len:8382
memory used:2780.0
now epsilon is 0.3500668266744462, the reward is 34.15999999999997 with loss [22.169136598706245, 36.6261522769928] in episode 675
Report: 
rewardSum:34.15999999999997
loss:[22.169136598706245, 36.6261522769928]
policies:[4, 0, 3]
qAverage:[61.2992197672526, 0.0]
ws:[4.734379688898723, 3.6984175046284995]
memory len:8396
memory used:2780.0
now epsilon is 0.34989181514028567, the reward is -1.0 with loss [14.363922119140625, 13.912535667419434] in episode 676
Report: 
rewardSum:-1.0
loss:[14.363922119140625, 13.912535667419434]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:8400
memory used:2780.0
now epsilon is 0.34910534535374327, the reward is 242.25 with loss [41.36773467063904, 48.85479348897934] in episode 677
Report: 
rewardSum:242.25
loss:[41.36773467063904, 48.85479348897934]
policies:[1, 3, 5]
qAverage:[22.287156677246095, 39.952694702148435]
ws:[2.038987636566162, 1.7161079883575439]
memory len:8418
memory used:2786.0
now epsilon is 0.34875637090107636, the reward is 247.25 with loss [15.767496824264526, 19.17963135242462] in episode 678
Report: 
rewardSum:247.25
loss:[15.767496824264526, 19.17963135242462]
policies:[2, 1, 1]
qAverage:[37.254007975260414, 22.840810139973957]
ws:[4.108783562978108, 1.8985009188763797]
memory len:8426
memory used:2786.0
now epsilon is 0.3482335631948566, the reward is 245.25 with loss [31.948363780975342, 30.69050097465515] in episode 679
Report: 
rewardSum:245.25
loss:[31.948363780975342, 30.69050097465515]
policies:[1, 2, 3]
qAverage:[27.358261108398438, 22.516845703125]
ws:[2.0241265296936035, 2.108966072400411]
memory len:8438
memory used:2786.0
now epsilon is 0.34788546019748473, the reward is 247.25 with loss [16.287756204605103, 14.586058378219604] in episode 680
Report: 
rewardSum:247.25
loss:[16.287756204605103, 14.586058378219604]
policies:[4, 0, 0]
qAverage:[58.1651357014974, 0.0]
ws:[1.1875662008921306, 0.9184476534525553]
memory len:8446
memory used:2786.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24*		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.3473639580411137, the reward is 245.25 with loss [27.86064600944519, 31.212636947631836] in episode 681
Report: 
rewardSum:245.25
loss:[27.86064600944519, 31.212636947631836]
policies:[3, 0, 3]
qAverage:[60.97185262044271, 0.0]
ws:[1.2146143640081088, 1.004834731419881]
memory len:8458
memory used:2786.0
now epsilon is 0.34666983770810955, the reward is 243.25 with loss [32.9766480922699, 47.70980679988861] in episode 682
Report: 
rewardSum:243.25
loss:[32.9766480922699, 47.70980679988861]
policies:[5, 1, 2]
qAverage:[77.1252670288086, 0.0]
ws:[2.929430305957794, -0.20270146802067757]
memory len:8474
memory used:2786.0
now epsilon is 0.345977104401668, the reward is 243.25 with loss [40.182497382164, 31.934662699699402] in episode 683
Report: 
rewardSum:243.25
loss:[40.182497382164, 31.934662699699402]
policies:[4, 1, 3]
qAverage:[75.75139465332032, 0.0]
ws:[2.6864200592041017, 0.6304941892623901]
memory len:8490
memory used:2786.0
now epsilon is 0.3456312570170583, the reward is 247.25 with loss [19.34430170059204, 10.811745762825012] in episode 684
Report: 
rewardSum:247.25
loss:[19.34430170059204, 10.811745762825012]
policies:[3, 1, 0]
qAverage:[71.01054000854492, 0.0]
ws:[1.662737471750006, -0.4787141904234886]
memory len:8498
memory used:2786.0
now epsilon is 0.3452857553501621, the reward is 247.25 with loss [20.250545978546143, 15.61847996711731] in episode 685
Report: 
rewardSum:247.25
loss:[20.250545978546143, 15.61847996711731]
policies:[2, 1, 1]
qAverage:[0.0, 32.75562286376953]
ws:[1.2109520435333252, 1.4448363780975342]
memory len:8506
memory used:2786.0
now epsilon is 0.344768150314651, the reward is 245.25 with loss [24.319413661956787, 18.534209489822388] in episode 686
Report: 
rewardSum:245.25
loss:[24.319413661956787, 18.534209489822388]
policies:[1, 4, 1]
qAverage:[26.54566192626953, 30.533635139465332]
ws:[4.104273825883865, 2.9661595933139324]
memory len:8518
memory used:2786.0
now epsilon is 0.3442513212016002, the reward is 245.25 with loss [31.940009593963623, 46.67112064361572] in episode 687
Report: 
rewardSum:245.25
loss:[31.940009593963623, 46.67112064361572]
policies:[2, 3, 1]
qAverage:[21.65229034423828, 36.05884704589844]
ws:[2.7004082798957825, 1.5294265270233154]
memory len:8530
memory used:2786.0
now epsilon is 0.34382122215339145, the reward is 246.25 with loss [21.698502898216248, 20.841577529907227] in episode 688
Report: 
rewardSum:246.25
loss:[21.698502898216248, 20.841577529907227]
policies:[0, 2, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:8540
memory used:2786.0
now epsilon is 0.34364933303114115, the reward is -1.0 with loss [16.9908447265625, 13.648239135742188] in episode 689
Report: 
rewardSum:-1.0
loss:[16.9908447265625, 13.648239135742188]
policies:[0, 1, 1]
qAverage:[0.0, 27.336889266967773]
ws:[0.4349305331707001, 0.6853026747703552]
memory len:8544
memory used:2786.0
now epsilon is 0.3428768947919498, the reward is 242.25 with loss [34.43581008911133, 36.68516659736633] in episode 690
Report: 
rewardSum:242.25
loss:[34.43581008911133, 36.68516659736633]
policies:[3, 0, 6]
qAverage:[62.949267069498696, 0.0]
ws:[1.6223737796147664, -0.42804720004399616]
memory len:8562
memory used:2787.0
now epsilon is 0.34244851291795136, the reward is -4.0 with loss [22.777207374572754, 28.049894332885742] in episode 691
Report: 
rewardSum:-4.0
loss:[22.777207374572754, 28.049894332885742]
policies:[0, 1, 4]
qAverage:[0.0, 27.43776512145996]
ws:[0.5124390125274658, 1.1682543754577637]
memory len:8572
memory used:2787.0
now epsilon is 0.34176421487746433, the reward is 243.25 with loss [42.09215474128723, 37.17929720878601] in episode 692
Report: 
rewardSum:243.25
loss:[42.09215474128723, 37.17929720878601]
policies:[3, 2, 3]
qAverage:[59.49290974934896, 0.0]
ws:[1.5420116186141968, 0.7515286008516947]
memory len:8588
memory used:2787.0
now epsilon is 0.3414225788028085, the reward is 247.25 with loss [20.532198429107666, 18.97712939977646] in episode 693
Report: 
rewardSum:247.25
loss:[20.532198429107666, 18.97712939977646]
policies:[2, 2, 0]
qAverage:[68.45548248291016, 0.0]
ws:[2.263214031855265, -0.06800611813863118]
memory len:8596
memory used:2787.0
now epsilon is 0.3409107649115974, the reward is 245.25 with loss [27.29111933708191, 31.907949030399323] in episode 694
Report: 
rewardSum:245.25
loss:[27.29111933708191, 31.907949030399323]
policies:[3, 1, 2]
qAverage:[65.97898610432942, 0.0]
ws:[6.029649416605632, 3.7907828291257224]
memory len:8608
memory used:2787.0
now epsilon is 0.3407403308360644, the reward is -1.0 with loss [9.729572772979736, 10.701692342758179] in episode 695
Report: 
rewardSum:-1.0
loss:[9.729572772979736, 10.701692342758179]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:8612
memory used:2787.0
now epsilon is 0.3403997182615575, the reward is 247.25 with loss [17.430094242095947, 15.120471000671387] in episode 696
Report: 
rewardSum:247.25
loss:[17.430094242095947, 15.120471000671387]
policies:[2, 0, 2]
qAverage:[56.258155822753906, 0.0]
ws:[3.56484317779541, 0.3394930362701416]
memory len:8620
memory used:2787.0
now epsilon is 0.33988943770254615, the reward is 245.25 with loss [36.12967038154602, 30.817646026611328] in episode 697
Report: 
rewardSum:245.25
loss:[36.12967038154602, 30.817646026611328]
policies:[3, 1, 2]
qAverage:[50.787357330322266, 0.0]
ws:[1.2579572200775146, 1.007272720336914]
memory len:8632
memory used:2787.0
now epsilon is 0.33937992208614465, the reward is 245.25 with loss [26.986422300338745, 31.95991826057434] in episode 698
Report: 
rewardSum:245.25
loss:[26.986422300338745, 31.95991826057434]
policies:[5, 0, 1]
qAverage:[71.72376251220703, 0.0]
ws:[1.2381463944911957, -1.6416201889514923]
memory len:8644
memory used:2787.0
now epsilon is 0.3390406694103194, the reward is 247.25 with loss [21.15694546699524, 15.714069128036499] in episode 699
Report: 
rewardSum:247.25
loss:[21.15694546699524, 15.714069128036499]
policies:[4, 0, 0]
qAverage:[76.44177703857422, 0.0]
ws:[3.9300432443618774, 1.74463951587677]
memory len:8652
memory used:2787.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23*		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.33844779304436345, the reward is 244.25 with loss [36.33090782165527, 43.15546691417694] in episode 700
Report: 
rewardSum:244.25
loss:[36.33090782165527, 43.15546691417694]
policies:[2, 0, 5]
qAverage:[72.2500508626302, 0.0]
ws:[4.178153038024902, 1.813312590122223]
memory len:8666
memory used:2787.0
now epsilon is 0.33794043854385786, the reward is 245.25 with loss [29.0330753326416, 26.81999707221985] in episode 701
Report: 
rewardSum:245.25
loss:[29.0330753326416, 26.81999707221985]
policies:[2, 1, 3]
qAverage:[28.714276631673176, 22.051546732584637]
ws:[2.0218584537506104, 1.904178222020467]
memory len:8678
memory used:2787.0
now epsilon is 0.3374338445996167, the reward is 245.25 with loss [35.77794814109802, 24.929484844207764] in episode 702
Report: 
rewardSum:245.25
loss:[35.77794814109802, 24.929484844207764]
policies:[2, 1, 3]
qAverage:[56.881710052490234, 0.0]
ws:[3.1662559509277344, -0.7928910255432129]
memory len:8690
memory used:2787.0
now epsilon is 0.3370965372716205, the reward is 247.25 with loss [20.331282138824463, 23.9989972114563] in episode 703
Report: 
rewardSum:247.25
loss:[20.331282138824463, 23.9989972114563]
policies:[2, 0, 2]
qAverage:[60.91828918457031, 0.0]
ws:[0.36108456055323285, 0.08875028292338054]
memory len:8698
memory used:2787.0
now epsilon is 0.3367595671244832, the reward is 247.25 with loss [15.94870924949646, 18.010910630226135] in episode 704
Report: 
rewardSum:247.25
loss:[15.94870924949646, 18.010910630226135]
policies:[1, 2, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:8706
memory used:2787.0
now epsilon is 0.33642293382115024, the reward is 247.25 with loss [15.986851692199707, 18.826578855514526] in episode 705
Report: 
rewardSum:247.25
loss:[15.986851692199707, 18.826578855514526]
policies:[2, 1, 1]
qAverage:[52.721405029296875, 13.067684173583984]
ws:[5.221281319856644, 3.1070931553840637]
memory len:8714
memory used:2793.0
now epsilon is 0.33617067969482783, the reward is -2.0 with loss [13.096147298812866, 11.44569206237793] in episode 706
Report: 
rewardSum:-2.0
loss:[13.096147298812866, 11.44569206237793]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:8720
memory used:2793.0
now epsilon is 0.3358346350581286, the reward is 247.25 with loss [9.435304641723633, 14.098568737506866] in episode 707
Report: 
rewardSum:247.25
loss:[9.435304641723633, 14.098568737506866]
policies:[2, 2, 0]
qAverage:[52.73961639404297, 17.490686416625977]
ws:[7.031553626060486, 5.682763934135437]
memory len:8728
memory used:2793.0
now epsilon is 0.33549892634007034, the reward is 247.25 with loss [23.90733242034912, 24.909128665924072] in episode 708
Report: 
rewardSum:247.25
loss:[23.90733242034912, 24.909128665924072]
policies:[1, 2, 1]
qAverage:[40.832499186197914, 23.292831420898438]
ws:[4.156931718190511, 2.345624407132467]
memory len:8736
memory used:2793.0
now epsilon is 0.33499599237598005, the reward is 245.25 with loss [34.33870553970337, 24.578408122062683] in episode 709
Report: 
rewardSum:245.25
loss:[34.33870553970337, 24.578408122062683]
policies:[3, 1, 2]
qAverage:[29.933453877766926, 23.185399373372395]
ws:[1.6245067914326985, 1.4693528811136882]
memory len:8748
memory used:2794.0
now epsilon is 0.3346611219861653, the reward is 247.25 with loss [22.51697063446045, 24.20207905769348] in episode 710
Report: 
rewardSum:247.25
loss:[22.51697063446045, 24.20207905769348]
policies:[4, 0, 0]
qAverage:[79.05940399169921, 0.0]
ws:[1.7417969465255738, 0.2752769649028778]
memory len:8756
memory used:2794.0
now epsilon is 0.3343265863411849, the reward is 247.25 with loss [22.116923809051514, 13.732368111610413] in episode 711
Report: 
rewardSum:247.25
loss:[22.116923809051514, 13.732368111610413]
policies:[2, 1, 1]
qAverage:[50.321767807006836, 17.25228500366211]
ws:[3.5162753388285637, 1.876008354127407]
memory len:8764
memory used:2794.0
now epsilon is 0.3339923851064195, the reward is 247.25 with loss [24.481216430664062, 33.21037769317627] in episode 712
Report: 
rewardSum:247.25
loss:[24.481216430664062, 33.21037769317627]
policies:[3, 0, 1]
qAverage:[76.92183876037598, 0.0]
ws:[4.689942836761475, 2.2034778892993927]
memory len:8772
memory used:2794.0
now epsilon is 0.33349170954226787, the reward is 245.25 with loss [24.43104124069214, 30.78578519821167] in episode 713
Report: 
rewardSum:245.25
loss:[24.43104124069214, 30.78578519821167]
policies:[3, 2, 1]
qAverage:[70.59710439046223, 0.0]
ws:[6.329357782999675, 0.8953633308410645]
memory len:8784
memory used:2794.0
now epsilon is 0.3331583428712747, the reward is 247.25 with loss [25.84299945831299, 19.09406089782715] in episode 714
Report: 
rewardSum:247.25
loss:[25.84299945831299, 19.09406089782715]
policies:[4, 0, 0]
qAverage:[39.572994232177734, 0.0]
ws:[-0.161423921585083, -1.2601245641708374]
memory len:8792
memory used:2794.0
now epsilon is 0.332825309441961, the reward is 247.25 with loss [27.724928379058838, 9.763886451721191] in episode 715
Report: 
rewardSum:247.25
loss:[27.724928379058838, 9.763886451721191]
policies:[3, 0, 1]
qAverage:[39.39506530761719, 0.0]
ws:[0.00808654073625803, -1.0138307809829712]
memory len:8800
memory used:2794.0
now epsilon is 0.33249260892120985, the reward is 247.25 with loss [16.40012502670288, 25.318162202835083] in episode 716
Report: 
rewardSum:247.25
loss:[16.40012502670288, 25.318162202835083]
policies:[4, 0, 0]
qAverage:[79.45713500976562, 0.0]
ws:[4.281367993354797, 1.2307244136929512]
memory len:8808
memory used:2793.0
now epsilon is 0.3319111830703606, the reward is 34.15999999999997 with loss [41.48246669769287, 42.75383424758911] in episode 717
Report: 
rewardSum:34.15999999999997
loss:[41.48246669769287, 42.75383424758911]
policies:[3, 0, 4]
qAverage:[69.35888290405273, 0.0]
ws:[4.525948226451874, 3.724177062511444]
memory len:8822
memory used:2793.0
now epsilon is 0.33157939633324074, the reward is 37.15999999999997 with loss [18.926355361938477, 15.034473180770874] in episode 718
Report: 
rewardSum:37.15999999999997
loss:[18.926355361938477, 15.034473180770874]
policies:[2, 0, 2]
qAverage:[63.398661295572914, 0.0]
ws:[1.535640041033427, 1.0139120916525524]
memory len:8830
memory used:2793.0
now epsilon is 0.3313307739519467, the reward is -2.0 with loss [8.864494562149048, 8.298253059387207] in episode 719
Report: 
rewardSum:-2.0
loss:[8.864494562149048, 8.298253059387207]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:8836
memory used:2793.0
now epsilon is 0.3311651292731441, the reward is -1.0 with loss [6.842877388000488, 6.5715272426605225] in episode 720
Report: 
rewardSum:-1.0
loss:[6.842877388000488, 6.5715272426605225]
policies:[1, 0, 1]
qAverage:[40.22904968261719, 0.0]
ws:[0.37583664059638977, 0.31705567240715027]
memory len:8840
memory used:2793.0
now epsilon is 0.3309995674063282, the reward is -1.0 with loss [15.435660600662231, 6.869901657104492] in episode 721
Report: 
rewardSum:-1.0
loss:[15.435660600662231, 6.869901657104492]
policies:[0, 1, 1]
qAverage:[0.0, 26.790685653686523]
ws:[0.7801786065101624, 0.966876745223999]
memory len:8844
memory used:2793.0
now epsilon is 0.330834088310098, the reward is -1.0 with loss [15.208138465881348, 8.132375717163086] in episode 722
Report: 
rewardSum:-1.0
loss:[15.208138465881348, 8.132375717163086]
policies:[0, 1, 1]
qAverage:[0.0, 25.829853057861328]
ws:[1.1942898035049438, 1.3955754041671753]
memory len:8848
memory used:2793.0
now epsilon is 0.3305033782638952, the reward is 37.15999999999997 with loss [16.25123429298401, 26.407368183135986] in episode 723
Report: 
rewardSum:37.15999999999997
loss:[16.25123429298401, 26.407368183135986]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:8856
memory used:2793.0
now epsilon is 0.330172998803743, the reward is 37.15999999999997 with loss [16.843842029571533, 15.168053150177002] in episode 724
Report: 
rewardSum:37.15999999999997
loss:[16.843842029571533, 15.168053150177002]
policies:[2, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:8864
memory used:2793.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22*		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.32984294959917937, the reward is 247.25 with loss [17.05915081501007, 16.704630613327026] in episode 725
Report: 
rewardSum:247.25
loss:[17.05915081501007, 16.704630613327026]
policies:[3, 0, 1]
qAverage:[82.85502815246582, 0.0]
ws:[6.547144651412964, 4.029246442019939]
memory len:8872
memory used:2793.0
now epsilon is 0.32951323032007246, the reward is 247.25 with loss [21.080448865890503, 16.48206701874733] in episode 726
Report: 
rewardSum:247.25
loss:[21.080448865890503, 16.48206701874733]
policies:[3, 0, 1]
qAverage:[68.43655395507812, 0.0]
ws:[2.8165077368418374, 2.2810227076212564]
memory len:8880
memory used:2799.0
now epsilon is 0.3291838406366205, the reward is 247.25 with loss [23.417879581451416, 22.32085371017456] in episode 727
Report: 
rewardSum:247.25
loss:[23.417879581451416, 22.32085371017456]
policies:[3, 1, 0]
qAverage:[69.4952335357666, 0.0]
ws:[4.6736409068107605, 4.238748699426651]
memory len:8888
memory used:2799.0
now epsilon is 0.3282797157960692, the reward is 240.25 with loss [49.59507751464844, 47.88152199983597] in episode 728
Report: 
rewardSum:240.25
loss:[49.59507751464844, 47.88152199983597]
policies:[3, 2, 6]
qAverage:[71.92641830444336, 0.0]
ws:[2.0586748719215393, 1.2934534400701523]
memory len:8910
memory used:2799.0
now epsilon is 0.3277876038820406, the reward is 245.25 with loss [44.72545623779297, 30.545376777648926] in episode 729
Report: 
rewardSum:245.25
loss:[44.72545623779297, 30.545376777648926]
policies:[3, 1, 2]
qAverage:[60.081451416015625, 0.0]
ws:[5.084007024765015, 4.609339157740275]
memory len:8922
memory used:2799.0
now epsilon is 0.32737807419323006, the reward is 246.25 with loss [21.787336111068726, 28.86705780029297] in episode 730
Report: 
rewardSum:246.25
loss:[21.787336111068726, 28.86705780029297]
policies:[2, 2, 1]
qAverage:[48.597835540771484, 0.0]
ws:[6.495974063873291, 5.759905815124512]
memory len:8932
memory used:2799.0
now epsilon is 0.3270508188653548, the reward is 247.25 with loss [26.899476051330566, 21.732242107391357] in episode 731
Report: 
rewardSum:247.25
loss:[26.899476051330566, 21.732242107391357]
policies:[2, 1, 1]
qAverage:[73.2162094116211, 0.0]
ws:[6.0055952072143555, 1.3038432598114014]
memory len:8940
memory used:2801.0
now epsilon is 0.32656054914501526, the reward is 245.25 with loss [32.159568071365356, 35.69216865301132] in episode 732
Report: 
rewardSum:245.25
loss:[32.159568071365356, 35.69216865301132]
policies:[2, 2, 2]
qAverage:[66.90773010253906, 0.0]
ws:[3.2281533082326255, 2.8030858834584556]
memory len:8952
memory used:2807.0
now epsilon is 0.32623411103566746, the reward is 247.25 with loss [11.522973954677582, 16.130040287971497] in episode 733
Report: 
rewardSum:247.25
loss:[11.522973954677582, 16.130040287971497]
policies:[3, 0, 1]
qAverage:[59.69061279296875, 0.0]
ws:[4.159362196922302, 3.518929777046045]
memory len:8960
memory used:2807.0
now epsilon is 0.3257450656116641, the reward is 35.15999999999997 with loss [30.991440057754517, 19.435161873698235] in episode 734
Report: 
rewardSum:35.15999999999997
loss:[30.991440057754517, 19.435161873698235]
policies:[1, 2, 3]
qAverage:[27.965487162272137, 25.65552266438802]
ws:[3.7512264251708984, 3.593294302622477]
memory len:8972
memory used:2807.0
now epsilon is 0.3254194426800943, the reward is 247.25 with loss [16.11598300933838, 9.70392607152462] in episode 735
Report: 
rewardSum:247.25
loss:[16.11598300933838, 9.70392607152462]
policies:[3, 1, 0]
qAverage:[90.47153854370117, 0.0]
ws:[7.725924015045166, 4.936030149459839]
memory len:8980
memory used:2807.0
now epsilon is 0.32493161849512725, the reward is 245.25 with loss [25.593964338302612, 32.3416993021965] in episode 736
Report: 
rewardSum:245.25
loss:[25.593964338302612, 32.3416993021965]
policies:[3, 0, 3]
qAverage:[75.53128433227539, 0.0]
ws:[5.684773564338684, 4.209359273314476]
memory len:8992
memory used:2806.0
now epsilon is 0.32460680870568215, the reward is 247.25 with loss [18.234344959259033, 17.633122205734253] in episode 737
Report: 
rewardSum:247.25
loss:[18.234344959259033, 17.633122205734253]
policies:[3, 1, 0]
qAverage:[58.80103047688802, 0.0]
ws:[4.263482848803203, 3.773300071557363]
memory len:9000
memory used:2806.0
now epsilon is 0.3242823236042431, the reward is 247.25 with loss [26.063796520233154, 16.09532594680786] in episode 738
Report: 
rewardSum:247.25
loss:[26.063796520233154, 16.09532594680786]
policies:[3, 1, 0]
qAverage:[91.64322471618652, 0.0]
ws:[6.075569987297058, 3.8165294528007507]
memory len:9008
memory used:2806.0
now epsilon is 0.32395816286624385, the reward is 247.25 with loss [12.476998805999756, 21.21669578552246] in episode 739
Report: 
rewardSum:247.25
loss:[12.476998805999756, 21.21669578552246]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:9016
memory used:2806.0
now epsilon is 0.32363432616744264, the reward is 247.25 with loss [38.63377141952515, 27.51901626586914] in episode 740
Report: 
rewardSum:247.25
loss:[38.63377141952515, 27.51901626586914]
policies:[1, 0, 3]
qAverage:[43.35231399536133, 0.0]
ws:[1.7168047428131104, 1.2826879024505615]
memory len:9024
memory used:2806.0
now epsilon is 0.3231491779842556, the reward is 245.25 with loss [40.231040477752686, 34.73443961143494] in episode 741
Report: 
rewardSum:245.25
loss:[40.231040477752686, 34.73443961143494]
policies:[1, 2, 3]
qAverage:[66.36552429199219, 0.0]
ws:[2.8236398696899414, 2.403648614883423]
memory len:9036
memory used:2806.0
now epsilon is 0.3228261499670176, the reward is 247.25 with loss [33.28298282623291, 16.769585609436035] in episode 742
Report: 
rewardSum:247.25
loss:[33.28298282623291, 16.769585609436035]
policies:[2, 1, 1]
qAverage:[61.97261301676432, 0.0]
ws:[4.237578789393107, 3.972382744153341]
memory len:9044
memory used:2806.0
now epsilon is 0.3223422132907184, the reward is 245.25 with loss [27.768547415733337, 27.052679419517517] in episode 743
Report: 
rewardSum:245.25
loss:[27.768547415733337, 27.052679419517517]
policies:[3, 1, 2]
qAverage:[92.95656840006511, 0.0]
ws:[9.373554547627768, 5.603647232055664]
memory len:9056
memory used:2806.0
now epsilon is 0.32201999193561254, the reward is 247.25 with loss [39.03958225250244, 10.335469365119934] in episode 744
Report: 
rewardSum:247.25
loss:[39.03958225250244, 10.335469365119934]
policies:[2, 1, 1]
qAverage:[65.14046478271484, 0.0]
ws:[4.248450756072998, 3.5917859077453613]
memory len:9064
memory used:2806.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32*		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.3213765152050478, the reward is 243.25 with loss [36.497217893600464, 38.495787143707275] in episode 745
Report: 
rewardSum:243.25
loss:[36.497217893600464, 38.495787143707275]
policies:[3, 2, 3]
qAverage:[45.74768218994141, 30.691905212402343]
ws:[5.5817299604415895, 4.388050103187561]
memory len:9080
memory used:2806.0
now epsilon is 0.32089475162231196, the reward is 245.25 with loss [27.868571519851685, 24.505940079689026] in episode 746
Report: 
rewardSum:245.25
loss:[27.868571519851685, 24.505940079689026]
policies:[4, 0, 2]
qAverage:[95.46030807495117, 0.0]
ws:[6.987564742565155, 4.651330411434174]
memory len:9092
memory used:2806.0
now epsilon is 0.3201734600233366, the reward is 242.25 with loss [57.42315053939819, 34.708348631858826] in episode 747
Report: 
rewardSum:242.25
loss:[57.42315053939819, 34.708348631858826]
policies:[4, 1, 4]
qAverage:[100.91645431518555, 0.0]
ws:[6.708021342754364, 4.123337388038635]
memory len:9110
memory used:2806.0
now epsilon is 0.3198534066083512, the reward is 247.25 with loss [15.38395643234253, 27.1285343170166] in episode 748
Report: 
rewardSum:247.25
loss:[15.38395643234253, 27.1285343170166]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:9118
memory used:2806.0
now epsilon is 0.31953367312678077, the reward is 247.25 with loss [37.49688243865967, 28.949617862701416] in episode 749
Report: 
rewardSum:247.25
loss:[37.49688243865967, 28.949617862701416]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:9126
memory used:2806.0
now epsilon is 0.3190546720800737, the reward is 245.25 with loss [31.391713619232178, 31.19457459449768] in episode 750
Report: 
rewardSum:245.25
loss:[31.391713619232178, 31.19457459449768]
policies:[3, 0, 3]
qAverage:[84.40749740600586, 0.0]
ws:[7.063904762268066, 6.178553581237793]
memory len:9138
memory used:2807.0
now epsilon is 0.31873573703355607, the reward is 247.25 with loss [16.137011766433716, 24.112727880477905] in episode 751
Report: 
rewardSum:247.25
loss:[16.137011766433716, 24.112727880477905]
policies:[2, 0, 2]
qAverage:[67.10785675048828, 0.0]
ws:[5.2660040855407715, 5.058246612548828]
memory len:9146
memory used:2807.0
now epsilon is 0.318257932143173, the reward is 245.25 with loss [23.247632414102554, 33.03695845603943] in episode 752
Report: 
rewardSum:245.25
loss:[23.247632414102554, 33.03695845603943]
policies:[3, 1, 2]
qAverage:[91.90629577636719, 0.0]
ws:[10.66839474439621, 6.595671057701111]
memory len:9158
memory used:2807.0
now epsilon is 0.3179397935378645, the reward is 247.25 with loss [15.420710325241089, 20.648420095443726] in episode 753
Report: 
rewardSum:247.25
loss:[15.420710325241089, 20.648420095443726]
policies:[3, 1, 0]
qAverage:[77.59001922607422, 0.0]
ws:[10.979628562927246, 4.772221565246582]
memory len:9166
memory used:2807.0
now epsilon is 0.3174631818167767, the reward is 245.25 with loss [29.221193552017212, 35.94451069831848] in episode 754
Report: 
rewardSum:245.25
loss:[29.221193552017212, 35.94451069831848]
policies:[5, 0, 1]
qAverage:[103.30537261962891, 0.0]
ws:[12.367867279052735, 7.589972591400146]
memory len:9178
memory used:2808.0
now epsilon is 0.3171458376638129, the reward is 247.25 with loss [19.967578172683716, 21.507827043533325] in episode 755
Report: 
rewardSum:247.25
loss:[19.967578172683716, 21.507827043533325]
policies:[3, 0, 1]
qAverage:[75.10349782307942, 0.0]
ws:[2.9540255069732666, 1.4718344410260518]
memory len:9186
memory used:2807.0
now epsilon is 0.3166704161324505, the reward is 245.25 with loss [27.025411367416382, 25.537334442138672] in episode 756
Report: 
rewardSum:245.25
loss:[27.025411367416382, 25.537334442138672]
policies:[5, 0, 1]
qAverage:[99.86418151855469, 0.0]
ws:[7.568460941314697, 4.75519523024559]
memory len:9198
memory used:2807.0
now epsilon is 0.3162747759818215, the reward is 246.25 with loss [24.879244327545166, 23.036820650100708] in episode 757
Report: 
rewardSum:246.25
loss:[24.879244327545166, 23.036820650100708]
policies:[4, 0, 1]
qAverage:[104.52200775146484, 0.0]
ws:[10.958645725250244, 7.456442213058471]
memory len:9208
memory used:2807.0
now epsilon is 0.3159586197891148, the reward is 247.25 with loss [22.703166484832764, 22.540502071380615] in episode 758
Report: 
rewardSum:247.25
loss:[22.703166484832764, 22.540502071380615]
policies:[4, 0, 0]
qAverage:[103.33505249023438, 0.0]
ws:[9.255754566192627, 6.217182970046997]
memory len:9216
memory used:2807.0
now epsilon is 0.31564277963406195, the reward is 247.25 with loss [25.392236709594727, 21.47047221660614] in episode 759
Report: 
rewardSum:247.25
loss:[25.392236709594727, 21.47047221660614]
policies:[2, 2, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:9224
memory used:2807.0
now epsilon is 0.31532725520074384, the reward is 247.25 with loss [22.913251876831055, 26.911339282989502] in episode 760
Report: 
rewardSum:247.25
loss:[22.913251876831055, 26.911339282989502]
policies:[4, 0, 0]
qAverage:[99.5108133951823, 0.0]
ws:[10.226481596628824, 6.2577314376831055]
memory len:9232
memory used:2807.0
now epsilon is 0.3150120461735571, the reward is 247.25 with loss [12.507977604866028, 26.488099098205566] in episode 761
Report: 
rewardSum:247.25
loss:[12.507977604866028, 26.488099098205566]
policies:[3, 0, 1]
qAverage:[70.94850158691406, 0.0]
ws:[3.153082291285197, 1.7603909969329834]
memory len:9240
memory used:2807.0
now epsilon is 0.3146971522372139, the reward is 247.25 with loss [20.27503728866577, 25.199878215789795] in episode 762
Report: 
rewardSum:247.25
loss:[20.27503728866577, 25.199878215789795]
policies:[3, 1, 0]
qAverage:[103.7206859588623, 0.0]
ws:[12.018409729003906, 8.308497428894043]
memory len:9248
memory used:2808.0
now epsilon is 0.31406830837748195, the reward is 243.25 with loss [52.753196239471436, 43.17575967311859] in episode 763
Report: 
rewardSum:243.25
loss:[52.753196239471436, 43.17575967311859]
policies:[4, 1, 3]
qAverage:[109.35501556396484, 0.0]
ws:[9.114855098724366, 5.800560092926025]
memory len:9264
memory used:2807.0
now epsilon is 0.3139112938525625, the reward is -1.0 with loss [5.155591011047363, 8.649000644683838] in episode 764
Report: 
rewardSum:-1.0
loss:[5.155591011047363, 8.649000644683838]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:9268
memory used:2807.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2*		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.3137543578250921, the reward is -1.0 with loss [16.854636192321777, 16.191658973693848] in episode 765
Report: 
rewardSum:-1.0
loss:[16.854636192321777, 16.191658973693848]
policies:[1, 0, 1]
qAverage:[52.098304748535156, 0.0]
ws:[-0.9123287796974182, -1.3803709745407104]
memory len:9272
memory used:2807.0
now epsilon is 0.3134407211055428, the reward is 247.25 with loss [12.490540027618408, 18.66103959083557] in episode 766
Report: 
rewardSum:247.25
loss:[12.490540027618408, 18.66103959083557]
policies:[3, 1, 0]
qAverage:[90.26440175374348, 0.0]
ws:[8.63078769048055, 4.4229733149210615]
memory len:9280
memory used:2807.0
now epsilon is 0.3131273979051188, the reward is 37.15999999999997 with loss [25.43733024597168, 21.26004409790039] in episode 767
Report: 
rewardSum:37.15999999999997
loss:[25.43733024597168, 21.26004409790039]
policies:[2, 1, 1]
qAverage:[75.07763926188152, 0.0]
ws:[2.703004519144694, 1.697727620601654]
memory len:9288
memory used:2807.0
now epsilon is 0.31234545949430564, the reward is 241.25 with loss [58.239495277404785, 40.73269844055176] in episode 768
Report: 
rewardSum:241.25
loss:[58.239495277404785, 40.73269844055176]
policies:[4, 1, 5]
qAverage:[95.6101303100586, 0.0]
ws:[4.6970213651657104, 4.231411457061768]
memory len:9308
memory used:2808.0
now epsilon is 0.31203323114483833, the reward is 247.25 with loss [21.32944941520691, 17.29849338531494] in episode 769
Report: 
rewardSum:247.25
loss:[21.32944941520691, 17.29849338531494]
policies:[4, 0, 0]
qAverage:[92.95713996887207, 0.0]
ws:[6.531960666179657, 3.4130125641822815]
memory len:9316
memory used:2808.0
now epsilon is 0.31140971046775945, the reward is 243.25 with loss [39.33729910850525, 28.03840923309326] in episode 770
Report: 
rewardSum:243.25
loss:[39.33729910850525, 28.03840923309326]
policies:[4, 0, 4]
qAverage:[81.20923360188802, 0.0]
ws:[5.910695523023605, 2.0634032686551413]
memory len:9332
memory used:2809.0
now epsilon is 0.31109841751647127, the reward is 247.25 with loss [19.027584552764893, 18.97835612297058] in episode 771
Report: 
rewardSum:247.25
loss:[19.027584552764893, 18.97835612297058]
policies:[3, 0, 1]
qAverage:[100.92712783813477, 0.0]
ws:[9.87899661064148, 6.561758399009705]
memory len:9340
memory used:2814.0
now epsilon is 0.31063206144776306, the reward is 245.25 with loss [31.24053716659546, 31.070114612579346] in episode 772
Report: 
rewardSum:245.25
loss:[31.24053716659546, 31.070114612579346]
policies:[4, 1, 1]
qAverage:[61.96171760559082, 20.513700485229492]
ws:[5.765631675720215, 3.399964153766632]
memory len:9352
memory used:2814.0
now epsilon is 0.3101664044760948, the reward is 245.25 with loss [17.599979639053345, 42.47318959236145] in episode 773
Report: 
rewardSum:245.25
loss:[17.599979639053345, 42.47318959236145]
policies:[4, 0, 2]
qAverage:[102.21565399169921, 0.0]
ws:[6.376062822341919, 4.2200514793396]
memory len:9364
memory used:2814.0
now epsilon is 0.3098563543646362, the reward is 37.15999999999997 with loss [19.718294858932495, 23.237314701080322] in episode 774
Report: 
rewardSum:37.15999999999997
loss:[19.718294858932495, 23.237314701080322]
policies:[3, 0, 1]
qAverage:[91.13733863830566, 0.0]
ws:[3.0846618711948395, 2.1539086252450943]
memory len:9372
memory used:2814.0
now epsilon is 0.3093918602266096, the reward is 245.25 with loss [30.738450288772583, 50.12047052383423] in episode 775
Report: 
rewardSum:245.25
loss:[30.738450288772583, 50.12047052383423]
policies:[4, 0, 2]
qAverage:[100.78695831298828, 0.0]
ws:[4.128163814544678, 0.3127200841903687]
memory len:9384
memory used:2814.0
now epsilon is 0.30908258436899483, the reward is 247.25 with loss [28.074607849121094, 17.126746654510498] in episode 776
Report: 
rewardSum:247.25
loss:[28.074607849121094, 17.126746654510498]
policies:[3, 0, 1]
qAverage:[90.85492134094238, 0.0]
ws:[2.260032970458269, 0.9086427390575409]
memory len:9392
memory used:2820.0
now epsilon is 0.30854209534825383, the reward is 244.25 with loss [30.2677059173584, 27.791816115379333] in episode 777
Report: 
rewardSum:244.25
loss:[30.2677059173584, 27.791816115379333]
policies:[5, 0, 2]
qAverage:[102.31998901367187, 0.0]
ws:[7.453996849060059, 4.709664678573608]
memory len:9406
memory used:2820.0
now epsilon is 0.3081566105196745, the reward is 36.15999999999997 with loss [24.53546178340912, 26.710594654083252] in episode 778
Report: 
rewardSum:36.15999999999997
loss:[24.53546178340912, 26.710594654083252]
policies:[1, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:9416
memory used:2820.0
now epsilon is 0.3073870851063015, the reward is 241.25 with loss [66.3626275062561, 50.437447905540466] in episode 779
Report: 
rewardSum:241.25
loss:[66.3626275062561, 50.437447905540466]
policies:[2, 1, 7]
qAverage:[64.46832466125488, 18.445192337036133]
ws:[7.962718963623047, 5.849632263183594]
memory len:9436
memory used:2820.0
now epsilon is 0.3072334107754412, the reward is -1.0 with loss [7.976603984832764, 9.60653042793274] in episode 780
Report: 
rewardSum:-1.0
loss:[7.976603984832764, 9.60653042793274]
policies:[0, 1, 1]
qAverage:[0.0, 36.23196792602539]
ws:[0.022642401978373528, 0.3136036992073059]
memory len:9440
memory used:2820.0
now epsilon is 0.306926292557994, the reward is 247.25 with loss [22.2502019405365, 26.184539079666138] in episode 781
Report: 
rewardSum:247.25
loss:[22.2502019405365, 26.184539079666138]
policies:[1, 3, 0]
qAverage:[35.909497578938804, 29.95025126139323]
ws:[0.7441901862621307, 0.7527534862359365]
memory len:9448
memory used:2820.0
now epsilon is 0.30661948134361405, the reward is 247.25 with loss [17.621919751167297, 16.528899431228638] in episode 782
Report: 
rewardSum:247.25
loss:[17.621919751167297, 16.528899431228638]
policies:[1, 2, 1]
qAverage:[48.070393880208336, 29.363909403483074]
ws:[6.231301148732503, 2.8252561887105307]
memory len:9456
memory used:2820.0
now epsilon is 0.3063129768254135, the reward is 247.25 with loss [22.823432683944702, 16.341291189193726] in episode 783
Report: 
rewardSum:247.25
loss:[22.823432683944702, 16.341291189193726]
policies:[2, 0, 2]
qAverage:[73.3973159790039, 0.0]
ws:[7.032089710235596, 2.395885705947876]
memory len:9464
memory used:2820.0
now epsilon is 0.30600677869681103, the reward is 37.15999999999997 with loss [16.324452996253967, 22.04480469226837] in episode 784
Report: 
rewardSum:37.15999999999997
loss:[16.324452996253967, 22.04480469226837]
policies:[0, 3, 1]
qAverage:[0.0, 59.89985656738281]
ws:[3.6123118847608566, 4.167997725307941]
memory len:9472
memory used:2820.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2*		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.3056244614298691, the reward is 246.25 with loss [27.493191719055176, 13.802637219429016] in episode 785
Report: 
rewardSum:246.25
loss:[27.493191719055176, 13.802637219429016]
policies:[1, 3, 1]
qAverage:[37.51140213012695, 40.82302665710449]
ws:[5.718080908060074, 4.1829564571380615]
memory len:9482
memory used:2819.0
now epsilon is 0.30531895155851196, the reward is 37.15999999999997 with loss [12.518532752990723, 16.392862796783447] in episode 786
Report: 
rewardSum:37.15999999999997
loss:[12.518532752990723, 16.392862796783447]
policies:[1, 2, 1]
qAverage:[0.0, 38.30888366699219]
ws:[6.941383361816406, 7.277220726013184]
memory len:9490
memory used:2819.0
now epsilon is 0.3051663111651672, the reward is -1.0 with loss [9.565173625946045, 8.00671124458313] in episode 787
Report: 
rewardSum:-1.0
loss:[9.565173625946045, 8.00671124458313]
policies:[1, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:9494
memory used:2819.0
now epsilon is 0.30486125927229707, the reward is 247.25 with loss [16.441482305526733, 20.923523902893066] in episode 788
Report: 
rewardSum:247.25
loss:[16.441482305526733, 20.923523902893066]
policies:[1, 3, 0]
qAverage:[29.942132568359376, 47.08633117675781]
ws:[7.8947532176971436, 6.53711462020874]
memory len:9502
memory used:2819.0
now epsilon is 0.30440425309556796, the reward is 245.25 with loss [36.09176158905029, 39.77410316467285] in episode 789
Report: 
rewardSum:245.25
loss:[36.09176158905029, 39.77410316467285]
policies:[2, 1, 3]
qAverage:[43.167704264322914, 24.522791544596355]
ws:[3.0143068631490073, 3.046621799468994]
memory len:9514
memory used:2819.0
now epsilon is 0.30394793199980336, the reward is 245.25 with loss [45.4680061340332, 29.365500926971436] in episode 790
Report: 
rewardSum:245.25
loss:[45.4680061340332, 29.365500926971436]
policies:[1, 3, 2]
qAverage:[36.97451400756836, 42.7302303314209]
ws:[7.537094920873642, 5.832575023174286]
memory len:9526
memory used:2820.0
now epsilon is 0.30364409802928255, the reward is 247.25 with loss [19.00822353363037, 26.083763599395752] in episode 791
Report: 
rewardSum:247.25
loss:[19.00822353363037, 26.083763599395752]
policies:[2, 2, 0]
qAverage:[72.68291982014973, 0.0]
ws:[1.3158351282278697, 0.9446284373601278]
memory len:9534
memory used:2820.0
now epsilon is 0.30334056777881346, the reward is 247.25 with loss [27.221547603607178, 20.504476308822632] in episode 792
Report: 
rewardSum:247.25
loss:[27.221547603607178, 20.504476308822632]
policies:[4, 0, 0]
qAverage:[100.39664001464844, 0.0]
ws:[6.912362143397331, 4.46689920425415]
memory len:9542
memory used:2821.0
now epsilon is 0.30303734094478996, the reward is 247.25 with loss [20.076017379760742, 21.88668918609619] in episode 793
Report: 
rewardSum:247.25
loss:[20.076017379760742, 21.88668918609619]
policies:[2, 2, 0]
qAverage:[43.83885192871094, 23.14776357014974]
ws:[2.5362884998321533, 2.520981550216675]
memory len:9550
memory used:2821.0
now epsilon is 0.3025830689361985, the reward is 245.25 with loss [29.245139122009277, 30.261082887649536] in episode 794
Report: 
rewardSum:245.25
loss:[29.245139122009277, 30.261082887649536]
policies:[4, 0, 2]
qAverage:[93.72660255432129, 0.0]
ws:[6.4976677894592285, 5.808936953544617]
memory len:9562
memory used:2821.0
now epsilon is 0.30205394554040443, the reward is 34.15999999999997 with loss [34.066224813461304, 35.28293454647064] in episode 795
Report: 
rewardSum:34.15999999999997
loss:[34.066224813461304, 35.28293454647064]
policies:[3, 0, 4]
qAverage:[86.79778289794922, 0.0]
ws:[2.2603960037231445, 1.3578337219078094]
memory len:9576
memory used:2821.0
now epsilon is 0.30175200484621645, the reward is 247.25 with loss [25.965092658996582, 17.826078414916992] in episode 796
Report: 
rewardSum:247.25
loss:[25.965092658996582, 17.826078414916992]
policies:[2, 0, 2]
qAverage:[82.43866221110027, 0.0]
ws:[3.8039352099100747, 2.629415273666382]
memory len:9584
memory used:2820.0
now epsilon is 0.30145036597951375, the reward is 247.25 with loss [17.83848810195923, 31.90056872367859] in episode 797
Report: 
rewardSum:247.25
loss:[17.83848810195923, 31.90056872367859]
policies:[2, 1, 1]
qAverage:[66.88356018066406, 0.0]
ws:[2.368086814880371, 1.3186734914779663]
memory len:9592
memory used:2820.0
now epsilon is 0.301149028638582, the reward is 247.25 with loss [20.55027174949646, 16.1224467754364] in episode 798
Report: 
rewardSum:247.25
loss:[20.55027174949646, 16.1224467754364]
policies:[3, 0, 1]
qAverage:[86.59029579162598, 0.0]
ws:[2.1651904433965683, 0.5217249095439911]
memory len:9600
memory used:2820.0
now epsilon is 0.30084799252200856, the reward is 247.25 with loss [20.71119737625122, 20.12787628173828] in episode 799
Report: 
rewardSum:247.25
loss:[20.71119737625122, 20.12787628173828]
policies:[3, 1, 0]
qAverage:[81.47464243570964, 0.0]
ws:[0.7896046837170919, 0.34201868375142414]
memory len:9608
memory used:2820.0
now epsilon is 0.3004721205143498, the reward is 246.25 with loss [28.16534399986267, 18.728655099868774] in episode 800
Report: 
rewardSum:246.25
loss:[28.16534399986267, 18.728655099868774]
policies:[3, 0, 2]
qAverage:[101.12482070922852, 0.0]
ws:[9.143558859825134, 5.587802052497864]
memory len:9618
memory used:2820.0
now epsilon is 0.30017176105210236, the reward is 247.25 with loss [18.235968589782715, 9.504841685295105] in episode 801
Report: 
rewardSum:247.25
loss:[18.235968589782715, 9.504841685295105]
policies:[3, 1, 0]
qAverage:[95.52796363830566, 0.0]
ws:[10.748537540435791, 7.216327786445618]
memory len:9626
memory used:2820.0
now epsilon is 0.29972178472776423, the reward is 245.25 with loss [38.76061487197876, 38.59732460975647] in episode 802
Report: 
rewardSum:245.25
loss:[38.76061487197876, 38.59732460975647]
policies:[1, 2, 3]
qAverage:[29.515581130981445, 40.813589096069336]
ws:[0.9621361494064331, -0.4356890320777893]
memory len:9638
memory used:2820.0
now epsilon is 0.2995719425680119, the reward is -1.0 with loss [12.212071895599365, 10.091221332550049] in episode 803
Report: 
rewardSum:-1.0
loss:[12.212071895599365, 10.091221332550049]
policies:[0, 1, 1]
qAverage:[0.0, 35.174774169921875]
ws:[-0.6987757682800293, -0.18270565569400787]
memory len:9642
memory used:2820.0
now epsilon is 0.2989733226717318, the reward is 33.15999999999997 with loss [34.3230094909668, 37.16234850883484] in episode 804
Report: 
rewardSum:33.15999999999997
loss:[34.3230094909668, 37.16234850883484]
policies:[2, 3, 3]
qAverage:[50.96519317626953, 28.856982421875]
ws:[2.3303812995553015, 2.278918242454529]
memory len:9658
memory used:2820.0
now epsilon is 0.29867446144537146, the reward is 247.25 with loss [20.67487072944641, 13.995922088623047] in episode 805
Report: 
rewardSum:247.25
loss:[20.67487072944641, 13.995922088623047]
policies:[3, 0, 1]
qAverage:[77.58903503417969, 0.0]
ws:[9.949410438537598, 5.313471794128418]
memory len:9666
memory used:2820.0
now epsilon is 0.2983758989681832, the reward is 247.25 with loss [25.653610229492188, 24.775269031524658] in episode 806
Report: 
rewardSum:247.25
loss:[25.653610229492188, 24.775269031524658]
policies:[3, 0, 1]
qAverage:[104.19686889648438, 0.0]
ws:[8.422012686729431, 5.2217758893966675]
memory len:9674
memory used:2820.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34*		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.29792861475391125, the reward is 35.15999999999997 with loss [30.5174880027771, 30.88796305656433] in episode 807
Report: 
rewardSum:35.15999999999997
loss:[30.5174880027771, 30.88796305656433]
policies:[3, 1, 2]
qAverage:[89.6151008605957, 0.0]
ws:[6.444321274757385, 5.784297406673431]
memory len:9686
memory used:2820.0
now epsilon is 0.2976307978437685, the reward is 247.25 with loss [18.88895869255066, 21.644023656845093] in episode 808
Report: 
rewardSum:247.25
loss:[18.88895869255066, 21.644023656845093]
policies:[3, 0, 1]
qAverage:[75.43390655517578, 0.0]
ws:[6.400953769683838, 5.8461752732594805]
memory len:9694
memory used:2832.0
now epsilon is 0.2973332786388732, the reward is 247.25 with loss [16.546369552612305, 32.445703983306885] in episode 809
Report: 
rewardSum:247.25
loss:[16.546369552612305, 32.445703983306885]
policies:[3, 1, 0]
qAverage:[80.18372344970703, 0.0]
ws:[10.991615295410156, 5.489435195922852]
memory len:9702
memory used:2839.0
now epsilon is 0.2969617978274213, the reward is 246.25 with loss [25.54402184486389, 33.58732461929321] in episode 810
Report: 
rewardSum:246.25
loss:[25.54402184486389, 33.58732461929321]
policies:[4, 0, 1]
qAverage:[109.83295135498047, 0.0]
ws:[6.491893649101257, 3.3328075379133226]
memory len:9712
memory used:2840.0
now epsilon is 0.2966649473717092, the reward is 247.25 with loss [20.164748191833496, 15.262285709381104] in episode 811
Report: 
rewardSum:247.25
loss:[20.164748191833496, 15.262285709381104]
policies:[3, 1, 0]
qAverage:[91.74349975585938, 0.0]
ws:[9.838074088096619, 5.824388424555461]
memory len:9720
memory used:2840.0
now epsilon is 0.29636839365515233, the reward is 37.15999999999997 with loss [19.26510262489319, 15.638821721076965] in episode 812
Report: 
rewardSum:37.15999999999997
loss:[19.26510262489319, 15.638821721076965]
policies:[3, 0, 1]
qAverage:[61.04311752319336, 0.0]
ws:[8.23073959350586, 7.79561710357666]
memory len:9728
memory used:2840.0
now epsilon is 0.29607213638112295, the reward is 247.25 with loss [15.07355284690857, 10.799263954162598] in episode 813
Report: 
rewardSum:247.25
loss:[15.07355284690857, 10.799263954162598]
policies:[2, 1, 1]
qAverage:[41.38095347086588, 33.75379435221354]
ws:[2.7470361391703286, 2.6678953965504966]
memory len:9736
memory used:2840.0
now epsilon is 0.29577617525328964, the reward is 247.25 with loss [16.681301593780518, 11.954762697219849] in episode 814
Report: 
rewardSum:247.25
loss:[16.681301593780518, 11.954762697219849]
policies:[1, 3, 0]
qAverage:[33.42899169921875, 52.28770904541015]
ws:[8.947316938638687, 7.656060683727264]
memory len:9744
memory used:2840.0
now epsilon is 0.2954066398481234, the reward is 246.25 with loss [27.535745859146118, 25.397205352783203] in episode 815
Report: 
rewardSum:246.25
loss:[27.535745859146118, 25.397205352783203]
policies:[3, 1, 1]
qAverage:[71.62788200378418, 25.351720809936523]
ws:[13.433353900909424, 10.9038245677948]
memory len:9754
memory used:2840.0
now epsilon is 0.29511134396730343, the reward is 247.25 with loss [14.336665749549866, 25.159172534942627] in episode 816
Report: 
rewardSum:247.25
loss:[14.336665749549866, 25.159172534942627]
policies:[4, 0, 0]
qAverage:[107.371484375, 0.0]
ws:[10.705084133148194, 8.23402910232544]
memory len:9762
memory used:2840.0
now epsilon is 0.29481634327164685, the reward is 247.25 with loss [16.292716026306152, 19.27850317955017] in episode 817
Report: 
rewardSum:247.25
loss:[16.292716026306152, 19.27850317955017]
policies:[3, 0, 1]
qAverage:[55.66952133178711, 0.0]
ws:[0.9517195224761963, 0.15186956524848938]
memory len:9770
memory used:2840.0
now epsilon is 0.2945216374660791, the reward is 247.25 with loss [10.750717639923096, 11.987449526786804] in episode 818
Report: 
rewardSum:247.25
loss:[10.750717639923096, 11.987449526786804]
policies:[3, 1, 0]
qAverage:[104.10150909423828, 0.0]
ws:[7.664751201868057, 3.6000985503196716]
memory len:9778
memory used:2840.0
now epsilon is 0.29422722625582065, the reward is 247.25 with loss [12.686576262116432, 25.803951740264893] in episode 819
Report: 
rewardSum:247.25
loss:[12.686576262116432, 25.803951740264893]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:9786
memory used:2840.0
now epsilon is 0.29393310934638667, the reward is 247.25 with loss [22.42440700531006, 21.066226482391357] in episode 820
Report: 
rewardSum:247.25
loss:[22.42440700531006, 21.066226482391357]
policies:[4, 0, 0]
qAverage:[108.42086791992188, 0.0]
ws:[7.4255534410476685, 4.454398465156555]
memory len:9794
memory used:2840.0
now epsilon is 0.2936392864435866, the reward is 37.15999999999997 with loss [33.17204713821411, 18.81943106651306] in episode 821
Report: 
rewardSum:37.15999999999997
loss:[33.17204713821411, 18.81943106651306]
policies:[2, 0, 2]
qAverage:[59.72233581542969, 0.0]
ws:[3.133035659790039, 2.1746678352355957]
memory len:9802
memory used:2840.0
now epsilon is 0.2933457572535242, the reward is 247.25 with loss [23.395848035812378, 20.192647218704224] in episode 822
Report: 
rewardSum:247.25
loss:[23.395848035812378, 20.192647218704224]
policies:[4, 0, 0]
qAverage:[110.56041717529297, 0.0]
ws:[10.953209114074706, 8.06058578491211]
memory len:9810
memory used:2840.0
now epsilon is 0.29305252148259664, the reward is 247.25 with loss [19.412201404571533, 36.48975944519043] in episode 823
Report: 
rewardSum:247.25
loss:[19.412201404571533, 36.48975944519043]
policies:[3, 1, 0]
qAverage:[93.4715347290039, 0.0]
ws:[8.719594160715738, 5.618264198303223]
memory len:9818
memory used:2840.0
now epsilon is 0.292759578837495, the reward is 247.25 with loss [16.164607048034668, 9.754822134971619] in episode 824
Report: 
rewardSum:247.25
loss:[16.164607048034668, 9.754822134971619]
policies:[3, 0, 1]
qAverage:[96.10970115661621, 0.0]
ws:[4.905455231666565, 3.0054449439048767]
memory len:9826
memory used:2840.0
now epsilon is 0.2923207138398737, the reward is 245.25 with loss [23.802784085273743, 31.45662569999695] in episode 825
Report: 
rewardSum:245.25
loss:[23.802784085273743, 31.45662569999695]
policies:[4, 1, 1]
qAverage:[107.91832885742187, 0.0]
ws:[6.992592716217041, 4.092954039573669]
memory len:9838
memory used:2840.0
now epsilon is 0.29188250672845006, the reward is 245.25 with loss [27.198593854904175, 33.073899269104004] in episode 826
Report: 
rewardSum:245.25
loss:[27.198593854904175, 33.073899269104004]
policies:[5, 0, 1]
qAverage:[113.4181760152181, 0.0]
ws:[6.777403672536214, 4.385503262281418]
memory len:9850
memory used:2840.0
now epsilon is 0.29144495651701136, the reward is 245.25 with loss [31.709030866622925, 31.029862701892853] in episode 827
Report: 
rewardSum:245.25
loss:[31.709030866622925, 31.029862701892853]
policies:[3, 0, 3]
qAverage:[107.3422139485677, 0.0]
ws:[11.397744496663412, 7.746462186177571]
memory len:9862
memory used:2840.0
now epsilon is 0.29115362083413887, the reward is 247.25 with loss [25.764702081680298, 15.346401929855347] in episode 828
Report: 
rewardSum:247.25
loss:[25.764702081680298, 15.346401929855347]
policies:[4, 0, 0]
qAverage:[98.30059305826823, 0.0]
ws:[14.506738662719727, 10.342494328816732]
memory len:9870
memory used:2840.0
now epsilon is 0.2908625763777167, the reward is 247.25 with loss [21.46877086162567, 13.775254964828491] in episode 829
Report: 
rewardSum:247.25
loss:[21.46877086162567, 13.775254964828491]
policies:[3, 0, 1]
qAverage:[93.23025004069011, 0.0]
ws:[7.916773994763692, 4.6233634154001875]
memory len:9878
memory used:2840.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22*		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.2904991799009132, the reward is 246.25 with loss [34.404170513153076, 30.378262042999268] in episode 830
Report: 
rewardSum:246.25
loss:[34.404170513153076, 30.378262042999268]
policies:[4, 0, 1]
qAverage:[110.88274841308593, 0.0]
ws:[9.327632066607475, 6.670289354026318]
memory len:9888
memory used:2840.0
now epsilon is 0.2898462099881538, the reward is 242.25 with loss [48.099326491355896, 55.15843629837036] in episode 831
Report: 
rewardSum:242.25
loss:[48.099326491355896, 55.15843629837036]
policies:[4, 1, 4]
qAverage:[76.2633071899414, 18.031698608398436]
ws:[4.63323073387146, 3.950443744659424]
memory len:9906
memory used:2840.0
now epsilon is 0.2895564724523802, the reward is 247.25 with loss [13.652792692184448, 12.489700317382812] in episode 832
Report: 
rewardSum:247.25
loss:[13.652792692184448, 12.489700317382812]
policies:[3, 1, 0]
qAverage:[102.9744364420573, 0.0]
ws:[11.172377427419027, 7.065142631530762]
memory len:9914
memory used:2840.0
now epsilon is 0.28912240911242515, the reward is 245.25 with loss [20.893465280532837, 32.28062725067139] in episode 833
Report: 
rewardSum:245.25
loss:[20.893465280532837, 32.28062725067139]
policies:[1, 2, 3]
qAverage:[66.25196075439453, 0.0]
ws:[9.661909103393555, 8.199651718139648]
memory len:9926
memory used:2840.0
now epsilon is 0.2887611867573706, the reward is 246.25 with loss [29.989765167236328, 22.26808452606201] in episode 834
Report: 
rewardSum:246.25
loss:[29.989765167236328, 22.26808452606201]
policies:[3, 1, 1]
qAverage:[116.8869400024414, 0.0]
ws:[12.593397736549377, 7.733523845672607]
memory len:9936
memory used:2840.0
now epsilon is 0.28854467000551326, the reward is -2.0 with loss [5.074578762054443, 9.398369908332825] in episode 835
Report: 
rewardSum:-2.0
loss:[5.074578762054443, 9.398369908332825]
policies:[1, 0, 2]
qAverage:[58.46806335449219, 0.0]
ws:[0.05930400267243385, -0.16895830631256104]
memory len:9942
memory used:2840.0
now epsilon is 0.2882562335217262, the reward is 37.15999999999997 with loss [16.695453882217407, 17.71199917793274] in episode 836
Report: 
rewardSum:37.15999999999997
loss:[16.695453882217407, 17.71199917793274]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:9950
memory used:2840.0
now epsilon is 0.2879680853662771, the reward is 247.25 with loss [17.46615433692932, 19.83634090423584] in episode 837
Report: 
rewardSum:247.25
loss:[17.46615433692932, 19.83634090423584]
policies:[2, 2, 0]
qAverage:[104.77339172363281, 0.0]
ws:[13.257733662923178, 7.692198117574056]
memory len:9958
memory used:2840.0
now epsilon is 0.28753640311833456, the reward is 245.25 with loss [22.698131561279297, 35.94673538208008] in episode 838
Report: 
rewardSum:245.25
loss:[22.698131561279297, 35.94673538208008]
policies:[3, 2, 1]
qAverage:[44.75366719563802, 30.820767720540363]
ws:[6.7305562098821, 5.787663578987122]
memory len:9970
memory used:2840.0
now epsilon is 0.2873926528878006, the reward is -1.0 with loss [0.5217460989952087, 8.70468282699585] in episode 839
Report: 
rewardSum:-1.0
loss:[0.5217460989952087, 8.70468282699585]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:9974
memory used:2840.0
now epsilon is 0.2871053679891968, the reward is 247.25 with loss [13.790437698364258, 8.058609694242477] in episode 840
Report: 
rewardSum:247.25
loss:[13.790437698364258, 8.058609694242477]
policies:[2, 0, 2]
qAverage:[91.56253814697266, 0.0]
ws:[11.456323623657227, 5.548121452331543]
memory len:9982
memory used:2840.0
now epsilon is 0.2867466656752107, the reward is 246.25 with loss [19.815260648727417, 27.17100191116333] in episode 841
Report: 
rewardSum:246.25
loss:[19.815260648727417, 27.17100191116333]
policies:[3, 1, 1]
qAverage:[104.54833984375, 0.0]
ws:[15.693895975748697, 10.573232332865397]
memory len:9992
memory used:2839.0
now epsilon is 0.2864600265216146, the reward is 247.25 with loss [22.618476629257202, 21.84156608581543] in episode 842
Report: 
rewardSum:247.25
loss:[22.618476629257202, 21.84156608581543]
policies:[4, 0, 0]
qAverage:[98.64806620279948, 0.0]
ws:[9.478811820348104, 5.001551151275635]
memory len:10000
memory used:2839.0
now epsilon is 0.2858876075230437, the reward is 243.25 with loss [60.286001682281494, 47.468098640441895] in episode 843
Report: 
rewardSum:243.25
loss:[60.286001682281494, 47.468098640441895]
policies:[4, 0, 4]
qAverage:[112.97625274658203, 0.0]
ws:[12.73792486190796, 8.59943054318428]
memory len:10000
memory used:2839.0
now epsilon is 0.2856018271055067, the reward is 247.25 with loss [15.571244239807129, 16.03585386276245] in episode 844
Report: 
rewardSum:247.25
loss:[15.571244239807129, 16.03585386276245]
policies:[3, 0, 1]
qAverage:[99.1100362141927, 0.0]
ws:[10.779637147982916, 5.829761346181233]
memory len:10000
memory used:2839.0
now epsilon is 0.2853163323612374, the reward is 247.25 with loss [23.639312744140625, 23.672661304473877] in episode 845
Report: 
rewardSum:247.25
loss:[23.639312744140625, 23.672661304473877]
policies:[3, 0, 1]
qAverage:[113.91755104064941, 0.0]
ws:[10.031009301543236, 5.240240432322025]
memory len:10000
memory used:2840.0
now epsilon is 0.2850311230046696, the reward is 247.25 with loss [26.084237098693848, 18.926674842834473] in episode 846
Report: 
rewardSum:247.25
loss:[26.084237098693848, 18.926674842834473]
policies:[1, 2, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:2839.0
now epsilon is 0.28474619875052276, the reward is 247.25 with loss [21.612809896469116, 17.890278816223145] in episode 847
Report: 
rewardSum:247.25
loss:[21.612809896469116, 17.890278816223145]
policies:[4, 0, 0]
qAverage:[115.3519344329834, 0.0]
ws:[11.499562382698059, 6.263584464788437]
memory len:10000
memory used:2839.0
now epsilon is 0.28446155931380124, the reward is 247.25 with loss [11.35777896642685, 14.739350318908691] in episode 848
Report: 
rewardSum:247.25
loss:[11.35777896642685, 14.739350318908691]
policies:[2, 0, 2]
qAverage:[92.25691731770833, 0.0]
ws:[3.5645340283711753, 2.8033802111943564]
memory len:10000
memory used:2839.0
now epsilon is 0.28431934631299177, the reward is -1.0 with loss [10.435422658920288, 16.65794563293457] in episode 849
Report: 
rewardSum:-1.0
loss:[10.435422658920288, 16.65794563293457]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:2847.0
now epsilon is 0.28403513356866483, the reward is 247.25 with loss [17.339611053466797, 21.89455771446228] in episode 850
Report: 
rewardSum:247.25
loss:[17.339611053466797, 21.89455771446228]
policies:[4, 0, 0]
qAverage:[117.65902709960938, 0.0]
ws:[10.125317931175232, 6.228126049041748]
memory len:10000
memory used:2848.0
now epsilon is 0.2837512049305202, the reward is 247.25 with loss [20.486530780792236, 30.924502849578857] in episode 851
Report: 
rewardSum:247.25
loss:[20.486530780792236, 30.924502849578857]
policies:[1, 2, 1]
qAverage:[68.22235107421875, 0.0]
ws:[2.4278781414031982, 2.116396188735962]
memory len:10000
memory used:2872.0
now epsilon is 0.2834675601145582, the reward is 247.25 with loss [13.110825061798096, 36.54157304763794] in episode 852
Report: 
rewardSum:247.25
loss:[13.110825061798096, 36.54157304763794]
policies:[3, 1, 0]
qAverage:[95.62084655761718, 16.097894287109376]
ws:[13.234676933288574, 10.028819847106934]
memory len:10000
memory used:2842.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22*		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.28318419883706314, the reward is 247.25 with loss [21.537039279937744, 24.250870943069458] in episode 853
Report: 
rewardSum:247.25
loss:[21.537039279937744, 24.250870943069458]
policies:[4, 0, 0]
qAverage:[118.37454986572266, 0.0]
ws:[10.511637306213379, 6.469386959075928]
memory len:10000
memory used:2841.0
now epsilon is 0.2829011208146027, the reward is 247.25 with loss [30.70129156112671, 22.852797031402588] in episode 854
Report: 
rewardSum:247.25
loss:[30.70129156112671, 22.852797031402588]
policies:[3, 1, 0]
qAverage:[102.48818778991699, 0.0]
ws:[1.9398387148976326, 0.9873697608709335]
memory len:10000
memory used:2841.0
now epsilon is 0.28261832576402823, the reward is 247.25 with loss [19.988717555999756, 20.46256971359253] in episode 855
Report: 
rewardSum:247.25
loss:[19.988717555999756, 20.46256971359253]
policies:[3, 0, 1]
qAverage:[104.14933522542317, 0.0]
ws:[13.786412477493286, 8.3446573416392]
memory len:10000
memory used:2841.0
now epsilon is 0.2824770342647916, the reward is -1.0 with loss [11.228043556213379, 7.347822844982147] in episode 856
Report: 
rewardSum:-1.0
loss:[11.228043556213379, 7.347822844982147]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:2841.0
now epsilon is 0.28219466314176095, the reward is 247.25 with loss [21.47327470779419, 23.978118419647217] in episode 857
Report: 
rewardSum:247.25
loss:[21.47327470779419, 23.978118419647217]
policies:[2, 0, 2]
qAverage:[107.96952819824219, 0.0]
ws:[19.19191614786784, 13.112483342488607]
memory len:10000
memory used:2841.0
now epsilon is 0.28184209614041084, the reward is 36.15999999999997 with loss [24.780254364013672, 24.175111293792725] in episode 858
Report: 
rewardSum:36.15999999999997
loss:[24.780254364013672, 24.175111293792725]
policies:[2, 1, 2]
qAverage:[64.17215728759766, 0.0]
ws:[-0.6910415291786194, -1.124381422996521]
memory len:10000
memory used:2841.0
now epsilon is 0.2814195971351063, the reward is 245.25 with loss [32.09249424934387, 29.024837970733643] in episode 859
Report: 
rewardSum:245.25
loss:[32.09249424934387, 29.024837970733643]
policies:[3, 2, 1]
qAverage:[114.5849609375, 0.0]
ws:[16.569299062093098, 11.948758761088053]
memory len:10000
memory used:2841.0
now epsilon is 0.28113828305273253, the reward is 247.25 with loss [19.939317226409912, 20.02888774871826] in episode 860
Report: 
rewardSum:247.25
loss:[19.939317226409912, 20.02888774871826]
policies:[3, 1, 0]
qAverage:[98.1735107421875, 17.240510559082033]
ws:[10.736195421218872, 7.299800300598145]
memory len:10000
memory used:2841.0
now epsilon is 0.2808572501789659, the reward is 247.25 with loss [25.720297813415527, 19.687891960144043] in episode 861
Report: 
rewardSum:247.25
loss:[25.720297813415527, 19.687891960144043]
policies:[3, 0, 1]
qAverage:[111.7554931640625, 0.0]
ws:[8.946376860141754, 5.748238623142242]
memory len:10000
memory used:2841.0
now epsilon is 0.28029602693312256, the reward is 243.25 with loss [29.78346061706543, 60.91809320449829] in episode 862
Report: 
rewardSum:243.25
loss:[29.78346061706543, 60.91809320449829]
policies:[6, 0, 2]
qAverage:[82.62221272786458, 0.0]
ws:[4.926050662994385, 4.045827945073445]
memory len:10000
memory used:2841.0
now epsilon is 0.27994583204068224, the reward is 246.25 with loss [30.15334963798523, 33.16836643218994] in episode 863
Report: 
rewardSum:246.25
loss:[30.15334963798523, 33.16836643218994]
policies:[1, 1, 3]
qAverage:[63.01549530029297, 0.0]
ws:[2.123863458633423, 1.4541802406311035]
memory len:10000
memory used:2841.0
now epsilon is 0.27980587662127643, the reward is -1.0 with loss [19.869105339050293, 5.298910021781921] in episode 864
Report: 
rewardSum:-1.0
loss:[19.869105339050293, 5.298910021781921]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:2841.0
now epsilon is 0.2795261756543721, the reward is 247.25 with loss [12.22346544265747, 21.33100461959839] in episode 865
Report: 
rewardSum:247.25
loss:[12.22346544265747, 21.33100461959839]
policies:[1, 1, 2]
qAverage:[81.19489288330078, 0.0]
ws:[5.001296520233154, 3.2391085624694824]
memory len:10000
memory used:2841.0
now epsilon is 0.2791071483593447, the reward is 245.25 with loss [21.44009780883789, 31.395487308502197] in episode 866
Report: 
rewardSum:245.25
loss:[21.44009780883789, 31.395487308502197]
policies:[4, 0, 2]
qAverage:[128.00227966308594, 0.0]
ws:[8.081602382659913, 5.120247220993042]
memory len:10000
memory used:2841.0
now epsilon is 0.27882814585872295, the reward is 247.25 with loss [21.372815132141113, 18.91208839416504] in episode 867
Report: 
rewardSum:247.25
loss:[21.372815132141113, 18.91208839416504]
policies:[3, 1, 0]
qAverage:[73.72613525390625, 0.0]
ws:[5.83159875869751, 4.364851474761963]
memory len:10000
memory used:2841.0
now epsilon is 0.2786887492125527, the reward is -1.0 with loss [12.947394371032715, 12.262632131576538] in episode 868
Report: 
rewardSum:-1.0
loss:[12.947394371032715, 12.262632131576538]
policies:[1, 0, 1]
qAverage:[67.00865936279297, 0.0]
ws:[3.6276872158050537, 2.029449224472046]
memory len:10000
memory used:2841.0
now epsilon is 0.2782014095280443, the reward is 34.15999999999997 with loss [34.75522518157959, 33.228739738464355] in episode 869
Report: 
rewardSum:34.15999999999997
loss:[34.75522518157959, 33.228739738464355]
policies:[3, 1, 3]
qAverage:[112.06951522827148, 0.0]
ws:[5.6376272439956665, 3.7592564821243286]
memory len:10000
memory used:2841.0
now epsilon is 0.27778436814065205, the reward is 245.25 with loss [44.41655778884888, 18.640400171279907] in episode 870
Report: 
rewardSum:245.25
loss:[44.41655778884888, 18.640400171279907]
policies:[3, 1, 2]
qAverage:[118.10082244873047, 0.0]
ws:[8.716056823730469, 7.092823266983032]
memory len:10000
memory used:2841.0
now epsilon is 0.2775066879242891, the reward is 247.25 with loss [25.470557689666748, 27.34500026702881] in episode 871
Report: 
rewardSum:247.25
loss:[25.470557689666748, 27.34500026702881]
policies:[3, 0, 1]
qAverage:[114.1494140625, 0.0]
ws:[14.28028647104899, 8.923139890034994]
memory len:10000
memory used:2841.0
now epsilon is 0.27709068796821806, the reward is 245.25 with loss [36.593346118927, 31.584006309509277] in episode 872
Report: 
rewardSum:245.25
loss:[36.593346118927, 31.584006309509277]
policies:[4, 0, 2]
qAverage:[120.4079376220703, 0.0]
ws:[5.934264922142029, 4.315204191207886]
memory len:10000
memory used:2841.0
now epsilon is 0.27681370117194076, the reward is 247.25 with loss [20.524210691452026, 14.163501739501953] in episode 873
Report: 
rewardSum:247.25
loss:[20.524210691452026, 14.163501739501953]
policies:[3, 0, 1]
qAverage:[117.31339263916016, 0.0]
ws:[10.584153197705746, 6.453912161290646]
memory len:10000
memory used:2841.0
now epsilon is 0.276536991258607, the reward is 247.25 with loss [23.668558597564697, 27.476716995239258] in episode 874
Report: 
rewardSum:247.25
loss:[23.668558597564697, 27.476716995239258]
policies:[3, 0, 1]
qAverage:[117.33783721923828, 0.0]
ws:[12.10368549823761, 8.118655502796173]
memory len:10000
memory used:2841.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12*		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.27598440097393023, the reward is 243.25 with loss [41.42339825630188, 33.0620493888855] in episode 875
Report: 
rewardSum:243.25
loss:[41.42339825630188, 33.0620493888855]
policies:[3, 2, 3]
qAverage:[75.08951187133789, 23.793594360351562]
ws:[6.266540467739105, 5.961134076118469]
memory len:10000
memory used:2842.0
now epsilon is 0.2756395929198463, the reward is 36.15999999999997 with loss [26.199840784072876, 19.28344178199768] in episode 876
Report: 
rewardSum:36.15999999999997
loss:[26.199840784072876, 19.28344178199768]
policies:[1, 1, 3]
qAverage:[51.10558064778646, 41.92205301920573]
ws:[5.5109225908915205, 5.226896603902181]
memory len:10000
memory used:2841.0
now epsilon is 0.2752263918564637, the reward is 245.25 with loss [35.759692668914795, 30.732977151870728] in episode 877
Report: 
rewardSum:245.25
loss:[35.759692668914795, 30.732977151870728]
policies:[2, 2, 2]
qAverage:[46.15493392944336, 61.72104835510254]
ws:[10.837082505226135, 8.112758159637451]
memory len:10000
memory used:2841.0
now epsilon is 0.2750200236632194, the reward is -2.0 with loss [13.21485984325409, 16.79376196861267] in episode 878
Report: 
rewardSum:-2.0
loss:[13.21485984325409, 16.79376196861267]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:2841.0
now epsilon is 0.27474510675487746, the reward is 247.25 with loss [24.532759189605713, 32.89115524291992] in episode 879
Report: 
rewardSum:247.25
loss:[24.532759189605713, 32.89115524291992]
policies:[2, 2, 0]
qAverage:[37.57224655151367, 55.11091613769531]
ws:[5.316794514656067, 5.981387585401535]
memory len:10000
memory used:2841.0
now epsilon is 0.27433324658244096, the reward is 245.25 with loss [32.39153444766998, 33.645174741744995] in episode 880
Report: 
rewardSum:245.25
loss:[32.39153444766998, 33.645174741744995]
policies:[1, 4, 1]
qAverage:[31.96937306722005, 75.09644190470378]
ws:[9.43540229399999, 8.00657476981481]
memory len:10000
memory used:2841.0
now epsilon is 0.2736481845140912, the reward is 241.25 with loss [56.65800225734711, 50.778638273477554] in episode 881
Report: 
rewardSum:241.25
loss:[56.65800225734711, 50.778638273477554]
policies:[2, 5, 3]
qAverage:[51.04883466448103, 65.0791756766183]
ws:[7.53177989221045, 6.80237603187561]
memory len:10000
memory used:2841.0
now epsilon is 0.272964833175459, the reward is 241.25 with loss [42.9830157160759, 46.62562704086304] in episode 882
Report: 
rewardSum:241.25
loss:[42.9830157160759, 46.62562704086304]
policies:[3, 3, 4]
qAverage:[39.84937438964844, 66.87603454589843]
ws:[9.857281827926636, 9.155528807640076]
memory len:10000
memory used:2841.0
now epsilon is 0.2726919706870368, the reward is 247.25 with loss [13.297107934951782, 25.05271005630493] in episode 883
Report: 
rewardSum:247.25
loss:[13.297107934951782, 25.05271005630493]
policies:[1, 3, 0]
qAverage:[0.0, 86.55516624450684]
ws:[4.037312865257263, 4.858300268650055]
memory len:10000
memory used:2841.0
now epsilon is 0.27241938095879664, the reward is 37.15999999999997 with loss [24.325504779815674, 26.139000415802002] in episode 884
Report: 
rewardSum:37.15999999999997
loss:[24.325504779815674, 26.139000415802002]
policies:[1, 2, 1]
qAverage:[0.0, 73.60214233398438]
ws:[3.0023630460103354, 3.5443963209788003]
memory len:10000
memory used:2841.0
now epsilon is 0.2721470637180806, the reward is 247.25 with loss [17.726739645004272, 19.923614025115967] in episode 885
Report: 
rewardSum:247.25
loss:[17.726739645004272, 19.923614025115967]
policies:[3, 1, 0]
qAverage:[99.50537719726563, 26.171609497070314]
ws:[10.43022961616516, 8.058917093276978]
memory len:10000
memory used:2841.0
now epsilon is 0.2718750186925033, the reward is 247.25 with loss [25.617494106292725, 13.984654188156128] in episode 886
Report: 
rewardSum:247.25
loss:[25.617494106292725, 13.984654188156128]
policies:[2, 2, 0]
qAverage:[50.15829849243164, 56.89532661437988]
ws:[11.098140954971313, 8.293865442276001]
memory len:10000
memory used:2841.0
now epsilon is 0.2716032456099517, the reward is 247.25 with loss [23.88545799255371, 18.192027807235718] in episode 887
Report: 
rewardSum:247.25
loss:[23.88545799255371, 18.192027807235718]
policies:[3, 0, 1]
qAverage:[115.15279006958008, 0.0]
ws:[7.135254085063934, 6.730352431535721]
memory len:10000
memory used:2841.0
now epsilon is 0.27126391126253513, the reward is 246.25 with loss [25.12132978439331, 31.093358755111694] in episode 888
Report: 
rewardSum:246.25
loss:[25.12132978439331, 31.093358755111694]
policies:[2, 2, 1]
qAverage:[94.30831909179688, 0.0]
ws:[5.344894886016846, 4.030657132466634]
memory len:10000
memory used:2841.0
now epsilon is 0.2708572696208041, the reward is 245.25 with loss [33.57277536392212, 23.818439245224] in episode 889
Report: 
rewardSum:245.25
loss:[33.57277536392212, 23.818439245224]
policies:[2, 1, 3]
qAverage:[100.03369140625, 0.0]
ws:[14.776642799377441, 9.064238548278809]
memory len:10000
memory used:2841.0
now epsilon is 0.2705865139057319, the reward is 247.25 with loss [26.95190143585205, 13.837048053741455] in episode 890
Report: 
rewardSum:247.25
loss:[26.95190143585205, 13.837048053741455]
policies:[3, 0, 1]
qAverage:[120.86205673217773, 0.0]
ws:[10.577403232455254, 7.523991793394089]
memory len:10000
memory used:2841.0
now epsilon is 0.27031602884485834, the reward is 247.25 with loss [20.05786371231079, 16.689534187316895] in episode 891
Report: 
rewardSum:247.25
loss:[20.05786371231079, 16.689534187316895]
policies:[2, 1, 1]
qAverage:[124.17955525716145, 0.0]
ws:[14.250494956970215, 10.211365222930908]
memory len:10000
memory used:2841.0
now epsilon is 0.26984333043637565, the reward is 244.25 with loss [41.2106237411499, 39.893693923950195] in episode 892
Report: 
rewardSum:244.25
loss:[41.2106237411499, 39.893693923950195]
policies:[3, 2, 2]
qAverage:[71.51101303100586, 33.1284294128418]
ws:[7.803854584693909, 7.349027633666992]
memory len:10000
memory used:2841.0
now epsilon is 0.2695735882803241, the reward is 247.25 with loss [18.54977035522461, 19.663657188415527] in episode 893
Report: 
rewardSum:247.25
loss:[18.54977035522461, 19.663657188415527]
policies:[2, 2, 0]
qAverage:[67.12271423339844, 45.25975341796875]
ws:[10.15288610458374, 8.554696846008301]
memory len:10000
memory used:2841.0
now epsilon is 0.2693041157652921, the reward is 247.25 with loss [24.384318351745605, 21.144080638885498] in episode 894
Report: 
rewardSum:247.25
loss:[24.384318351745605, 21.144080638885498]
policies:[0, 3, 1]
qAverage:[0.0, 84.92864418029785]
ws:[3.6385132670402527, 4.191004812717438]
memory len:10000
memory used:2841.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2*		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.26903491262173984, the reward is 37.15999999999997 with loss [23.307979106903076, 15.10891342163086] in episode 895
Report: 
rewardSum:37.15999999999997
loss:[23.307979106903076, 15.10891342163086]
policies:[0, 2, 2]
qAverage:[0.0, 65.97520446777344]
ws:[2.049131155014038, 2.932269811630249]
memory len:10000
memory used:2841.0
now epsilon is 0.26876597858039675, the reward is 247.25 with loss [24.777420043945312, 16.352287650108337] in episode 896
Report: 
rewardSum:247.25
loss:[24.777420043945312, 16.352287650108337]
policies:[2, 2, 0]
qAverage:[67.94902648925782, 46.81325225830078]
ws:[10.464594888687134, 8.878340196609496]
memory len:10000
memory used:2841.0
now epsilon is 0.26849731337226157, the reward is 247.25 with loss [22.149628400802612, 23.088135242462158] in episode 897
Report: 
rewardSum:247.25
loss:[22.149628400802612, 23.088135242462158]
policies:[1, 3, 0]
qAverage:[36.9411865234375, 66.99239196777344]
ws:[8.885691118240356, 7.285481333732605]
memory len:10000
memory used:2841.0
now epsilon is 0.26809481903454485, the reward is 245.25 with loss [17.770593404769897, 27.946293830871582] in episode 898
Report: 
rewardSum:245.25
loss:[17.770593404769897, 27.946293830871582]
policies:[0, 4, 2]
qAverage:[0.0, 94.9145751953125]
ws:[3.5224470496177673, 4.269157099723816]
memory len:10000
memory used:2841.0
now epsilon is 0.2678268247343126, the reward is 247.25 with loss [10.294484972953796, 26.115506529808044] in episode 899
Report: 
rewardSum:247.25
loss:[10.294484972953796, 26.115506529808044]
policies:[3, 0, 1]
qAverage:[130.68128967285156, 0.0]
ws:[12.292028188705444, 9.563273072242737]
memory len:10000
memory used:2841.0
now epsilon is 0.2671580104334658, the reward is 241.25 with loss [44.079865872859955, 39.278220891952515] in episode 900
Report: 
rewardSum:241.25
loss:[44.079865872859955, 39.278220891952515]
policies:[4, 2, 4]
qAverage:[127.69027099609374, 0.0]
ws:[10.600951409339904, 8.37046582698822]
memory len:10000
memory used:2840.0
now epsilon is 0.26689095259059, the reward is 247.25 with loss [23.88888931274414, 20.29087519645691] in episode 901
Report: 
rewardSum:247.25
loss:[23.88888931274414, 20.29087519645691]
policies:[3, 1, 0]
qAverage:[102.16707865397136, 0.0]
ws:[2.420021096865336, 1.7783250013987224]
memory len:10000
memory used:2841.0
now epsilon is 0.266624161705427, the reward is 247.25 with loss [22.537587642669678, 24.24801778793335] in episode 902
Report: 
rewardSum:247.25
loss:[22.537587642669678, 24.24801778793335]
policies:[3, 1, 0]
qAverage:[121.86510467529297, 0.0]
ws:[10.328964829444885, 6.911418356001377]
memory len:10000
memory used:2841.0
now epsilon is 0.2663576375111193, the reward is 247.25 with loss [21.52269697189331, 18.663776397705078] in episode 903
Report: 
rewardSum:247.25
loss:[21.52269697189331, 18.663776397705078]
policies:[3, 0, 1]
qAverage:[101.17705790201823, 0.0]
ws:[1.7099701166152954, 1.4334220066666603]
memory len:10000
memory used:2841.0
now epsilon is 0.26609137974107594, the reward is 247.25 with loss [27.412603855133057, 28.748247623443604] in episode 904
Report: 
rewardSum:247.25
loss:[27.412603855133057, 28.748247623443604]
policies:[2, 1, 1]
qAverage:[85.86913299560547, 0.0]
ws:[2.3449134826660156, 2.268033027648926]
memory len:10000
memory used:2841.0
now epsilon is 0.2656924920489949, the reward is 245.25 with loss [32.82060790061951, 28.41772723197937] in episode 905
Report: 
rewardSum:245.25
loss:[32.82060790061951, 28.41772723197937]
policies:[1, 0, 5]
qAverage:[66.4044189453125, 0.0]
ws:[0.9285790920257568, 0.3428865075111389]
memory len:10000
memory used:2841.0
now epsilon is 0.2654268991750257, the reward is 247.25 with loss [22.836811065673828, 22.003806829452515] in episode 906
Report: 
rewardSum:247.25
loss:[22.836811065673828, 22.003806829452515]
policies:[3, 1, 0]
qAverage:[131.5642433166504, 0.0]
ws:[8.32795000076294, 5.0980366468429565]
memory len:10000
memory used:2841.0
now epsilon is 0.2648302855141629, the reward is 242.25 with loss [44.36780905723572, 52.49661350250244] in episode 907
Report: 
rewardSum:242.25
loss:[44.36780905723572, 52.49661350250244]
policies:[5, 0, 4]
qAverage:[132.18392028808594, 0.0]
ws:[13.132737827301025, 9.915433835983276]
memory len:10000
memory used:2842.0
now epsilon is 0.26436717995947, the reward is 244.25 with loss [41.46665668487549, 37.002984046936035] in episode 908
Report: 
rewardSum:244.25
loss:[41.46665668487549, 37.002984046936035]
policies:[2, 2, 3]
qAverage:[51.507181803385414, 39.033531188964844]
ws:[4.706450382868449, 4.594693094491959]
memory len:10000
memory used:2841.0
now epsilon is 0.2639708769511629, the reward is 245.25 with loss [30.66640043258667, 20.878307580947876] in episode 909
Report: 
rewardSum:245.25
loss:[30.66640043258667, 20.878307580947876]
policies:[1, 3, 2]
qAverage:[0.0, 71.03843434651692]
ws:[-0.010227044423421225, 0.3947685162226359]
memory len:10000
memory used:2842.0
now epsilon is 0.2634433969153929, the reward is 243.25 with loss [45.80080950260162, 54.48810005187988] in episode 910
Report: 
rewardSum:243.25
loss:[45.80080950260162, 54.48810005187988]
policies:[2, 3, 3]
qAverage:[57.86766560872396, 56.73692321777344]
ws:[8.513058642546335, 7.455178946256638]
memory len:10000
memory used:2842.0
now epsilon is 0.2631800522932872, the reward is 247.25 with loss [18.595080852508545, 15.455146789550781] in episode 911
Report: 
rewardSum:247.25
loss:[18.595080852508545, 15.455146789550781]
policies:[1, 3, 0]
qAverage:[49.3913688659668, 60.17164993286133]
ws:[11.235799193382263, 9.65350067615509]
memory len:10000
memory used:2842.0
now epsilon is 0.26285124167433666, the reward is 36.15999999999997 with loss [23.012644290924072, 23.680261611938477] in episode 912
Report: 
rewardSum:36.15999999999997
loss:[23.012644290924072, 23.680261611938477]
policies:[0, 3, 2]
qAverage:[0.0, 87.2838020324707]
ws:[4.266361139714718, 5.039296478033066]
memory len:10000
memory used:2842.0
now epsilon is 0.26258848898545084, the reward is 247.25 with loss [20.048312187194824, 21.90427303314209] in episode 913
Report: 
rewardSum:247.25
loss:[20.048312187194824, 21.90427303314209]
policies:[1, 2, 1]
qAverage:[50.098636627197266, 60.03851318359375]
ws:[11.334176540374756, 9.07657790184021]
memory len:10000
memory used:2843.0
############# STATE ###############
0-		10-		20*		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.26232599895073805, the reward is 247.25 with loss [20.0368914604187, 23.264346599578857] in episode 914
Report: 
rewardSum:247.25
loss:[20.0368914604187, 23.264346599578857]
policies:[3, 1, 0]
qAverage:[93.00716018676758, 28.421350479125977]
ws:[9.136745572090149, 6.834842085838318]
memory len:10000
memory used:2842.0
now epsilon is 0.2619982553648157, the reward is 246.25 with loss [31.821165084838867, 22.392073392868042] in episode 915
Report: 
rewardSum:246.25
loss:[31.821165084838867, 22.392073392868042]
policies:[2, 1, 2]
qAverage:[88.31661605834961, 26.68177032470703]
ws:[11.478423655033112, 9.395284652709961]
memory len:10000
memory used:2841.0
now epsilon is 0.2617363553424228, the reward is 247.25 with loss [21.473217248916626, 17.230131149291992] in episode 916
Report: 
rewardSum:247.25
loss:[21.473217248916626, 17.230131149291992]
policies:[2, 1, 1]
qAverage:[88.42078399658203, 25.794727325439453]
ws:[12.599053859710693, 9.7831289768219]
memory len:10000
memory used:2841.0
now epsilon is 0.2614747171218562, the reward is -3.0 with loss [19.120059967041016, 22.58158040046692] in episode 917
Report: 
rewardSum:-3.0
loss:[19.120059967041016, 22.58158040046692]
policies:[2, 0, 2]
qAverage:[98.37205505371094, 0.0]
ws:[6.603425741195679, 6.015981356302897]
memory len:10000
memory used:2841.0
now epsilon is 0.26121334044141215, the reward is 37.15999999999997 with loss [23.09667420387268, 14.75059723854065] in episode 918
Report: 
rewardSum:37.15999999999997
loss:[23.09667420387268, 14.75059723854065]
policies:[2, 0, 2]
qAverage:[105.67823282877605, 0.0]
ws:[4.169045766194661, 3.539569536844889]
memory len:10000
memory used:2841.0
now epsilon is 0.2609522250396486, the reward is 247.25 with loss [19.905412673950195, 14.168403506278992] in episode 919
Report: 
rewardSum:247.25
loss:[19.905412673950195, 14.168403506278992]
policies:[4, 0, 0]
qAverage:[133.88221740722656, 0.0]
ws:[11.18349027633667, 8.059237003326416]
memory len:10000
memory used:2841.0
now epsilon is 0.260626197812721, the reward is 246.25 with loss [27.184181213378906, 34.50613069534302] in episode 920
Report: 
rewardSum:246.25
loss:[27.184181213378906, 34.50613069534302]
policies:[4, 0, 1]
qAverage:[132.94894104003907, 0.0]
ws:[12.563058567047118, 8.854695892333984]
memory len:10000
memory used:2841.0
now epsilon is 0.2603656693334444, the reward is 247.25 with loss [16.00656008720398, 16.96992564201355] in episode 921
Report: 
rewardSum:247.25
loss:[16.00656008720398, 16.96992564201355]
policies:[2, 2, 0]
qAverage:[111.74101257324219, 0.0]
ws:[5.4975307782491045, 3.9498875935872397]
memory len:10000
memory used:2841.0
now epsilon is 0.2601054012849652, the reward is 247.25 with loss [17.72649335861206, 18.96684694290161] in episode 922
Report: 
rewardSum:247.25
loss:[17.72649335861206, 18.96684694290161]
policies:[2, 0, 2]
qAverage:[111.1866963704427, 0.0]
ws:[9.006213823954264, 7.231296857198079]
memory len:10000
memory used:2841.0
now epsilon is 0.2597154869505838, the reward is 35.15999999999997 with loss [20.708662509918213, 38.308887004852295] in episode 923
Report: 
rewardSum:35.15999999999997
loss:[20.708662509918213, 38.308887004852295]
policies:[4, 0, 2]
qAverage:[123.77096252441406, 0.0]
ws:[3.465793418884277, 2.5068968772888183]
memory len:10000
memory used:2841.0
now epsilon is 0.25932615712228113, the reward is 245.25 with loss [32.36284875869751, 42.49338960647583] in episode 924
Report: 
rewardSum:245.25
loss:[32.36284875869751, 42.49338960647583]
policies:[3, 1, 2]
qAverage:[132.1774787902832, 0.0]
ws:[14.033436059951782, 10.677126049995422]
memory len:10000
memory used:2841.0
now epsilon is 0.2590669281962609, the reward is 247.25 with loss [14.148636817932129, 28.911903381347656] in episode 925
Report: 
rewardSum:247.25
loss:[14.148636817932129, 28.911903381347656]
policies:[4, 0, 0]
qAverage:[139.65846557617186, 0.0]
ws:[11.688253676891327, 9.132316875457764]
memory len:10000
memory used:2841.0
now epsilon is 0.2586785705982685, the reward is 245.25 with loss [36.71717447042465, 37.27692890167236] in episode 926
Report: 
rewardSum:245.25
loss:[36.71717447042465, 37.27692890167236]
policies:[3, 2, 1]
qAverage:[94.12686538696289, 26.72852897644043]
ws:[14.180675461888313, 10.817709192633629]
memory len:10000
memory used:2842.0
now epsilon is 0.25841998901596785, the reward is 247.25 with loss [17.80962884426117, 19.285526752471924] in episode 927
Report: 
rewardSum:247.25
loss:[17.80962884426117, 19.285526752471924]
policies:[3, 0, 1]
qAverage:[72.79601287841797, 0.0]
ws:[0.490651935338974, 0.22233504056930542]
memory len:10000
memory used:2842.0
now epsilon is 0.2580326012204425, the reward is 245.25 with loss [45.33979892730713, 34.514888763427734] in episode 928
Report: 
rewardSum:245.25
loss:[45.33979892730713, 34.514888763427734]
policies:[4, 0, 2]
qAverage:[136.99403381347656, 0.0]
ws:[13.979049396514892, 9.192587232589721]
memory len:10000
memory used:2842.0
now epsilon is 0.2575169873493458, the reward is 243.25 with loss [40.20628798007965, 42.22945702075958] in episode 929
Report: 
rewardSum:243.25
loss:[40.20628798007965, 42.22945702075958]
policies:[5, 0, 3]
qAverage:[136.3095672607422, 0.0]
ws:[11.120627379417419, 7.360014235973358]
memory len:10000
memory used:2842.0
now epsilon is 0.257259566914773, the reward is 37.15999999999997 with loss [19.178883910179138, 23.426613092422485] in episode 930
Report: 
rewardSum:37.15999999999997
loss:[19.178883910179138, 23.426613092422485]
policies:[3, 0, 1]
qAverage:[117.40898132324219, 0.0]
ws:[4.0892097155253095, 2.7559319337209067]
memory len:10000
memory used:2842.0
now epsilon is 0.2569381532031671, the reward is 246.25 with loss [36.009531021118164, 22.354397773742676] in episode 931
Report: 
rewardSum:246.25
loss:[36.009531021118164, 22.354397773742676]
policies:[3, 1, 1]
qAverage:[94.02902221679688, 0.0]
ws:[4.641889572143555, 4.059950828552246]
memory len:10000
memory used:2842.0
now epsilon is 0.25668131138571376, the reward is 247.25 with loss [18.264418840408325, 29.185280799865723] in episode 932
Report: 
rewardSum:247.25
loss:[18.264418840408325, 29.185280799865723]
policies:[3, 0, 1]
qAverage:[122.05801010131836, 0.0]
ws:[6.046458899974823, 4.6829514503479]
memory len:10000
memory used:2842.0
now epsilon is 0.2564247263137782, the reward is 37.15999999999997 with loss [19.97810697555542, 22.782410144805908] in episode 933
Report: 
rewardSum:37.15999999999997
loss:[19.97810697555542, 22.782410144805908]
policies:[2, 1, 1]
qAverage:[102.56067403157552, 0.0]
ws:[4.540793975194295, 2.877282698949178]
memory len:10000
memory used:2842.0
now epsilon is 0.2561683977307113, the reward is 247.25 with loss [10.974927425384521, 14.417988777160645] in episode 934
Report: 
rewardSum:247.25
loss:[10.974927425384521, 14.417988777160645]
policies:[3, 0, 1]
qAverage:[142.78808212280273, 0.0]
ws:[13.02718698978424, 7.702641069889069]
memory len:10000
memory used:2843.0
now epsilon is 0.2559123253801202, the reward is 247.25 with loss [29.66099500656128, 26.83650040626526] in episode 935
Report: 
rewardSum:247.25
loss:[29.66099500656128, 26.83650040626526]
policies:[3, 0, 1]
qAverage:[69.69376373291016, 0.0]
ws:[0.1790051907300949, -0.5943812727928162]
memory len:10000
memory used:2843.0
############# STATE ###############
0-		10*		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.2556565090058686, the reward is 37.15999999999997 with loss [17.379142999649048, 15.921815514564514] in episode 936
Report: 
rewardSum:37.15999999999997
loss:[17.379142999649048, 15.921815514564514]
policies:[2, 1, 1]
qAverage:[118.99446105957031, 0.0]
ws:[4.5947585105896, 3.894203503926595]
memory len:10000
memory used:2842.0
now epsilon is 0.2552732638404594, the reward is 245.25 with loss [38.03825044631958, 34.57300019264221] in episode 937
Report: 
rewardSum:245.25
loss:[38.03825044631958, 34.57300019264221]
policies:[2, 2, 2]
qAverage:[0.0, 86.45523071289062]
ws:[3.7928972244262695, 4.157494068145752]
memory len:10000
memory used:2853.0
now epsilon is 0.2550180862881394, the reward is 247.25 with loss [27.889346599578857, 22.779252767562866] in episode 938
Report: 
rewardSum:247.25
loss:[27.889346599578857, 22.779252767562866]
policies:[1, 3, 0]
qAverage:[0.0, 80.24451955159505]
ws:[3.8669172128041587, 4.720395008722941]
memory len:10000
memory used:2853.0
now epsilon is 0.254763163817696, the reward is 247.25 with loss [8.65413522720337, 20.084054470062256] in episode 939
Report: 
rewardSum:247.25
loss:[8.65413522720337, 20.084054470062256]
policies:[1, 2, 1]
qAverage:[41.3515739440918, 60.4456787109375]
ws:[7.635077595710754, 8.053180754184723]
memory len:10000
memory used:2853.0
now epsilon is 0.254508496174143, the reward is 37.15999999999997 with loss [33.937132358551025, 20.880274772644043] in episode 940
Report: 
rewardSum:37.15999999999997
loss:[33.937132358551025, 20.880274772644043]
policies:[2, 1, 1]
qAverage:[0.0, 60.998680114746094]
ws:[1.6113810539245605, 1.9063624143600464]
memory len:10000
memory used:2853.0
now epsilon is 0.25412697195207806, the reward is 245.25 with loss [40.61721110343933, 29.671869039535522] in episode 941
Report: 
rewardSum:245.25
loss:[40.61721110343933, 29.671869039535522]
policies:[0, 4, 2]
qAverage:[0.0, 95.38998222351074]
ws:[8.701657891273499, 9.829286575317383]
memory len:10000
memory used:2853.0
now epsilon is 0.2538729402618586, the reward is 247.25 with loss [18.422987461090088, 24.23918080329895] in episode 942
Report: 
rewardSum:247.25
loss:[18.422987461090088, 24.23918080329895]
policies:[0, 4, 0]
qAverage:[0.0, 91.76086235046387]
ws:[5.395791709423065, 5.922857880592346]
memory len:10000
memory used:2853.0
now epsilon is 0.2534923687780269, the reward is 245.25 with loss [34.56096005439758, 35.92332363128662] in episode 943
Report: 
rewardSum:245.25
loss:[34.56096005439758, 35.92332363128662]
policies:[1, 4, 1]
qAverage:[50.14435958862305, 57.72364044189453]
ws:[10.992757230997086, 8.22206825017929]
memory len:10000
memory used:2853.0
now epsilon is 0.2532389714530449, the reward is 247.25 with loss [33.13634014129639, 13.219434022903442] in episode 944
Report: 
rewardSum:247.25
loss:[33.13634014129639, 13.219434022903442]
policies:[2, 2, 0]
qAverage:[41.38832092285156, 60.3897590637207]
ws:[6.622832108289003, 6.830591380596161]
memory len:10000
memory used:2853.0
now epsilon is 0.2529858274303798, the reward is 247.25 with loss [26.668357610702515, 25.202281951904297] in episode 945
Report: 
rewardSum:247.25
loss:[26.668357610702515, 25.202281951904297]
policies:[2, 0, 2]
qAverage:[120.871337890625, 0.0]
ws:[20.38163884480794, 15.06133778889974]
memory len:10000
memory used:2853.0
now epsilon is 0.2526065857844043, the reward is 245.25 with loss [34.05410051345825, 29.640960216522217] in episode 946
Report: 
rewardSum:245.25
loss:[34.05410051345825, 29.640960216522217]
policies:[3, 2, 1]
qAverage:[99.24306030273438, 21.219215393066406]
ws:[6.787448251247406, 6.151746225357056]
memory len:10000
memory used:2853.0
now epsilon is 0.25235407391030273, the reward is 247.25 with loss [19.53370237350464, 14.996885895729065] in episode 947
Report: 
rewardSum:247.25
loss:[19.53370237350464, 14.996885895729065]
policies:[3, 1, 0]
qAverage:[102.72874450683594, 21.58924560546875]
ws:[10.156597148068249, 6.955209827423095]
memory len:10000
memory used:2853.0
now epsilon is 0.2521018144533991, the reward is 247.25 with loss [15.975473701953888, 15.358206272125244] in episode 948
Report: 
rewardSum:247.25
loss:[15.975473701953888, 15.358206272125244]
policies:[3, 0, 1]
qAverage:[85.21952819824219, 0.0]
ws:[2.4879558086395264, 2.4128286838531494]
memory len:10000
memory used:2853.0
now epsilon is 0.25184980716137073, the reward is 247.25 with loss [20.50486969947815, 28.677939414978027] in episode 949
Report: 
rewardSum:247.25
loss:[20.50486969947815, 28.677939414978027]
policies:[3, 1, 0]
qAverage:[53.31505839029948, 36.012916564941406]
ws:[6.504014203945796, 6.183228082954884]
memory len:10000
memory used:2853.0
now epsilon is 0.25159805178214745, the reward is 247.25 with loss [18.748483896255493, 31.278037548065186] in episode 950
Report: 
rewardSum:247.25
loss:[18.748483896255493, 31.278037548065186]
policies:[3, 1, 0]
qAverage:[125.28511047363281, 0.0]
ws:[10.103626370429993, 5.165378749370575]
memory len:10000
memory used:2853.0
now epsilon is 0.2512208904990382, the reward is 35.15999999999997 with loss [42.51178789138794, 51.02052688598633] in episode 951
Report: 
rewardSum:35.15999999999997
loss:[42.51178789138794, 51.02052688598633]
policies:[4, 0, 2]
qAverage:[103.72959899902344, 0.0]
ws:[4.7560848792394, 3.258484125137329]
memory len:10000
memory used:2853.0
now epsilon is 0.25096976380067276, the reward is 247.25 with loss [25.163949489593506, 20.95957899093628] in episode 952
Report: 
rewardSum:247.25
loss:[25.163949489593506, 20.95957899093628]
policies:[2, 0, 2]
qAverage:[113.71129862467448, 0.0]
ws:[8.89016580581665, 7.389412562052409]
memory len:10000
memory used:2853.0
now epsilon is 0.2504682632506282, the reward is 243.25 with loss [48.593230962753296, 31.147894978523254] in episode 953
Report: 
rewardSum:243.25
loss:[48.593230962753296, 31.147894978523254]
policies:[4, 0, 4]
qAverage:[131.18546447753906, 0.0]
ws:[11.893751955032348, 7.253421330451966]
memory len:10000
memory used:2853.0
now epsilon is 0.25009279559149244, the reward is 245.25 with loss [26.326112985610962, 31.5705726146698] in episode 954
Report: 
rewardSum:245.25
loss:[26.326112985610962, 31.5705726146698]
policies:[5, 0, 1]
qAverage:[134.38080139160155, 0.0]
ws:[6.847880601882935, 2.777536952495575]
memory len:10000
memory used:2853.0
now epsilon is 0.2498427965650695, the reward is 247.25 with loss [30.38823652267456, 23.446028232574463] in episode 955
Report: 
rewardSum:247.25
loss:[30.38823652267456, 23.446028232574463]
policies:[2, 1, 1]
qAverage:[89.68887710571289, 26.910964965820312]
ws:[7.141587324440479, 2.954774461686611]
memory len:10000
memory used:2853.0
now epsilon is 0.24959304744393895, the reward is 247.25 with loss [15.700482487678528, 19.99997901916504] in episode 956
Report: 
rewardSum:247.25
loss:[15.700482487678528, 19.99997901916504]
policies:[3, 1, 0]
qAverage:[86.06758880615234, 27.162145614624023]
ws:[6.4493304044008255, 2.0310793668031693]
memory len:10000
memory used:2853.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22*		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.24934354797828925, the reward is 247.25 with loss [18.251144409179688, 24.624074697494507] in episode 957
Report: 
rewardSum:247.25
loss:[18.251144409179688, 24.624074697494507]
policies:[3, 1, 0]
qAverage:[78.0623550415039, 26.158666610717773]
ws:[1.6717352867126465, 1.392756711691618]
memory len:10000
memory used:2853.0
now epsilon is 0.24909429791855853, the reward is 37.15999999999997 with loss [19.69268798828125, 24.806018352508545] in episode 958
Report: 
rewardSum:37.15999999999997
loss:[19.69268798828125, 24.806018352508545]
policies:[2, 1, 1]
qAverage:[79.48069381713867, 26.04775619506836]
ws:[5.254365496337414, 4.387426406145096]
memory len:10000
memory used:2853.0
now epsilon is 0.2488452970154343, the reward is 247.25 with loss [21.592758893966675, 24.895031452178955] in episode 959
Report: 
rewardSum:247.25
loss:[21.592758893966675, 24.895031452178955]
policies:[2, 1, 1]
qAverage:[97.32094319661458, 0.0]
ws:[1.9931531051794689, 1.321726421515147]
memory len:10000
memory used:2853.0
now epsilon is 0.2485965450198534, the reward is 247.25 with loss [20.176459312438965, 17.959413766860962] in episode 960
Report: 
rewardSum:247.25
loss:[20.176459312438965, 17.959413766860962]
policies:[2, 1, 1]
qAverage:[77.96142959594727, 26.428861618041992]
ws:[4.550900638103485, 4.184544563293457]
memory len:10000
memory used:2853.0
now epsilon is 0.24828595467258088, the reward is 246.25 with loss [32.46657466888428, 22.665339946746826] in episode 961
Report: 
rewardSum:246.25
loss:[32.46657466888428, 22.665339946746826]
policies:[3, 1, 1]
qAverage:[118.52457046508789, 0.0]
ws:[5.387148551642895, 2.0343570560216904]
memory len:10000
memory used:2853.0
now epsilon is 0.24803776180962442, the reward is 247.25 with loss [32.36428356170654, 23.79862880706787] in episode 962
Report: 
rewardSum:247.25
loss:[32.36428356170654, 23.79862880706787]
policies:[1, 3, 0]
qAverage:[34.150994873046876, 66.46984558105468]
ws:[3.949955177307129, 1.744766640663147]
memory len:10000
memory used:2853.0
now epsilon is 0.24778981704647413, the reward is 247.25 with loss [20.990537643432617, 22.606028079986572] in episode 963
Report: 
rewardSum:247.25
loss:[20.990537643432617, 22.606028079986572]
policies:[1, 3, 0]
qAverage:[33.28318481445312, 65.25870971679687]
ws:[5.767910766601562, 3.8665736258029937]
memory len:10000
memory used:2853.0
now epsilon is 0.24723284716010577, the reward is 242.25 with loss [45.557799100875854, 57.65839242935181] in episode 964
Report: 
rewardSum:242.25
loss:[45.557799100875854, 57.65839242935181]
policies:[4, 1, 4]
qAverage:[54.3229014078776, 37.53063456217448]
ws:[9.29377293586731, 4.8651862144470215]
memory len:10000
memory used:2853.0
now epsilon is 0.24673881390700697, the reward is 243.25 with loss [43.59024453163147, 51.62098789215088] in episode 965
Report: 
rewardSum:243.25
loss:[43.59024453163147, 51.62098789215088]
policies:[2, 2, 4]
qAverage:[76.67299270629883, 29.262210845947266]
ws:[6.519359856843948, 3.1653649657964706]
memory len:10000
memory used:2853.0
now epsilon is 0.246492167604735, the reward is 247.25 with loss [42.59345245361328, 21.051625967025757] in episode 966
Report: 
rewardSum:247.25
loss:[42.59345245361328, 21.051625967025757]
policies:[2, 2, 0]
qAverage:[39.97907257080078, 55.44814109802246]
ws:[7.513679310679436, 4.886905252933502]
memory len:10000
memory used:2853.0
now epsilon is 0.24612266036272068, the reward is 245.25 with loss [39.740806579589844, 35.763473987579346] in episode 967
Report: 
rewardSum:245.25
loss:[39.740806579589844, 35.763473987579346]
policies:[3, 2, 1]
qAverage:[86.55996704101562, 20.030491638183594]
ws:[11.47567982673645, 8.393447303771973]
memory len:10000
memory used:2853.0
now epsilon is 0.2458766299829739, the reward is 247.25 with loss [25.413536071777344, 11.956383109092712] in episode 968
Report: 
rewardSum:247.25
loss:[25.413536071777344, 11.956383109092712]
policies:[3, 1, 0]
qAverage:[71.88084030151367, 24.90209197998047]
ws:[8.071546494960785, 4.701022684574127]
memory len:10000
memory used:2853.0
now epsilon is 0.24495620443920324, the reward is 236.25 with loss [87.14758324623108, 75.50030493736267] in episode 969
Report: 
rewardSum:236.25
loss:[87.14758324623108, 75.50030493736267]
policies:[3, 5, 7]
qAverage:[24.54334767659505, 67.28384653727214]
ws:[3.868148572742939, 2.3297059386968613]
memory len:10000
memory used:2853.0
now epsilon is 0.2447113400780319, the reward is 247.25 with loss [12.308178067207336, 23.28799796104431] in episode 970
Report: 
rewardSum:247.25
loss:[12.308178067207336, 23.28799796104431]
policies:[1, 2, 1]
qAverage:[35.120914459228516, 49.769548416137695]
ws:[4.116777658462524, 4.304117679595947]
memory len:10000
memory used:2852.0
now epsilon is 0.24434450240833827, the reward is 245.25 with loss [20.170061349868774, 35.76494264602661] in episode 971
Report: 
rewardSum:245.25
loss:[20.170061349868774, 35.76494264602661]
policies:[2, 3, 1]
qAverage:[36.04964828491211, 50.148149490356445]
ws:[4.716523103415966, 2.202023595571518]
memory len:10000
memory used:2852.0
now epsilon is 0.2441002495198478, the reward is 247.25 with loss [19.781091451644897, 17.871116638183594] in episode 972
Report: 
rewardSum:247.25
loss:[19.781091451644897, 17.871116638183594]
policies:[1, 2, 1]
qAverage:[36.660430908203125, 50.835527420043945]
ws:[5.357578240334988, 2.8006030917167664]
memory len:10000
memory used:2852.0
now epsilon is 0.24385624079266624, the reward is 247.25 with loss [23.058651328086853, 19.164611339569092] in episode 973
Report: 
rewardSum:247.25
loss:[23.058651328086853, 19.164611339569092]
policies:[1, 2, 1]
qAverage:[0.0, 67.60562133789062]
ws:[2.1690614223480225, 2.467949072519938]
memory len:10000
memory used:2853.0
now epsilon is 0.24349068497051224, the reward is 35.15999999999997 with loss [32.00306701660156, 28.457794189453125] in episode 974
Report: 
rewardSum:35.15999999999997
loss:[32.00306701660156, 28.457794189453125]
policies:[0, 2, 4]
qAverage:[0.0, 66.79395039876302]
ws:[0.45704331000645954, 1.4804753462473552]
memory len:10000
memory used:2853.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26*		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.24324728557933142, the reward is 247.25 with loss [22.972854137420654, 28.987549304962158] in episode 975
Report: 
rewardSum:247.25
loss:[22.972854137420654, 28.987549304962158]
policies:[1, 2, 1]
qAverage:[70.92133331298828, 0.0]
ws:[7.073233604431152, 5.890480041503906]
memory len:10000
memory used:2853.0
############# STATE ###############
0*		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.24288264261929216, the reward is 245.25 with loss [47.81251764297485, 29.304110050201416] in episode 976
Report: 
rewardSum:245.25
loss:[47.81251764297485, 29.304110050201416]
policies:[0, 5, 1]
qAverage:[0.0, 67.48280334472656]
ws:[1.6819381713867188, 2.169818639755249]
memory len:10000
memory used:2853.0
now epsilon is 0.24263985104248464, the reward is 247.25 with loss [27.33469581604004, 27.056745052337646] in episode 977
Report: 
rewardSum:247.25
loss:[27.33469581604004, 27.056745052337646]
policies:[2, 1, 1]
qAverage:[90.53943379720052, 0.0]
ws:[4.211320559183757, 3.294584274291992]
memory len:10000
memory used:2853.0
now epsilon is 0.2423973021662223, the reward is 247.25 with loss [24.238261699676514, 20.978143572807312] in episode 978
Report: 
rewardSum:247.25
loss:[24.238261699676514, 20.978143572807312]
policies:[2, 0, 2]
qAverage:[63.82678985595703, 0.0]
ws:[0.9511300325393677, 0.5782827138900757]
memory len:10000
memory used:2853.0
now epsilon is 0.24197342490136262, the reward is 34.15999999999997 with loss [36.40053582191467, 27.25545907020569] in episode 979
Report: 
rewardSum:34.15999999999997
loss:[36.40053582191467, 27.25545907020569]
policies:[3, 2, 2]
qAverage:[53.10592956542969, 38.90296936035156]
ws:[1.6914701521396638, 1.6003817737102508]
memory len:10000
memory used:2853.0
now epsilon is 0.24167110931582292, the reward is 246.25 with loss [31.28056526184082, 24.46229863166809] in episode 980
Report: 
rewardSum:246.25
loss:[31.28056526184082, 24.46229863166809]
policies:[3, 1, 1]
qAverage:[81.48641967773438, 18.39099884033203]
ws:[4.581166195869446, 2.2196877241134643]
memory len:10000
memory used:2853.0
now epsilon is 0.2409470920509241, the reward is 239.25 with loss [59.05406630039215, 49.20171785354614] in episode 981
Report: 
rewardSum:239.25
loss:[59.05406630039215, 49.20171785354614]
policies:[6, 3, 3]
qAverage:[86.46726608276367, 15.351678212483725]
ws:[3.432006925344467, 3.0788092017173767]
memory len:10000
memory used:2853.0
now epsilon is 0.24058589722546472, the reward is 245.25 with loss [36.2802517414093, 35.94904589653015] in episode 982
Report: 
rewardSum:245.25
loss:[36.2802517414093, 35.94904589653015]
policies:[3, 1, 2]
qAverage:[80.16825408935547, 19.028375244140626]
ws:[6.056959867477417, 3.2305212020874023]
memory len:10000
memory used:2853.0
now epsilon is 0.24034540153291506, the reward is 247.25 with loss [23.01522970199585, 13.935143947601318] in episode 983
Report: 
rewardSum:247.25
loss:[23.01522970199585, 13.935143947601318]
policies:[3, 1, 0]
qAverage:[80.43157348632812, 19.110508728027344]
ws:[2.228937494754791, -0.009013009071350098]
memory len:10000
memory used:2853.0
now epsilon is 0.23938582041598042, the reward is -15.0 with loss [78.64849519729614, 86.48028087615967] in episode 984
Report: 
rewardSum:-15.0
loss:[78.64849519729614, 86.48028087615967]
policies:[8, 3, 5]
qAverage:[94.96670761108399, 8.481208801269531]
ws:[8.763202679157256, 7.743453431129455]
memory len:10000
memory used:2853.0
now epsilon is 0.23890746749093705, the reward is 243.25 with loss [44.16148519515991, 52.52531957626343] in episode 985
Report: 
rewardSum:243.25
loss:[44.16148519515991, 52.52531957626343]
policies:[4, 0, 4]
qAverage:[100.33184814453125, 0.0]
ws:[4.5192968368530275, 1.7814359664916992]
memory len:10000
memory used:2853.0
now epsilon is 0.23860898243641598, the reward is 246.25 with loss [22.26588487625122, 14.895653009414673] in episode 986
Report: 
rewardSum:246.25
loss:[22.26588487625122, 14.895653009414673]
policies:[4, 1, 0]
qAverage:[96.11673164367676, 0.0]
ws:[7.875064134597778, 4.034241825342178]
memory len:10000
memory used:2853.0
now epsilon is 0.23837046291743588, the reward is 247.25 with loss [19.78339195251465, 19.01487159729004] in episode 987
Report: 
rewardSum:247.25
loss:[19.78339195251465, 19.01487159729004]
policies:[3, 0, 1]
qAverage:[97.80415344238281, 0.0]
ws:[5.038238167762756, 1.0186037421226501]
memory len:10000
memory used:2853.0
now epsilon is 0.2378346653966693, the reward is 242.25 with loss [32.63410258293152, 51.02477550506592] in episode 988
Report: 
rewardSum:242.25
loss:[32.63410258293152, 51.02477550506592]
policies:[5, 2, 2]
qAverage:[103.71967951456706, 0.0]
ws:[6.338847855726878, 3.2054107089837394]
memory len:10000
memory used:2853.0
now epsilon is 0.23759691990440845, the reward is 247.25 with loss [19.674225568771362, 28.897116661071777] in episode 989
Report: 
rewardSum:247.25
loss:[19.674225568771362, 28.897116661071777]
policies:[3, 1, 0]
qAverage:[62.81721496582031, 20.690040588378906]
ws:[1.203509896993637, 0.689302509650588]
memory len:10000
memory used:2853.0
now epsilon is 0.23724074719742913, the reward is 245.25 with loss [41.37438440322876, 29.253568649291992] in episode 990
Report: 
rewardSum:245.25
loss:[41.37438440322876, 29.253568649291992]
policies:[4, 0, 2]
qAverage:[87.32747459411621, 0.0]
ws:[6.590516090393066, 5.230035334825516]
memory len:10000
memory used:2854.0
############# STATE ###############
0-		10-		20*		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.2367666806668211, the reward is 33.15999999999997 with loss [41.10157036781311, 41.63973593711853] in episode 991
Report: 
rewardSum:33.15999999999997
loss:[41.10157036781311, 41.63973593711853]
policies:[2, 3, 3]
qAverage:[43.64683532714844, 31.750628662109374]
ws:[1.2523678064346313, 1.0317854642868043]
memory len:10000
memory used:2853.0
now epsilon is 0.2364708702581729, the reward is 36.15999999999997 with loss [22.563838720321655, 19.819214820861816] in episode 992
Report: 
rewardSum:36.15999999999997
loss:[22.563838720321655, 19.819214820861816]
policies:[2, 1, 2]
qAverage:[37.43809254964193, 25.456919352213543]
ws:[4.184065371751785, 4.262263794740041]
memory len:10000
memory used:2853.0
now epsilon is 0.2362344880497126, the reward is 247.25 with loss [21.192543506622314, 18.663716077804565] in episode 993
Report: 
rewardSum:247.25
loss:[21.192543506622314, 18.663716077804565]
policies:[2, 2, 0]
qAverage:[61.446367263793945, 18.76277732849121]
ws:[7.513927334919572, 4.593255922198296]
memory len:10000
memory used:2853.0
now epsilon is 0.2359983421348322, the reward is 247.25 with loss [14.456307470798492, 18.94348430633545] in episode 994
Report: 
rewardSum:247.25
loss:[14.456307470798492, 18.94348430633545]
policies:[0, 2, 2]
qAverage:[0.0, 51.21367390950521]
ws:[1.3979387084643047, 1.7420405546824138]
memory len:10000
memory used:2853.0
now epsilon is 0.2357624322773267, the reward is 247.25 with loss [26.06102418899536, 20.749163389205933] in episode 995
Report: 
rewardSum:247.25
loss:[26.06102418899536, 20.749163389205933]
policies:[3, 1, 0]
qAverage:[82.34962209065755, 0.0]
ws:[4.5748560428619385, 0.43970124920209247]
memory len:10000
memory used:2853.0
now epsilon is 0.2355267582412273, the reward is 247.25 with loss [20.53097677230835, 19.55716633796692] in episode 996
Report: 
rewardSum:247.25
loss:[20.53097677230835, 19.55716633796692]
policies:[2, 1, 1]
qAverage:[60.47917556762695, 0.0]
ws:[0.19381970167160034, 0.09863484650850296]
memory len:10000
memory used:2853.0
now epsilon is 0.23540900958252908, the reward is -1.0 with loss [8.574803352355957, 11.2539644241333] in episode 997
Report: 
rewardSum:-1.0
loss:[8.574803352355957, 11.2539644241333]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:2853.0
now epsilon is 0.23517368883661305, the reward is 247.25 with loss [17.380113124847412, 23.906468868255615] in episode 998
Report: 
rewardSum:247.25
loss:[17.380113124847412, 23.906468868255615]
policies:[1, 2, 1]
qAverage:[0.0, 37.84467697143555]
ws:[-0.45283424854278564, 0.07413644343614578]
memory len:10000
memory used:2853.0
now epsilon is 0.23493860332321234, the reward is 247.25 with loss [19.13961935043335, 13.234994411468506] in episode 999
Report: 
rewardSum:247.25
loss:[19.13961935043335, 13.234994411468506]
policies:[1, 2, 1]
qAverage:[0.0, 43.17839431762695]
ws:[1.3049899339675903, 1.8586269617080688]
memory len:10000
memory used:2853.0
now epsilon is 0.23470375280718267, the reward is 247.25 with loss [26.757338523864746, 20.538403749465942] in episode 1000
Report: 
rewardSum:247.25
loss:[26.757338523864746, 20.538403749465942]
policies:[2, 2, 0]
qAverage:[33.825565338134766, 40.46237373352051]
ws:[4.9539942145347595, 2.2732847332954407]
memory len:10000
memory used:2853.0
now epsilon is 0.23441051976935134, the reward is 36.15999999999997 with loss [30.913437366485596, 30.41191005706787] in episode 1001
Report: 
rewardSum:36.15999999999997
loss:[30.913437366485596, 30.41191005706787]
policies:[0, 3, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:2853.0
now epsilon is 0.23417619713887716, the reward is 247.25 with loss [24.36185073852539, 25.25686550140381] in episode 1002
Report: 
rewardSum:247.25
loss:[24.36185073852539, 25.25686550140381]
policies:[3, 1, 0]
qAverage:[90.38229942321777, 0.0]
ws:[8.696107149124146, 4.796393990516663]
memory len:10000
memory used:2853.0
now epsilon is 0.23394210874317717, the reward is 37.15999999999997 with loss [24.074323892593384, 19.133576154708862] in episode 1003
Report: 
rewardSum:37.15999999999997
loss:[24.074323892593384, 19.133576154708862]
policies:[2, 1, 1]
qAverage:[60.14572715759277, 18.510799407958984]
ws:[2.967205822467804, 2.4059427194297314]
memory len:10000
memory used:2853.0
now epsilon is 0.23370825434810433, the reward is 247.25 with loss [19.001886129379272, 20.732988595962524] in episode 1004
Report: 
rewardSum:247.25
loss:[19.001886129379272, 20.732988595962524]
policies:[2, 1, 1]
qAverage:[60.39703369140625, 17.560585021972656]
ws:[7.527434825897217, 3.68399715423584]
memory len:10000
memory used:2853.0
now epsilon is 0.23347463371974578, the reward is 247.25 with loss [19.195070266723633, 27.296427726745605] in episode 1005
Report: 
rewardSum:247.25
loss:[19.195070266723633, 27.296427726745605]
policies:[3, 1, 0]
qAverage:[75.39297637939453, 14.60086669921875]
ws:[7.603812742233276, 3.659908318519592]
memory len:10000
memory used:2853.0
now epsilon is 0.2331246405786882, the reward is 245.25 with loss [38.05746269226074, 23.820849418640137] in episode 1006
Report: 
rewardSum:245.25
loss:[38.05746269226074, 23.820849418640137]
policies:[5, 0, 1]
qAverage:[91.95861625671387, 0.0]
ws:[7.176251292228699, 3.1434046924114227]
memory len:10000
memory used:2852.0
now epsilon is 0.23277517209933293, the reward is 245.25 with loss [31.431063175201416, 35.72930097579956] in episode 1007
Report: 
rewardSum:245.25
loss:[31.431063175201416, 35.72930097579956]
policies:[0, 3, 3]
qAverage:[0.0, 51.061920166015625]
ws:[0.6694662868976593, 1.3162210782368977]
memory len:10000
memory used:2852.0
now epsilon is 0.23254248420337562, the reward is 247.25 with loss [27.52006435394287, 17.49416184425354] in episode 1008
Report: 
rewardSum:247.25
loss:[27.52006435394287, 17.49416184425354]
policies:[2, 2, 0]
qAverage:[45.52131144205729, 24.33582051595052]
ws:[4.964492956797282, 2.195108731587728]
memory len:10000
memory used:2852.0
now epsilon is 0.23231002890807084, the reward is 247.25 with loss [34.61138582229614, 15.753969192504883] in episode 1009
Report: 
rewardSum:247.25
loss:[34.61138582229614, 15.753969192504883]
policies:[1, 3, 0]
qAverage:[26.996270751953126, 45.988153076171876]
ws:[5.557620334625244, 4.091336441040039]
memory len:10000
memory used:2852.0
now epsilon is 0.23201978652940994, the reward is 246.25 with loss [24.698678255081177, 26.302654027938843] in episode 1010
Report: 
rewardSum:246.25
loss:[24.698678255081177, 26.302654027938843]
policies:[1, 2, 2]
qAverage:[44.69549560546875, 24.437169392903645]
ws:[4.975272178649902, 1.7022059361139934]
memory len:10000
memory used:2852.0
now epsilon is 0.23178785373580016, the reward is 247.25 with loss [16.590983390808105, 19.345722675323486] in episode 1011
Report: 
rewardSum:247.25
loss:[16.590983390808105, 19.345722675323486]
policies:[4, 0, 0]
qAverage:[93.39129066467285, 0.0]
ws:[5.81852263212204, 2.916469968855381]
memory len:10000
memory used:2853.0
now epsilon is 0.2315561527880237, the reward is 247.25 with loss [16.74504590034485, 27.26311707496643] in episode 1012
Report: 
rewardSum:247.25
loss:[16.74504590034485, 27.26311707496643]
policies:[3, 1, 0]
qAverage:[72.97008514404297, 14.355377197265625]
ws:[4.6732937574386595, 2.4027487695217133]
memory len:10000
memory used:2852.0
now epsilon is 0.2313246834543216, the reward is 247.25 with loss [25.176469326019287, 19.79850149154663] in episode 1013
Report: 
rewardSum:247.25
loss:[25.176469326019287, 19.79850149154663]
policies:[2, 2, 0]
qAverage:[64.48059844970703, 0.0]
ws:[6.196433067321777, 0.5556640625]
memory len:10000
memory used:2852.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12*		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.23097791322375547, the reward is 245.25 with loss [36.340139389038086, 28.73082673549652] in episode 1014
Report: 
rewardSum:245.25
loss:[36.340139389038086, 28.73082673549652]
policies:[3, 2, 1]
qAverage:[42.7786148071289, 30.61851348876953]
ws:[6.01188371181488, 5.929138803482056]
memory len:10000
memory used:2852.0
now epsilon is 0.23074702191281402, the reward is 247.25 with loss [18.052448272705078, 19.726081132888794] in episode 1015
Report: 
rewardSum:247.25
loss:[18.052448272705078, 19.726081132888794]
policies:[3, 1, 0]
qAverage:[72.30423736572266, 13.507933044433594]
ws:[6.770756936073303, 4.0141974806785585]
memory len:10000
memory used:2852.0
now epsilon is 0.23051636140661363, the reward is 37.15999999999997 with loss [26.993345260620117, 15.910369634628296] in episode 1016
Report: 
rewardSum:37.15999999999997
loss:[26.993345260620117, 15.910369634628296]
policies:[2, 1, 1]
qAverage:[59.49353790283203, 0.0]
ws:[1.3812470436096191, 1.205572485923767]
memory len:10000
memory used:2852.0
now epsilon is 0.2302859314744362, the reward is 247.25 with loss [19.484606072306633, 26.91844415664673] in episode 1017
Report: 
rewardSum:247.25
loss:[19.484606072306633, 26.91844415664673]
policies:[3, 1, 0]
qAverage:[85.0093485514323, 0.0]
ws:[4.388010581334432, 0.7265090445677439]
memory len:10000
memory used:2852.0
now epsilon is 0.23005573188579412, the reward is 247.25 with loss [16.35170865058899, 20.74534034729004] in episode 1018
Report: 
rewardSum:247.25
loss:[16.35170865058899, 20.74534034729004]
policies:[2, 2, 0]
qAverage:[26.995086669921875, 37.622880935668945]
ws:[3.295638132840395, 3.639527305960655]
memory len:10000
memory used:2853.0
now epsilon is 0.22982576241043023, the reward is 247.25 with loss [23.431358814239502, 19.97486424446106] in episode 1019
Report: 
rewardSum:247.25
loss:[23.431358814239502, 19.97486424446106]
policies:[0, 4, 0]
qAverage:[0.0, 56.72839546203613]
ws:[3.8137239329516888, 4.273802220821381]
memory len:10000
memory used:2853.0
now epsilon is 0.22959602281831754, the reward is 247.25 with loss [20.21016025543213, 20.967644929885864] in episode 1020
Report: 
rewardSum:247.25
loss:[20.21016025543213, 20.967644929885864]
policies:[1, 2, 1]
qAverage:[54.41465377807617, 0.0]
ws:[6.337324619293213, 6.312281131744385]
memory len:10000
memory used:2853.0
now epsilon is 0.22936651287965895, the reward is 247.25 with loss [24.51778221130371, 17.500457763671875] in episode 1021
Report: 
rewardSum:247.25
loss:[24.51778221130371, 17.500457763671875]
policies:[3, 1, 0]
qAverage:[71.5656723022461, 13.673561096191406]
ws:[7.025663280487061, 4.324253416061401]
memory len:10000
memory used:2853.0
now epsilon is 0.22913723236488714, the reward is 247.25 with loss [23.377991676330566, 25.689942836761475] in episode 1022
Report: 
rewardSum:247.25
loss:[23.377991676330566, 25.689942836761475]
policies:[3, 1, 0]
qAverage:[72.7127899169922, 13.814210510253906]
ws:[6.244724225997925, 3.150491988658905]
memory len:10000
memory used:2854.0
now epsilon is 0.22890818104466423, the reward is 37.15999999999997 with loss [14.616162896156311, 19.597975969314575] in episode 1023
Report: 
rewardSum:37.15999999999997
loss:[14.616162896156311, 19.597975969314575]
policies:[2, 1, 1]
qAverage:[40.39551289876302, 22.353505452473957]
ws:[1.091980258623759, 0.7127604981263479]
memory len:10000
memory used:2854.0
now epsilon is 0.22867935868988165, the reward is 37.15999999999997 with loss [24.91848635673523, 24.598648071289062] in episode 1024
Report: 
rewardSum:37.15999999999997
loss:[24.91848635673523, 24.598648071289062]
policies:[3, 0, 1]
qAverage:[50.1335334777832, 0.0]
ws:[-0.021319905295968056, -0.4386448562145233]
memory len:10000
memory used:2854.0
now epsilon is 0.22833655396729674, the reward is 245.25 with loss [25.317703247070312, 44.77046775817871] in episode 1025
Report: 
rewardSum:245.25
loss:[25.317703247070312, 44.77046775817871]
policies:[2, 1, 3]
qAverage:[52.77812194824219, 0.0]
ws:[5.529516220092773, 5.1298828125]
memory len:10000
memory used:2853.0
now epsilon is 0.22810830302526708, the reward is 247.25 with loss [11.281553626060486, 23.325005531311035] in episode 1026
Report: 
rewardSum:247.25
loss:[11.281553626060486, 23.325005531311035]
policies:[3, 1, 0]
qAverage:[69.86580352783203, 13.482411193847657]
ws:[7.101637899875641, 4.415027618408203]
memory len:10000
memory used:2853.0
now epsilon is 0.22782331017853746, the reward is 246.25 with loss [28.194565773010254, 31.316291093826294] in episode 1027
Report: 
rewardSum:246.25
loss:[28.194565773010254, 31.316291093826294]
policies:[2, 0, 3]
qAverage:[76.41012064615886, 0.0]
ws:[2.6159133911132812, 2.146188815434774]
memory len:10000
memory used:2853.0
now epsilon is 0.22759557228786217, the reward is 247.25 with loss [19.590223789215088, 26.87158203125] in episode 1028
Report: 
rewardSum:247.25
loss:[19.590223789215088, 26.87158203125]
policies:[4, 0, 0]
qAverage:[89.48371887207031, 0.0]
ws:[7.540737724304199, 3.9500022411346434]
memory len:10000
memory used:2853.0
now epsilon is 0.2271407792364541, the reward is 243.25 with loss [41.11904048919678, 36.89208686351776] in episode 1029
Report: 
rewardSum:243.25
loss:[41.11904048919678, 36.89208686351776]
policies:[4, 1, 3]
qAverage:[80.4702262878418, 0.0]
ws:[3.774030476808548, 2.9969986379146576]
memory len:10000
memory used:2853.0
now epsilon is 0.22702722304313458, the reward is -1.0 with loss [12.333967208862305, 16.573311805725098] in episode 1030
Report: 
rewardSum:-1.0
loss:[12.333967208862305, 16.573311805725098]
policies:[0, 1, 1]
qAverage:[0.0, 33.06978225708008]
ws:[-0.495099812746048, -0.11712833493947983]
memory len:10000
memory used:2853.0
now epsilon is 0.2268002809411118, the reward is 247.25 with loss [20.514703273773193, 17.75490951538086] in episode 1031
Report: 
rewardSum:247.25
loss:[20.514703273773193, 17.75490951538086]
policies:[2, 1, 1]
qAverage:[74.63434346516927, 0.0]
ws:[4.423176447550456, 4.031020998954773]
memory len:10000
memory used:2853.0
now epsilon is 0.22657356569610193, the reward is 247.25 with loss [16.38735556602478, 20.392574310302734] in episode 1032
Report: 
rewardSum:247.25
loss:[16.38735556602478, 20.392574310302734]
policies:[3, 1, 0]
qAverage:[69.51396789550782, 13.35875244140625]
ws:[4.663911366462708, 2.312175524234772]
memory len:10000
memory used:2853.0
now epsilon is 0.22634707708133303, the reward is 247.25 with loss [21.301491737365723, 31.631237983703613] in episode 1033
Report: 
rewardSum:247.25
loss:[21.301491737365723, 31.631237983703613]
policies:[2, 2, 0]
qAverage:[46.37231292724609, 29.51017608642578]
ws:[5.642384022474289, 3.3106745570898055]
memory len:10000
memory used:2853.0
now epsilon is 0.2258383051418543, the reward is 242.25 with loss [46.10976696014404, 41.63275766372681] in episode 1034
Report: 
rewardSum:242.25
loss:[46.10976696014404, 41.63275766372681]
policies:[3, 3, 3]
qAverage:[57.945359547932945, 24.17945607503255]
ws:[2.9337097046275935, 0.9816875557104746]
memory len:10000
memory used:2853.0
now epsilon is 0.2256125515119629, the reward is 247.25 with loss [27.70516300201416, 20.579244375228882] in episode 1035
Report: 
rewardSum:247.25
loss:[27.70516300201416, 20.579244375228882]
policies:[1, 0, 3]
qAverage:[59.7905158996582, 0.0]
ws:[5.779541969299316, 0.0802045688033104]
memory len:10000
memory used:2853.0
now epsilon is 0.22538702355105789, the reward is 247.25 with loss [15.84605023264885, 22.35232925415039] in episode 1036
Report: 
rewardSum:247.25
loss:[15.84605023264885, 22.35232925415039]
policies:[2, 2, 0]
qAverage:[0.0, 50.370190938313804]
ws:[1.9140676657358806, 2.5959672927856445]
memory len:10000
memory used:2854.0
############# STATE ###############
0-		10-		20-		30-		40-		
1*		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.2251617210335549, the reward is 247.25 with loss [20.667904376983643, 21.333087921142578] in episode 1037
Report: 
rewardSum:247.25
loss:[20.667904376983643, 21.333087921142578]
policies:[2, 2, 0]
qAverage:[57.45021057128906, 16.054399490356445]
ws:[3.191161770373583, -0.03598327934741974]
memory len:10000
memory used:2854.0
now epsilon is 0.22482418947076824, the reward is 245.25 with loss [39.488011837005615, 40.07745599746704] in episode 1038
Report: 
rewardSum:245.25
loss:[39.488011837005615, 40.07745599746704]
policies:[5, 0, 1]
qAverage:[88.81711959838867, 0.0]
ws:[3.538764258225759, 0.33308074871699017]
memory len:10000
memory used:2853.0
now epsilon is 0.22448716388899537, the reward is 245.25 with loss [29.90902090072632, 25.164222240447998] in episode 1039
Report: 
rewardSum:245.25
loss:[29.90902090072632, 25.164222240447998]
policies:[3, 0, 3]
qAverage:[75.01568349202473, 0.0]
ws:[7.953635533650716, 2.150548299153646]
memory len:10000
memory used:2853.0
now epsilon is 0.22415064352973896, the reward is 245.25 with loss [28.031102061271667, 37.42303276062012] in episode 1040
Report: 
rewardSum:245.25
loss:[28.031102061271667, 37.42303276062012]
policies:[3, 2, 1]
qAverage:[71.45331064860027, 0.0]
ws:[4.817981481552124, -0.08618420362472534]
memory len:10000
memory used:2853.0
now epsilon is 0.22387059528445985, the reward is 246.25 with loss [25.744918823242188, 35.605355739593506] in episode 1041
Report: 
rewardSum:246.25
loss:[25.744918823242188, 35.605355739593506]
policies:[0, 3, 2]
qAverage:[0.0, 49.89259592692057]
ws:[1.2075193325678508, 1.242287238438924]
memory len:10000
memory used:2854.0
now epsilon is 0.22364680862665762, the reward is 247.25 with loss [21.146278858184814, 14.370368242263794] in episode 1042
Report: 
rewardSum:247.25
loss:[21.146278858184814, 14.370368242263794]
policies:[1, 3, 0]
qAverage:[24.24274444580078, 42.37638549804687]
ws:[5.430778419971466, 2.9524603962898253]
memory len:10000
memory used:2854.0
now epsilon is 0.22342324567160718, the reward is 247.25 with loss [20.644550800323486, 12.101211667060852] in episode 1043
Report: 
rewardSum:247.25
loss:[20.644550800323486, 12.101211667060852]
policies:[0, 3, 1]
qAverage:[0.0, 35.46235275268555]
ws:[6.798340320587158, 6.9282002449035645]
memory len:10000
memory used:2853.0
now epsilon is 0.22314410621914071, the reward is 246.25 with loss [19.843578815460205, 20.80387544631958] in episode 1044
Report: 
rewardSum:246.25
loss:[19.843578815460205, 20.80387544631958]
policies:[3, 0, 2]
qAverage:[77.26583099365234, 0.0]
ws:[6.43369706471761, 1.2350560029347737]
memory len:10000
memory used:2853.0
now epsilon is 0.22292104577801578, the reward is 247.25 with loss [21.464635372161865, 27.383944988250732] in episode 1045
Report: 
rewardSum:247.25
loss:[21.464635372161865, 27.383944988250732]
policies:[2, 2, 0]
qAverage:[29.275394439697266, 34.60923385620117]
ws:[3.892331637442112, 0.047046318650245667]
memory len:10000
memory used:2854.0
now epsilon is 0.22258687312817946, the reward is 245.25 with loss [37.064767360687256, 24.0958833694458] in episode 1046
Report: 
rewardSum:245.25
loss:[37.064767360687256, 24.0958833694458]
policies:[3, 1, 2]
qAverage:[57.00382614135742, 19.23402214050293]
ws:[3.474765568971634, 0.10398823022842407]
memory len:10000
memory used:2854.0
now epsilon is 0.22236436971121792, the reward is 247.25 with loss [33.46634292602539, 20.978894233703613] in episode 1047
Report: 
rewardSum:247.25
loss:[33.46634292602539, 20.978894233703613]
policies:[2, 2, 0]
qAverage:[0.0, 37.036094665527344]
ws:[1.3203046321868896, 1.6760056018829346]
memory len:10000
memory used:2854.0
now epsilon is 0.22214208871424848, the reward is 247.25 with loss [15.908660411834717, 14.170942068099976] in episode 1048
Report: 
rewardSum:247.25
loss:[15.908660411834717, 14.170942068099976]
policies:[2, 2, 0]
qAverage:[0.0, 34.846370697021484]
ws:[0.22063535451889038, 0.5248528122901917]
memory len:10000
memory used:2854.0
now epsilon is 0.22192002991493454, the reward is 247.25 with loss [12.301016092300415, 21.82545757293701] in episode 1049
Report: 
rewardSum:247.25
loss:[12.301016092300415, 21.82545757293701]
policies:[4, 0, 0]
qAverage:[58.814903259277344, 0.0]
ws:[3.921619415283203, -2.163821220397949]
memory len:10000
memory used:2854.0
now epsilon is 0.22147657802103768, the reward is 243.25 with loss [52.027544021606445, 47.561442136764526] in episode 1050
Report: 
rewardSum:243.25
loss:[52.027544021606445, 47.561442136764526]
policies:[2, 3, 3]
qAverage:[23.66203155517578, 41.83921356201172]
ws:[1.7173408314585685, -0.5773860931396484]
memory len:10000
memory used:2854.0
now epsilon is 0.22125518448289203, the reward is 247.25 with loss [18.96051487326622, 17.379295229911804] in episode 1051
Report: 
rewardSum:247.25
loss:[18.96051487326622, 17.379295229911804]
policies:[1, 2, 1]
qAverage:[29.30520248413086, 34.700735092163086]
ws:[0.6352603957056999, -2.708430737257004]
memory len:10000
memory used:2854.0
now epsilon is 0.22103401225527575, the reward is 247.25 with loss [19.972291946411133, 24.100430011749268] in episode 1052
Report: 
rewardSum:247.25
loss:[19.972291946411133, 24.100430011749268]
policies:[0, 3, 1]
qAverage:[0.0, 51.98141002655029]
ws:[0.5886697173118591, 1.6716970950365067]
memory len:10000
memory used:2853.0
now epsilon is 0.22059233084694235, the reward is 243.25 with loss [48.19682574272156, 43.79322361946106] in episode 1053
Report: 
rewardSum:243.25
loss:[48.19682574272156, 43.79322361946106]
policies:[1, 5, 2]
qAverage:[19.26272710164388, 46.18978055318197]
ws:[5.2820645570755005, 4.214960704247157]
memory len:10000
memory used:2854.0
now epsilon is 0.22037182122443336, the reward is 247.25 with loss [20.66591912508011, 22.00332200527191] in episode 1054
Report: 
rewardSum:247.25
loss:[20.66591912508011, 22.00332200527191]
policies:[2, 2, 0]
qAverage:[38.83806355794271, 21.162681579589844]
ws:[4.6805396775404615, 0.5642375946044922]
memory len:10000
memory used:2854.0
now epsilon is 0.22015153202886956, the reward is 247.25 with loss [23.08828830718994, 22.51268434524536] in episode 1055
Report: 
rewardSum:247.25
loss:[23.08828830718994, 22.51268434524536]
policies:[1, 2, 1]
qAverage:[27.09230613708496, 32.28283500671387]
ws:[4.101682774722576, 1.0365888327360153]
memory len:10000
memory used:2854.0
now epsilon is 0.21993146303990663, the reward is 247.25 with loss [17.42412006855011, 19.96554470062256] in episode 1056
Report: 
rewardSum:247.25
loss:[17.42412006855011, 19.96554470062256]
policies:[1, 3, 0]
qAverage:[22.41892852783203, 39.221439361572266]
ws:[4.006278992444277, 1.9655004262924194]
memory len:10000
memory used:2854.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23*		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.21971161403742054, the reward is 247.25 with loss [22.519123554229736, 18.589579582214355] in episode 1057
Report: 
rewardSum:247.25
loss:[22.519123554229736, 18.589579582214355]
policies:[1, 3, 0]
qAverage:[28.02300453186035, 32.63594055175781]
ws:[5.2192064970731735, 2.5201147347688675]
memory len:10000
memory used:2854.0
now epsilon is 0.2194371118053069, the reward is -4.0 with loss [28.152196407318115, 31.571744203567505] in episode 1058
Report: 
rewardSum:-4.0
loss:[28.152196407318115, 31.571744203567505]
policies:[0, 1, 4]
qAverage:[0.0, 30.367589950561523]
ws:[-0.8870989680290222, -0.07104389369487762]
memory len:10000
memory used:2853.0
now epsilon is 0.21921775696870457, the reward is 247.25 with loss [17.096742630004883, 16.090624928474426] in episode 1059
Report: 
rewardSum:247.25
loss:[17.096742630004883, 16.090624928474426]
policies:[1, 1, 2]
qAverage:[37.94304911295573, 21.15704345703125]
ws:[0.8271771272023519, -2.9026037057240806]
memory len:10000
memory used:2853.0
now epsilon is 0.21910816179133005, the reward is -1.0 with loss [7.733242988586426, 9.52939510345459] in episode 1060
Report: 
rewardSum:-1.0
loss:[7.733242988586426, 9.52939510345459]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:2853.0
now epsilon is 0.21888913578140604, the reward is 247.25 with loss [19.57076597213745, 23.8745756149292] in episode 1061
Report: 
rewardSum:247.25
loss:[19.57076597213745, 23.8745756149292]
policies:[0, 3, 1]
qAverage:[0.0, 47.142913818359375]
ws:[2.3748433589935303, 2.885484298070272]
memory len:10000
memory used:2853.0
now epsilon is 0.21867032871537084, the reward is 247.25 with loss [16.94449806213379, 24.04617166519165] in episode 1062
Report: 
rewardSum:247.25
loss:[16.94449806213379, 24.04617166519165]
policies:[2, 2, 0]
qAverage:[42.11652374267578, 26.447801971435545]
ws:[4.1846163988113405, 1.999159574508667]
memory len:10000
memory used:2853.0
now epsilon is 0.21856100721790872, the reward is -1.0 with loss [8.240033149719238, 6.119566440582275] in episode 1063
Report: 
rewardSum:-1.0
loss:[8.240033149719238, 6.119566440582275]
policies:[0, 1, 1]
qAverage:[0.0, 30.468050003051758]
ws:[0.07158764451742172, 0.49890604615211487]
memory len:10000
memory used:2853.0
now epsilon is 0.21834252815740934, the reward is 247.25 with loss [18.753485664725304, 15.496580362319946] in episode 1064
Report: 
rewardSum:247.25
loss:[18.753485664725304, 15.496580362319946]
policies:[0, 3, 1]
qAverage:[0.0, 49.98622512817383]
ws:[3.5858126059174538, 4.144118174910545]
memory len:10000
memory used:2853.0
now epsilon is 0.21812426749405447, the reward is 247.25 with loss [24.86616849899292, 17.262815475463867] in episode 1065
Report: 
rewardSum:247.25
loss:[24.86616849899292, 17.262815475463867]
policies:[2, 2, 0]
qAverage:[29.230173110961914, 31.83867645263672]
ws:[5.494795143604279, 3.5540765151381493]
memory len:10000
memory used:2853.0
now epsilon is 0.21790622500952886, the reward is 37.15999999999997 with loss [17.647087812423706, 15.75973105430603] in episode 1066
Report: 
rewardSum:37.15999999999997
loss:[17.647087812423706, 15.75973105430603]
policies:[0, 3, 1]
qAverage:[0.0, 48.892802238464355]
ws:[1.6335042268037796, 3.081210970878601]
memory len:10000
memory used:2853.0
now epsilon is 0.2175795698910176, the reward is 245.25 with loss [34.642250537872314, 31.750345945358276] in episode 1067
Report: 
rewardSum:245.25
loss:[34.642250537872314, 31.750345945358276]
policies:[1, 3, 2]
qAverage:[22.565882873535156, 42.390934753417966]
ws:[3.1323984622955323, 2.228832572698593]
memory len:10000
memory used:2853.0
now epsilon is 0.217253404449047, the reward is 245.25 with loss [26.69360661506653, 27.398743867874146] in episode 1068
Report: 
rewardSum:245.25
loss:[26.69360661506653, 27.398743867874146]
policies:[1, 4, 1]
qAverage:[21.864669799804688, 37.08468475341797]
ws:[2.809783959388733, 1.869283378124237]
memory len:10000
memory used:2853.0
now epsilon is 0.2169819734429219, the reward is 36.15999999999997 with loss [24.563416719436646, 23.755967140197754] in episode 1069
Report: 
rewardSum:36.15999999999997
loss:[24.563416719436646, 23.755967140197754]
policies:[1, 2, 2]
qAverage:[23.03352928161621, 32.51568603515625]
ws:[3.082873612642288, 3.5172598361968994]
memory len:10000
memory used:2853.0
now epsilon is 0.21676507282415852, the reward is 247.25 with loss [24.099557399749756, 25.759469985961914] in episode 1070
Report: 
rewardSum:247.25
loss:[24.099557399749756, 25.759469985961914]
policies:[0, 4, 0]
qAverage:[0.0, 47.52682113647461]
ws:[0.8774914825335145, 1.308307908475399]
memory len:10000
memory used:2853.0
now epsilon is 0.21654838902468973, the reward is 247.25 with loss [16.437272548675537, 19.136637449264526] in episode 1071
Report: 
rewardSum:247.25
loss:[16.437272548675537, 19.136637449264526]
policies:[2, 2, 0]
qAverage:[26.575578689575195, 32.23717498779297]
ws:[2.166285067796707, 0.18177763372659683]
memory len:10000
memory used:2853.0
now epsilon is 0.21633192182777755, the reward is 247.25 with loss [18.04738736152649, 19.053266525268555] in episode 1072
Report: 
rewardSum:247.25
loss:[18.04738736152649, 19.053266525268555]
policies:[1, 3, 0]
qAverage:[21.345845031738282, 38.33732299804687]
ws:[3.5912872552871704, 2.0564419507980345]
memory len:10000
memory used:2853.0
now epsilon is 0.21611567101690057, the reward is 247.25 with loss [26.754094123840332, 30.212899208068848] in episode 1073
Report: 
rewardSum:247.25
loss:[26.754094123840332, 30.212899208068848]
policies:[1, 2, 1]
qAverage:[27.07114601135254, 33.22731971740723]
ws:[5.380510151386261, 3.2392100989818573]
memory len:10000
memory used:2853.0
now epsilon is 0.2157917000512934, the reward is 245.25 with loss [26.906306505203247, 44.08622455596924] in episode 1074
Report: 
rewardSum:245.25
loss:[26.906306505203247, 44.08622455596924]
policies:[1, 3, 2]
qAverage:[26.923213958740234, 32.32794952392578]
ws:[2.632370039820671, 0.691719051450491]
memory len:10000
memory used:2853.0
now epsilon is 0.2155759892596435, the reward is 247.25 with loss [27.71161127090454, 12.19998037815094] in episode 1075
Report: 
rewardSum:247.25
loss:[27.71161127090454, 12.19998037815094]
policies:[1, 3, 0]
qAverage:[21.759239196777344, 38.22593765258789]
ws:[4.948366916179657, 4.357509446144104]
memory len:10000
memory used:2853.0
now epsilon is 0.2153604940979072, the reward is 247.25 with loss [22.736212730407715, 34.74240446090698] in episode 1076
Report: 
rewardSum:247.25
loss:[22.736212730407715, 34.74240446090698]
policies:[1, 2, 1]
qAverage:[26.379541397094727, 32.04823112487793]
ws:[4.296652302145958, 2.7447256445884705]
memory len:10000
memory used:2853.0
now epsilon is 0.2151452143505354, the reward is 247.25 with loss [16.85669755935669, 35.50443410873413] in episode 1077
Report: 
rewardSum:247.25
loss:[16.85669755935669, 35.50443410873413]
policies:[0, 4, 0]
qAverage:[0.0, 44.872520446777344]
ws:[3.420209914445877, 4.867294708887736]
memory len:10000
memory used:2855.0
now epsilon is 0.21493014980219455, the reward is 247.25 with loss [14.692602276802063, 18.97446358203888] in episode 1078
Report: 
rewardSum:247.25
loss:[14.692602276802063, 18.97446358203888]
policies:[1, 2, 1]
qAverage:[26.76468276977539, 32.7486629486084]
ws:[4.623422893695533, 1.9034945964813232]
memory len:10000
memory used:2855.0
now epsilon is 0.21471530023776628, the reward is 247.25 with loss [23.27251148223877, 25.967326641082764] in episode 1079
Report: 
rewardSum:247.25
loss:[23.27251148223877, 25.967326641082764]
policies:[1, 2, 1]
qAverage:[25.897306442260742, 31.971091270446777]
ws:[4.675870552659035, 2.9522247910499573]
memory len:10000
memory used:2855.0
now epsilon is 0.21450066544234725, the reward is 247.25 with loss [24.007099390029907, 26.689385890960693] in episode 1080
Report: 
rewardSum:247.25
loss:[24.007099390029907, 26.689385890960693]
policies:[1, 3, 0]
qAverage:[26.15262222290039, 32.29082202911377]
ws:[4.552621245384216, 2.0088774412870407]
memory len:10000
memory used:2855.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24*		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.2141791154715387, the reward is 245.25 with loss [22.66533327102661, 27.609612822532654] in episode 1081
Report: 
rewardSum:245.25
loss:[22.66533327102661, 27.609612822532654]
policies:[1, 3, 2]
qAverage:[25.332536697387695, 31.09570026397705]
ws:[3.6568731367588043, 0.9160281978547573]
memory len:10000
memory used:2855.0
now epsilon is 0.21396501665985013, the reward is 247.25 with loss [22.79236364364624, 16.06395721435547] in episode 1082
Report: 
rewardSum:247.25
loss:[22.79236364364624, 16.06395721435547]
policies:[1, 2, 1]
qAverage:[0.0, 31.247453689575195]
ws:[1.0266289710998535, 1.217705249786377]
memory len:10000
memory used:2854.0
now epsilon is 0.21375113186669958, the reward is 247.25 with loss [21.909520149230957, 23.774471044540405] in episode 1083
Report: 
rewardSum:247.25
loss:[21.909520149230957, 23.774471044540405]
policies:[1, 3, 0]
qAverage:[20.723826599121093, 37.803532409667966]
ws:[4.545414817333222, 2.6739155799150467]
memory len:10000
memory used:2854.0
now epsilon is 0.21364426966021197, the reward is -1.0 with loss [9.137760162353516, 6.1398656368255615] in episode 1084
Report: 
rewardSum:-1.0
loss:[9.137760162353516, 6.1398656368255615]
policies:[0, 1, 1]
qAverage:[0.0, 28.420934677124023]
ws:[0.5887112617492676, 0.8608837127685547]
memory len:10000
memory used:2854.0
now epsilon is 0.21343070549380097, the reward is 247.25 with loss [19.902117252349854, 22.445565700531006] in episode 1085
Report: 
rewardSum:247.25
loss:[19.902117252349854, 22.445565700531006]
policies:[1, 3, 0]
qAverage:[21.369046020507813, 38.14034423828125]
ws:[4.932936185598374, 2.7554964125156403]
memory len:10000
memory used:2854.0
now epsilon is 0.21321735481148318, the reward is 247.25 with loss [21.433541774749756, 20.151910066604614] in episode 1086
Report: 
rewardSum:247.25
loss:[21.433541774749756, 20.151910066604614]
policies:[2, 1, 1]
qAverage:[35.90813954671224, 20.697677612304688]
ws:[6.346187591552734, 2.081577777862549]
memory len:10000
memory used:2854.0
now epsilon is 0.2130042173998545, the reward is 247.25 with loss [20.34779667854309, 25.829720497131348] in episode 1087
Report: 
rewardSum:247.25
loss:[20.34779667854309, 25.829720497131348]
policies:[1, 3, 0]
qAverage:[21.32781524658203, 37.92180633544922]
ws:[3.8208623833954336, 2.1297130584716797]
memory len:10000
memory used:2854.0
now epsilon is 0.21268491069865725, the reward is 245.25 with loss [22.332303881645203, 27.98960852622986] in episode 1088
Report: 
rewardSum:245.25
loss:[22.332303881645203, 27.98960852622986]
policies:[1, 4, 1]
qAverage:[17.86386235555013, 41.84497388203939]
ws:[4.367755889892578, 3.030302549401919]
memory len:10000
memory used:2854.0
now epsilon is 0.21247230553150817, the reward is 247.25 with loss [20.8662428855896, 22.175387859344482] in episode 1089
Report: 
rewardSum:247.25
loss:[20.8662428855896, 22.175387859344482]
policies:[1, 3, 0]
qAverage:[27.070985794067383, 29.577938079833984]
ws:[4.324136957526207, 2.3695566952228546]
memory len:10000
memory used:2854.0
now epsilon is 0.21204773256112472, the reward is 243.25 with loss [37.9931275844574, 42.924654483795166] in episode 1090
Report: 
rewardSum:243.25
loss:[37.9931275844574, 42.924654483795166]
policies:[3, 1, 4]
qAverage:[28.6686528523763, 22.720125834147137]
ws:[1.2984100580215454, -0.053834756215413414]
memory len:10000
memory used:2854.0
now epsilon is 0.21183576433321116, the reward is 247.25 with loss [34.841259479522705, 12.375364065170288] in episode 1091
Report: 
rewardSum:247.25
loss:[34.841259479522705, 12.375364065170288]
policies:[1, 3, 0]
qAverage:[26.938405990600586, 28.787002563476562]
ws:[4.565407203510404, 3.11458158493042]
memory len:10000
memory used:2854.0
now epsilon is 0.2116240079940507, the reward is 247.25 with loss [21.94482421875, 26.64979362487793] in episode 1092
Report: 
rewardSum:247.25
loss:[21.94482421875, 26.64979362487793]
policies:[1, 3, 0]
qAverage:[26.339021682739258, 30.98705005645752]
ws:[2.870354562997818, 1.912569910287857]
memory len:10000
memory used:2854.0
now epsilon is 0.21141246333183403, the reward is 247.25 with loss [14.295923709869385, 17.73515295982361] in episode 1093
Report: 
rewardSum:247.25
loss:[14.295923709869385, 17.73515295982361]
policies:[1, 3, 0]
qAverage:[26.738723754882812, 29.172334671020508]
ws:[2.025092124938965, 0.7921745777130127]
memory len:10000
memory used:2853.0
now epsilon is 0.2112011301349635, the reward is 247.25 with loss [16.628669947385788, 17.442078590393066] in episode 1094
Report: 
rewardSum:247.25
loss:[16.628669947385788, 17.442078590393066]
policies:[1, 2, 1]
qAverage:[34.86112976074219, 20.03137969970703]
ws:[5.690561294555664, 3.6868536692733564]
memory len:10000
memory used:2853.0
now epsilon is 0.2107264025176051, the reward is 242.25 with loss [43.341882944107056, 44.74404191970825] in episode 1095
Report: 
rewardSum:242.25
loss:[43.341882944107056, 44.74404191970825]
policies:[1, 5, 3]
qAverage:[17.826189676920574, 42.06814765930176]
ws:[3.436154584089915, 2.5979149540265403]
memory len:10000
memory used:2853.0
now epsilon is 0.21041051040399142, the reward is 245.25 with loss [33.51731514930725, 37.95571756362915] in episode 1096
Report: 
rewardSum:245.25
loss:[33.51731514930725, 37.95571756362915]
policies:[4, 1, 1]
qAverage:[54.13353424072265, 11.21850128173828]
ws:[2.466479539871216, 0.5619679600000381]
memory len:10000
memory used:2853.0
now epsilon is 0.21014762873968293, the reward is 246.25 with loss [29.77914834022522, 36.00868844985962] in episode 1097
Report: 
rewardSum:246.25
loss:[29.77914834022522, 36.00868844985962]
policies:[2, 2, 1]
qAverage:[36.880836486816406, 25.99392166137695]
ws:[2.836958277225494, 0.865789794921875]
memory len:10000
memory used:2853.0
now epsilon is 0.20993755990317065, the reward is 247.25 with loss [23.499046802520752, 31.689138889312744] in episode 1098
Report: 
rewardSum:247.25
loss:[23.499046802520752, 31.689138889312744]
policies:[1, 3, 0]
qAverage:[20.742750549316405, 37.37350082397461]
ws:[3.025896689295769, 1.002426564693451]
memory len:10000
memory used:2854.0
now epsilon is 0.2097277010567322, the reward is 247.25 with loss [23.821998119354248, 22.80404043197632] in episode 1099
Report: 
rewardSum:247.25
loss:[23.821998119354248, 22.80404043197632]
policies:[0, 3, 1]
qAverage:[0.0, 41.646246592203774]
ws:[1.1665421724319458, 1.4971944491068523]
memory len:10000
memory used:2853.0
now epsilon is 0.20930861249464122, the reward is 243.25 with loss [59.118903160095215, 36.3326370716095] in episode 1100
Report: 
rewardSum:243.25
loss:[59.118903160095215, 36.3326370716095]
policies:[1, 5, 2]
qAverage:[14.044386727469307, 43.6080082484654]
ws:[1.6878596182380403, 1.4768523850611277]
memory len:10000
memory used:2853.0
now epsilon is 0.20909938235979533, the reward is 37.15999999999997 with loss [25.110167026519775, 21.32097554206848] in episode 1101
Report: 
rewardSum:37.15999999999997
loss:[25.110167026519775, 21.32097554206848]
policies:[0, 3, 1]
qAverage:[0.0, 45.98948001861572]
ws:[-0.6714348047971725, 1.8747073411941528]
memory len:10000
memory used:2854.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26*		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.20889036137663602, the reward is 247.25 with loss [22.54864001274109, 16.740990102291107] in episode 1102
Report: 
rewardSum:247.25
loss:[22.54864001274109, 16.740990102291107]
policies:[0, 4, 0]
qAverage:[0.0, 32.72510528564453]
ws:[-0.37922385334968567, 1.4323208332061768]
memory len:10000
memory used:2853.0
############# STATE ###############
0*		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.2086815493360901, the reward is 247.25 with loss [30.613171100616455, 22.319368720054626] in episode 1103
Report: 
rewardSum:247.25
loss:[30.613171100616455, 22.319368720054626]
policies:[1, 2, 1]
qAverage:[25.614519119262695, 28.49905300140381]
ws:[0.5861868262290955, 0.7314774394035339]
memory len:10000
memory used:2853.0
now epsilon is 0.2083166304051913, the reward is 244.25 with loss [39.68323373794556, 34.60263180732727] in episode 1104
Report: 
rewardSum:244.25
loss:[39.68323373794556, 34.60263180732727]
policies:[1, 3, 3]
qAverage:[20.125639343261717, 37.56680068969727]
ws:[5.222918891906739, 4.811831557750702]
memory len:10000
memory used:2853.0
now epsilon is 0.20779642432927856, the reward is 241.25 with loss [66.26619172096252, 60.72986799478531] in episode 1105
Report: 
rewardSum:241.25
loss:[66.26619172096252, 60.72986799478531]
policies:[3, 3, 4]
qAverage:[47.063618977864586, 20.153888702392578]
ws:[4.279499808947246, 2.235763351122538]
memory len:10000
memory used:2853.0
now epsilon is 0.20758870581562197, the reward is 247.25 with loss [18.425063371658325, 21.355069279670715] in episode 1106
Report: 
rewardSum:247.25
loss:[18.425063371658325, 21.355069279670715]
policies:[2, 2, 0]
qAverage:[37.80430908203125, 22.778292846679687]
ws:[4.352274775505066, 2.5603758335113525]
memory len:10000
memory used:2853.0
now epsilon is 0.20738119494259755, the reward is 247.25 with loss [17.107227206230164, 27.44459629058838] in episode 1107
Report: 
rewardSum:247.25
loss:[17.107227206230164, 27.44459629058838]
policies:[3, 1, 0]
qAverage:[48.05625534057617, 13.770498275756836]
ws:[4.549609303474426, 2.063477724790573]
memory len:10000
memory used:2853.0
now epsilon is 0.20717389150264257, the reward is 247.25 with loss [20.006706476211548, 16.734477519989014] in episode 1108
Report: 
rewardSum:247.25
loss:[20.006706476211548, 16.734477519989014]
policies:[2, 2, 0]
qAverage:[49.321590423583984, 0.0]
ws:[4.646969318389893, 0.008407235145568848]
memory len:10000
memory used:2853.0
now epsilon is 0.2068633248261822, the reward is 245.25 with loss [33.61166310310364, 40.2014684677124] in episode 1109
Report: 
rewardSum:245.25
loss:[33.61166310310364, 40.2014684677124]
policies:[2, 3, 1]
qAverage:[18.729661560058595, 37.62060775756836]
ws:[3.0522559762001036, 3.003062105178833]
memory len:10000
memory used:2854.0
now epsilon is 0.2065532237086773, the reward is 35.15999999999997 with loss [43.85072708129883, 48.23292827606201] in episode 1110
Report: 
rewardSum:35.15999999999997
loss:[43.85072708129883, 48.23292827606201]
policies:[0, 2, 4]
qAverage:[0.0, 40.46188481648763]
ws:[-2.8619136015574136, -2.0117330153783164]
memory len:10000
memory used:2853.0
now epsilon is 0.20634674792951876, the reward is 247.25 with loss [25.427454233169556, 22.16945743560791] in episode 1111
Report: 
rewardSum:247.25
loss:[25.427454233169556, 22.16945743560791]
policies:[1, 2, 1]
qAverage:[23.942012786865234, 29.45821762084961]
ws:[-0.5523135960102081, -2.4950440526008606]
memory len:10000
memory used:2853.0
now epsilon is 0.20614047854872386, the reward is 247.25 with loss [24.74442410469055, 20.395652532577515] in episode 1112
Report: 
rewardSum:247.25
loss:[24.74442410469055, 20.395652532577515]
policies:[1, 3, 0]
qAverage:[19.43444366455078, 34.220767211914065]
ws:[3.2766064405441284, 2.502777910232544]
memory len:10000
memory used:2853.0
now epsilon is 0.20593441535997162, the reward is 247.25 with loss [26.205199718475342, 26.11352252960205] in episode 1113
Report: 
rewardSum:247.25
loss:[26.205199718475342, 26.11352252960205]
policies:[2, 2, 0]
qAverage:[24.284969329833984, 30.273698806762695]
ws:[4.389979660511017, 4.271050691604614]
memory len:10000
memory used:2854.0
now epsilon is 0.2057285581571473, the reward is 247.25 with loss [17.909231901168823, 23.083733558654785] in episode 1114
Report: 
rewardSum:247.25
loss:[17.909231901168823, 23.083733558654785]
policies:[1, 3, 0]
qAverage:[19.794827270507813, 34.550515747070314]
ws:[1.677286195755005, 2.2287671089172365]
memory len:10000
memory used:2854.0
now epsilon is 0.20552290673434223, the reward is 247.25 with loss [19.03028130531311, 37.79545783996582] in episode 1115
Report: 
rewardSum:247.25
loss:[19.03028130531311, 37.79545783996582]
policies:[2, 2, 0]
qAverage:[24.53854751586914, 29.281855583190918]
ws:[0.9279867708683014, -0.5171520709991455]
memory len:10000
memory used:2854.0
now epsilon is 0.20531746088585356, the reward is 247.25 with loss [23.514748096466064, 18.230787992477417] in episode 1116
Report: 
rewardSum:247.25
loss:[23.514748096466064, 18.230787992477417]
policies:[1, 3, 0]
qAverage:[20.274029541015626, 35.12522583007812]
ws:[-0.0197314977645874, -1.0238878458738327]
memory len:10000
memory used:2854.0
now epsilon is 0.20500967711549473, the reward is 245.25 with loss [27.552476406097412, 31.10385036468506] in episode 1117
Report: 
rewardSum:245.25
loss:[27.552476406097412, 31.10385036468506]
policies:[3, 2, 1]
qAverage:[34.062538146972656, 19.259278615315754]
ws:[4.641264915466309, 3.7357196311155954]
memory len:10000
memory used:2854.0
now epsilon is 0.20480474430419587, the reward is 247.25 with loss [21.927181214094162, 25.514892578125] in episode 1118
Report: 
rewardSum:247.25
loss:[21.927181214094162, 25.514892578125]
policies:[1, 3, 0]
qAverage:[19.763766479492187, 34.9070930480957]
ws:[4.488577079772949, 3.7531835079193114]
memory len:10000
memory used:2854.0
now epsilon is 0.2047023547323403, the reward is -1.0 with loss [7.471986293792725, 8.783447623252869] in episode 1119
Report: 
rewardSum:-1.0
loss:[7.471986293792725, 8.783447623252869]
policies:[0, 1, 1]
qAverage:[0.0, 26.873004913330078]
ws:[-0.48622891306877136, -0.339030921459198]
memory len:10000
memory used:2854.0
now epsilon is 0.2044977291281979, the reward is 247.25 with loss [17.74137830734253, 26.251500844955444] in episode 1120
Report: 
rewardSum:247.25
loss:[17.74137830734253, 26.251500844955444]
policies:[1, 2, 1]
qAverage:[50.16645431518555, 0.0]
ws:[2.1080992221832275, -2.3889780044555664]
memory len:10000
memory used:2853.0
now epsilon is 0.20429330807293783, the reward is 247.25 with loss [18.088417530059814, 31.900743007659912] in episode 1121
Report: 
rewardSum:247.25
loss:[18.088417530059814, 31.900743007659912]
policies:[1, 3, 0]
qAverage:[19.79385986328125, 34.38757400512695]
ws:[3.962355302274227, 2.928625428676605]
memory len:10000
memory used:2853.0
now epsilon is 0.2040890913620879, the reward is 247.25 with loss [19.671457529067993, 19.788845777511597] in episode 1122
Report: 
rewardSum:247.25
loss:[19.671457529067993, 19.788845777511597]
policies:[1, 3, 0]
qAverage:[19.46007385253906, 33.749610137939456]
ws:[6.181017541885376, 5.697046494483947]
memory len:10000
memory used:2853.0
now epsilon is 0.20388507879138032, the reward is 247.25 with loss [25.831074714660645, 25.754741191864014] in episode 1123
Report: 
rewardSum:247.25
loss:[25.831074714660645, 25.754741191864014]
policies:[0, 3, 1]
qAverage:[0.0, 42.02909564971924]
ws:[4.43345183134079, 5.032608151435852]
memory len:10000
memory used:2853.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26*		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.20368127015675147, the reward is 247.25 with loss [17.42628574371338, 18.457003116607666] in episode 1124
Report: 
rewardSum:247.25
loss:[17.42628574371338, 18.457003116607666]
policies:[3, 1, 0]
qAverage:[32.17642720540365, 20.98751449584961]
ws:[0.7825298309326172, -1.9714355667432149]
memory len:10000
memory used:2853.0
############# STATE ###############
0*		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.2034776652543418, the reward is 37.15999999999997 with loss [22.396252632141113, 22.69010305404663] in episode 1125
Report: 
rewardSum:37.15999999999997
loss:[22.396252632141113, 22.69010305404663]
policies:[1, 2, 1]
qAverage:[0.0, 35.14354451497396]
ws:[-1.0229446416099865, -0.05356105168660482]
memory len:10000
memory used:2853.0
now epsilon is 0.2032742638804954, the reward is 247.25 with loss [29.78420329093933, 26.295697689056396] in episode 1126
Report: 
rewardSum:247.25
loss:[29.78420329093933, 26.295697689056396]
policies:[1, 3, 0]
qAverage:[19.41737060546875, 33.85981826782226]
ws:[2.90728577375412, 1.84871985912323]
memory len:10000
memory used:2853.0
now epsilon is 0.2029695429907858, the reward is 245.25 with loss [26.192381247878075, 33.64662837982178] in episode 1127
Report: 
rewardSum:245.25
loss:[26.192381247878075, 33.64662837982178]
policies:[1, 3, 2]
qAverage:[0.0, 26.953166961669922]
ws:[3.387129545211792, 3.5338613986968994]
memory len:10000
memory used:2853.0
now epsilon is 0.20266527889683011, the reward is 245.25 with loss [33.617122411727905, 27.07482671737671] in episode 1128
Report: 
rewardSum:245.25
loss:[33.617122411727905, 27.07482671737671]
policies:[2, 3, 1]
qAverage:[0.0, 35.26403935750326]
ws:[1.732513189315796, 2.0361851851145425]
memory len:10000
memory used:2853.0
now epsilon is 0.20246268960474711, the reward is 247.25 with loss [17.743141412734985, 17.43735647201538] in episode 1129
Report: 
rewardSum:247.25
loss:[17.743141412734985, 17.43735647201538]
policies:[0, 3, 1]
qAverage:[0.0, 42.47251510620117]
ws:[1.986780822277069, 2.4014888405799866]
memory len:10000
memory used:2853.0
now epsilon is 0.20226030282599786, the reward is 247.25 with loss [26.80121421813965, 18.375536680221558] in episode 1130
Report: 
rewardSum:247.25
loss:[26.80121421813965, 18.375536680221558]
policies:[1, 3, 0]
qAverage:[19.493402099609376, 33.87099685668945]
ws:[1.723176857829094, 0.7684218525886536]
memory len:10000
memory used:2853.0
now epsilon is 0.20205811835814494, the reward is 247.25 with loss [22.52399444580078, 20.811821699142456] in episode 1131
Report: 
rewardSum:247.25
loss:[22.52399444580078, 20.811821699142456]
policies:[1, 3, 0]
qAverage:[19.256611633300782, 33.62275390625]
ws:[2.1766412973403932, 1.1441620469093323]
memory len:10000
memory used:2853.0
now epsilon is 0.20185613599895336, the reward is 247.25 with loss [29.662970542907715, 24.965495109558105] in episode 1132
Report: 
rewardSum:247.25
loss:[29.662970542907715, 24.965495109558105]
policies:[1, 3, 0]
qAverage:[24.16533851623535, 29.02430534362793]
ws:[1.092066876590252, -0.417087584733963]
memory len:10000
memory used:2853.0
now epsilon is 0.20165435554639022, the reward is 247.25 with loss [27.437752723693848, 27.393187046051025] in episode 1133
Report: 
rewardSum:247.25
loss:[27.437752723693848, 27.393187046051025]
policies:[1, 2, 1]
qAverage:[23.513689041137695, 28.035202026367188]
ws:[-0.3115442618727684, -1.8484449312090874]
memory len:10000
memory used:2853.0
now epsilon is 0.20145277679862458, the reward is 37.15999999999997 with loss [23.260141849517822, 23.414910554885864] in episode 1134
Report: 
rewardSum:37.15999999999997
loss:[23.260141849517822, 23.414910554885864]
policies:[1, 2, 1]
qAverage:[0.0, 34.09250513712565]
ws:[1.5086814165115356, 1.8179778258005779]
memory len:10000
memory used:2853.0
now epsilon is 0.20125139955402727, the reward is 247.25 with loss [20.405808925628662, 15.920291900634766] in episode 1135
Report: 
rewardSum:247.25
loss:[20.405808925628662, 15.920291900634766]
policies:[2, 2, 0]
qAverage:[37.10952453613281, 21.043334197998046]
ws:[2.234206630848348, 1.0407442927360535]
memory len:10000
memory used:2853.0
now epsilon is 0.2009497110650041, the reward is 245.25 with loss [40.27165412902832, 31.08755385875702] in episode 1136
Report: 
rewardSum:245.25
loss:[40.27165412902832, 31.08755385875702]
policies:[2, 3, 1]
qAverage:[15.237844848632813, 33.30335540771485]
ws:[2.8307483196258545, 2.895570492744446]
memory len:10000
memory used:2853.0
now epsilon is 0.20074883669752216, the reward is 247.25 with loss [23.684826135635376, 29.157090663909912] in episode 1137
Report: 
rewardSum:247.25
loss:[23.684826135635376, 29.157090663909912]
policies:[1, 2, 1]
qAverage:[0.0, 37.61336771647135]
ws:[1.0615476270516713, 1.4717767834663391]
memory len:10000
memory used:2853.0
now epsilon is 0.2005481631290924, the reward is 247.25 with loss [18.983675479888916, 20.90347719192505] in episode 1138
Report: 
rewardSum:247.25
loss:[18.983675479888916, 20.90347719192505]
policies:[1, 3, 0]
qAverage:[0.0, 42.3292818069458]
ws:[3.299879103899002, 3.900067239999771]
memory len:10000
memory used:2853.0
now epsilon is 0.20024752883564217, the reward is 245.25 with loss [38.155699253082275, 44.45210838317871] in episode 1139
Report: 
rewardSum:245.25
loss:[38.155699253082275, 44.45210838317871]
policies:[0, 5, 1]
qAverage:[0.0, 43.7097692489624]
ws:[3.1207435727119446, 4.200806021690369]
memory len:10000
memory used:2854.0
now epsilon is 0.20004735638711518, the reward is 247.25 with loss [18.109487771987915, 18.514291524887085] in episode 1140
Report: 
rewardSum:247.25
loss:[18.109487771987915, 18.514291524887085]
policies:[2, 2, 0]
qAverage:[25.720901489257812, 26.714219093322754]
ws:[1.47345869243145, 0.7284785360097885]
memory len:10000
memory used:2854.0
now epsilon is 0.19984738403598454, the reward is 247.25 with loss [23.61453676223755, 24.505475997924805] in episode 1141
Report: 
rewardSum:247.25
loss:[23.61453676223755, 24.505475997924805]
policies:[1, 3, 0]
qAverage:[25.806365966796875, 27.783944129943848]
ws:[1.6022637858986855, 1.422855257987976]
memory len:10000
memory used:2854.0
now epsilon is 0.1996476115822279, the reward is 247.25 with loss [19.773405075073242, 23.67137384414673] in episode 1142
Report: 
rewardSum:247.25
loss:[19.773405075073242, 23.67137384414673]
policies:[1, 2, 1]
qAverage:[25.429401397705078, 26.44004726409912]
ws:[3.7480281591415405, 3.3038244619965553]
memory len:10000
memory used:2854.0
now epsilon is 0.19934832727211227, the reward is 245.25 with loss [30.88559865951538, 34.6956901550293] in episode 1143
Report: 
rewardSum:245.25
loss:[30.88559865951538, 34.6956901550293]
policies:[2, 3, 1]
qAverage:[19.78730926513672, 33.630367279052734]
ws:[3.4553276538848876, 3.2457136511802673]
memory len:10000
memory used:2854.0
now epsilon is 0.19899972923507425, the reward is 244.25 with loss [36.13721179962158, 41.63762331008911] in episode 1144
Report: 
rewardSum:244.25
loss:[36.13721179962158, 41.63762331008911]
policies:[4, 0, 3]
qAverage:[56.43025779724121, 0.0]
ws:[1.2714155316352844, 0.9459760040044785]
memory len:10000
memory used:2854.0
now epsilon is 0.19870141614129208, the reward is 245.25 with loss [31.959205627441406, 34.01642966270447] in episode 1145
Report: 
rewardSum:245.25
loss:[31.959205627441406, 34.01642966270447]
policies:[3, 2, 1]
qAverage:[36.95438194274902, 16.205821990966797]
ws:[2.173630528151989, 2.1059931814670563]
memory len:10000
memory used:2854.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22*		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.19850278922576378, the reward is 247.25 with loss [22.04118061065674, 37.5194206237793] in episode 1146
Report: 
rewardSum:247.25
loss:[22.04118061065674, 37.5194206237793]
policies:[0, 4, 0]
qAverage:[0.0, 37.39441426595052]
ws:[1.8111292521158855, 2.272096554438273]
memory len:10000
memory used:2854.0
now epsilon is 0.19830436086267836, the reward is 247.25 with loss [23.545180320739746, 23.922016382217407] in episode 1147
Report: 
rewardSum:247.25
loss:[23.545180320739746, 23.922016382217407]
policies:[1, 2, 1]
qAverage:[25.486501693725586, 31.294358253479004]
ws:[2.5617193579673767, 1.353111445903778]
memory len:10000
memory used:2854.0
now epsilon is 0.19810613085355777, the reward is 247.25 with loss [21.470886945724487, 20.042068362236023] in episode 1148
Report: 
rewardSum:247.25
loss:[21.470886945724487, 20.042068362236023]
policies:[2, 2, 0]
qAverage:[0.0, 37.8935292561849]
ws:[1.4877535104751587, 2.252285043398539]
memory len:10000
memory used:2854.0
now epsilon is 0.19790809900012243, the reward is 247.25 with loss [20.57788848876953, 24.189156532287598] in episode 1149
Report: 
rewardSum:247.25
loss:[20.57788848876953, 24.189156532287598]
policies:[1, 3, 0]
qAverage:[20.325901794433594, 34.754662322998044]
ws:[1.3924041502177715, 0.051536199450492856]
memory len:10000
memory used:2854.0
now epsilon is 0.197710265104291, the reward is 247.25 with loss [20.42926836013794, 20.471619606018066] in episode 1150
Report: 
rewardSum:247.25
loss:[20.42926836013794, 20.471619606018066]
policies:[1, 2, 1]
qAverage:[26.374103546142578, 30.296083450317383]
ws:[-0.37161748856306076, -2.01203952729702]
memory len:10000
memory used:2854.0
now epsilon is 0.1974138849982352, the reward is 245.25 with loss [39.09232568740845, 38.57716989517212] in episode 1151
Report: 
rewardSum:245.25
loss:[39.09232568740845, 38.57716989517212]
policies:[0, 4, 2]
qAverage:[0.0, 46.8634895324707]
ws:[2.7012293100357057, 3.958636236190796]
memory len:10000
memory used:2854.0
now epsilon is 0.1971672409948235, the reward is 246.25 with loss [28.1235990524292, 29.07082509994507] in episode 1152
Report: 
rewardSum:246.25
loss:[28.1235990524292, 29.07082509994507]
policies:[2, 2, 1]
qAverage:[26.116064071655273, 26.943578720092773]
ws:[2.3979531824588776, 1.1607894897460938]
memory len:10000
memory used:2854.0
now epsilon is 0.19697014767922189, the reward is 247.25 with loss [21.871527194976807, 33.94800615310669] in episode 1153
Report: 
rewardSum:247.25
loss:[21.871527194976807, 33.94800615310669]
policies:[1, 3, 0]
qAverage:[21.237081909179686, 34.398450469970705]
ws:[1.4292875289916993, 0.708772349357605]
memory len:10000
memory used:2854.0
now epsilon is 0.19687167491601654, the reward is -1.0 with loss [9.276907444000244, 17.27185344696045] in episode 1154
Report: 
rewardSum:-1.0
loss:[9.276907444000244, 17.27185344696045]
policies:[0, 1, 1]
qAverage:[0.0, 25.263132095336914]
ws:[-0.20265908539295197, 0.3016436994075775]
memory len:10000
memory used:2854.0
now epsilon is 0.19667487705567493, the reward is 247.25 with loss [31.05323553085327, 16.124922513961792] in episode 1155
Report: 
rewardSum:247.25
loss:[31.05323553085327, 16.124922513961792]
policies:[2, 2, 0]
qAverage:[27.101051330566406, 26.56093120574951]
ws:[2.231610879302025, 0.7414492364041507]
memory len:10000
memory used:2854.0
now epsilon is 0.19647827591940678, the reward is 247.25 with loss [30.187140464782715, 25.831353187561035] in episode 1156
Report: 
rewardSum:247.25
loss:[30.187140464782715, 25.831353187561035]
policies:[1, 3, 0]
qAverage:[0.0, 44.72898864746094]
ws:[3.8228288888931274, 4.922737896442413]
memory len:10000
memory used:2854.0
now epsilon is 0.19618374264252345, the reward is 245.25 with loss [37.81657648086548, 38.189152240753174] in episode 1157
Report: 
rewardSum:245.25
loss:[37.81657648086548, 38.189152240753174]
policies:[2, 2, 2]
qAverage:[27.36046028137207, 30.75398826599121]
ws:[3.1446037888526917, 2.9382659196853638]
memory len:10000
memory used:2854.0
now epsilon is 0.1959876324565237, the reward is 247.25 with loss [24.33112859725952, 18.53755497932434] in episode 1158
Report: 
rewardSum:247.25
loss:[24.33112859725952, 18.53755497932434]
policies:[1, 3, 0]
qAverage:[27.636260986328125, 26.358312606811523]
ws:[2.33752503991127, 1.8641676306724548]
memory len:10000
memory used:2854.0
now epsilon is 0.19569383468500973, the reward is 245.25 with loss [24.240868091583252, 38.40019702911377] in episode 1159
Report: 
rewardSum:245.25
loss:[24.240868091583252, 38.40019702911377]
policies:[1, 3, 2]
qAverage:[27.34296226501465, 29.699225425720215]
ws:[4.721319615840912, 3.6578699350357056]
memory len:10000
memory used:2854.0
now epsilon is 0.19549821422328262, the reward is 247.25 with loss [21.406260013580322, 18.48677706718445] in episode 1160
Report: 
rewardSum:247.25
loss:[21.406260013580322, 18.48677706718445]
policies:[2, 1, 1]
qAverage:[46.81150817871094, 17.072223663330078]
ws:[5.131133079528809, 3.4801858942955732]
memory len:10000
memory used:2860.0
now epsilon is 0.19530278930867181, the reward is 247.25 with loss [15.977915048599243, 28.01395893096924] in episode 1161
Report: 
rewardSum:247.25
loss:[15.977915048599243, 28.01395893096924]
policies:[0, 3, 1]
qAverage:[0.0, 42.32420984903971]
ws:[3.6964211463928223, 3.829195181528727]
memory len:10000
memory used:2860.0
now epsilon is 0.19510755974570348, the reward is 247.25 with loss [17.817420721054077, 24.988603591918945] in episode 1162
Report: 
rewardSum:247.25
loss:[17.817420721054077, 24.988603591918945]
policies:[2, 2, 0]
qAverage:[37.13155975341797, 23.594688415527344]
ws:[2.6300204753875733, 1.4736475110054017]
memory len:10000
memory used:2860.0
now epsilon is 0.19491252533909922, the reward is 247.25 with loss [23.196596145629883, 15.163969993591309] in episode 1163
Report: 
rewardSum:247.25
loss:[23.196596145629883, 15.163969993591309]
policies:[1, 2, 1]
qAverage:[35.897361755371094, 16.826435089111328]
ws:[2.7032246192296348, 0.7594465414683024]
memory len:10000
memory used:2860.0
now epsilon is 0.1945716841358792, the reward is 244.25 with loss [47.394720792770386, 34.81771731376648] in episode 1164
Report: 
rewardSum:244.25
loss:[47.394720792770386, 34.81771731376648]
policies:[1, 4, 2]
qAverage:[18.039534250895183, 38.53792826334635]
ws:[4.102213462193807, 3.2210534115632377]
memory len:10000
memory used:2860.0
now epsilon is 0.19437718540396492, the reward is 247.25 with loss [22.80260944366455, 34.944334506988525] in episode 1165
Report: 
rewardSum:247.25
loss:[22.80260944366455, 34.944334506988525]
policies:[0, 2, 2]
qAverage:[0.0, 42.836751302083336]
ws:[1.737518072128296, 2.0269834995269775]
memory len:10000
memory used:2860.0
now epsilon is 0.1939887710232046, the reward is 243.25 with loss [42.78499221801758, 55.502623558044434] in episode 1166
Report: 
rewardSum:243.25
loss:[42.78499221801758, 55.502623558044434]
policies:[2, 4, 2]
qAverage:[22.638394165039063, 37.591162109375]
ws:[3.190183627605438, 2.7643586874008177]
memory len:10000
memory used:2860.0
now epsilon is 0.19384331581480072, the reward is -2.0 with loss [19.88218116760254, 22.934572458267212] in episode 1167
Report: 
rewardSum:-2.0
loss:[19.88218116760254, 22.934572458267212]
policies:[0, 1, 2]
qAverage:[0.0, 26.919675827026367]
ws:[1.5533777475357056, 2.28763747215271]
memory len:10000
memory used:2860.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26*		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.19364954517811492, the reward is 247.25 with loss [32.60389041900635, 20.971712350845337] in episode 1168
Report: 
rewardSum:247.25
loss:[32.60389041900635, 20.971712350845337]
policies:[0, 3, 1]
qAverage:[0.0, 48.122013092041016]
ws:[2.3350969552993774, 3.8624701499938965]
memory len:10000
memory used:2860.0
############# STATE ###############
0*		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.19331091253320565, the reward is 244.25 with loss [36.6797571182251, 40.475414991378784] in episode 1169
Report: 
rewardSum:244.25
loss:[36.6797571182251, 40.475414991378784]
policies:[1, 4, 2]
qAverage:[0.0, 50.79377822875976]
ws:[2.9272027969360352, 4.912593650817871]
memory len:10000
memory used:2860.0
now epsilon is 0.1931176741001835, the reward is 37.15999999999997 with loss [21.141178607940674, 25.034194707870483] in episode 1170
Report: 
rewardSum:37.15999999999997
loss:[21.141178607940674, 25.034194707870483]
policies:[0, 2, 2]
qAverage:[0.0, 42.12829717000326]
ws:[0.6493092974026998, 1.9652507702509563]
memory len:10000
memory used:2860.0
now epsilon is 0.19292462883314201, the reward is 247.25 with loss [22.8134548664093, 23.057846069335938] in episode 1171
Report: 
rewardSum:247.25
loss:[22.8134548664093, 23.057846069335938]
policies:[1, 3, 0]
qAverage:[0.0, 42.088418324788414]
ws:[1.4819631725549698, 3.161282936731974]
memory len:10000
memory used:2860.0
now epsilon is 0.19273177653898765, the reward is 247.25 with loss [18.973167419433594, 21.14602565765381] in episode 1172
Report: 
rewardSum:247.25
loss:[18.973167419433594, 21.14602565765381]
policies:[1, 3, 0]
qAverage:[31.985198974609375, 34.55029773712158]
ws:[8.490785121917725, 8.007116794586182]
memory len:10000
memory used:2860.0
now epsilon is 0.1925391170248199, the reward is 247.25 with loss [25.1841938495636, 25.348556518554688] in episode 1173
Report: 
rewardSum:247.25
loss:[25.1841938495636, 25.348556518554688]
policies:[1, 3, 0]
qAverage:[25.861367797851564, 37.89050369262695]
ws:[5.216341543197632, 4.079615116119385]
memory len:10000
memory used:2860.0
now epsilon is 0.1923466500979311, the reward is 247.25 with loss [23.91728115081787, 22.048266410827637] in episode 1174
Report: 
rewardSum:247.25
loss:[23.91728115081787, 22.048266410827637]
policies:[1, 2, 1]
qAverage:[31.653003692626953, 28.864973068237305]
ws:[3.767734732478857, 2.804136723279953]
memory len:10000
memory used:2860.0
now epsilon is 0.1921063369719146, the reward is 246.25 with loss [21.56381583213806, 29.450069427490234] in episode 1175
Report: 
rewardSum:246.25
loss:[21.56381583213806, 29.450069427490234]
policies:[1, 3, 1]
qAverage:[25.044517517089844, 38.48712844848633]
ws:[3.6803653299808503, 3.491515374183655]
memory len:10000
memory used:2860.0
now epsilon is 0.19191430266281317, the reward is 247.25 with loss [22.234578609466553, 26.359644889831543] in episode 1176
Report: 
rewardSum:247.25
loss:[22.234578609466553, 26.359644889831543]
policies:[1, 2, 1]
qAverage:[33.65953063964844, 28.848546981811523]
ws:[3.962377019226551, 3.28783842921257]
memory len:10000
memory used:2860.0
now epsilon is 0.19172246031602, the reward is 247.25 with loss [16.820428371429443, 21.921496391296387] in episode 1177
Report: 
rewardSum:247.25
loss:[16.820428371429443, 21.921496391296387]
policies:[1, 2, 1]
qAverage:[35.797698974609375, 39.18411636352539]
ws:[6.33346152305603, 6.284114837646484]
memory len:10000
memory used:2860.0
now epsilon is 0.1914350563054505, the reward is 245.25 with loss [47.30054569244385, 34.33885097503662] in episode 1178
Report: 
rewardSum:245.25
loss:[47.30054569244385, 34.33885097503662]
policies:[1, 4, 1]
qAverage:[23.109619140625, 45.45712407430013]
ws:[6.018038868904114, 6.770204067230225]
memory len:10000
memory used:2860.0
now epsilon is 0.19124369302532723, the reward is 247.25 with loss [17.556448221206665, 16.17010736465454] in episode 1179
Report: 
rewardSum:247.25
loss:[17.556448221206665, 16.17010736465454]
policies:[2, 1, 1]
qAverage:[0.0, 28.996274948120117]
ws:[0.12508685886859894, 1.2122734785079956]
memory len:10000
memory used:2866.0
now epsilon is 0.1911480831315454, the reward is -1.0 with loss [13.420344352722168, 6.371514439582825] in episode 1180
Report: 
rewardSum:-1.0
loss:[13.420344352722168, 6.371514439582825]
policies:[0, 1, 1]
qAverage:[0.0, 27.45906639099121]
ws:[0.7033491134643555, 2.46284556388855]
memory len:10000
memory used:2866.0
now epsilon is 0.19095700671699908, the reward is 247.25 with loss [21.512537002563477, 21.871580123901367] in episode 1181
Report: 
rewardSum:247.25
loss:[21.512537002563477, 21.871580123901367]
policies:[1, 3, 0]
qAverage:[0.0, 33.5757942199707]
ws:[5.296771049499512, 6.388239860534668]
memory len:10000
memory used:2866.0
now epsilon is 0.19071842977689873, the reward is 246.25 with loss [26.881990909576416, 25.390401363372803] in episode 1182
Report: 
rewardSum:246.25
loss:[26.881990909576416, 25.390401363372803]
policies:[1, 2, 2]
qAverage:[0.0, 32.4007453918457]
ws:[0.9449944496154785, 2.001171350479126]
memory len:10000
memory used:2872.0
now epsilon is 0.19052778285461386, the reward is 247.25 with loss [26.35830521583557, 22.094064712524414] in episode 1183
Report: 
rewardSum:247.25
loss:[26.35830521583557, 22.094064712524414]
policies:[1, 3, 0]
qAverage:[28.521975708007812, 42.67494277954101]
ws:[3.927909326553345, 4.0746933102607725]
memory len:10000
memory used:2872.0
now epsilon is 0.1903373265077706, the reward is 247.25 with loss [22.10617971420288, 27.187018871307373] in episode 1184
Report: 
rewardSum:247.25
loss:[22.10617971420288, 27.187018871307373]
policies:[1, 3, 0]
qAverage:[35.88562774658203, 37.67837333679199]
ws:[5.7296841740608215, 5.605732321739197]
memory len:10000
memory used:2873.0
now epsilon is 0.18990949553238873, the reward is 242.25 with loss [61.62060022354126, 46.936107873916626] in episode 1185
Report: 
rewardSum:242.25
loss:[61.62060022354126, 46.936107873916626]
policies:[1, 5, 3]
qAverage:[23.409578959147137, 46.1894416809082]
ws:[4.920861462752025, 5.028136134147644]
memory len:10000
memory used:2873.0
now epsilon is 0.1895774030675892, the reward is 244.25 with loss [40.27282643318176, 35.18279838562012] in episode 1186
Report: 
rewardSum:244.25
loss:[40.27282643318176, 35.18279838562012]
policies:[3, 2, 2]
qAverage:[36.87683614095052, 21.6852289835612]
ws:[3.383389155069987, 3.629063844680786]
memory len:10000
memory used:2873.0
now epsilon is 0.18938789674419992, the reward is 247.25 with loss [28.95806121826172, 25.326226472854614] in episode 1187
Report: 
rewardSum:247.25
loss:[28.95806121826172, 25.326226472854614]
policies:[2, 2, 0]
qAverage:[42.05330276489258, 40.49745273590088]
ws:[5.52192759513855, 5.019022047519684]
memory len:10000
memory used:2873.0
now epsilon is 0.18919857985608102, the reward is 247.25 with loss [31.20183038711548, 23.751628875732422] in episode 1188
Report: 
rewardSum:247.25
loss:[31.20183038711548, 23.751628875732422]
policies:[1, 3, 0]
qAverage:[40.54499053955078, 44.23122787475586]
ws:[9.430935382843018, 9.567569255828857]
memory len:10000
memory used:2873.0
now epsilon is 0.18900945221386822, the reward is 247.25 with loss [22.303127765655518, 29.2403621673584] in episode 1189
Report: 
rewardSum:247.25
loss:[22.303127765655518, 29.2403621673584]
policies:[0, 4, 0]
qAverage:[0.0, 57.50555992126465]
ws:[6.622546911239624, 8.21765398979187]
memory len:10000
memory used:2873.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26*		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.1888205136283866, the reward is 247.25 with loss [22.58892583847046, 18.72375988960266] in episode 1190
Report: 
rewardSum:247.25
loss:[22.58892583847046, 18.72375988960266]
policies:[2, 2, 0]
qAverage:[41.65121841430664, 35.56974506378174]
ws:[8.08085560798645, 7.220010995864868]
memory len:10000
memory used:2873.0
############# STATE ###############
0*		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.18863176391065029, the reward is 247.25 with loss [23.22636342048645, 14.910096645355225] in episode 1191
Report: 
rewardSum:247.25
loss:[23.22636342048645, 14.910096645355225]
policies:[0, 3, 1]
qAverage:[0.0, 59.5685977935791]
ws:[3.5767606496810913, 4.3228230476379395]
memory len:10000
memory used:2874.0
now epsilon is 0.1884432028718624, the reward is 247.25 with loss [30.709369897842407, 14.051525592803955] in episode 1192
Report: 
rewardSum:247.25
loss:[30.709369897842407, 14.051525592803955]
policies:[2, 2, 0]
qAverage:[53.64579772949219, 26.072245279947918]
ws:[9.171210686365763, 7.423488616943359]
memory len:10000
memory used:2874.0
now epsilon is 0.18825483032341467, the reward is 247.25 with loss [22.55950689315796, 18.216399013996124] in episode 1193
Report: 
rewardSum:247.25
loss:[22.55950689315796, 18.216399013996124]
policies:[1, 3, 0]
qAverage:[33.02302551269531, 48.0350227355957]
ws:[6.412748718261719, 6.937579393386841]
memory len:10000
memory used:2874.0
now epsilon is 0.18806664607688747, the reward is 247.25 with loss [22.896036624908447, 21.114707469940186] in episode 1194
Report: 
rewardSum:247.25
loss:[22.896036624908447, 21.114707469940186]
policies:[1, 3, 0]
qAverage:[31.475396728515626, 46.75067977905273]
ws:[5.085081466287375, 7.103873443603516]
memory len:10000
memory used:2874.0
now epsilon is 0.18787864994404946, the reward is 247.25 with loss [19.014286518096924, 25.342519760131836] in episode 1195
Report: 
rewardSum:247.25
loss:[19.014286518096924, 25.342519760131836]
policies:[0, 3, 1]
qAverage:[0.0, 47.08020909627279]
ws:[3.4291741847991943, 5.118143081665039]
memory len:10000
memory used:2873.0
now epsilon is 0.18769084173685746, the reward is 247.25 with loss [31.89854621887207, 26.746724605560303] in episode 1196
Report: 
rewardSum:247.25
loss:[31.89854621887207, 26.746724605560303]
policies:[0, 4, 0]
qAverage:[0.0, 58.73873805999756]
ws:[2.784765362739563, 4.643467664718628]
memory len:10000
memory used:2874.0
now epsilon is 0.18750322126745633, the reward is 247.25 with loss [28.868072032928467, 26.40357255935669] in episode 1197
Report: 
rewardSum:247.25
loss:[28.868072032928467, 26.40357255935669]
policies:[0, 3, 1]
qAverage:[0.0, 60.25301615397135]
ws:[-0.25760023792584735, 1.7283769845962524]
memory len:10000
memory used:2874.0
now epsilon is 0.18731578834817864, the reward is 247.25 with loss [24.80138397216797, 22.506259441375732] in episode 1198
Report: 
rewardSum:247.25
loss:[24.80138397216797, 22.506259441375732]
policies:[1, 3, 0]
qAverage:[33.30855712890625, 47.33258743286133]
ws:[7.474225480668247, 6.998432159423828]
memory len:10000
memory used:2873.0
now epsilon is 0.1871285427915446, the reward is 247.25 with loss [36.40822505950928, 26.433603286743164] in episode 1199
Report: 
rewardSum:247.25
loss:[36.40822505950928, 26.433603286743164]
policies:[1, 2, 1]
qAverage:[44.69373321533203, 36.043264389038086]
ws:[9.403639435768127, 7.467020511627197]
memory len:10000
memory used:2886.0
now epsilon is 0.1869414844102618, the reward is 247.25 with loss [32.81319761276245, 20.970269203186035] in episode 1200
Report: 
rewardSum:247.25
loss:[32.81319761276245, 20.970269203186035]
policies:[2, 2, 0]
qAverage:[62.30654602050781, 39.12098083496094]
ws:[7.745794594287872, 6.541028618812561]
memory len:10000
memory used:2886.0
now epsilon is 0.1867546130172251, the reward is 247.25 with loss [26.641820430755615, 24.062307357788086] in episode 1201
Report: 
rewardSum:247.25
loss:[26.641820430755615, 24.062307357788086]
policies:[4, 0, 0]
qAverage:[96.00989786783855, 0.0]
ws:[11.845094045003256, 9.433607737223307]
memory len:10000
memory used:2886.0
now epsilon is 0.18642803745776867, the reward is 244.25 with loss [43.78411555290222, 53.57405233383179] in episode 1202
Report: 
rewardSum:244.25
loss:[43.78411555290222, 53.57405233383179]
policies:[4, 1, 2]
qAverage:[74.28977661132812, 22.60973663330078]
ws:[8.843473339080811, 7.603645896911621]
memory len:10000
memory used:2874.0
now epsilon is 0.18624167931917396, the reward is 247.25 with loss [31.24629831314087, 17.373424291610718] in episode 1203
Report: 
rewardSum:247.25
loss:[31.24629831314087, 17.373424291610718]
policies:[0, 1, 3]
qAverage:[0.0, 55.404197692871094]
ws:[3.9388771057128906, 4.8277201652526855]
memory len:10000
memory used:2874.0
now epsilon is 0.1860555074688452, the reward is 247.25 with loss [39.14733409881592, 24.465023040771484] in episode 1204
Report: 
rewardSum:247.25
loss:[39.14733409881592, 24.465023040771484]
policies:[1, 3, 0]
qAverage:[45.48767852783203, 47.96732521057129]
ws:[7.902587354183197, 7.252091288566589]
memory len:10000
memory used:2874.0
now epsilon is 0.18577659857654877, the reward is 245.25 with loss [37.95596122741699, 41.76759457588196] in episode 1205
Report: 
rewardSum:245.25
loss:[37.95596122741699, 41.76759457588196]
policies:[2, 2, 2]
qAverage:[84.62090301513672, 17.366239547729492]
ws:[9.324388921260834, 6.87316107749939]
memory len:10000
memory used:2875.0
now epsilon is 0.18559089163258638, the reward is 247.25 with loss [26.661882877349854, 27.13916277885437] in episode 1206
Report: 
rewardSum:247.25
loss:[26.661882877349854, 27.13916277885437]
policies:[3, 0, 1]
qAverage:[105.86931991577148, 0.0]
ws:[12.436567544937134, 8.32408881187439]
memory len:10000
memory used:2874.0
now epsilon is 0.18540537032593946, the reward is 247.25 with loss [18.780970573425293, 16.815810203552246] in episode 1207
Report: 
rewardSum:247.25
loss:[18.780970573425293, 16.815810203552246]
policies:[1, 2, 1]
qAverage:[61.30393981933594, 0.0]
ws:[7.465429306030273, 6.908385753631592]
memory len:10000
memory used:2874.0
now epsilon is 0.18512743603005696, the reward is 245.25 with loss [35.88002252578735, 31.179237365722656] in episode 1208
Report: 
rewardSum:245.25
loss:[35.88002252578735, 31.179237365722656]
policies:[4, 1, 1]
qAverage:[103.92072296142578, 0.0]
ws:[11.705641078948975, 9.46929874420166]
memory len:10000
memory used:2874.0
now epsilon is 0.1849423780052457, the reward is 247.25 with loss [30.57408618927002, 34.78765296936035] in episode 1209
Report: 
rewardSum:247.25
loss:[30.57408618927002, 34.78765296936035]
policies:[1, 2, 1]
qAverage:[45.631900787353516, 40.1171875]
ws:[10.690118938684464, 7.9796943962574005]
memory len:10000
memory used:2874.0
now epsilon is 0.18475750496907406, the reward is 247.25 with loss [31.78612232208252, 24.30105447769165] in episode 1210
Report: 
rewardSum:247.25
loss:[31.78612232208252, 24.30105447769165]
policies:[2, 1, 1]
qAverage:[43.70242818196615, 43.90504455566406]
ws:[2.572540203730265, 2.711601654688517]
memory len:10000
memory used:2874.0
now epsilon is 0.18438831312315734, the reward is 33.15999999999997 with loss [61.218419551849365, 54.88182544708252] in episode 1211
Report: 
rewardSum:33.15999999999997
loss:[61.218419551849365, 54.88182544708252]
policies:[0, 5, 3]
qAverage:[0.0, 85.8773078918457]
ws:[5.975716253121694, 7.160844092257321]
memory len:10000
memory used:2874.0
now epsilon is 0.18420399394412806, the reward is 247.25 with loss [37.64761686325073, 27.852739810943604] in episode 1212
Report: 
rewardSum:247.25
loss:[37.64761686325073, 27.852739810943604]
policies:[1, 3, 0]
qAverage:[40.20421142578125, 65.00481567382812]
ws:[10.725868940353394, 9.436872625350953]
memory len:10000
memory used:2874.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21*		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.18401985901516965, the reward is 247.25 with loss [35.4597282409668, 21.527499437332153] in episode 1213
Report: 
rewardSum:247.25
loss:[35.4597282409668, 21.527499437332153]
policies:[1, 3, 0]
qAverage:[39.14749450683594, 60.14342498779297]
ws:[11.99013376235962, 10.51635913848877]
memory len:10000
memory used:2874.0
now epsilon is 0.1838359081521011, the reward is 247.25 with loss [31.23512601852417, 25.215694427490234] in episode 1214
Report: 
rewardSum:247.25
loss:[31.23512601852417, 25.215694427490234]
policies:[1, 3, 0]
qAverage:[39.35606689453125, 63.755307006835935]
ws:[12.10451307296753, 10.354776954650879]
memory len:10000
memory used:2873.0
now epsilon is 0.18365214117092557, the reward is 37.15999999999997 with loss [22.923426151275635, 21.622345685958862] in episode 1215
Report: 
rewardSum:37.15999999999997
loss:[22.923426151275635, 21.622345685958862]
policies:[2, 1, 1]
qAverage:[74.08549118041992, 19.974136352539062]
ws:[6.405637621879578, 6.1095863580703735]
memory len:10000
memory used:2874.0
now epsilon is 0.18346855788783006, the reward is 247.25 with loss [18.755285263061523, 19.269636869430542] in episode 1216
Report: 
rewardSum:247.25
loss:[18.755285263061523, 19.269636869430542]
policies:[3, 0, 1]
qAverage:[124.09880065917969, 0.0]
ws:[12.664082288742065, 9.42074704170227]
memory len:10000
memory used:2874.0
now epsilon is 0.18328515811918542, the reward is 247.25 with loss [19.170258045196533, 20.58169460296631] in episode 1217
Report: 
rewardSum:247.25
loss:[19.170258045196533, 20.58169460296631]
policies:[1, 3, 0]
qAverage:[41.107278442382814, 62.87872314453125]
ws:[10.018919801712036, 8.077287626266479]
memory len:10000
memory used:2874.0
now epsilon is 0.18310194168154592, the reward is 247.25 with loss [24.795493364334106, 29.422578811645508] in episode 1218
Report: 
rewardSum:247.25
loss:[24.795493364334106, 29.422578811645508]
policies:[1, 2, 1]
qAverage:[49.87196350097656, 51.21419334411621]
ws:[13.34377932548523, 11.023799538612366]
memory len:10000
memory used:2873.0
now epsilon is 0.1829189083916494, the reward is 247.25 with loss [28.730809688568115, 24.679003477096558] in episode 1219
Report: 
rewardSum:247.25
loss:[28.730809688568115, 24.679003477096558]
policies:[3, 1, 0]
qAverage:[65.70395406087239, 34.83661905924479]
ws:[17.416758219401043, 13.905285199483236]
memory len:10000
memory used:2874.0
now epsilon is 0.18264470145838715, the reward is 245.25 with loss [31.706912517547607, 36.00249147415161] in episode 1220
Report: 
rewardSum:245.25
loss:[31.706912517547607, 36.00249147415161]
policies:[2, 3, 1]
qAverage:[39.16998291015625, 70.64657897949219]
ws:[14.414724731445313, 12.751378440856934]
memory len:10000
memory used:2874.0
now epsilon is 0.18246212523727726, the reward is 247.25 with loss [35.3241662979126, 25.3787260055542] in episode 1221
Report: 
rewardSum:247.25
loss:[35.3241662979126, 25.3787260055542]
policies:[1, 1, 2]
qAverage:[70.36288452148438, 44.76102193196615]
ws:[15.565460840861002, 11.323410034179688]
memory len:10000
memory used:2874.0
now epsilon is 0.18223416159105282, the reward is 246.25 with loss [29.847196102142334, 31.712307929992676] in episode 1222
Report: 
rewardSum:246.25
loss:[29.847196102142334, 31.712307929992676]
policies:[2, 2, 1]
qAverage:[67.3031514485677, 31.488983154296875]
ws:[14.27819554011027, 10.727237861603498]
memory len:10000
memory used:2874.0
now epsilon is 0.18205199575588346, the reward is 37.15999999999997 with loss [24.527340412139893, 21.668269157409668] in episode 1223
Report: 
rewardSum:37.15999999999997
loss:[24.527340412139893, 21.668269157409668]
policies:[0, 3, 1]
qAverage:[0.0, 75.58334604899089]
ws:[2.56480081876119, 3.972137371699015]
memory len:10000
memory used:2874.0
now epsilon is 0.1816882101961186, the reward is 243.25 with loss [35.349419832229614, 46.390860080718994] in episode 1224
Report: 
rewardSum:243.25
loss:[35.349419832229614, 46.390860080718994]
policies:[1, 5, 2]
qAverage:[29.225287301199778, 78.0985107421875]
ws:[10.843794856752668, 10.151403631482806]
memory len:10000
memory used:2874.0
now epsilon is 0.18150659010764653, the reward is 247.25 with loss [35.4402551651001, 25.498481273651123] in episode 1225
Report: 
rewardSum:247.25
loss:[35.4402551651001, 25.498481273651123]
policies:[1, 3, 0]
qAverage:[0.0, 78.00794982910156]
ws:[3.948714017868042, 4.856566588083903]
memory len:10000
memory used:2874.0
now epsilon is 0.18132515157116674, the reward is 247.25 with loss [27.44286060333252, 27.02192783355713] in episode 1226
Report: 
rewardSum:247.25
loss:[27.44286060333252, 27.02192783355713]
policies:[1, 3, 0]
qAverage:[0.0, 76.3601582845052]
ws:[3.7845640977223716, 4.253006935119629]
memory len:10000
memory used:2875.0
now epsilon is 0.18114389440519535, the reward is 247.25 with loss [28.283358573913574, 26.061152696609497] in episode 1227
Report: 
rewardSum:247.25
loss:[28.283358573913574, 26.061152696609497]
policies:[1, 3, 0]
qAverage:[40.52461242675781, 67.65660095214844]
ws:[9.57917034626007, 8.656814002990723]
memory len:10000
memory used:2875.0
now epsilon is 0.18087234832939172, the reward is 245.25 with loss [39.78329944610596, 24.923433303833008] in episode 1228
Report: 
rewardSum:245.25
loss:[39.78329944610596, 24.923433303833008]
policies:[1, 3, 2]
qAverage:[68.77064514160156, 46.06773376464844]
ws:[16.25183169047038, 13.118647893269857]
memory len:10000
memory used:2875.0
now epsilon is 0.18069154379688915, the reward is 247.25 with loss [29.509169816970825, 15.112408757209778] in episode 1229
Report: 
rewardSum:247.25
loss:[29.509169816970825, 15.112408757209778]
policies:[1, 3, 0]
qAverage:[42.574832153320315, 69.00449981689454]
ws:[11.748205614089965, 10.153679943084716]
memory len:10000
memory used:2875.0
now epsilon is 0.18051092000112867, the reward is 247.25 with loss [21.157389998435974, 24.52141046524048] in episode 1230
Report: 
rewardSum:247.25
loss:[21.157389998435974, 24.52141046524048]
policies:[2, 2, 0]
qAverage:[51.98760223388672, 56.46346092224121]
ws:[12.784815788269043, 10.280661761760712]
memory len:10000
memory used:2875.0
now epsilon is 0.18033047676144132, the reward is 247.25 with loss [28.434304237365723, 34.123470306396484] in episode 1231
Report: 
rewardSum:247.25
loss:[28.434304237365723, 34.123470306396484]
policies:[1, 2, 1]
qAverage:[54.011531829833984, 64.19525527954102]
ws:[13.605658411979675, 11.664979577064514]
memory len:10000
memory used:2875.0
now epsilon is 0.1801502138973387, the reward is 247.25 with loss [17.514334440231323, 39.26923847198486] in episode 1232
Report: 
rewardSum:247.25
loss:[17.514334440231323, 39.26923847198486]
policies:[0, 4, 0]
qAverage:[0.0, 94.60371780395508]
ws:[4.229510009288788, 5.410718560218811]
memory len:10000
memory used:2875.0
now epsilon is 0.17997013122851294, the reward is 247.25 with loss [13.129489660263062, 27.36210060119629] in episode 1233
Report: 
rewardSum:247.25
loss:[13.129489660263062, 27.36210060119629]
policies:[1, 3, 0]
qAverage:[44.90854187011719, 76.50818328857422]
ws:[12.996982288360595, 12.494758796691894]
memory len:10000
memory used:2875.0
now epsilon is 0.17979022857483623, the reward is 247.25 with loss [22.43723773956299, 22.752902030944824] in episode 1234
Report: 
rewardSum:247.25
loss:[22.43723773956299, 22.752902030944824]
policies:[1, 3, 0]
qAverage:[47.84546813964844, 76.13349151611328]
ws:[13.239784336090088, 12.153658199310303]
memory len:10000
memory used:2875.0
now epsilon is 0.17961050575636092, the reward is 247.25 with loss [22.559293746948242, 28.178091526031494] in episode 1235
Report: 
rewardSum:247.25
loss:[22.559293746948242, 28.178091526031494]
policies:[1, 3, 0]
qAverage:[44.86343078613281, 75.9445571899414]
ws:[15.025941848754883, 14.435015869140624]
memory len:10000
memory used:2875.0
############# STATE ###############
0-		10-		20-		30*		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.17934125832645778, the reward is 245.25 with loss [32.32441973686218, 31.390233039855957] in episode 1236
Report: 
rewardSum:245.25
loss:[32.32441973686218, 31.390233039855957]
policies:[1, 3, 2]
qAverage:[57.90409469604492, 57.875160217285156]
ws:[13.525919377803802, 12.096271395683289]
memory len:10000
memory used:2875.0
now epsilon is 0.1791619843098951, the reward is 247.25 with loss [24.254456043243408, 17.3807110786438] in episode 1237
Report: 
rewardSum:247.25
loss:[24.254456043243408, 17.3807110786438]
policies:[1, 3, 0]
qAverage:[47.47657775878906, 75.31805877685547]
ws:[11.581667768955231, 11.883276510238648]
memory len:10000
memory used:2875.0
now epsilon is 0.1789828895001324, the reward is 247.25 with loss [33.123459458351135, 30.78546380996704] in episode 1238
Report: 
rewardSum:247.25
loss:[33.123459458351135, 30.78546380996704]
policies:[1, 3, 0]
qAverage:[44.97652893066406, 75.63921813964843]
ws:[14.223586797714233, 14.790867614746094]
memory len:10000
memory used:2875.0
now epsilon is 0.1788039737180301, the reward is 247.25 with loss [18.38406467437744, 17.143862009048462] in episode 1239
Report: 
rewardSum:247.25
loss:[18.38406467437744, 17.143862009048462]
policies:[1, 3, 0]
qAverage:[47.35018615722656, 76.87215576171874]
ws:[15.929457330703736, 16.475472927093506]
memory len:10000
memory used:2875.0
now epsilon is 0.17871458290641945, the reward is -1.0 with loss [3.142443060874939, 19.08299446105957] in episode 1240
Report: 
rewardSum:-1.0
loss:[3.142443060874939, 19.08299446105957]
policies:[0, 1, 1]
qAverage:[0.0, 51.415771484375]
ws:[1.6195770502090454, 2.9969513416290283]
memory len:10000
memory used:2875.0
now epsilon is 0.1785359353303127, the reward is 37.15999999999997 with loss [28.977019786834717, 22.341213703155518] in episode 1241
Report: 
rewardSum:37.15999999999997
loss:[28.977019786834717, 22.341213703155518]
policies:[0, 3, 1]
qAverage:[0.0, 93.76058387756348]
ws:[5.89550244808197, 8.415996074676514]
memory len:10000
memory used:2875.0
now epsilon is 0.17835746633480035, the reward is 247.25 with loss [22.025453329086304, 17.924680471420288] in episode 1242
Report: 
rewardSum:247.25
loss:[22.025453329086304, 17.924680471420288]
policies:[1, 2, 1]
qAverage:[56.917083740234375, 57.39741325378418]
ws:[14.08236289024353, 12.224365711212158]
memory len:10000
memory used:2875.0
now epsilon is 0.17817917574136877, the reward is 247.25 with loss [29.021275520324707, 22.32372808456421] in episode 1243
Report: 
rewardSum:247.25
loss:[29.021275520324707, 22.32372808456421]
policies:[1, 3, 0]
qAverage:[46.19779968261719, 77.33444519042969]
ws:[12.27942519634962, 11.422847843170166]
memory len:10000
memory used:2875.0
now epsilon is 0.17791207396506348, the reward is 245.25 with loss [40.21786022186279, 35.182743072509766] in episode 1244
Report: 
rewardSum:245.25
loss:[40.21786022186279, 35.182743072509766]
policies:[1, 4, 1]
qAverage:[41.93144734700521, 87.43046951293945]
ws:[9.73601096868515, 9.432586431503296]
memory len:10000
memory used:2875.0
now epsilon is 0.17773422859700738, the reward is 247.25 with loss [24.19915771484375, 22.6808500289917] in episode 1245
Report: 
rewardSum:247.25
loss:[24.19915771484375, 22.6808500289917]
policies:[2, 2, 0]
qAverage:[0.0, 90.54573567708333]
ws:[2.426305413246155, 4.218485116958618]
memory len:10000
memory used:2875.0
now epsilon is 0.1775565610076384, the reward is 247.25 with loss [26.87284755706787, 28.47873544692993] in episode 1246
Report: 
rewardSum:247.25
loss:[26.87284755706787, 28.47873544692993]
policies:[1, 3, 0]
qAverage:[48.6655029296875, 80.30231628417968]
ws:[15.979778575897218, 14.70752773284912]
memory len:10000
memory used:2874.0
now epsilon is 0.17746779382441966, the reward is -1.0 with loss [9.745603799819946, 9.117071151733398] in episode 1247
Report: 
rewardSum:-1.0
loss:[9.745603799819946, 9.117071151733398]
policies:[0, 1, 1]
qAverage:[0.0, 57.173397064208984]
ws:[2.3613414764404297, 3.4663469791412354]
memory len:10000
memory used:2875.0
now epsilon is 0.17729039256992687, the reward is 247.25 with loss [34.16419267654419, 25.850959062576294] in episode 1248
Report: 
rewardSum:247.25
loss:[34.16419267654419, 25.850959062576294]
policies:[1, 3, 0]
qAverage:[45.74217834472656, 78.52526550292968]
ws:[14.999714374542236, 13.690015029907226]
memory len:10000
memory used:2875.0
now epsilon is 0.17715745801467792, the reward is -2.0 with loss [33.723854541778564, 12.870576620101929] in episode 1249
Report: 
rewardSum:-2.0
loss:[33.723854541778564, 12.870576620101929]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:2875.0
now epsilon is 0.1769803669796384, the reward is 247.25 with loss [25.595332145690918, 28.432588577270508] in episode 1250
Report: 
rewardSum:247.25
loss:[25.595332145690918, 28.432588577270508]
policies:[1, 2, 1]
qAverage:[56.756839752197266, 66.24227523803711]
ws:[14.180188834667206, 13.103998184204102]
memory len:10000
memory used:2875.0
now epsilon is 0.176715062292967, the reward is 245.25 with loss [44.20761585235596, 45.65821099281311] in episode 1251
Report: 
rewardSum:245.25
loss:[44.20761585235596, 45.65821099281311]
policies:[1, 4, 1]
qAverage:[36.68343607584635, 89.69624328613281]
ws:[12.710358500480652, 11.704373583197594]
memory len:10000
memory used:2875.0
now epsilon is 0.17653841348777838, the reward is 247.25 with loss [23.36355972290039, 28.44446563720703] in episode 1252
Report: 
rewardSum:247.25
loss:[23.36355972290039, 28.44446563720703]
policies:[1, 3, 0]
qAverage:[47.36993408203125, 81.22514495849609]
ws:[14.306199741363525, 11.7000892162323]
memory len:10000
memory used:2874.0
now epsilon is 0.17645015531468536, the reward is -1.0 with loss [19.967555046081543, 19.820338249206543] in episode 1253
Report: 
rewardSum:-1.0
loss:[19.967555046081543, 19.820338249206543]
policies:[1, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:2874.0
now epsilon is 0.1762737713171515, the reward is 247.25 with loss [17.693231105804443, 22.10166573524475] in episode 1254
Report: 
rewardSum:247.25
loss:[17.693231105804443, 22.10166573524475]
policies:[2, 2, 0]
qAverage:[75.18148295084636, 36.940948486328125]
ws:[18.296542803446453, 14.633811950683594]
memory len:10000
memory used:2874.0
now epsilon is 0.1760535392465728, the reward is 246.25 with loss [19.694376707077026, 27.242398023605347] in episode 1255
Report: 
rewardSum:246.25
loss:[19.694376707077026, 27.242398023605347]
policies:[1, 3, 1]
qAverage:[58.3062744140625, 73.80964660644531]
ws:[11.704488098621368, 10.192655086517334]
memory len:10000
memory used:2874.0
now epsilon is 0.1758775517164008, the reward is 37.15999999999997 with loss [32.71183252334595, 29.460135459899902] in episode 1256
Report: 
rewardSum:37.15999999999997
loss:[32.71183252334595, 29.460135459899902]
policies:[0, 1, 3]
qAverage:[0.0, 65.40736389160156]
ws:[6.06265115737915, 7.092214107513428]
memory len:10000
memory used:2874.0
now epsilon is 0.17570174010777467, the reward is 247.25 with loss [31.892948150634766, 27.340201377868652] in episode 1257
Report: 
rewardSum:247.25
loss:[31.892948150634766, 27.340201377868652]
policies:[1, 3, 0]
qAverage:[48.99381713867187, 88.56974792480469]
ws:[17.479860687255858, 16.49028491973877]
memory len:10000
memory used:2874.0
now epsilon is 0.1754383521630979, the reward is 245.25 with loss [35.082701444625854, 35.84313631057739] in episode 1258
Report: 
rewardSum:245.25
loss:[35.082701444625854, 35.84313631057739]
policies:[1, 4, 1]
qAverage:[0.0, 110.60832595825195]
ws:[6.947699785232544, 7.6777565479278564]
memory len:10000
memory used:2874.0
now epsilon is 0.17526297958935266, the reward is 247.25 with loss [16.426360607147217, 16.126537680625916] in episode 1259
Report: 
rewardSum:247.25
loss:[16.426360607147217, 16.126537680625916]
policies:[0, 4, 0]
qAverage:[0.0, 106.97574234008789]
ws:[5.383388340473175, 6.517858266830444]
memory len:10000
memory used:2874.0
now epsilon is 0.17508778232242742, the reward is 37.15999999999997 with loss [26.522720336914062, 26.576762199401855] in episode 1260
Report: 
rewardSum:37.15999999999997
loss:[26.522720336914062, 26.576762199401855]
policies:[0, 3, 1]
qAverage:[0.0, 73.52930450439453]
ws:[4.295188903808594, 5.292903423309326]
memory len:10000
memory used:2874.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24*		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.17491276018708107, the reward is 247.25 with loss [18.269317269325256, 25.880157947540283] in episode 1261
Report: 
rewardSum:247.25
loss:[18.269317269325256, 25.880157947540283]
policies:[1, 2, 1]
qAverage:[0.0, 86.81250508626302]
ws:[4.399601459503174, 6.206322034200032]
memory len:10000
memory used:2874.0
now epsilon is 0.1747379130082477, the reward is -3.0 with loss [27.844171047210693, 21.063852548599243] in episode 1262
Report: 
rewardSum:-3.0
loss:[27.844171047210693, 21.063852548599243]
policies:[0, 1, 3]
qAverage:[0.0, 62.510658264160156]
ws:[0.6324101090431213, 1.9061769247055054]
memory len:10000
memory used:2874.0
now epsilon is 0.17447596990093342, the reward is 245.25 with loss [38.45211863517761, 38.359294414520264] in episode 1263
Report: 
rewardSum:245.25
loss:[38.45211863517761, 38.359294414520264]
policies:[2, 3, 1]
qAverage:[45.82491760253906, 88.53455810546875]
ws:[13.393012619018554, 13.172817039489747]
memory len:10000
memory used:2874.0
now epsilon is 0.17421441946279032, the reward is 245.25 with loss [35.4246289730072, 45.08297681808472] in episode 1264
Report: 
rewardSum:245.25
loss:[35.4246289730072, 45.08297681808472]
policies:[0, 4, 2]
qAverage:[0.0, 121.14869232177735]
ws:[6.50498251914978, 7.9616447448730465]
memory len:10000
memory used:2875.0
now epsilon is 0.1740402703628471, the reward is 247.25 with loss [29.82407808303833, 30.361051559448242] in episode 1265
Report: 
rewardSum:247.25
loss:[29.82407808303833, 30.361051559448242]
policies:[1, 3, 0]
qAverage:[56.82251739501953, 77.07586288452148]
ws:[14.643470883369446, 13.091135501861572]
memory len:10000
memory used:2875.0
now epsilon is 0.17382282877287217, the reward is 246.25 with loss [39.24125623703003, 33.25713634490967] in episode 1266
Report: 
rewardSum:246.25
loss:[39.24125623703003, 33.25713634490967]
policies:[1, 3, 1]
qAverage:[54.8280029296875, 68.10696983337402]
ws:[16.194147050380707, 13.391000747680664]
memory len:10000
memory used:2875.0
now epsilon is 0.17347548715322936, the reward is 243.25 with loss [42.65400218963623, 53.43297362327576] in episode 1267
Report: 
rewardSum:243.25
loss:[42.65400218963623, 53.43297362327576]
policies:[1, 4, 3]
qAverage:[31.102761840820314, 86.23731384277343]
ws:[4.123364400863648, 4.449805068969726]
memory len:10000
memory used:2874.0
now epsilon is 0.17330207670854228, the reward is 247.25 with loss [22.707257986068726, 18.428529024124146] in episode 1268
Report: 
rewardSum:247.25
loss:[22.707257986068726, 18.428529024124146]
policies:[2, 2, 0]
qAverage:[55.613319396972656, 69.33269119262695]
ws:[12.726791752502322, 10.440404534339905]
memory len:10000
memory used:2875.0
now epsilon is 0.17304228601002966, the reward is 245.25 with loss [38.22260570526123, 52.6149046421051] in episode 1269
Report: 
rewardSum:245.25
loss:[38.22260570526123, 52.6149046421051]
policies:[0, 4, 2]
qAverage:[0.0, 118.99764099121094]
ws:[2.323500859737396, 4.206860387325287]
memory len:10000
memory used:2875.0
now epsilon is 0.17286930860406244, the reward is 247.25 with loss [26.48890209197998, 41.26496410369873] in episode 1270
Report: 
rewardSum:247.25
loss:[26.48890209197998, 41.26496410369873]
policies:[1, 3, 0]
qAverage:[58.45348358154297, 74.06553649902344]
ws:[12.452659457921982, 11.453234076499939]
memory len:10000
memory used:2875.0
now epsilon is 0.17261016665212164, the reward is 245.25 with loss [48.80791759490967, 39.24638557434082] in episode 1271
Report: 
rewardSum:245.25
loss:[48.80791759490967, 39.24638557434082]
policies:[0, 2, 4]
qAverage:[0.0, 101.81936136881511]
ws:[2.9961307843526206, 5.276419639587402]
memory len:10000
memory used:2875.0
now epsilon is 0.17243762120349457, the reward is 37.15999999999997 with loss [25.04598569869995, 30.97965407371521] in episode 1272
Report: 
rewardSum:37.15999999999997
loss:[25.04598569869995, 30.97965407371521]
policies:[0, 3, 1]
qAverage:[0.0, 108.6532211303711]
ws:[2.6307040005922318, 5.454499900341034]
memory len:10000
memory used:2875.0
now epsilon is 0.1721791263780826, the reward is 245.25 with loss [38.1352915763855, 50.84137201309204] in episode 1273
Report: 
rewardSum:245.25
loss:[38.1352915763855, 50.84137201309204]
policies:[1, 4, 1]
qAverage:[37.38933563232422, 101.84256744384766]
ws:[12.171554803848267, 13.216102917989096]
memory len:10000
memory used:2875.0
now epsilon is 0.17209304757608898, the reward is -1.0 with loss [9.943670272827148, 11.824912071228027] in episode 1274
Report: 
rewardSum:-1.0
loss:[9.943670272827148, 11.824912071228027]
policies:[0, 1, 1]
qAverage:[0.0, 65.04949951171875]
ws:[-0.25312501192092896, 1.573865532875061]
memory len:10000
memory used:2875.0
now epsilon is 0.17187803879788746, the reward is 246.25 with loss [36.2038836479187, 47.308924198150635] in episode 1275
Report: 
rewardSum:246.25
loss:[36.2038836479187, 47.308924198150635]
policies:[1, 3, 1]
qAverage:[44.887454223632815, 87.18677368164063]
ws:[10.673350858688355, 11.497799587249755]
memory len:10000
memory used:2875.0
now epsilon is 0.17179211052086593, the reward is -1.0 with loss [19.742486000061035, 10.72157907485962] in episode 1276
Report: 
rewardSum:-1.0
loss:[19.742486000061035, 10.72157907485962]
policies:[0, 1, 1]
qAverage:[0.0, 66.07791137695312]
ws:[0.7970092296600342, 2.369680643081665]
memory len:10000
memory used:2875.0
now epsilon is 0.1716203828216502, the reward is 247.25 with loss [32.829280853271484, 18.178418159484863] in episode 1277
Report: 
rewardSum:247.25
loss:[32.829280853271484, 18.178418159484863]
policies:[1, 3, 0]
qAverage:[55.46318435668945, 71.92239761352539]
ws:[15.520829916000366, 14.734604716300964]
memory len:10000
memory used:2875.0
now epsilon is 0.17144882678574655, the reward is 247.25 with loss [33.36039733886719, 26.983373165130615] in episode 1278
Report: 
rewardSum:247.25
loss:[33.36039733886719, 26.983373165130615]
policies:[1, 3, 0]
qAverage:[46.02815246582031, 91.54804077148438]
ws:[14.3481050491333, 13.860516834259034]
memory len:10000
memory used:2875.0
now epsilon is 0.171277442241556, the reward is 247.25 with loss [28.060251474380493, 21.492958068847656] in episode 1279
Report: 
rewardSum:247.25
loss:[28.060251474380493, 21.492958068847656]
policies:[2, 2, 0]
qAverage:[53.842864990234375, 74.39840698242188]
ws:[13.68045099079609, 12.252179741859436]
memory len:10000
memory used:2875.0
now epsilon is 0.17110622901765116, the reward is 247.25 with loss [21.83392333984375, 28.206807613372803] in episode 1280
Report: 
rewardSum:247.25
loss:[21.83392333984375, 28.206807613372803]
policies:[1, 3, 0]
qAverage:[44.54599609375, 92.23169555664063]
ws:[11.810631275177002, 10.639790296554565]
memory len:10000
memory used:2874.0
now epsilon is 0.17102068659728165, the reward is -1.0 with loss [15.051599025726318, 20.9522762298584] in episode 1281
Report: 
rewardSum:-1.0
loss:[15.051599025726318, 20.9522762298584]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:2875.0
now epsilon is 0.17084973003275372, the reward is 247.25 with loss [30.376056671142578, 17.851746320724487] in episode 1282
Report: 
rewardSum:247.25
loss:[30.376056671142578, 17.851746320724487]
policies:[2, 2, 0]
qAverage:[53.4736442565918, 70.28631973266602]
ws:[15.397855043411255, 13.37171983718872]
memory len:10000
memory used:2876.0
now epsilon is 0.1706789443606923, the reward is 247.25 with loss [24.070496559143066, 29.940950393676758] in episode 1283
Report: 
rewardSum:247.25
loss:[24.070496559143066, 29.940950393676758]
policies:[1, 2, 1]
qAverage:[52.76178741455078, 77.8608627319336]
ws:[19.633090376853943, 17.752057790756226]
memory len:10000
memory used:2876.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25*		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.17050832941026903, the reward is 247.25 with loss [30.608237266540527, 34.23517370223999] in episode 1284
Report: 
rewardSum:247.25
loss:[30.608237266540527, 34.23517370223999]
policies:[1, 3, 0]
qAverage:[41.23768615722656, 90.68011169433593]
ws:[13.734353971481323, 12.574371147155762]
memory len:10000
memory used:2876.0
now epsilon is 0.17025272671443858, the reward is 245.25 with loss [27.102992296218872, 32.02361857891083] in episode 1285
Report: 
rewardSum:245.25
loss:[27.102992296218872, 32.02361857891083]
policies:[1, 4, 1]
qAverage:[33.721394856770836, 98.88890329996745]
ws:[11.116869976123175, 10.819784800211588]
memory len:10000
memory used:2875.0
now epsilon is 0.16999750718310427, the reward is 245.25 with loss [43.19194412231445, 50.43917536735535] in episode 1286
Report: 
rewardSum:245.25
loss:[43.19194412231445, 50.43917536735535]
policies:[0, 5, 1]
qAverage:[0.0, 113.14386367797852]
ws:[0.6150882542133331, 2.492019459605217]
memory len:10000
memory used:2876.0
now epsilon is 0.16982757341436222, the reward is 247.25 with loss [21.983922719955444, 32.78465700149536] in episode 1287
Report: 
rewardSum:247.25
loss:[21.983922719955444, 32.78465700149536]
policies:[1, 3, 0]
qAverage:[40.556356811523436, 90.69437561035156]
ws:[11.608894896507262, 11.085906744003296]
memory len:10000
memory used:2875.0
now epsilon is 0.16965780951567436, the reward is 247.25 with loss [19.90049386024475, 27.848095417022705] in episode 1288
Report: 
rewardSum:247.25
loss:[19.90049386024475, 27.848095417022705]
policies:[1, 3, 0]
qAverage:[37.03105773925781, 89.28729858398438]
ws:[12.746349155902863, 12.024682760238647]
memory len:10000
memory used:2875.0
now epsilon is 0.16940348180258913, the reward is 245.25 with loss [43.90746450424194, 43.37158489227295] in episode 1289
Report: 
rewardSum:245.25
loss:[43.90746450424194, 43.37158489227295]
policies:[2, 1, 3]
qAverage:[65.25076293945312, 56.2846425374349]
ws:[15.519579410552979, 12.800296147664389]
memory len:10000
memory used:2875.0
now epsilon is 0.16923414183650515, the reward is 247.25 with loss [34.30053091049194, 26.047878742218018] in episode 1290
Report: 
rewardSum:247.25
loss:[34.30053091049194, 26.047878742218018]
policies:[0, 4, 0]
qAverage:[0.0, 95.73426310221355]
ws:[0.4887715180714925, 2.067809780438741]
memory len:10000
memory used:2875.0
now epsilon is 0.1690649711468954, the reward is 37.15999999999997 with loss [37.3041934967041, 35.45918846130371] in episode 1291
Report: 
rewardSum:37.15999999999997
loss:[37.3041934967041, 35.45918846130371]
policies:[0, 3, 1]
qAverage:[0.0, 106.26545333862305]
ws:[0.3465389907360077, 2.1604975759983063]
memory len:10000
memory used:2875.0
now epsilon is 0.1688959695645468, the reward is 247.25 with loss [25.115158081054688, 29.53244161605835] in episode 1292
Report: 
rewardSum:247.25
loss:[25.115158081054688, 29.53244161605835]
policies:[0, 2, 2]
qAverage:[0.0, 94.99846903483073]
ws:[1.4821795026461284, 2.9776333967844644]
memory len:10000
memory used:2875.0
now epsilon is 0.16872713692041552, the reward is 247.25 with loss [34.94041728973389, 27.10963487625122] in episode 1293
Report: 
rewardSum:247.25
loss:[34.94041728973389, 27.10963487625122]
policies:[1, 3, 0]
qAverage:[38.7828857421875, 85.95195617675782]
ws:[8.331551849842072, 7.389465475082398]
memory len:10000
memory used:2875.0
now epsilon is 0.16851633342736527, the reward is 246.25 with loss [30.289228439331055, 39.348326206207275] in episode 1294
Report: 
rewardSum:246.25
loss:[30.289228439331055, 39.348326206207275]
policies:[1, 2, 2]
qAverage:[48.397239685058594, 69.56428527832031]
ws:[9.766548454761505, 8.330280363559723]
memory len:10000
memory used:2875.0
now epsilon is 0.16834788027703138, the reward is 247.25 with loss [25.168747901916504, 31.369948863983154] in episode 1295
Report: 
rewardSum:247.25
loss:[25.168747901916504, 31.369948863983154]
policies:[0, 4, 0]
qAverage:[0.0, 108.4897346496582]
ws:[3.745892046019435, 5.5506831407547]
memory len:10000
memory used:2875.0
now epsilon is 0.16817959551668837, the reward is 247.25 with loss [35.8438835144043, 20.82099986076355] in episode 1296
Report: 
rewardSum:247.25
loss:[35.8438835144043, 20.82099986076355]
policies:[2, 2, 0]
qAverage:[49.56748580932617, 74.30261993408203]
ws:[12.012891173362732, 11.263435125350952]
memory len:10000
memory used:2875.0
now epsilon is 0.16801147897800947, the reward is 247.25 with loss [29.998987197875977, 24.01087474822998] in episode 1297
Report: 
rewardSum:247.25
loss:[29.998987197875977, 24.01087474822998]
policies:[2, 2, 0]
qAverage:[48.439422607421875, 73.13884735107422]
ws:[11.801192045211792, 10.428201079368591]
memory len:10000
memory used:2875.0
now epsilon is 0.16780156961021286, the reward is 246.25 with loss [36.07343292236328, 44.27957582473755] in episode 1298
Report: 
rewardSum:246.25
loss:[36.07343292236328, 44.27957582473755]
policies:[2, 1, 2]
qAverage:[62.653727213541664, 49.90479532877604]
ws:[14.879126071929932, 11.319759527842203]
memory len:10000
memory used:2875.0
now epsilon is 0.16763383095570433, the reward is 247.25 with loss [18.120301246643066, 24.615010857582092] in episode 1299
Report: 
rewardSum:247.25
loss:[18.120301246643066, 24.615010857582092]
policies:[1, 3, 0]
qAverage:[37.253378295898436, 85.02195434570312]
ws:[9.666285741329194, 8.594430184364318]
memory len:10000
memory used:2874.0
now epsilon is 0.16738253731361158, the reward is 245.25 with loss [43.388197898864746, 33.92434573173523] in episode 1300
Report: 
rewardSum:245.25
loss:[43.388197898864746, 33.92434573173523]
policies:[1, 4, 1]
qAverage:[0.0, 114.31552429199219]
ws:[1.6884375780820846, 3.90829758644104]
memory len:10000
memory used:2875.0
now epsilon is 0.16671426106997916, the reward is 235.25 with loss [148.77545738220215, 102.9533417224884] in episode 1301
Report: 
rewardSum:235.25
loss:[148.77545738220215, 102.9533417224884]
policies:[5, 8, 3]
qAverage:[63.09855324881418, 81.9840066092355]
ws:[12.8115970151765, 11.675250853810992]
memory len:10000
memory used:2874.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44*		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.16460188519911859, the reward is -50.0 with loss [350.192015171051, 337.5390100479126] in episode 1302
Report: 
rewardSum:-50.0
loss:[350.192015171051, 337.5390100479126]
policies:[7, 40, 4]
qAverage:[16.220300674438477, 117.78352375030518]
ws:[6.592627838253975, 6.408717620372772]
memory len:10000
memory used:2875.0
now epsilon is 0.16398570716015418, the reward is 236.25 with loss [73.78819453716278, 87.01937556266785] in episode 1303
Report: 
rewardSum:236.25
loss:[73.78819453716278, 87.01937556266785]
policies:[2, 10, 3]
qAverage:[23.534004798302284, 94.43988741361179]
ws:[2.4456171003671794, 2.484858895723636]
memory len:10000
memory used:2875.0
now epsilon is 0.1633309927906075, the reward is 235.25 with loss [100.28036427497864, 103.09658527374268] in episode 1304
Report: 
rewardSum:235.25
loss:[100.28036427497864, 103.09658527374268]
policies:[2, 11, 3]
qAverage:[12.910245259602865, 102.07815869649251]
ws:[2.9706302247941494, 3.5219129820664725]
memory len:10000
memory used:2875.0
now epsilon is 0.163086149373196, the reward is 245.25 with loss [42.83021593093872, 40.05177307128906] in episode 1305
Report: 
rewardSum:245.25
loss:[42.83021593093872, 40.05177307128906]
policies:[1, 4, 1]
qAverage:[30.533981323242188, 73.50276184082031]
ws:[6.184986472129822, 5.86545958518982]
memory len:10000
memory used:2875.0
now epsilon is 0.1628416729914464, the reward is 245.25 with loss [36.30820441246033, 27.869786977767944] in episode 1306
Report: 
rewardSum:245.25
loss:[36.30820441246033, 27.869786977767944]
policies:[0, 4, 2]
qAverage:[0.0, 88.66297149658203]
ws:[3.6979440689086913, 5.255960321426391]
memory len:10000
memory used:2875.0
now epsilon is 0.16231323068013598, the reward is 238.25 with loss [105.18717241287231, 77.69461607933044] in episode 1307
Report: 
rewardSum:238.25
loss:[105.18717241287231, 77.69461607933044]
policies:[0, 9, 4]
qAverage:[0.0, 99.25340488978794]
ws:[1.0798060122345174, 2.082404170717512]
memory len:10000
memory used:2875.0
############# STATE ###############
0-		10*		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.16190790380519274, the reward is 241.25 with loss [53.14582732319832, 54.842933654785156] in episode 1308
Report: 
rewardSum:241.25
loss:[53.14582732319832, 54.842933654785156]
policies:[1, 7, 2]
qAverage:[17.75094223022461, 85.08698177337646]
ws:[3.031284200027585, 3.2324428111314774]
memory len:10000
memory used:2875.0
now epsilon is 0.16174605660673289, the reward is 37.15999999999997 with loss [26.0644793510437, 27.218568086624146] in episode 1309
Report: 
rewardSum:37.15999999999997
loss:[26.0644793510437, 27.218568086624146]
policies:[0, 3, 1]
qAverage:[0.0, 86.04751396179199]
ws:[0.962434858083725, 2.2771424055099487]
memory len:10000
memory used:2875.0
now epsilon is 0.15822606523876537, the reward is 163.25 with loss [535.7888574004173, 536.4696359634399] in episode 1310
Report: 
rewardSum:163.25
loss:[535.7888574004173, 536.4696359634399]
policies:[33, 47, 8]
qAverage:[48.00392294621122, 63.76380876181782]
ws:[4.63240220596123, 3.554184562553638]
memory len:10000
memory used:2875.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2*		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.15767317302188338, the reward is 237.25 with loss [75.26153445243835, 79.181480884552] in episode 1311
Report: 
rewardSum:237.25
loss:[75.26153445243835, 79.181480884552]
policies:[1, 9, 4]
qAverage:[0.0, 85.07498677571614]
ws:[0.45171359678109485, 1.1487902829216585]
memory len:10000
memory used:2876.0
now epsilon is 0.156847455050952, the reward is 230.25 with loss [140.963858127594, 139.8315725326538] in episode 1312
Report: 
rewardSum:230.25
loss:[140.963858127594, 139.8315725326538]
policies:[2, 18, 1]
qAverage:[12.503543006049263, 76.98150210910373]
ws:[1.9449395810564358, 2.114519871564375]
memory len:10000
memory used:2876.0
now epsilon is 0.15649490097807614, the reward is 242.25 with loss [54.24514675140381, 60.1548810005188] in episode 1313
Report: 
rewardSum:242.25
loss:[54.24514675140381, 60.1548810005188]
policies:[1, 7, 1]
qAverage:[15.224041530064174, 62.18425532749721]
ws:[1.475015364587307, 2.1381392500230243]
memory len:10000
memory used:2876.0
now epsilon is 0.15594805799994912, the reward is 237.25 with loss [76.62388062477112, 86.6189022064209] in episode 1314
Report: 
rewardSum:237.25
loss:[76.62388062477112, 86.6189022064209]
policies:[2, 12, 0]
qAverage:[15.262572697230748, 68.24130467006138]
ws:[2.2644345802920207, 2.1478721126914024]
memory len:10000
memory used:2876.0
now epsilon is 0.15579216841272484, the reward is 247.25 with loss [21.4726459980011, 16.059955835342407] in episode 1315
Report: 
rewardSum:247.25
loss:[21.4726459980011, 16.059955835342407]
policies:[1, 3, 0]
qAverage:[21.790765380859376, 52.41549377441406]
ws:[4.199787497520447, 3.4445435285568236]
memory len:10000
memory used:2876.0
now epsilon is 0.15439615586886807, the reward is 5.159999999999968 with loss [199.00610160827637, 242.97168385982513] in episode 1316
Report: 
rewardSum:5.159999999999968
loss:[199.00610160827637, 242.97168385982513]
policies:[23, 6, 7]
qAverage:[79.06641734730114, 15.750145998868076]
ws:[0.8386310744522647, -0.04011641535907984]
memory len:10000
memory used:2875.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42*		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.15385664657393883, the reward is 237.25 with loss [67.55691438913345, 76.62162065505981] in episode 1317
Report: 
rewardSum:237.25
loss:[67.55691438913345, 76.62162065505981]
policies:[8, 2, 4]
qAverage:[72.01080460981889, 14.44125713001598]
ws:[-0.10370214418931441, -0.48030964759263123]
memory len:10000
memory used:2875.0
now epsilon is 0.1532040619741981, the reward is 234.25 with loss [100.3401985168457, 86.12606090307236] in episode 1318
Report: 
rewardSum:234.25
loss:[100.3401985168457, 86.12606090307236]
policies:[9, 6, 2]
qAverage:[59.317884826660155, 26.14453633626302]
ws:[-0.3757884989182154, -0.7772509607175986]
memory len:10000
memory used:2875.0
now epsilon is 0.15224940366481302, the reward is 226.25 with loss [149.1136872768402, 149.8455991744995] in episode 1319
Report: 
rewardSum:226.25
loss:[149.1136872768402, 149.8455991744995]
policies:[21, 2, 2]
qAverage:[78.69916687011718, 3.7117462158203125]
ws:[1.200291790277697, -1.1719684571027755]
memory len:10000
memory used:2876.0
now epsilon is 0.1515278438600592, the reward is 232.25 with loss [116.87258070707321, 95.33678114414215] in episode 1320
Report: 
rewardSum:232.25
loss:[116.87258070707321, 95.33678114414215]
policies:[3, 12, 4]
qAverage:[8.60076904296875, 57.697845458984375]
ws:[-0.12929504026066174, 0.31990526616573334]
memory len:10000
memory used:2876.0
now epsilon is 0.1509606077705986, the reward is 236.25 with loss [96.13533234596252, 96.3993833065033] in episode 1321
Report: 
rewardSum:236.25
loss:[96.13533234596252, 96.3993833065033]
policies:[3, 10, 2]
qAverage:[7.853691101074219, 53.74002109874379]
ws:[-0.7841705497015606, 0.2722462591799823]
memory len:10000
memory used:2876.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44*		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.1502451559918299, the reward is 232.25 with loss [103.03654336929321, 103.69541454315186] in episode 1322
Report: 
rewardSum:232.25
loss:[103.03654336929321, 103.69541454315186]
policies:[6, 10, 3]
qAverage:[26.795362345377605, 38.59310404459635]
ws:[-1.7289836605389912, -1.8750646114349365]
memory len:10000
memory used:2875.0
now epsilon is 0.14986996538476513, the reward is 241.25 with loss [69.66126847267151, 74.46728134155273] in episode 1323
Report: 
rewardSum:241.25
loss:[69.66126847267151, 74.46728134155273]
policies:[7, 1, 2]
qAverage:[70.04211711883545, 0.0]
ws:[-2.6943072825670242, -4.171802267432213]
memory len:10000
memory used:2876.0
now epsilon is 0.14964530089295497, the reward is 245.25 with loss [36.45264983177185, 39.002352714538574] in episode 1324
Report: 
rewardSum:245.25
loss:[36.45264983177185, 39.002352714538574]
policies:[1, 5, 0]
qAverage:[15.81707305908203, 38.550537109375]
ws:[0.5491178929805756, 0.9530239343643189]
memory len:10000
memory used:2876.0
now epsilon is 0.14875000574271036, the reward is 227.25 with loss [124.79088687896729, 163.16282749176025] in episode 1325
Report: 
rewardSum:227.25
loss:[124.79088687896729, 163.16282749176025]
policies:[11, 11, 2]
qAverage:[34.73996935392681, 27.69957733154297]
ws:[0.30687301605939865, -0.02858828714019374]
memory len:10000
memory used:2876.0
now epsilon is 0.14819316833636106, the reward is 236.25 with loss [87.89482006430626, 76.15229284763336] in episode 1326
Report: 
rewardSum:236.25
loss:[87.89482006430626, 76.15229284763336]
policies:[2, 10, 3]
qAverage:[12.418086369832357, 42.79264863332113]
ws:[-0.7911655160132796, -0.14518450697263083]
memory len:10000
memory used:2876.0
now epsilon is 0.1479710174686502, the reward is -5.0 with loss [29.082974195480347, 24.109140634536743] in episode 1327
Report: 
rewardSum:-5.0
loss:[29.082974195480347, 24.109140634536743]
policies:[0, 4, 2]
qAverage:[0.0, 46.058258056640625]
ws:[0.6831380963325501, 1.8044118344783784]
memory len:10000
memory used:2876.0
now epsilon is 0.14774919961904387, the reward is 245.25 with loss [21.569625973701477, 35.91194677352905] in episode 1328
Report: 
rewardSum:245.25
loss:[21.569625973701477, 35.91194677352905]
policies:[1, 3, 2]
qAverage:[19.16515350341797, 28.46067523956299]
ws:[-0.4997896132990718, -0.4348948895931244]
memory len:10000
memory used:2876.0
now epsilon is 0.14719610867497793, the reward is 236.25 with loss [98.2171802520752, 75.84170365333557] in episode 1329
Report: 
rewardSum:236.25
loss:[98.2171802520752, 75.84170365333557]
policies:[8, 6, 1]
qAverage:[27.216604232788086, 26.764211018880207]
ws:[0.14956310018897057, 0.1386459917606165]
memory len:10000
memory used:2876.0
############# STATE ###############
0*		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.1461692058994785, the reward is 223.25 with loss [162.2229917049408, 149.8930259346962] in episode 1330
Report: 
rewardSum:223.25
loss:[162.2229917049408, 149.8930259346962]
policies:[6, 19, 3]
qAverage:[8.571734221085258, 42.04371991364852]
ws:[-1.2760987232601189, -1.0370240166135456]
memory len:10000
memory used:2877.0
now epsilon is 0.1458406538751428, the reward is 242.25 with loss [56.023314237594604, 62.06683397293091] in episode 1331
Report: 
rewardSum:242.25
loss:[56.023314237594604, 62.06683397293091]
policies:[5, 2, 2]
qAverage:[36.70841979980469, 14.666491372244698]
ws:[-3.878483845187085, -5.204919134931905]
memory len:10000
memory used:2876.0
now epsilon is 0.14547646214396234, the reward is 241.25 with loss [64.45864629745483, 60.246906757354736] in episode 1332
Report: 
rewardSum:241.25
loss:[64.45864629745483, 60.246906757354736]
policies:[3, 3, 4]
qAverage:[12.42383270263672, 27.60995864868164]
ws:[0.1648324489593506, 0.09824585914611816]
memory len:10000
memory used:2876.0
now epsilon is 0.14525838378947678, the reward is 245.25 with loss [39.40613079071045, 33.747708797454834] in episode 1333
Report: 
rewardSum:245.25
loss:[39.40613079071045, 33.747708797454834]
policies:[5, 1, 0]
qAverage:[39.257405598958336, 7.708445866902669]
ws:[-1.0222128356496494, -1.7623800138632457]
memory len:10000
memory used:2876.0
now epsilon is 0.14500437219005563, the reward is 244.25 with loss [43.24349546432495, 33.585987627506256] in episode 1334
Report: 
rewardSum:244.25
loss:[43.24349546432495, 33.585987627506256]
policies:[3, 4, 0]
qAverage:[17.2921142578125, 26.036496298653738]
ws:[-2.652915194630623, -2.5491989254951477]
memory len:10000
memory used:2876.0
now epsilon is 0.14453381422916892, the reward is 238.25 with loss [67.49280285835266, 77.82070884108543] in episode 1335
Report: 
rewardSum:238.25
loss:[67.49280285835266, 77.82070884108543]
policies:[2, 9, 2]
qAverage:[5.684603604403409, 37.069853349165484]
ws:[-1.3400912975723094, -0.48535567928444256]
memory len:10000
memory used:2876.0
now epsilon is 0.14446155635541771, the reward is -1.0 with loss [13.296160697937012, 16.197731018066406] in episode 1336
Report: 
rewardSum:-1.0
loss:[13.296160697937012, 16.197731018066406]
policies:[0, 1, 1]
qAverage:[0.0, 22.605037689208984]
ws:[-0.06718570739030838, 0.09571661055088043]
memory len:10000
memory used:2876.0
now epsilon is 0.1442089381586058, the reward is 244.25 with loss [33.06822091341019, 46.62297487258911] in episode 1337
Report: 
rewardSum:244.25
loss:[33.06822091341019, 46.62297487258911]
policies:[2, 4, 1]
qAverage:[11.378643798828126, 26.972393798828126]
ws:[-1.6750699281692505, -1.31400386095047]
memory len:10000
memory used:2876.0
now epsilon is 0.1439927599021906, the reward is 245.25 with loss [33.64418601989746, 44.399484634399414] in episode 1338
Report: 
rewardSum:245.25
loss:[33.64418601989746, 44.399484634399414]
policies:[2, 4, 0]
qAverage:[16.94888414655413, 26.133529663085938]
ws:[-2.7436061464250088, -2.6527491126741682]
memory len:10000
memory used:2876.0
now epsilon is 0.14377690571056043, the reward is 245.25 with loss [45.806246280670166, 43.738346576690674] in episode 1339
Report: 
rewardSum:245.25
loss:[45.806246280670166, 43.738346576690674]
policies:[2, 4, 0]
qAverage:[13.581130981445312, 20.64890956878662]
ws:[-0.15020126104354858, 0.10113835334777832]
memory len:10000
memory used:2876.0
now epsilon is 0.14356137509792188, the reward is 245.25 with loss [41.96176540851593, 32.365517377853394] in episode 1340
Report: 
rewardSum:245.25
loss:[41.96176540851593, 32.365517377853394]
policies:[1, 4, 1]
qAverage:[11.516157531738282, 25.116272735595704]
ws:[-2.935910499095917, -2.88338690251112]
memory len:10000
memory used:2876.0
now epsilon is 0.14334616757920965, the reward is 245.25 with loss [32.07939875125885, 31.855427742004395] in episode 1341
Report: 
rewardSum:245.25
loss:[32.07939875125885, 31.855427742004395]
policies:[4, 2, 0]
qAverage:[21.32927703857422, 16.061400604248046]
ws:[-4.792390236258507, -4.817324829101563]
memory len:10000
memory used:2876.0
now epsilon is 0.14313128267008568, the reward is 245.25 with loss [27.424322366714478, 28.130114793777466] in episode 1342
Report: 
rewardSum:245.25
loss:[27.424322366714478, 28.130114793777466]
policies:[1, 5, 0]
qAverage:[0.0, 35.13148816426595]
ws:[-1.9752054810523987, -0.8256551337738832]
memory len:10000
memory used:2876.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41*		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.14288099070696622, the reward is 244.25 with loss [28.222474336624146, 35.21388602256775] in episode 1343
Report: 
rewardSum:244.25
loss:[28.222474336624146, 35.21388602256775]
policies:[1, 5, 1]
qAverage:[8.013150351388115, 29.428936549595424]
ws:[-0.5465145877429417, 0.5201843466077533]
memory len:10000
memory used:2876.0
now epsilon is 0.14259547864230426, the reward is 243.25 with loss [60.47887992858887, 46.85343861579895] in episode 1344
Report: 
rewardSum:243.25
loss:[60.47887992858887, 46.85343861579895]
policies:[3, 4, 1]
qAverage:[15.480699266706194, 23.256079537527903]
ws:[-2.2904060690530708, -2.2487617124404227]
memory len:10000
memory used:2876.0
now epsilon is 0.1423817190630493, the reward is 245.25 with loss [41.44561696052551, 30.495840787887573] in episode 1345
Report: 
rewardSum:245.25
loss:[41.44561696052551, 30.495840787887573]
policies:[1, 4, 1]
qAverage:[9.258200963338217, 27.41794204711914]
ws:[-2.9574922571579614, -2.6802861417333284]
memory len:10000
memory used:2876.0
now epsilon is 0.14216827992283046, the reward is 245.25 with loss [31.85818886756897, 36.74630951881409] in episode 1346
Report: 
rewardSum:245.25
loss:[31.85818886756897, 36.74630951881409]
policies:[1, 3, 2]
qAverage:[11.081920623779297, 24.67272186279297]
ws:[-2.6542235970497132, -2.293558830022812]
memory len:10000
memory used:2876.0
now epsilon is 0.1419551607412894, the reward is 245.25 with loss [27.12790822982788, 42.61988139152527] in episode 1347
Report: 
rewardSum:245.25
loss:[27.12790822982788, 42.61988139152527]
policies:[1, 5, 0]
qAverage:[6.945839473179409, 26.544592721121653]
ws:[-1.885450532393796, -1.6349487943308694]
memory len:10000
memory used:2876.0
now epsilon is 0.1417069254485283, the reward is 244.25 with loss [49.41144394874573, 34.10749292373657] in episode 1348
Report: 
rewardSum:244.25
loss:[49.41144394874573, 34.10749292373657]
policies:[2, 3, 2]
qAverage:[20.3956657409668, 14.712499237060547]
ws:[-5.841926026344299, -5.829740976728499]
memory len:10000
memory used:2876.0
now epsilon is 0.1412117563621927, the reward is 237.25 with loss [77.48169469833374, 78.40231418609619] in episode 1349
Report: 
rewardSum:237.25
loss:[77.48169469833374, 78.40231418609619]
policies:[6, 6, 2]
qAverage:[20.288771947224934, 18.531554222106934]
ws:[-5.737806469202042, -5.398337374130885]
memory len:10000
memory used:2876.0
now epsilon is 0.14075350593068206, the reward is 238.25 with loss [64.5685710310936, 92.80847024917603] in episode 1350
Report: 
rewardSum:238.25
loss:[64.5685710310936, 92.80847024917603]
policies:[1, 10, 2]
qAverage:[4.320809682210286, 30.5766388575236]
ws:[-3.6830728178222976, -2.990374132990837]
memory len:10000
memory used:2876.0
now epsilon is 0.14050737195732463, the reward is 244.25 with loss [37.87170743942261, 35.2647665143013] in episode 1351
Report: 
rewardSum:244.25
loss:[37.87170743942261, 35.2647665143013]
policies:[1, 5, 1]
qAverage:[7.210569654192243, 26.47370801653181]
ws:[-2.592273541859218, -1.3435615982328142]
memory len:10000
memory used:2876.0
now epsilon is 0.14029674258114957, the reward is 245.25 with loss [24.791430950164795, 34.17941498756409] in episode 1352
Report: 
rewardSum:245.25
loss:[24.791430950164795, 34.17941498756409]
policies:[0, 6, 0]
qAverage:[0.0, 29.945784250895183]
ws:[-2.8458178589741387, -1.570419818162918]
memory len:10000
memory used:2875.0
now epsilon is 0.14001639449256556, the reward is 243.25 with loss [49.269718170166016, 45.01041579246521] in episode 1353
Report: 
rewardSum:243.25
loss:[49.269718170166016, 45.01041579246521]
policies:[1, 4, 3]
qAverage:[0.0, 26.832418823242186]
ws:[0.10491003990173339, 0.9333954095840454]
memory len:10000
memory used:2875.0
now epsilon is 0.13970167245814227, the reward is 242.25 with loss [52.080212354660034, 35.68704283237457] in episode 1354
Report: 
rewardSum:242.25
loss:[52.080212354660034, 35.68704283237457]
policies:[1, 7, 1]
qAverage:[0.0, 29.8948917388916]
ws:[-2.525311451870948, -1.2286894861608744]
memory len:10000
memory used:2875.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25*		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.1394922508761244, the reward is 245.25 with loss [36.938759088516235, 35.6445095539093] in episode 1355
Report: 
rewardSum:245.25
loss:[36.938759088516235, 35.6445095539093]
policies:[0, 3, 3]
qAverage:[0.0, 22.443804423014324]
ws:[0.42639245465397835, 1.5134934186935425]
memory len:10000
memory used:2876.0
now epsilon is 0.13935281092612464, the reward is 247.25 with loss [21.080793380737305, 13.878438800573349] in episode 1356
Report: 
rewardSum:247.25
loss:[21.080793380737305, 13.878438800573349]
policies:[0, 3, 1]
qAverage:[0.0, 25.152921676635742]
ws:[-3.397119700908661, -2.2852186262607574]
memory len:10000
memory used:2876.0
now epsilon is 0.13910912633137876, the reward is 244.25 with loss [39.46306586265564, 35.04939150810242] in episode 1357
Report: 
rewardSum:244.25
loss:[39.46306586265564, 35.04939150810242]
policies:[4, 1, 2]
qAverage:[34.689171600341794, 0.0]
ws:[-3.435613441467285, -4.090507435798645]
memory len:10000
memory used:2877.0
now epsilon is 0.13886586786497088, the reward is 244.25 with loss [32.54547584056854, 39.755876541137695] in episode 1358
Report: 
rewardSum:244.25
loss:[32.54547584056854, 39.755876541137695]
policies:[1, 6, 0]
qAverage:[6.5706585475376675, 24.05867603846959]
ws:[-1.1569215570177351, -1.074286311864853]
memory len:10000
memory used:2876.0
now epsilon is 0.13865769920653712, the reward is 245.25 with loss [36.461135387420654, 36.28510808944702] in episode 1359
Report: 
rewardSum:245.25
loss:[36.461135387420654, 36.28510808944702]
policies:[2, 4, 0]
qAverage:[12.752720424107142, 19.212841033935547]
ws:[0.1820629324231829, -0.0013551712036132812]
memory len:10000
memory used:2876.0
now epsilon is 0.13844984260599794, the reward is 245.25 with loss [25.433743238449097, 52.64981555938721] in episode 1360
Report: 
rewardSum:245.25
loss:[25.433743238449097, 52.64981555938721]
policies:[3, 3, 0]
qAverage:[16.90704127720424, 12.924889700753349]
ws:[-0.5924099045140403, -0.969586546931948]
memory len:10000
memory used:2876.0
now epsilon is 0.1383806263378101, the reward is -1.0 with loss [8.105183124542236, 13.468939304351807] in episode 1361
Report: 
rewardSum:-1.0
loss:[8.105183124542236, 13.468939304351807]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:2876.0
now epsilon is 0.13824229759555892, the reward is 247.25 with loss [34.857163429260254, 21.479143619537354] in episode 1362
Report: 
rewardSum:247.25
loss:[34.857163429260254, 21.479143619537354]
policies:[1, 1, 2]
qAverage:[13.3443603515625, 10.259872436523438]
ws:[-0.08178468545277913, 0.27586297194163006]
memory len:10000
memory used:2876.0
now epsilon is 0.138035063708127, the reward is 245.25 with loss [31.124706983566284, 38.16304588317871] in episode 1363
Report: 
rewardSum:245.25
loss:[31.124706983566284, 38.16304588317871]
policies:[3, 2, 1]
qAverage:[20.4116636912028, 10.084200859069824]
ws:[-3.989558001359304, -3.9312929113705954]
memory len:10000
memory used:2876.0
now epsilon is 0.1378970803989411, the reward is 247.25 with loss [26.018098831176758, 27.14957070350647] in episode 1364
Report: 
rewardSum:247.25
loss:[26.018098831176758, 27.14957070350647]
policies:[1, 3, 0]
qAverage:[7.377288055419922, 18.868027114868163]
ws:[-5.303348910808563, -4.507820105552673]
memory len:10000
memory used:2876.0
now epsilon is 0.13751833709371972, the reward is 240.25 with loss [38.53045439720154, 63.54649269580841] in episode 1365
Report: 
rewardSum:240.25
loss:[38.53045439720154, 63.54649269580841]
policies:[5, 3, 3]
qAverage:[20.298542976379395, 11.286816120147705]
ws:[-1.6890590395778418, -1.9485890418291092]
memory len:10000
memory used:2882.0
now epsilon is 0.1372778604214366, the reward is 244.25 with loss [43.495712757110596, 35.027543783187866] in episode 1366
Report: 
rewardSum:244.25
loss:[43.495712757110596, 35.027543783187866]
policies:[3, 3, 1]
qAverage:[17.17831529889788, 13.183770315987724]
ws:[-2.7340751928942546, -2.7506769640105113]
memory len:10000
memory used:2889.0
now epsilon is 0.13707207228590734, the reward is 245.25 with loss [39.162352561950684, 57.38336992263794] in episode 1367
Report: 
rewardSum:245.25
loss:[39.162352561950684, 57.38336992263794]
policies:[3, 3, 0]
qAverage:[17.830698830740793, 12.980082375662667]
ws:[-4.117063560656139, -4.739184177347592]
memory len:10000
memory used:2889.0
now epsilon is 0.1369350516070821, the reward is -3.0 with loss [19.76410436630249, 27.35948896408081] in episode 1368
Report: 
rewardSum:-3.0
loss:[19.76410436630249, 27.35948896408081]
policies:[1, 0, 3]
qAverage:[18.94304656982422, 0.0]
ws:[-1.4161094427108765, -1.6600719690322876]
memory len:10000
memory used:2890.0
now epsilon is 0.13672977736349823, the reward is 245.25 with loss [46.605876445770264, 33.887887954711914] in episode 1369
Report: 
rewardSum:245.25
loss:[46.605876445770264, 33.887887954711914]
policies:[3, 3, 0]
qAverage:[7.067317199707031, 16.646240234375]
ws:[0.9798669815063477, 1.5754798412323]
memory len:10000
memory used:2890.0
now epsilon is 0.13659309885125617, the reward is 247.25 with loss [28.907435417175293, 20.01022458076477] in episode 1370
Report: 
rewardSum:247.25
loss:[28.907435417175293, 20.01022458076477]
policies:[0, 4, 0]
qAverage:[0.0, 22.877717208862304]
ws:[-0.0029645442962646486, 0.9130422115325928]
memory len:10000
memory used:2889.0
now epsilon is 0.13652481083889922, the reward is -1.0 with loss [9.077190637588501, 10.915390491485596] in episode 1371
Report: 
rewardSum:-1.0
loss:[9.077190637588501, 10.915390491485596]
policies:[0, 1, 1]
qAverage:[0.0, 13.726570129394531]
ws:[1.3727548122406006, 1.596411943435669]
memory len:10000
memory used:2890.0
now epsilon is 0.13638833721633212, the reward is 37.15999999999997 with loss [18.48685622215271, 23.559916019439697] in episode 1372
Report: 
rewardSum:37.15999999999997
loss:[18.48685622215271, 23.559916019439697]
policies:[2, 1, 1]
qAverage:[17.449987411499023, 6.773250102996826]
ws:[1.8462333679199219, 2.099690157920122]
memory len:10000
memory used:2889.0
now epsilon is 0.13625200001621854, the reward is 247.25 with loss [24.250109910964966, 34.93070650100708] in episode 1373
Report: 
rewardSum:247.25
loss:[24.250109910964966, 34.93070650100708]
policies:[1, 3, 0]
qAverage:[9.563493728637695, 13.909429550170898]
ws:[-2.470491297543049, -2.4564935714006424]
memory len:10000
memory used:2889.0
now epsilon is 0.13611579910218713, the reward is 247.25 with loss [26.693041801452637, 35.45663356781006] in episode 1374
Report: 
rewardSum:247.25
loss:[26.693041801452637, 35.45663356781006]
policies:[1, 3, 0]
qAverage:[6.771183013916016, 17.80293502807617]
ws:[-3.363822031021118, -3.1208274997770786]
memory len:10000
memory used:2889.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25*		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.1359797343380029, the reward is 247.25 with loss [18.787900686264038, 26.215044498443604] in episode 1375
Report: 
rewardSum:247.25
loss:[18.787900686264038, 26.215044498443604]
policies:[1, 3, 0]
qAverage:[7.500318145751953, 16.37235908508301]
ws:[-1.1080566883087157, -0.6742160439491272]
memory len:10000
memory used:2889.0
now epsilon is 0.13577589217501115, the reward is 245.25 with loss [47.53807616233826, 40.11149859428406] in episode 1376
Report: 
rewardSum:245.25
loss:[47.53807616233826, 40.11149859428406]
policies:[2, 3, 1]
qAverage:[15.418087768554688, 11.589070510864257]
ws:[-7.145764541625977, -7.843226699531078]
memory len:10000
memory used:2889.0
now epsilon is 0.13564016719031027, the reward is 247.25 with loss [23.75273895263672, 18.834222078323364] in episode 1377
Report: 
rewardSum:247.25
loss:[23.75273895263672, 18.834222078323364]
policies:[0, 4, 0]
qAverage:[0.0, 21.821006298065186]
ws:[-4.45873036980629, -3.4270593523979187]
memory len:10000
memory used:2889.0
now epsilon is 0.13550457787970568, the reward is 247.25 with loss [18.843011260032654, 22.973549604415894] in episode 1378
Report: 
rewardSum:247.25
loss:[18.843011260032654, 22.973549604415894]
policies:[0, 4, 0]
qAverage:[0.0, 22.80372886657715]
ws:[-3.950486445426941, -2.936533683538437]
memory len:10000
memory used:2889.0
now epsilon is 0.1351999972869935, the reward is 242.25 with loss [53.84525716304779, 53.858994007110596] in episode 1379
Report: 
rewardSum:242.25
loss:[53.84525716304779, 53.858994007110596]
policies:[2, 6, 1]
qAverage:[4.634766101837158, 20.52405881881714]
ws:[-4.230011239647865, -4.617069743573666]
memory len:10000
memory used:2889.0
now epsilon is 0.135064847981256, the reward is 247.25 with loss [21.87740993499756, 29.51619291305542] in episode 1380
Report: 
rewardSum:247.25
loss:[21.87740993499756, 29.51619291305542]
policies:[1, 3, 0]
qAverage:[6.86444320678711, 14.759650802612304]
ws:[-1.1850654363632203, -0.741111421585083]
memory len:10000
memory used:2889.0
now epsilon is 0.13492983377415174, the reward is 247.25 with loss [20.768498063087463, 21.260412216186523] in episode 1381
Report: 
rewardSum:247.25
loss:[20.768498063087463, 21.260412216186523]
policies:[2, 2, 0]
qAverage:[16.68649911880493, 6.164975166320801]
ws:[-3.0116478465497494, -3.1165896728634834]
memory len:10000
memory used:2889.0
now epsilon is 0.1347949545306327, the reward is 247.25 with loss [23.694496631622314, 25.829020500183105] in episode 1382
Report: 
rewardSum:247.25
loss:[23.694496631622314, 25.829020500183105]
policies:[2, 2, 0]
qAverage:[13.496410369873047, 10.530579757690429]
ws:[-3.16752405166626, -2.9933365404605867]
memory len:10000
memory used:2889.0
now epsilon is 0.13455924020488436, the reward is 34.15999999999997 with loss [44.128143310546875, 41.872151374816895] in episode 1383
Report: 
rewardSum:34.15999999999997
loss:[44.128143310546875, 41.872151374816895]
policies:[3, 2, 2]
qAverage:[7.594690799713135, 13.179044246673584]
ws:[-0.08099630963988602, 0.717899352312088]
memory len:10000
memory used:2889.0
now epsilon is 0.13442473141598513, the reward is 247.25 with loss [21.8326096534729, 20.5484778881073] in episode 1384
Report: 
rewardSum:247.25
loss:[21.8326096534729, 20.5484778881073]
policies:[0, 4, 0]
qAverage:[0.0, 19.234793186187744]
ws:[1.183751793578267, 2.1524053029716015]
memory len:10000
memory used:2889.0
now epsilon is 0.1342903570854424, the reward is 247.25 with loss [16.462038278579712, 19.840704679489136] in episode 1385
Report: 
rewardSum:247.25
loss:[16.462038278579712, 19.840704679489136]
policies:[0, 4, 0]
qAverage:[0.0, 21.30376777648926]
ws:[-1.8746972680091858, -0.9226841926574707]
memory len:10000
memory used:2889.0
now epsilon is 0.13415611707884825, the reward is 247.25 with loss [18.387568801641464, 20.813586235046387] in episode 1386
Report: 
rewardSum:247.25
loss:[18.387568801641464, 20.813586235046387]
policies:[1, 3, 0]
qAverage:[7.139893341064453, 15.41288070678711]
ws:[-1.7112046718597411, -0.75091552734375]
memory len:10000
memory used:2889.0
now epsilon is 0.13402201126192909, the reward is 247.25 with loss [22.63009786605835, 19.37950301170349] in episode 1387
Report: 
rewardSum:247.25
loss:[22.63009786605835, 19.37950301170349]
policies:[1, 3, 0]
qAverage:[8.962329864501953, 12.96048641204834]
ws:[-3.555336158722639, -3.0381759703159332]
memory len:10000
memory used:2889.0
now epsilon is 0.13388803950054556, the reward is 247.25 with loss [24.040109872817993, 24.106948852539062] in episode 1388
Report: 
rewardSum:247.25
loss:[24.040109872817993, 24.106948852539062]
policies:[1, 3, 0]
qAverage:[9.016039848327637, 13.035181999206543]
ws:[-4.308740671724081, -3.8859008476138115]
memory len:10000
memory used:2889.0
now epsilon is 0.13375420166069235, the reward is 247.25 with loss [13.373563885688782, 23.814255237579346] in episode 1389
Report: 
rewardSum:247.25
loss:[13.373563885688782, 23.814255237579346]
policies:[0, 4, 0]
qAverage:[0.0, 21.317919921875]
ws:[-3.891245150566101, -3.144555127620697]
memory len:10000
memory used:2889.0
now epsilon is 0.13362049760849817, the reward is 247.25 with loss [30.14394521713257, 28.89258122444153] in episode 1390
Report: 
rewardSum:247.25
loss:[30.14394521713257, 28.89258122444153]
policies:[1, 2, 1]
qAverage:[0.0, 19.40760548909505]
ws:[-7.085688869158427, -5.952495828270912]
memory len:10000
memory used:2889.0
now epsilon is 0.1334869272102255, the reward is 247.25 with loss [22.15139865875244, 30.395408630371094] in episode 1391
Report: 
rewardSum:247.25
loss:[22.15139865875244, 30.395408630371094]
policies:[0, 4, 0]
qAverage:[0.0, 21.167573547363283]
ws:[-4.274579405784607, -3.2555471923202277]
memory len:10000
memory used:2889.0
now epsilon is 0.13342019208955336, the reward is -1.0 with loss [15.897698402404785, 12.054277896881104] in episode 1392
Report: 
rewardSum:-1.0
loss:[15.897698402404785, 12.054277896881104]
policies:[0, 1, 1]
qAverage:[0.0, 11.215890884399414]
ws:[-0.3692314326763153, -0.10281151533126831]
memory len:10000
memory used:2889.0
now epsilon is 0.1332868219216976, the reward is 247.25 with loss [36.04908895492554, 27.640791416168213] in episode 1393
Report: 
rewardSum:247.25
loss:[36.04908895492554, 27.640791416168213]
policies:[0, 3, 1]
qAverage:[0.0, 19.707219123840332]
ws:[-5.493000715970993, -4.78893855214119]
memory len:10000
memory used:2888.0
now epsilon is 0.1330870166035663, the reward is 245.25 with loss [33.15162181854248, 40.38356161117554] in episode 1394
Report: 
rewardSum:245.25
loss:[33.15162181854248, 40.38356161117554]
policies:[1, 4, 1]
qAverage:[5.563076019287109, 16.77048969268799]
ws:[-1.9746516247590382, -2.0503172278404236]
memory len:10000
memory used:2889.0
now epsilon is 0.13295397948627657, the reward is 247.25 with loss [18.090638279914856, 34.20752573013306] in episode 1395
Report: 
rewardSum:247.25
loss:[18.090638279914856, 34.20752573013306]
policies:[0, 4, 0]
qAverage:[0.0, 20.806502914428712]
ws:[-4.217791471630335, -3.526924043893814]
memory len:10000
memory used:2889.0
now epsilon is 0.13282107535622353, the reward is 247.25 with loss [26.838550567626953, 15.28522139787674] in episode 1396
Report: 
rewardSum:247.25
loss:[26.838550567626953, 15.28522139787674]
policies:[1, 3, 0]
qAverage:[0.0, 16.8922545115153]
ws:[-0.036720593770345054, 0.6158391733964285]
memory len:10000
memory used:2889.0
now epsilon is 0.1326883040804698, the reward is 247.25 with loss [14.561486005783081, 19.995041847229004] in episode 1397
Report: 
rewardSum:247.25
loss:[14.561486005783081, 19.995041847229004]
policies:[0, 3, 1]
qAverage:[0.0, 18.491217613220215]
ws:[1.9161758869886398, 3.5670436024665833]
memory len:10000
memory used:2889.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23*		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.1325556655262109, the reward is 247.25 with loss [27.000155448913574, 19.892268419265747] in episode 1398
Report: 
rewardSum:247.25
loss:[27.000155448913574, 19.892268419265747]
policies:[0, 4, 0]
qAverage:[0.0, 21.120727920532225]
ws:[-1.289815855026245, 0.8803086280822754]
memory len:10000
memory used:2889.0
now epsilon is 0.13242315956077508, the reward is 247.25 with loss [27.874340415000916, 23.330610275268555] in episode 1399
Report: 
rewardSum:247.25
loss:[27.874340415000916, 23.330610275268555]
policies:[0, 4, 0]
qAverage:[0.0, 21.349907302856444]
ws:[-2.8681771576404573, -0.8561792850494385]
memory len:10000
memory used:2890.0
now epsilon is 0.13225771335511033, the reward is 36.15999999999997 with loss [35.99054002761841, 36.20507287979126] in episode 1400
Report: 
rewardSum:36.15999999999997
loss:[35.99054002761841, 36.20507287979126]
policies:[0, 3, 2]
qAverage:[0.0, 18.612914562225342]
ws:[-0.7986781373620033, -0.053828924894332886]
memory len:10000
memory used:2889.0
now epsilon is 0.13212550523013214, the reward is 247.25 with loss [27.97142219543457, 28.81044626235962] in episode 1401
Report: 
rewardSum:247.25
loss:[27.97142219543457, 28.81044626235962]
policies:[1, 2, 1]
qAverage:[7.1949143409729, 14.947947978973389]
ws:[-7.1990020871162415, -6.701868444681168]
memory len:10000
memory used:2889.0
now epsilon is 0.13199342926370916, the reward is 37.15999999999997 with loss [25.11669683456421, 28.215540885925293] in episode 1402
Report: 
rewardSum:37.15999999999997
loss:[25.11669683456421, 28.215540885925293]
policies:[0, 2, 2]
qAverage:[0.0, 17.064198176066082]
ws:[-0.02955027421315511, 0.8597630908091863]
memory len:10000
memory used:2889.0
now epsilon is 0.13186148532373235, the reward is 247.25 with loss [29.468306064605713, 26.78403949737549] in episode 1403
Report: 
rewardSum:247.25
loss:[29.468306064605713, 26.78403949737549]
policies:[0, 4, 0]
qAverage:[0.0, 21.328677368164062]
ws:[-4.285400083661079, -2.6536108255386353]
memory len:10000
memory used:2889.0
now epsilon is 0.1317296732782248, the reward is 247.25 with loss [35.21245861053467, 27.143963098526] in episode 1404
Report: 
rewardSum:247.25
loss:[35.21245861053467, 27.143963098526]
policies:[1, 3, 0]
qAverage:[0.0, 18.09930992126465]
ws:[1.327356182038784, 2.6697073876857758]
memory len:10000
memory used:2889.0
now epsilon is 0.13159799299534147, the reward is -3.0 with loss [14.372438311576843, 28.738274097442627] in episode 1405
Report: 
rewardSum:-3.0
loss:[14.372438311576843, 28.738274097442627]
policies:[0, 2, 2]
qAverage:[0.0, 15.255991617838541]
ws:[1.125239074230194, 2.1141019662221274]
memory len:10000
memory used:2889.0
now epsilon is 0.13146644434336915, the reward is 247.25 with loss [20.906948804855347, 21.554489135742188] in episode 1406
Report: 
rewardSum:247.25
loss:[20.906948804855347, 21.554489135742188]
policies:[1, 3, 0]
qAverage:[0.0, 19.710015773773193]
ws:[-5.001525640487671, -3.5122867226600647]
memory len:10000
memory used:2890.0
now epsilon is 0.1313350271907263, the reward is 247.25 with loss [32.256866455078125, 23.231211185455322] in episode 1407
Report: 
rewardSum:247.25
loss:[32.256866455078125, 23.231211185455322]
policies:[1, 3, 0]
qAverage:[0.0, 19.37934684753418]
ws:[-4.573200040496886, -3.500206470489502]
memory len:10000
memory used:2889.0
now epsilon is 0.13120374140596283, the reward is 247.25 with loss [25.911749362945557, 19.256136417388916] in episode 1408
Report: 
rewardSum:247.25
loss:[25.911749362945557, 19.256136417388916]
policies:[0, 4, 0]
qAverage:[0.0, 21.71607360839844]
ws:[-2.421227824687958, -0.9068750619888306]
memory len:10000
memory used:2890.0
now epsilon is 0.13107258685776021, the reward is 247.25 with loss [15.237926721572876, 29.617262363433838] in episode 1409
Report: 
rewardSum:247.25
loss:[15.237926721572876, 29.617262363433838]
policies:[0, 4, 0]
qAverage:[0.0, 18.32252597808838]
ws:[1.7650627195835114, 2.850103050470352]
memory len:10000
memory used:2890.0
now epsilon is 0.13094156341493102, the reward is 247.25 with loss [21.507158517837524, 22.93180513381958] in episode 1410
Report: 
rewardSum:247.25
loss:[21.507158517837524, 22.93180513381958]
policies:[1, 3, 0]
qAverage:[0.0, 19.89162588119507]
ws:[-2.7417437732219696, -1.4561355113983154]
memory len:10000
memory used:2890.0
now epsilon is 0.13081067094641907, the reward is 247.25 with loss [33.02049732208252, 25.438447952270508] in episode 1411
Report: 
rewardSum:247.25
loss:[33.02049732208252, 25.438447952270508]
policies:[1, 3, 0]
qAverage:[0.0, 18.562076568603516]
ws:[-5.578514605760574, -4.047635734081268]
memory len:10000
memory used:2890.0
now epsilon is 0.1306799093212991, the reward is 247.25 with loss [30.14464521408081, 34.53488540649414] in episode 1412
Report: 
rewardSum:247.25
loss:[30.14464521408081, 34.53488540649414]
policies:[0, 3, 1]
qAverage:[0.0, 17.613683700561523]
ws:[0.8911445339520773, 2.0965269804000854]
memory len:10000
memory used:2890.0
now epsilon is 0.13048401192890235, the reward is 245.25 with loss [38.97377419471741, 40.913132429122925] in episode 1413
Report: 
rewardSum:245.25
loss:[38.97377419471741, 40.913132429122925]
policies:[0, 4, 2]
qAverage:[0.0, 21.64722671508789]
ws:[-3.5406382083892822, -2.2587897062301634]
memory len:10000
memory used:2890.0
now epsilon is 0.1303535768403232, the reward is 247.25 with loss [27.24645233154297, 28.10062074661255] in episode 1414
Report: 
rewardSum:247.25
loss:[27.24645233154297, 28.10062074661255]
policies:[0, 3, 1]
qAverage:[0.0, 22.318096160888672]
ws:[-6.992256551980972, -4.884805724024773]
memory len:10000
memory used:2890.0
now epsilon is 0.13022327213792761, the reward is 247.25 with loss [31.220484256744385, 26.24804401397705] in episode 1415
Report: 
rewardSum:247.25
loss:[31.220484256744385, 26.24804401397705]
policies:[1, 3, 0]
qAverage:[0.0, 20.473469734191895]
ws:[-5.765551939606667, -3.961758214980364]
memory len:10000
memory used:2890.0
now epsilon is 0.1300280592733512, the reward is 245.25 with loss [35.15515494346619, 38.74242305755615] in episode 1416
Report: 
rewardSum:245.25
loss:[35.15515494346619, 38.74242305755615]
policies:[0, 4, 2]
qAverage:[0.0, 23.071113204956056]
ws:[-2.7934998631477357, -2.266294278204441]
memory len:10000
memory used:2891.0
now epsilon is 0.12989807996647385, the reward is 37.15999999999997 with loss [17.39562737941742, 20.32841682434082] in episode 1417
Report: 
rewardSum:37.15999999999997
loss:[17.39562737941742, 20.32841682434082]
policies:[1, 2, 1]
qAverage:[7.030054569244385, 14.002899169921875]
ws:[2.3716507256031036, 3.148266524076462]
memory len:10000
memory used:2891.0
now epsilon is 0.12976823059016926, the reward is 247.25 with loss [25.3711576461792, 17.178863048553467] in episode 1418
Report: 
rewardSum:247.25
loss:[25.3711576461792, 17.178863048553467]
policies:[0, 4, 0]
qAverage:[0.0, 22.979005813598633]
ws:[-2.2571353912353516, -0.43628127574920655]
memory len:10000
memory used:2891.0
now epsilon is 0.12963851101455556, the reward is 37.15999999999997 with loss [22.264294147491455, 23.24887180328369] in episode 1419
Report: 
rewardSum:37.15999999999997
loss:[22.264294147491455, 23.24887180328369]
policies:[1, 2, 1]
qAverage:[0.0, 19.01667849222819]
ws:[0.5262727538744608, 1.5540525515874226]
memory len:10000
memory used:2891.0
now epsilon is 0.12950892110988074, the reward is 247.25 with loss [24.036923229694366, 15.49569821357727] in episode 1420
Report: 
rewardSum:247.25
loss:[24.036923229694366, 15.49569821357727]
policies:[1, 2, 1]
qAverage:[0.0, 18.641766866048176]
ws:[-0.009345571200052897, 0.9771160284678141]
memory len:10000
memory used:2891.0
now epsilon is 0.12937946074652248, the reward is 247.25 with loss [29.599157333374023, 14.435748815536499] in episode 1421
Report: 
rewardSum:247.25
loss:[29.599157333374023, 14.435748815536499]
policies:[0, 3, 1]
qAverage:[0.0, 20.995503107706707]
ws:[-5.457611958185832, -1.7997716267903645]
memory len:10000
memory used:2891.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26*		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.12925012979498804, the reward is 247.25 with loss [23.715652227401733, 24.508838176727295] in episode 1422
Report: 
rewardSum:247.25
loss:[23.715652227401733, 24.508838176727295]
policies:[0, 4, 0]
qAverage:[0.0, 22.909705352783202]
ws:[-2.798646104335785, 0.2087470054626465]
memory len:10000
memory used:2891.0
############# STATE ###############
0*		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.12912092812591414, the reward is 247.25 with loss [21.608086585998535, 22.719932556152344] in episode 1423
Report: 
rewardSum:247.25
loss:[21.608086585998535, 22.719932556152344]
policies:[0, 4, 0]
qAverage:[0.0, 23.188920974731445]
ws:[-1.5466022968292237, 0.3836520671844482]
memory len:10000
memory used:2891.0
now epsilon is 0.12899185561006674, the reward is 247.25 with loss [20.8856463432312, 18.634556353092194] in episode 1424
Report: 
rewardSum:247.25
loss:[20.8856463432312, 18.634556353092194]
policies:[0, 4, 0]
qAverage:[0.0, 22.72917251586914]
ws:[-2.2878095328807833, -0.8561965227127075]
memory len:10000
memory used:2891.0
now epsilon is 0.12886291211834103, the reward is 37.15999999999997 with loss [26.600887775421143, 15.199022054672241] in episode 1425
Report: 
rewardSum:37.15999999999997
loss:[26.600887775421143, 15.199022054672241]
policies:[0, 3, 1]
qAverage:[0.0, 19.663931846618652]
ws:[0.9890021532773972, 1.3678329288959503]
memory len:10000
memory used:2891.0
now epsilon is 0.12873409752176135, the reward is 247.25 with loss [24.411301851272583, 23.53339910507202] in episode 1426
Report: 
rewardSum:247.25
loss:[24.411301851272583, 23.53339910507202]
policies:[0, 4, 0]
qAverage:[0.0, 22.897352981567384]
ws:[-0.9148363828659057, 0.7177835464477539]
memory len:10000
memory used:2891.0
now epsilon is 0.12860541169148076, the reward is 247.25 with loss [26.046688079833984, 25.389644622802734] in episode 1427
Report: 
rewardSum:247.25
loss:[26.046688079833984, 25.389644622802734]
policies:[1, 3, 0]
qAverage:[8.376056671142578, 8.597478230794271]
ws:[0.5146434307098389, 0.538816511631012]
memory len:10000
memory used:2895.0
now epsilon is 0.12847685449878135, the reward is 247.25 with loss [21.552908897399902, 21.888310432434082] in episode 1428
Report: 
rewardSum:247.25
loss:[21.552908897399902, 21.888310432434082]
policies:[2, 2, 0]
qAverage:[7.024003982543945, 17.06609344482422]
ws:[-3.0671579390764236, -1.764297440648079]
memory len:10000
memory used:2901.0
now epsilon is 0.1283484258150737, the reward is 247.25 with loss [20.186423540115356, 22.170918226242065] in episode 1429
Report: 
rewardSum:247.25
loss:[20.186423540115356, 22.170918226242065]
policies:[1, 3, 0]
qAverage:[0.0, 24.836228370666504]
ws:[-2.5823380649089813, -1.1633195877075195]
memory len:10000
memory used:2901.0
now epsilon is 0.12822012551189702, the reward is 247.25 with loss [24.42475438117981, 12.079790115356445] in episode 1430
Report: 
rewardSum:247.25
loss:[24.42475438117981, 12.079790115356445]
policies:[0, 4, 0]
qAverage:[0.0, 25.68644428253174]
ws:[-3.8247075714170933, -1.940872922539711]
memory len:10000
memory used:2902.0
now epsilon is 0.12809195346091895, the reward is 247.25 with loss [30.87463617324829, 34.50212574005127] in episode 1431
Report: 
rewardSum:247.25
loss:[30.87463617324829, 34.50212574005127]
policies:[0, 3, 1]
qAverage:[0.0, 20.90134286880493]
ws:[0.7635277509689331, 2.2207423746585846]
memory len:10000
memory used:2901.0
now epsilon is 0.1277720835958184, the reward is 241.25 with loss [63.59284329414368, 52.6063449382782] in episode 1432
Report: 
rewardSum:241.25
loss:[63.59284329414368, 52.6063449382782]
policies:[0, 7, 3]
qAverage:[0.0, 25.76059103012085]
ws:[-1.452227283269167, 0.6872981488704681]
memory len:10000
memory used:2902.0
now epsilon is 0.12764435941876867, the reward is 247.25 with loss [22.387187480926514, 27.770341396331787] in episode 1433
Report: 
rewardSum:247.25
loss:[22.387187480926514, 27.770341396331787]
policies:[0, 4, 0]
qAverage:[0.0, 25.044575119018553]
ws:[-3.8373013973236083, -1.5851192116737365]
memory len:10000
memory used:2902.0
now epsilon is 0.12745301250634614, the reward is 245.25 with loss [37.812575340270996, 34.27102041244507] in episode 1434
Report: 
rewardSum:245.25
loss:[37.812575340270996, 34.27102041244507]
policies:[1, 4, 1]
qAverage:[0.0, 26.995846557617188]
ws:[-2.8034460499882696, -0.9845983922481537]
memory len:10000
memory used:2902.0
now epsilon is 0.12732560728075415, the reward is 247.25 with loss [24.770187854766846, 22.96645200252533] in episode 1435
Report: 
rewardSum:247.25
loss:[24.770187854766846, 22.96645200252533]
policies:[0, 4, 0]
qAverage:[0.0, 24.969102096557616]
ws:[-3.592895531654358, -1.5640195995569228]
memory len:10000
memory used:2902.0
now epsilon is 0.12719832941261877, the reward is 247.25 with loss [22.505826950073242, 25.422144412994385] in episode 1436
Report: 
rewardSum:247.25
loss:[22.505826950073242, 25.422144412994385]
policies:[0, 4, 0]
qAverage:[0.0, 25.037499237060548]
ws:[-2.92979781255126, -1.0717878341674805]
memory len:10000
memory used:2902.0
now epsilon is 0.12707117877463028, the reward is 247.25 with loss [15.640147924423218, 24.469062328338623] in episode 1437
Report: 
rewardSum:247.25
loss:[15.640147924423218, 24.469062328338623]
policies:[0, 2, 2]
qAverage:[0.0, 25.411905924479168]
ws:[-5.09059460957845, -3.0950207710266113]
memory len:10000
memory used:2902.0
now epsilon is 0.12688069109599615, the reward is 245.25 with loss [38.46515488624573, 27.93107795715332] in episode 1438
Report: 
rewardSum:245.25
loss:[38.46515488624573, 27.93107795715332]
policies:[2, 3, 1]
qAverage:[9.734010378519693, 19.85229555765788]
ws:[-1.5008538961410522, -0.6211829322079817]
memory len:10000
memory used:2902.0
now epsilon is 0.12675385797722977, the reward is 247.25 with loss [19.573089361190796, 24.538886785507202] in episode 1439
Report: 
rewardSum:247.25
loss:[19.573089361190796, 24.538886785507202]
policies:[1, 2, 1]
qAverage:[7.903174877166748, 18.89658546447754]
ws:[-1.7253182381391525, 0.1455589234828949]
memory len:10000
memory used:2902.0
now epsilon is 0.12662715164402769, the reward is 247.25 with loss [27.142382621765137, 22.95535171031952] in episode 1440
Report: 
rewardSum:247.25
loss:[27.142382621765137, 22.95535171031952]
policies:[0, 4, 0]
qAverage:[0.0, 27.38881034851074]
ws:[-0.19533817768096923, 2.129248094558716]
memory len:10000
memory used:2902.0
now epsilon is 0.12650057196965184, the reward is 247.25 with loss [20.637394428253174, 22.541364669799805] in episode 1441
Report: 
rewardSum:247.25
loss:[20.637394428253174, 22.541364669799805]
policies:[0, 4, 0]
qAverage:[0.0, 28.245464706420897]
ws:[-0.423821234703064, 1.6375998497009276]
memory len:10000
memory used:2902.0
now epsilon is 0.1263741188274909, the reward is 247.25 with loss [21.66005849838257, 16.97114849090576] in episode 1442
Report: 
rewardSum:247.25
loss:[21.66005849838257, 16.97114849090576]
policies:[0, 4, 0]
qAverage:[0.0, 27.516771697998045]
ws:[-1.3603139758110045, 0.5512198209762573]
memory len:10000
memory used:2902.0
now epsilon is 0.1261846760855016, the reward is 245.25 with loss [31.404101610183716, 43.449134826660156] in episode 1443
Report: 
rewardSum:245.25
loss:[31.404101610183716, 43.449134826660156]
policies:[0, 3, 3]
qAverage:[0.0, 28.62288522720337]
ws:[-0.944517582654953, 1.7171582579612732]
memory len:10000
memory used:2902.0
now epsilon is 0.1260585387207836, the reward is 247.25 with loss [23.614314556121826, 26.551796674728394] in episode 1444
Report: 
rewardSum:247.25
loss:[23.614314556121826, 26.551796674728394]
policies:[0, 4, 0]
qAverage:[0.0, 27.88087730407715]
ws:[-2.149095270037651, -0.4856573581695557]
memory len:10000
memory used:2902.0
############# STATE ###############
0-		10-		20-		30-		40-		
1*		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.1259325274461367, the reward is 247.25 with loss [18.989793300628662, 17.160998344421387] in episode 1445
Report: 
rewardSum:247.25
loss:[18.989793300628662, 17.160998344421387]
policies:[0, 4, 0]
qAverage:[0.0, 22.697463353474934]
ws:[0.645307340969642, 1.8429205020268757]
memory len:10000
memory used:2902.0
now epsilon is 0.1258066421355181, the reward is 247.25 with loss [25.48575735092163, 20.051755011081696] in episode 1446
Report: 
rewardSum:247.25
loss:[25.48575735092163, 20.051755011081696]
policies:[0, 4, 0]
qAverage:[0.0, 27.402786636352538]
ws:[-3.331017315387726, -1.392911782860756]
memory len:10000
memory used:2902.0
now epsilon is 0.12568088266301097, the reward is 247.25 with loss [29.12554907798767, 29.340951442718506] in episode 1447
Report: 
rewardSum:247.25
loss:[29.12554907798767, 29.340951442718506]
policies:[0, 4, 0]
qAverage:[0.0, 28.416644668579103]
ws:[-2.8004683598876, -0.614048981666565]
memory len:10000
memory used:2902.0
now epsilon is 0.12555524890282443, the reward is 247.25 with loss [25.079955577850342, 19.77487540245056] in episode 1448
Report: 
rewardSum:247.25
loss:[25.079955577850342, 19.77487540245056]
policies:[0, 4, 0]
qAverage:[0.0, 27.627022171020506]
ws:[-2.850261116027832, -0.37370251417160033]
memory len:10000
memory used:2902.0
now epsilon is 0.12539838329411096, the reward is 246.25 with loss [37.036919713020325, 30.0547935962677] in episode 1449
Report: 
rewardSum:246.25
loss:[37.036919713020325, 30.0547935962677]
policies:[0, 4, 1]
qAverage:[0.0, 30.13450469970703]
ws:[-3.2854635179042817, 0.06724395751953124]
memory len:10000
memory used:2869.0
now epsilon is 0.1252730319273737, the reward is 247.25 with loss [26.129698276519775, 24.30028772354126] in episode 1450
Report: 
rewardSum:247.25
loss:[26.129698276519775, 24.30028772354126]
policies:[1, 2, 1]
qAverage:[0.0, 24.81602668762207]
ws:[0.1468884473045667, 1.8711340030034382]
memory len:10000
memory used:2880.0
now epsilon is 0.12514780586500426, the reward is 247.25 with loss [24.6729474067688, 14.18124771118164] in episode 1451
Report: 
rewardSum:247.25
loss:[24.6729474067688, 14.18124771118164]
policies:[0, 4, 0]
qAverage:[0.0, 31.13063907623291]
ws:[-3.1376521587371826, 0.8607616424560547]
memory len:10000
memory used:2880.0
now epsilon is 0.1249602014431734, the reward is 245.25 with loss [25.910923719406128, 37.24800705909729] in episode 1452
Report: 
rewardSum:245.25
loss:[25.910923719406128, 37.24800705909729]
policies:[0, 5, 1]
qAverage:[0.0, 33.034300804138184]
ws:[-0.6298014720280966, 2.139637549718221]
memory len:10000
memory used:2880.0
now epsilon is 0.12483528809399624, the reward is 247.25 with loss [21.191264629364014, 20.029839277267456] in episode 1453
Report: 
rewardSum:247.25
loss:[21.191264629364014, 20.029839277267456]
policies:[1, 3, 0]
qAverage:[0.0, 25.47370433807373]
ws:[2.3414222598075867, 3.9802252650260925]
memory len:10000
memory used:2881.0
now epsilon is 0.12471049961133358, the reward is 247.25 with loss [22.510282516479492, 28.024658679962158] in episode 1454
Report: 
rewardSum:247.25
loss:[22.510282516479492, 28.024658679962158]
policies:[0, 4, 0]
qAverage:[0.0, 31.978311157226564]
ws:[-2.8337308764457703, -0.2804903030395508]
memory len:10000
memory used:2881.0
now epsilon is 0.12452355073904528, the reward is 245.25 with loss [29.949443817138672, 30.854559659957886] in episode 1455
Report: 
rewardSum:245.25
loss:[29.949443817138672, 30.854559659957886]
policies:[0, 5, 1]
qAverage:[0.0, 31.634461402893066]
ws:[-1.458951433499654, 1.0036760171254475]
memory len:10000
memory used:2887.0
now epsilon is 0.12439907387685553, the reward is 247.25 with loss [25.09830093383789, 24.531851768493652] in episode 1456
Report: 
rewardSum:247.25
loss:[25.09830093383789, 24.531851768493652]
policies:[1, 3, 0]
qAverage:[6.358278274536133, 26.607752990722656]
ws:[-2.030743622779846, -0.09602165222167969]
memory len:10000
memory used:2887.0
now epsilon is 0.12421259185130462, the reward is 245.25 with loss [42.087849140167236, 34.35600733757019] in episode 1457
Report: 
rewardSum:245.25
loss:[42.087849140167236, 34.35600733757019]
policies:[0, 5, 1]
qAverage:[0.0, 33.309560012817386]
ws:[-1.553812575340271, 0.14846577644348144]
memory len:10000
memory used:2887.0
now epsilon is 0.12408842583141247, the reward is 247.25 with loss [28.937800884246826, 23.85062685608864] in episode 1458
Report: 
rewardSum:247.25
loss:[28.937800884246826, 23.85062685608864]
policies:[0, 4, 0]
qAverage:[0.0, 31.388786792755127]
ws:[-3.248281642794609, -0.7376347929239273]
memory len:10000
memory used:2889.0
now epsilon is 0.1239643839309857, the reward is 247.25 with loss [15.945343017578125, 21.555343627929688] in episode 1459
Report: 
rewardSum:247.25
loss:[15.945343017578125, 21.555343627929688]
policies:[2, 2, 0]
qAverage:[7.9906392097473145, 19.065335273742676]
ws:[1.2315046042203903, 1.7086943164467812]
memory len:10000
memory used:2888.0
now epsilon is 0.12371667199236068, the reward is 243.25 with loss [47.02901363372803, 45.94395065307617] in episode 1460
Report: 
rewardSum:243.25
loss:[47.02901363372803, 45.94395065307617]
policies:[1, 4, 3]
qAverage:[0.0, 39.08170928955078]
ws:[-2.378545308113098, -0.27214022278785704]
memory len:10000
memory used:2888.0
now epsilon is 0.12359300170638853, the reward is 247.25 with loss [23.89502239227295, 20.70293664932251] in episode 1461
Report: 
rewardSum:247.25
loss:[23.89502239227295, 20.70293664932251]
policies:[2, 2, 0]
qAverage:[9.569841384887695, 25.17375087738037]
ws:[-1.6575566232204437, -0.0115928053855896]
memory len:10000
memory used:2888.0
now epsilon is 0.12343858768057263, the reward is 246.25 with loss [28.500591278076172, 24.783858060836792] in episode 1462
Report: 
rewardSum:246.25
loss:[28.500591278076172, 24.783858060836792]
policies:[1, 3, 1]
qAverage:[9.00725269317627, 29.227243423461914]
ws:[-1.8371147513389587, -0.21665897965431213]
memory len:10000
memory used:2888.0
now epsilon is 0.12319192641476494, the reward is 992.0 with loss [34.32395875453949, 49.55750012397766] in episode 1463
Report: 
rewardSum:992.0
loss:[34.32395875453949, 49.55750012397766]
policies:[1, 3, 4]
qAverage:[7.277587890625, 23.637339401245118]
ws:[-9.961905479431152, -15.304453563690185]
memory len:10000
memory used:2888.0
now epsilon is 0.12306878067762358, the reward is 247.25 with loss [11.915856838226318, 24.024474620819092] in episode 1464
Report: 
rewardSum:247.25
loss:[11.915856838226318, 24.024474620819092]
policies:[1, 3, 0]
qAverage:[0.0, 32.748785972595215]
ws:[-1.566929578781128, 1.3980321288108826]
memory len:10000
memory used:2888.0
now epsilon is 0.1229457580400474, the reward is 247.25 with loss [20.35413408279419, 22.812822341918945] in episode 1465
Report: 
rewardSum:247.25
loss:[20.35413408279419, 22.812822341918945]
policies:[0, 4, 0]
qAverage:[0.0, 37.23302040100098]
ws:[-0.535513973236084, 2.8469635248184204]
memory len:10000
memory used:2888.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2*		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.122822858378983, the reward is 37.15999999999997 with loss [21.236973762512207, 24.166260957717896] in episode 1466
Report: 
rewardSum:37.15999999999997
loss:[21.236973762512207, 24.166260957717896]
policies:[0, 3, 1]
qAverage:[0.0, 28.986629962921143]
ws:[2.31696355342865, 4.307779788970947]
memory len:10000
memory used:2888.0
now epsilon is 0.12266940655110711, the reward is 246.25 with loss [29.570314407348633, 34.140706300735474] in episode 1467
Report: 
rewardSum:246.25
loss:[29.570314407348633, 34.140706300735474]
policies:[1, 3, 1]
qAverage:[0.0, 33.46246337890625]
ws:[-0.6590372622013092, 2.5031160041689873]
memory len:10000
memory used:2888.0
now epsilon is 0.12254678313791713, the reward is 247.25 with loss [17.606653451919556, 17.55085563659668] in episode 1468
Report: 
rewardSum:247.25
loss:[17.606653451919556, 17.55085563659668]
policies:[0, 4, 0]
qAverage:[0.0, 36.88798561096191]
ws:[-0.310602593421936, 2.7103353261947634]
memory len:10000
memory used:2888.0
now epsilon is 0.12242428230216422, the reward is 247.25 with loss [20.390326023101807, 28.9777774810791] in episode 1469
Report: 
rewardSum:247.25
loss:[20.390326023101807, 28.9777774810791]
policies:[1, 3, 0]
qAverage:[0.0, 38.21564483642578]
ws:[0.3479800224304199, 4.045382261276245]
memory len:10000
memory used:2888.0
now epsilon is 0.12224076061322524, the reward is 35.15999999999997 with loss [40.94331645965576, 37.522977352142334] in episode 1470
Report: 
rewardSum:35.15999999999997
loss:[40.94331645965576, 37.522977352142334]
policies:[1, 3, 2]
qAverage:[0.0, 33.00424003601074]
ws:[1.9591167569160461, 3.291812241077423]
memory len:10000
memory used:2889.0
now epsilon is 0.12211856568525767, the reward is 247.25 with loss [19.439613580703735, 33.55491018295288] in episode 1471
Report: 
rewardSum:247.25
loss:[19.439613580703735, 33.55491018295288]
policies:[1, 3, 0]
qAverage:[0.0, 39.11414337158203]
ws:[0.13690119981765747, 2.3924491703510284]
memory len:10000
memory used:2890.0
now epsilon is 0.12193550228473024, the reward is 245.25 with loss [38.97171235084534, 45.26995801925659] in episode 1472
Report: 
rewardSum:245.25
loss:[38.97171235084534, 45.26995801925659]
policies:[1, 3, 2]
qAverage:[10.181922149658202, 21.921771240234374]
ws:[-2.639226698875427, -2.172296667098999]
memory len:10000
memory used:2890.0
now epsilon is 0.12181361250063838, the reward is 247.25 with loss [23.381808280944824, 33.930357456207275] in episode 1473
Report: 
rewardSum:247.25
loss:[23.381808280944824, 33.930357456207275]
policies:[1, 3, 0]
qAverage:[8.33751220703125, 35.4814094543457]
ws:[-0.22281265258789062, 0.5164379820227623]
memory len:10000
memory used:2890.0
now epsilon is 0.12169184456062956, the reward is 247.25 with loss [17.02542507648468, 18.54647397994995] in episode 1474
Report: 
rewardSum:247.25
loss:[17.02542507648468, 18.54647397994995]
policies:[2, 1, 1]
qAverage:[23.479421615600586, 14.624887466430664]
ws:[1.2146941721439362, 1.2218137308955193]
memory len:10000
memory used:2890.0
now epsilon is 0.12157019834290539, the reward is 247.25 with loss [16.562130451202393, 16.327452898025513] in episode 1475
Report: 
rewardSum:247.25
loss:[16.562130451202393, 16.327452898025513]
policies:[2, 2, 0]
qAverage:[10.241908073425293, 32.93760871887207]
ws:[-0.28124794363975525, 0.16524557024240494]
memory len:10000
memory used:2890.0
now epsilon is 0.1214486737257892, the reward is 247.25 with loss [25.072274446487427, 26.870450019836426] in episode 1476
Report: 
rewardSum:247.25
loss:[25.072274446487427, 26.870450019836426]
policies:[2, 2, 0]
qAverage:[13.619095802307129, 33.77531337738037]
ws:[0.08335739374160767, 0.8457081615924835]
memory len:10000
memory used:2890.0
now epsilon is 0.12126661453538656, the reward is 245.25 with loss [41.79664659500122, 37.36029863357544] in episode 1477
Report: 
rewardSum:245.25
loss:[41.79664659500122, 37.36029863357544]
policies:[1, 4, 1]
qAverage:[8.180197143554688, 35.94616775512695]
ws:[0.9662730932235718, 2.131556272506714]
memory len:10000
memory used:2890.0
now epsilon is 0.12114539338825295, the reward is 247.25 with loss [20.336361408233643, 19.948846578598022] in episode 1478
Report: 
rewardSum:247.25
loss:[20.336361408233643, 19.948846578598022]
policies:[0, 4, 0]
qAverage:[0.0, 42.353842163085936]
ws:[0.7317330718040467, 2.2055514097213744]
memory len:10000
memory used:2891.0
now epsilon is 0.12102429341681611, the reward is 247.25 with loss [21.182433605194092, 20.969773769378662] in episode 1479
Report: 
rewardSum:247.25
loss:[21.182433605194092, 20.969773769378662]
policies:[0, 4, 0]
qAverage:[0.0, 42.53054656982422]
ws:[0.7691570043563842, 1.9503808081150056]
memory len:10000
memory used:2891.0
now epsilon is 0.12090331449994579, the reward is 247.25 with loss [34.014392375946045, 36.34849452972412] in episode 1480
Report: 
rewardSum:247.25
loss:[34.014392375946045, 36.34849452972412]
policies:[0, 4, 0]
qAverage:[0.0, 42.43015747070312]
ws:[1.4091668367385863, 2.8050711393356322]
memory len:10000
memory used:2891.0
now epsilon is 0.1207824565166328, the reward is 247.25 with loss [37.23625922203064, 16.932577252388] in episode 1481
Report: 
rewardSum:247.25
loss:[37.23625922203064, 16.932577252388]
policies:[0, 4, 0]
qAverage:[0.0, 46.98378677368164]
ws:[0.6459879159927369, 1.8232093095779418]
memory len:10000
memory used:2891.0
now epsilon is 0.12066171934598895, the reward is 247.25 with loss [21.62303924560547, 16.384421348571777] in episode 1482
Report: 
rewardSum:247.25
loss:[21.62303924560547, 16.384421348571777]
policies:[0, 4, 0]
qAverage:[0.0, 52.695071411132815]
ws:[-0.32663201689720156, 1.4808366775512696]
memory len:10000
memory used:2890.0
now epsilon is 0.12054110286724684, the reward is 247.25 with loss [27.07471990585327, 25.4267840385437] in episode 1483
Report: 
rewardSum:247.25
loss:[27.07471990585327, 25.4267840385437]
policies:[0, 4, 0]
qAverage:[0.0, 46.72074737548828]
ws:[0.16024563312530518, 2.6640240490436553]
memory len:10000
memory used:2890.0
now epsilon is 0.12039050180801988, the reward is 246.25 with loss [33.51846742630005, 33.1513557434082] in episode 1484
Report: 
rewardSum:246.25
loss:[33.51846742630005, 33.1513557434082]
policies:[0, 3, 2]
qAverage:[0.0, 54.6746940612793]
ws:[0.5346183180809021, 3.8223512768745422]
memory len:10000
memory used:2889.0
now epsilon is 0.12027015644512612, the reward is 247.25 with loss [26.38541603088379, 26.78727388381958] in episode 1485
Report: 
rewardSum:247.25
loss:[26.38541603088379, 26.78727388381958]
policies:[0, 4, 0]
qAverage:[0.0, 49.45675811767578]
ws:[1.1508436679840088, 3.896537947654724]
memory len:10000
memory used:2889.0
now epsilon is 0.12014993138247326, the reward is 247.25 with loss [22.947083473205566, 24.2089262008667] in episode 1486
Report: 
rewardSum:247.25
loss:[22.947083473205566, 24.2089262008667]
policies:[1, 3, 0]
qAverage:[0.0, 45.01465702056885]
ws:[0.723711758852005, 2.3335677087306976]
memory len:10000
memory used:2890.0
now epsilon is 0.12002982649980616, the reward is 247.25 with loss [30.656356811523438, 22.199764728546143] in episode 1487
Report: 
rewardSum:247.25
loss:[30.656356811523438, 22.199764728546143]
policies:[0, 3, 1]
qAverage:[0.0, 53.31221675872803]
ws:[1.2202352285385132, 2.598878413438797]
memory len:10000
memory used:2902.0
now epsilon is 0.11990984167698993, the reward is 247.25 with loss [24.990504026412964, 22.849164962768555] in episode 1488
Report: 
rewardSum:247.25
loss:[24.990504026412964, 22.849164962768555]
policies:[2, 2, 0]
qAverage:[0.0, 40.52355448404948]
ws:[2.919119358062744, 3.967650572458903]
memory len:10000
memory used:2902.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22*		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.11978997679400967, the reward is 247.25 with loss [30.757845401763916, 15.253958702087402] in episode 1489
Report: 
rewardSum:247.25
loss:[30.757845401763916, 15.253958702087402]
policies:[0, 4, 0]
qAverage:[0.0, 50.21902618408203]
ws:[0.816063666343689, 1.2696263074874878]
memory len:10000
memory used:2902.0
now epsilon is 0.11967023173097056, the reward is 247.25 with loss [30.256922245025635, 23.274441480636597] in episode 1490
Report: 
rewardSum:247.25
loss:[30.256922245025635, 23.274441480636597]
policies:[1, 3, 0]
qAverage:[9.615183258056641, 43.60926971435547]
ws:[1.352665865421295, 1.758406613767147]
memory len:10000
memory used:2902.0
now epsilon is 0.11955060636809758, the reward is 247.25 with loss [21.291065216064453, 25.55494499206543] in episode 1491
Report: 
rewardSum:247.25
loss:[21.291065216064453, 25.55494499206543]
policies:[0, 4, 0]
qAverage:[0.0, 50.09460220336914]
ws:[1.7548513412475586, 3.036087465286255]
memory len:10000
memory used:2890.0
now epsilon is 0.11943110058573544, the reward is 247.25 with loss [26.80197525024414, 20.334572076797485] in episode 1492
Report: 
rewardSum:247.25
loss:[26.80197525024414, 20.334572076797485]
policies:[0, 2, 2]
qAverage:[0.0, 41.80009969075521]
ws:[2.2652528285980225, 3.752417723337809]
memory len:10000
memory used:2890.0
now epsilon is 0.11931171426434846, the reward is 247.25 with loss [30.11140537261963, 28.004451751708984] in episode 1493
Report: 
rewardSum:247.25
loss:[30.11140537261963, 28.004451751708984]
policies:[1, 3, 0]
qAverage:[0.0, 46.48568916320801]
ws:[1.250319890677929, 3.221930205821991]
memory len:10000
memory used:2890.0
now epsilon is 0.11919244728452047, the reward is 247.25 with loss [23.76746368408203, 22.908166885375977] in episode 1494
Report: 
rewardSum:247.25
loss:[23.76746368408203, 22.908166885375977]
policies:[0, 4, 0]
qAverage:[0.0, 59.98732299804688]
ws:[1.276318794488907, 3.480390214920044]
memory len:10000
memory used:2890.0
now epsilon is 0.11907329952695463, the reward is 247.25 with loss [26.715003967285156, 22.49893093109131] in episode 1495
Report: 
rewardSum:247.25
loss:[26.715003967285156, 22.49893093109131]
policies:[0, 4, 0]
qAverage:[0.0, 59.445570373535155]
ws:[2.138457638025284, 4.7286779403686525]
memory len:10000
memory used:2890.0
now epsilon is 0.1189542708724734, the reward is 247.25 with loss [32.27498245239258, 27.000484943389893] in episode 1496
Report: 
rewardSum:247.25
loss:[32.27498245239258, 27.000484943389893]
policies:[0, 4, 0]
qAverage:[0.0, 56.94308319091797]
ws:[2.888161826133728, 5.8581342697143555]
memory len:10000
memory used:2890.0
now epsilon is 0.11883536120201835, the reward is 247.25 with loss [24.339462757110596, 23.59468686580658] in episode 1497
Report: 
rewardSum:247.25
loss:[24.339462757110596, 23.59468686580658]
policies:[0, 4, 0]
qAverage:[0.0, 59.86355361938477]
ws:[4.479802751541138, 7.012763786315918]
memory len:10000
memory used:2890.0
now epsilon is 0.11865721953123737, the reward is 245.25 with loss [31.913060903549194, 38.28798866271973] in episode 1498
Report: 
rewardSum:245.25
loss:[31.913060903549194, 38.28798866271973]
policies:[0, 4, 2]
qAverage:[0.0, 41.77918720245361]
ws:[2.7014326453208923, 4.653950095176697]
memory len:10000
memory used:2889.0
now epsilon is 0.11853860680074785, the reward is 37.15999999999997 with loss [27.68594264984131, 22.51249408721924] in episode 1499
Report: 
rewardSum:37.15999999999997
loss:[27.68594264984131, 22.51249408721924]
policies:[0, 3, 1]
qAverage:[0.0, 46.49707889556885]
ws:[0.6441331133246422, 1.9228966683149338]
memory len:10000
memory used:2890.0
now epsilon is 0.11839050761035684, the reward is 246.25 with loss [25.013221263885498, 25.160374402999878] in episode 1500
Report: 
rewardSum:246.25
loss:[25.013221263885498, 25.160374402999878]
policies:[0, 4, 1]
qAverage:[0.0, 58.815399169921875]
ws:[2.497348737716675, 3.8367560863494874]
memory len:10000
memory used:2890.0
now epsilon is 0.11815393367496513, the reward is 243.25 with loss [48.66348886489868, 48.42099595069885] in episode 1501
Report: 
rewardSum:243.25
loss:[48.66348886489868, 48.42099595069885]
policies:[0, 4, 4]
qAverage:[0.0, 46.997371673583984]
ws:[2.8345146119594573, 4.763192749023437]
memory len:10000
memory used:2890.0
now epsilon is 0.11803582404163114, the reward is 247.25 with loss [23.404510974884033, 22.584434032440186] in episode 1502
Report: 
rewardSum:247.25
loss:[23.404510974884033, 22.584434032440186]
policies:[0, 4, 0]
qAverage:[0.0, 60.60214614868164]
ws:[3.5188167095184326, 5.844818711280823]
memory len:10000
memory used:2890.0
now epsilon is 0.11791783247364675, the reward is 247.25 with loss [19.004254817962646, 22.468833208084106] in episode 1503
Report: 
rewardSum:247.25
loss:[19.004254817962646, 22.468833208084106]
policies:[0, 4, 0]
qAverage:[0.0, 59.05020370483398]
ws:[4.5518183469772335, 7.62883415222168]
memory len:10000
memory used:2890.0
now epsilon is 0.1177999588529909, the reward is 247.25 with loss [30.251423835754395, 24.02696442604065] in episode 1504
Report: 
rewardSum:247.25
loss:[30.251423835754395, 24.02696442604065]
policies:[0, 3, 1]
qAverage:[0.0, 63.66280746459961]
ws:[3.611691564321518, 6.416900634765625]
memory len:10000
memory used:2890.0
now epsilon is 0.11768220306176047, the reward is 247.25 with loss [35.198050022125244, 19.665616035461426] in episode 1505
Report: 
rewardSum:247.25
loss:[35.198050022125244, 19.665616035461426]
policies:[0, 4, 0]
qAverage:[0.0, 72.50888290405274]
ws:[2.5230300813913344, 5.176096343994141]
memory len:10000
memory used:2889.0
now epsilon is 0.11756456498217019, the reward is 247.25 with loss [26.947232723236084, 26.845710277557373] in episode 1506
Report: 
rewardSum:247.25
loss:[26.947232723236084, 26.845710277557373]
policies:[0, 3, 1]
qAverage:[0.0, 70.7431468963623]
ws:[4.185024440288544, 6.68142557144165]
memory len:10000
memory used:2889.0
now epsilon is 0.11744704449655259, the reward is 247.25 with loss [22.703893184661865, 21.167291164398193] in episode 1507
Report: 
rewardSum:247.25
loss:[22.703893184661865, 21.167291164398193]
policies:[0, 4, 0]
qAverage:[0.0, 70.60145492553711]
ws:[4.823040699958801, 6.94119815826416]
memory len:10000
memory used:2890.0
now epsilon is 0.11730030907698591, the reward is 246.25 with loss [30.729776620864868, 28.5529842376709] in episode 1508
Report: 
rewardSum:246.25
loss:[30.729776620864868, 28.5529842376709]
policies:[0, 4, 1]
qAverage:[0.0, 69.04981307983398]
ws:[7.782364058494568, 10.131822109222412]
memory len:10000
memory used:2889.0
now epsilon is 0.11718305274819403, the reward is 247.25 with loss [26.8294358253479, 20.149449586868286] in episode 1509
Report: 
rewardSum:247.25
loss:[26.8294358253479, 20.149449586868286]
policies:[1, 2, 1]
qAverage:[38.38226318359375, 38.23347568511963]
ws:[8.534478187561035, 10.272610187530518]
memory len:10000
memory used:2889.0
now epsilon is 0.11706591363176716, the reward is 247.25 with loss [19.00264263153076, 18.445804953575134] in episode 1510
Report: 
rewardSum:247.25
loss:[19.00264263153076, 18.445804953575134]
policies:[1, 3, 0]
qAverage:[29.866207885742188, 43.52828140258789]
ws:[5.704992318153382, 7.670434474945068]
memory len:10000
memory used:2890.0
now epsilon is 0.11694889161053684, the reward is 247.25 with loss [23.225229740142822, 23.513027667999268] in episode 1511
Report: 
rewardSum:247.25
loss:[23.225229740142822, 23.513027667999268]
policies:[0, 4, 0]
qAverage:[0.0, 70.63777008056641]
ws:[3.704709732532501, 6.125191402435303]
memory len:10000
memory used:2890.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23*		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.11683198656745182, the reward is 247.25 with loss [19.793689370155334, 18.575250148773193] in episode 1512
Report: 
rewardSum:247.25
loss:[19.793689370155334, 18.575250148773193]
policies:[0, 4, 0]
qAverage:[0.0, 70.12899703979492]
ws:[6.366556411981582, 8.948786783218384]
memory len:10000
memory used:2890.0
now epsilon is 0.11671519838557781, the reward is 247.25 with loss [17.23892879486084, 25.909332752227783] in episode 1513
Report: 
rewardSum:247.25
loss:[17.23892879486084, 25.909332752227783]
policies:[1, 2, 1]
qAverage:[0.0, 55.224039713541664]
ws:[3.5903294881184897, 6.8651472727457685]
memory len:10000
memory used:2890.0
now epsilon is 0.11659852694809739, the reward is 247.25 with loss [28.691208600997925, 29.793826580047607] in episode 1514
Report: 
rewardSum:247.25
loss:[28.691208600997925, 29.793826580047607]
policies:[0, 4, 0]
qAverage:[0.0, 70.83368530273438]
ws:[8.857734107971192, 11.851398372650147]
memory len:10000
memory used:2890.0
now epsilon is 0.11648197213830995, the reward is 247.25 with loss [17.94702184200287, 29.733566284179688] in episode 1515
Report: 
rewardSum:247.25
loss:[17.94702184200287, 29.733566284179688]
policies:[0, 4, 0]
qAverage:[0.0, 54.4289665222168]
ws:[4.414992809295654, 7.4110411405563354]
memory len:10000
memory used:2890.0
now epsilon is 0.11636553383963154, the reward is 247.25 with loss [28.992605686187744, 33.730692863464355] in episode 1516
Report: 
rewardSum:247.25
loss:[28.992605686187744, 33.730692863464355]
policies:[0, 4, 0]
qAverage:[0.0, 76.88938827514649]
ws:[7.1322669506073, 9.594191551208496]
memory len:10000
memory used:2890.0
now epsilon is 0.1162492119355947, the reward is 247.25 with loss [28.928821563720703, 29.06899070739746] in episode 1517
Report: 
rewardSum:247.25
loss:[28.928821563720703, 29.06899070739746]
policies:[0, 4, 0]
qAverage:[0.0, 87.31976928710938]
ws:[7.259583055973053, 10.431691741943359]
memory len:10000
memory used:2890.0
now epsilon is 0.11613300630984848, the reward is 247.25 with loss [22.295313119888306, 24.746436595916748] in episode 1518
Report: 
rewardSum:247.25
loss:[22.295313119888306, 24.746436595916748]
policies:[1, 3, 0]
qAverage:[0.0, 84.69864082336426]
ws:[12.580480337142944, 15.460343837738037]
memory len:10000
memory used:2890.0
now epsilon is 0.11601691684615814, the reward is 247.25 with loss [22.933355808258057, 35.43595314025879] in episode 1519
Report: 
rewardSum:247.25
loss:[22.933355808258057, 35.43595314025879]
policies:[0, 4, 0]
qAverage:[0.0, 85.74953269958496]
ws:[10.330623865127563, 13.933554649353027]
memory len:10000
memory used:2890.0
now epsilon is 0.1158430002005, the reward is 245.25 with loss [32.699119329452515, 37.42441129684448] in episode 1520
Report: 
rewardSum:245.25
loss:[32.699119329452515, 37.42441129684448]
policies:[0, 5, 1]
qAverage:[0.0, 88.49011866251628]
ws:[6.756078104178111, 10.859252214431763]
memory len:10000
memory used:2890.0
now epsilon is 0.11572720063418485, the reward is 247.25 with loss [35.09507179260254, 21.71326780319214] in episode 1521
Report: 
rewardSum:247.25
loss:[35.09507179260254, 21.71326780319214]
policies:[0, 4, 0]
qAverage:[0.0, 85.28415985107422]
ws:[8.126540490984917, 11.954030609130859]
memory len:10000
memory used:2889.0
now epsilon is 0.1156115168240184, the reward is 247.25 with loss [22.103294849395752, 18.656518697738647] in episode 1522
Report: 
rewardSum:247.25
loss:[22.103294849395752, 18.656518697738647]
policies:[0, 3, 1]
qAverage:[0.0, 82.2885627746582]
ws:[11.337400317192078, 14.45727789402008]
memory len:10000
memory used:2890.0
now epsilon is 0.11549594865428794, the reward is 247.25 with loss [29.862122535705566, 24.26705265045166] in episode 1523
Report: 
rewardSum:247.25
loss:[29.862122535705566, 24.26705265045166]
policies:[0, 4, 0]
qAverage:[0.0, 91.31995964050293]
ws:[8.18715614080429, 11.295432448387146]
memory len:10000
memory used:2890.0
now epsilon is 0.11538049600939637, the reward is 247.25 with loss [19.919299364089966, 25.67063617706299] in episode 1524
Report: 
rewardSum:247.25
loss:[19.919299364089966, 25.67063617706299]
policies:[2, 2, 0]
qAverage:[0.0, 49.18658955891927]
ws:[1.191683014233907, 3.5563081900278726]
memory len:10000
memory used:2890.0
now epsilon is 0.11520753339854765, the reward is 35.15999999999997 with loss [37.800333976745605, 29.94057869911194] in episode 1525
Report: 
rewardSum:35.15999999999997
loss:[37.800333976745605, 29.94057869911194]
policies:[0, 4, 2]
qAverage:[0.0, 82.42644882202148]
ws:[4.924066007137299, 8.50152325630188]
memory len:10000
memory used:2890.0
now epsilon is 0.11514993683231922, the reward is -1.0 with loss [11.296708822250366, 14.996202945709229] in episode 1526
Report: 
rewardSum:-1.0
loss:[11.296708822250366, 14.996202945709229]
policies:[0, 1, 1]
qAverage:[0.0, 31.46507453918457]
ws:[2.5248968601226807, 5.1511335372924805]
memory len:10000
memory used:2890.0
now epsilon is 0.1150348300695168, the reward is 247.25 with loss [22.979912042617798, 26.882851600646973] in episode 1527
Report: 
rewardSum:247.25
loss:[22.979912042617798, 26.882851600646973]
policies:[0, 4, 0]
qAverage:[0.0, 86.07488555908203]
ws:[10.300768089294433, 15.17325496673584]
memory len:10000
memory used:2890.0
now epsilon is 0.11491983837031933, the reward is 247.25 with loss [35.8623628616333, 35.21841478347778] in episode 1528
Report: 
rewardSum:247.25
loss:[35.8623628616333, 35.21841478347778]
policies:[0, 4, 0]
qAverage:[0.0, 91.40422248840332]
ws:[9.099891185760498, 13.508605241775513]
memory len:10000
memory used:2890.0
now epsilon is 0.1147475663142066, the reward is 245.25 with loss [37.98454403877258, 39.40594005584717] in episode 1529
Report: 
rewardSum:245.25
loss:[37.98454403877258, 39.40594005584717]
policies:[1, 4, 1]
qAverage:[0.0, 104.10587768554687]
ws:[7.429672768712043, 12.047002983093261]
memory len:10000
memory used:2891.0
now epsilon is 0.1146328617710585, the reward is 247.25 with loss [28.184706211090088, 25.43550395965576] in episode 1530
Report: 
rewardSum:247.25
loss:[28.184706211090088, 25.43550395965576]
policies:[0, 3, 1]
qAverage:[0.0, 97.52863311767578]
ws:[15.74202823638916, 19.645694255828857]
memory len:10000
memory used:2891.0
now epsilon is 0.11451827188944652, the reward is 247.25 with loss [23.215931177139282, 27.033266067504883] in episode 1531
Report: 
rewardSum:247.25
loss:[23.215931177139282, 27.033266067504883]
policies:[1, 3, 0]
qAverage:[41.13261108398437, 63.137384033203126]
ws:[12.229594898223876, 13.777780151367187]
memory len:10000
memory used:2891.0
now epsilon is 0.11440379655475211, the reward is 247.25 with loss [22.66812801361084, 24.878434658050537] in episode 1532
Report: 
rewardSum:247.25
loss:[22.66812801361084, 24.878434658050537]
policies:[2, 2, 0]
qAverage:[47.49448013305664, 45.83642387390137]
ws:[10.550044596195221, 10.143848896026611]
memory len:10000
memory used:2891.0
now epsilon is 0.11417518906821456, the reward is 243.25 with loss [46.37860631942749, 46.54809045791626] in episode 1533
Report: 
rewardSum:243.25
loss:[46.37860631942749, 46.54809045791626]
policies:[0, 5, 3]
qAverage:[0.0, 71.92877388000488]
ws:[0.014109969139099121, 2.196794033050537]
memory len:10000
memory used:2891.0
now epsilon is 0.11406105668770675, the reward is 247.25 with loss [25.947704315185547, 26.850898265838623] in episode 1534
Report: 
rewardSum:247.25
loss:[25.947704315185547, 26.850898265838623]
policies:[1, 3, 0]
qAverage:[40.60884094238281, 61.88511505126953]
ws:[10.817522287368774, 12.722247886657716]
memory len:10000
memory used:2891.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25*		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.11394703839678695, the reward is 247.25 with loss [20.926525115966797, 26.030433654785156] in episode 1535
Report: 
rewardSum:247.25
loss:[20.926525115966797, 26.030433654785156]
policies:[1, 3, 0]
qAverage:[0.0, 79.70392417907715]
ws:[4.282073676586151, 8.029837489128113]
memory len:10000
memory used:2891.0
now epsilon is 0.11383313408140833, the reward is 247.25 with loss [30.450119018554688, 26.893216609954834] in episode 1536
Report: 
rewardSum:247.25
loss:[30.450119018554688, 26.893216609954834]
policies:[1, 3, 0]
qAverage:[0.0, 103.21444511413574]
ws:[13.716837406158447, 16.568819284439087]
memory len:10000
memory used:2891.0
now epsilon is 0.11371934362763808, the reward is 247.25 with loss [28.407379150390625, 19.681849479675293] in episode 1537
Report: 
rewardSum:247.25
loss:[28.407379150390625, 19.681849479675293]
policies:[0, 4, 0]
qAverage:[0.0, 97.44992370605469]
ws:[10.821035408973694, 14.56044635772705]
memory len:10000
memory used:2890.0
now epsilon is 0.1136056669216573, the reward is 247.25 with loss [25.148839473724365, 22.214387893676758] in episode 1538
Report: 
rewardSum:247.25
loss:[25.148839473724365, 22.214387893676758]
policies:[1, 1, 2]
qAverage:[67.9979756673177, 44.69353739420573]
ws:[17.70582326253255, 20.087561289469402]
memory len:10000
memory used:2891.0
now epsilon is 0.11349210384976084, the reward is 247.25 with loss [36.717710971832275, 26.809113025665283] in episode 1539
Report: 
rewardSum:247.25
loss:[36.717710971832275, 26.809113025665283]
policies:[1, 3, 0]
qAverage:[44.440249633789065, 67.56633148193359]
ws:[11.792697143554687, 14.571703624725341]
memory len:10000
memory used:2891.0
now epsilon is 0.11337865429835724, the reward is 247.25 with loss [32.048351764678955, 35.536200523376465] in episode 1540
Report: 
rewardSum:247.25
loss:[32.048351764678955, 35.536200523376465]
policies:[1, 3, 0]
qAverage:[46.940493774414065, 79.3768295288086]
ws:[10.02612238228321, 12.535450792312622]
memory len:10000
memory used:2891.0
now epsilon is 0.11326531815396852, the reward is 247.25 with loss [24.966752529144287, 27.761834621429443] in episode 1541
Report: 
rewardSum:247.25
loss:[24.966752529144287, 27.761834621429443]
policies:[1, 3, 0]
qAverage:[42.452206420898435, 73.63457794189453]
ws:[12.584693229198455, 15.274674320220948]
memory len:10000
memory used:2891.0
now epsilon is 0.11315209530323023, the reward is 247.25 with loss [23.613608598709106, 32.034247398376465] in episode 1542
Report: 
rewardSum:247.25
loss:[23.613608598709106, 32.034247398376465]
policies:[1, 3, 0]
qAverage:[47.41709594726562, 77.06624298095703]
ws:[13.925443172454834, 16.11370964050293]
memory len:10000
memory used:2891.0
now epsilon is 0.11303898563289119, the reward is 247.25 with loss [22.817917108535767, 22.531159162521362] in episode 1543
Report: 
rewardSum:247.25
loss:[22.817917108535767, 22.531159162521362]
policies:[1, 3, 0]
qAverage:[56.38129425048828, 72.45915031433105]
ws:[15.610874891281128, 17.80740189552307]
memory len:10000
memory used:2891.0
now epsilon is 0.11292598902981343, the reward is 247.25 with loss [27.865598440170288, 22.29449987411499] in episode 1544
Report: 
rewardSum:247.25
loss:[27.865598440170288, 22.29449987411499]
policies:[1, 3, 0]
qAverage:[44.06734924316406, 72.88752746582031]
ws:[12.93782081604004, 15.241291618347168]
memory len:10000
memory used:2892.0
now epsilon is 0.1128131053809721, the reward is 247.25 with loss [24.21207857131958, 24.90004253387451] in episode 1545
Report: 
rewardSum:247.25
loss:[24.21207857131958, 24.90004253387451]
policies:[1, 3, 0]
qAverage:[46.22618408203125, 79.03536834716797]
ws:[13.442194509506226, 16.306503200531004]
memory len:10000
memory used:2891.0
now epsilon is 0.11270033457345528, the reward is 247.25 with loss [16.59493136405945, 24.620342254638672] in episode 1546
Report: 
rewardSum:247.25
loss:[16.59493136405945, 24.620342254638672]
policies:[1, 3, 0]
qAverage:[0.0, 93.4592170715332]
ws:[5.8088390827178955, 10.464210033416748]
memory len:10000
memory used:2891.0
now epsilon is 0.11258767649446397, the reward is 247.25 with loss [21.302279710769653, 19.396050453186035] in episode 1547
Report: 
rewardSum:247.25
loss:[21.302279710769653, 19.396050453186035]
policies:[2, 2, 0]
qAverage:[57.260520935058594, 55.974578857421875]
ws:[12.26381927728653, 14.151792287826538]
memory len:10000
memory used:2890.0
now epsilon is 0.1124751310313119, the reward is 247.25 with loss [23.865588903427124, 38.03389883041382] in episode 1548
Report: 
rewardSum:247.25
loss:[23.865588903427124, 38.03389883041382]
policies:[1, 3, 0]
qAverage:[47.03507690429687, 75.73984985351562]
ws:[12.666816806793213, 16.0697998046875]
memory len:10000
memory used:2891.0
now epsilon is 0.11236269807142549, the reward is 247.25 with loss [21.330466508865356, 18.757884979248047] in episode 1549
Report: 
rewardSum:247.25
loss:[21.330466508865356, 18.757884979248047]
policies:[1, 3, 0]
qAverage:[47.87542724609375, 76.11456909179688]
ws:[10.597757983207703, 13.546807861328125]
memory len:10000
memory used:2891.0
now epsilon is 0.11225037750234362, the reward is 247.25 with loss [27.445265769958496, 25.439358711242676] in episode 1550
Report: 
rewardSum:247.25
loss:[27.445265769958496, 25.439358711242676]
policies:[0, 4, 0]
qAverage:[0.0, 116.26674041748046]
ws:[10.700206941366195, 14.188925552368165]
memory len:10000
memory used:2890.0
now epsilon is 0.11213816921171765, the reward is 247.25 with loss [26.056303024291992, 29.207959175109863] in episode 1551
Report: 
rewardSum:247.25
loss:[26.056303024291992, 29.207959175109863]
policies:[0, 4, 0]
qAverage:[0.0, 119.99373474121094]
ws:[12.475965118408203, 15.938883113861085]
memory len:10000
memory used:2890.0
now epsilon is 0.1120260730873112, the reward is 247.25 with loss [29.69938039779663, 31.884674549102783] in episode 1552
Report: 
rewardSum:247.25
loss:[29.69938039779663, 31.884674549102783]
policies:[1, 3, 0]
qAverage:[56.27197265625, 86.95794372558593]
ws:[15.099200010299683, 18.58860731124878]
memory len:10000
memory used:2892.0
now epsilon is 0.11191408901700012, the reward is 247.25 with loss [47.99961805343628, 31.474446296691895] in episode 1553
Report: 
rewardSum:247.25
loss:[47.99961805343628, 31.474446296691895]
policies:[2, 1, 1]
qAverage:[80.17014058430989, 40.68250020345052]
ws:[23.86843736966451, 25.151426633199055]
memory len:10000
memory used:2891.0
now epsilon is 0.1118022168887723, the reward is 247.25 with loss [27.124830722808838, 28.915631771087646] in episode 1554
Report: 
rewardSum:247.25
loss:[27.124830722808838, 28.915631771087646]
policies:[1, 2, 1]
qAverage:[59.41127395629883, 70.90607452392578]
ws:[17.707934975624084, 19.220853090286255]
memory len:10000
memory used:2891.0
now epsilon is 0.11169045659072767, the reward is 247.25 with loss [32.032179832458496, 24.649881839752197] in episode 1555
Report: 
rewardSum:247.25
loss:[32.032179832458496, 24.649881839752197]
policies:[1, 2, 1]
qAverage:[63.711143493652344, 66.97872352600098]
ws:[11.742664486169815, 13.178284406661987]
memory len:10000
memory used:2891.0
now epsilon is 0.11157880801107797, the reward is 247.25 with loss [32.39801788330078, 26.044036388397217] in episode 1556
Report: 
rewardSum:247.25
loss:[32.39801788330078, 26.044036388397217]
policies:[0, 4, 0]
qAverage:[0.0, 133.28451538085938]
ws:[14.263373374938965, 17.391096830368042]
memory len:10000
memory used:2892.0
now epsilon is 0.11146727103814667, the reward is 247.25 with loss [31.898681640625, 30.9186429977417] in episode 1557
Report: 
rewardSum:247.25
loss:[31.898681640625, 30.9186429977417]
policies:[1, 3, 0]
qAverage:[79.27925618489583, 39.25494893391927]
ws:[19.145883520444233, 20.467531045277912]
memory len:10000
memory used:2891.0
now epsilon is 0.11135584556036891, the reward is 247.25 with loss [32.65192985534668, 16.74827814102173] in episode 1558
Report: 
rewardSum:247.25
loss:[32.65192985534668, 16.74827814102173]
policies:[1, 3, 0]
qAverage:[58.082130432128906, 74.83468055725098]
ws:[18.805065035820007, 19.793596744537354]
memory len:10000
memory used:2891.0
now epsilon is 0.11124453146629133, the reward is 247.25 with loss [20.390735507011414, 25.948938608169556] in episode 1559
Report: 
rewardSum:247.25
loss:[20.390735507011414, 25.948938608169556]
policies:[1, 3, 0]
qAverage:[0.0, 115.92501258850098]
ws:[5.130989253520966, 7.003409743309021]
memory len:10000
memory used:2891.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25*		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.11113332864457201, the reward is 247.25 with loss [32.718085289001465, 20.83722972869873] in episode 1560
Report: 
rewardSum:247.25
loss:[32.718085289001465, 20.83722972869873]
policies:[1, 3, 0]
qAverage:[46.998748779296875, 86.99483947753906]
ws:[12.184452104568482, 14.110286855697632]
memory len:10000
memory used:2891.0
now epsilon is 0.1110222369839803, the reward is 247.25 with loss [28.610981941223145, 31.296581745147705] in episode 1561
Report: 
rewardSum:247.25
loss:[28.610981941223145, 31.296581745147705]
policies:[1, 3, 0]
qAverage:[48.80881958007812, 92.11124877929687]
ws:[14.904295539855957, 16.988079071044922]
memory len:10000
memory used:2891.0
now epsilon is 0.11085580767716358, the reward is 245.25 with loss [33.11210322380066, 26.614272356033325] in episode 1562
Report: 
rewardSum:245.25
loss:[33.11210322380066, 26.614272356033325]
policies:[0, 5, 1]
qAverage:[0.0, 121.07070465087891]
ws:[6.527556037902832, 10.334284591674805]
memory len:10000
memory used:2891.0
now epsilon is 0.11074499343348623, the reward is 247.25 with loss [23.68729043006897, 28.012476921081543] in episode 1563
Report: 
rewardSum:247.25
loss:[23.68729043006897, 28.012476921081543]
policies:[1, 3, 0]
qAverage:[48.109396362304686, 88.05741882324219]
ws:[12.392897367477417, 15.523839855194092]
memory len:10000
memory used:2891.0
now epsilon is 0.11063428996250417, the reward is 247.25 with loss [42.53324317932129, 36.04859638214111] in episode 1564
Report: 
rewardSum:247.25
loss:[42.53324317932129, 36.04859638214111]
policies:[0, 4, 0]
qAverage:[0.0, 140.73096923828126]
ws:[14.075089979171754, 18.636358165740965]
memory len:10000
memory used:2891.0
now epsilon is 0.1105236971534862, the reward is 247.25 with loss [22.86535620689392, 26.053269863128662] in episode 1565
Report: 
rewardSum:247.25
loss:[22.86535620689392, 26.053269863128662]
policies:[0, 4, 0]
qAverage:[0.0, 142.22333526611328]
ws:[18.387134432792664, 24.22734546661377]
memory len:10000
memory used:2891.0
now epsilon is 0.11041321489581185, the reward is 247.25 with loss [16.97783327102661, 22.81422758102417] in episode 1566
Report: 
rewardSum:247.25
loss:[16.97783327102661, 22.81422758102417]
policies:[0, 4, 0]
qAverage:[0.0, 144.01907653808593]
ws:[13.951971578598023, 19.772187805175783]
memory len:10000
memory used:2891.0
now epsilon is 0.11030284307897124, the reward is 247.25 with loss [24.503462314605713, 30.46984052658081] in episode 1567
Report: 
rewardSum:247.25
loss:[24.503462314605713, 30.46984052658081]
policies:[1, 3, 0]
qAverage:[0.0, 127.35893630981445]
ws:[14.428380496799946, 20.06099557876587]
memory len:10000
memory used:2892.0
now epsilon is 0.11019258159256495, the reward is 247.25 with loss [23.399112701416016, 10.835116863250732] in episode 1568
Report: 
rewardSum:247.25
loss:[23.399112701416016, 10.835116863250732]
policies:[0, 3, 1]
qAverage:[0.0, 147.77222061157227]
ws:[13.703947067260742, 20.210714101791382]
memory len:10000
memory used:2892.0
now epsilon is 0.1100824303263039, the reward is 247.25 with loss [25.71637535095215, 25.385730266571045] in episode 1569
Report: 
rewardSum:247.25
loss:[25.71637535095215, 25.385730266571045]
policies:[0, 4, 0]
qAverage:[0.0, 140.85115432739258]
ws:[15.266965627670288, 22.25187611579895]
memory len:10000
memory used:2892.0
now epsilon is 0.10994489607271675, the reward is 246.25 with loss [30.880716383457184, 38.359339237213135] in episode 1570
Report: 
rewardSum:246.25
loss:[30.880716383457184, 38.359339237213135]
policies:[0, 4, 1]
qAverage:[0.0, 141.09429016113282]
ws:[16.267154741287232, 23.395376777648927]
memory len:10000
memory used:2892.0
now epsilon is 0.10978008176759645, the reward is 245.25 with loss [52.55012273788452, 43.985766887664795] in episode 1571
Report: 
rewardSum:245.25
loss:[52.55012273788452, 43.985766887664795]
policies:[0, 5, 1]
qAverage:[0.0, 145.52594375610352]
ws:[14.555339892705282, 22.16186285018921]
memory len:10000
memory used:2892.0
now epsilon is 0.1096155145294719, the reward is 245.25 with loss [32.60238170623779, 34.80730175971985] in episode 1572
Report: 
rewardSum:245.25
loss:[32.60238170623779, 34.80730175971985]
policies:[0, 3, 3]
qAverage:[0.0, 132.58439254760742]
ws:[4.110199421644211, 10.836230039596558]
memory len:10000
memory used:2892.0
now epsilon is 0.10950594011390984, the reward is 247.25 with loss [27.854724645614624, 23.25420069694519] in episode 1573
Report: 
rewardSum:247.25
loss:[27.854724645614624, 23.25420069694519]
policies:[0, 3, 1]
qAverage:[0.0, 133.27465057373047]
ws:[13.43503312766552, 18.455353498458862]
memory len:10000
memory used:2892.0
now epsilon is 0.1093964752316798, the reward is 247.25 with loss [35.89577674865723, 32.61673045158386] in episode 1574
Report: 
rewardSum:247.25
loss:[35.89577674865723, 32.61673045158386]
policies:[0, 4, 0]
qAverage:[0.0, 141.18023223876952]
ws:[13.284216755628586, 18.663955307006837]
memory len:10000
memory used:2892.0
now epsilon is 0.10928711977328949, the reward is 247.25 with loss [42.571890354156494, 30.481149196624756] in episode 1575
Report: 
rewardSum:247.25
loss:[42.571890354156494, 30.481149196624756]
policies:[0, 4, 0]
qAverage:[0.0, 143.49619750976564]
ws:[12.652035355567932, 17.55666971206665]
memory len:10000
memory used:2891.0
now epsilon is 0.10917787362935612, the reward is 247.25 with loss [30.794233322143555, 31.923712730407715] in episode 1576
Report: 
rewardSum:247.25
loss:[30.794233322143555, 31.923712730407715]
policies:[0, 3, 1]
qAverage:[0.0, 131.9908447265625]
ws:[1.7669351994991302, 7.183117032051086]
memory len:10000
memory used:2891.0
now epsilon is 0.10904146950643355, the reward is 246.25 with loss [33.97547674179077, 23.240891098976135] in episode 1577
Report: 
rewardSum:246.25
loss:[33.97547674179077, 23.240891098976135]
policies:[0, 4, 1]
qAverage:[0.0, 145.63345031738282]
ws:[15.043563938140869, 21.641490364074706]
memory len:10000
memory used:2891.0
now epsilon is 0.10893246892066352, the reward is 247.25 with loss [23.73338270187378, 22.551424264907837] in episode 1578
Report: 
rewardSum:247.25
loss:[23.73338270187378, 22.551424264907837]
policies:[0, 4, 0]
qAverage:[0.0, 148.7117126464844]
ws:[13.756258916854858, 20.742934226989746]
memory len:10000
memory used:2891.0
now epsilon is 0.10876917230743714, the reward is 245.25 with loss [35.68555521965027, 49.02299690246582] in episode 1579
Report: 
rewardSum:245.25
loss:[35.68555521965027, 49.02299690246582]
policies:[1, 3, 2]
qAverage:[0.0, 134.44143676757812]
ws:[2.202899068593979, 7.60397732257843]
memory len:10000
memory used:2891.0
now epsilon is 0.1087147945193567, the reward is -1.0 with loss [10.162082195281982, 9.409410953521729] in episode 1580
Report: 
rewardSum:-1.0
loss:[10.162082195281982, 9.409410953521729]
policies:[0, 1, 1]
qAverage:[0.0, 73.54219055175781]
ws:[-1.4800289869308472, 1.5879806280136108]
memory len:10000
memory used:2891.0
now epsilon is 0.10860612048609104, the reward is 247.25 with loss [22.094558715820312, 24.910613775253296] in episode 1581
Report: 
rewardSum:247.25
loss:[22.094558715820312, 24.910613775253296]
policies:[0, 4, 0]
qAverage:[0.0, 145.36105041503907]
ws:[10.623283338546752, 17.65446834564209]
memory len:10000
memory used:2891.0
now epsilon is 0.1084975550861127, the reward is 247.25 with loss [24.242679595947266, 24.845089435577393] in episode 1582
Report: 
rewardSum:247.25
loss:[24.242679595947266, 24.845089435577393]
policies:[0, 4, 0]
qAverage:[0.0, 146.85463256835936]
ws:[9.755494904518127, 15.935105991363525]
memory len:10000
memory used:2891.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26*		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.10838909821082908, the reward is 247.25 with loss [24.836533546447754, 9.83044970035553] in episode 1583
Report: 
rewardSum:247.25
loss:[24.836533546447754, 9.83044970035553]
policies:[0, 4, 0]
qAverage:[0.0, 146.43536071777345]
ws:[10.459392601251603, 16.45289840698242]
memory len:10000
memory used:2891.0
############# STATE ###############
0*		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.10819955949039109, the reward is 244.25 with loss [45.93347907066345, 39.222572326660156] in episode 1584
Report: 
rewardSum:244.25
loss:[45.93347907066345, 39.222572326660156]
policies:[0, 4, 3]
qAverage:[0.0, 137.6927146911621]
ws:[9.641537517309189, 13.519491791725159]
memory len:10000
memory used:2891.0
now epsilon is 0.10803736155443654, the reward is 245.25 with loss [35.56556844711304, 49.37306451797485] in episode 1585
Report: 
rewardSum:245.25
loss:[35.56556844711304, 49.37306451797485]
policies:[0, 5, 1]
qAverage:[0.0, 148.61023864746093]
ws:[10.381594675779343, 14.196264791488648]
memory len:10000
memory used:2891.0
now epsilon is 0.10792936470014079, the reward is 247.25 with loss [21.976671934127808, 33.9078893661499] in episode 1586
Report: 
rewardSum:247.25
loss:[21.976671934127808, 33.9078893661499]
policies:[1, 3, 0]
qAverage:[0.0, 140.0061912536621]
ws:[12.644412439316511, 18.663100719451904]
memory len:10000
memory used:2891.0
now epsilon is 0.10782147580220726, the reward is 247.25 with loss [23.771785259246826, 29.69789743423462] in episode 1587
Report: 
rewardSum:247.25
loss:[23.771785259246826, 29.69789743423462]
policies:[0, 4, 0]
qAverage:[0.0, 152.20078125]
ws:[11.802378535270691, 19.572705268859863]
memory len:10000
memory used:2891.0
now epsilon is 0.10771369475272007, the reward is 247.25 with loss [21.2359516620636, 35.67831230163574] in episode 1588
Report: 
rewardSum:247.25
loss:[21.2359516620636, 35.67831230163574]
policies:[0, 4, 0]
qAverage:[0.0, 154.19765930175782]
ws:[12.262377148866653, 21.053884983062744]
memory len:10000
memory used:2891.0
now epsilon is 0.1076060214438712, the reward is 247.25 with loss [19.570098876953125, 28.344464778900146] in episode 1589
Report: 
rewardSum:247.25
loss:[19.570098876953125, 28.344464778900146]
policies:[0, 4, 0]
qAverage:[0.0, 150.76298904418945]
ws:[13.321508526802063, 20.84510898590088]
memory len:10000
memory used:2892.0
now epsilon is 0.10749845576796042, the reward is 247.25 with loss [40.76961612701416, 24.83694291114807] in episode 1590
Report: 
rewardSum:247.25
loss:[40.76961612701416, 24.83694291114807]
policies:[0, 4, 0]
qAverage:[0.0, 150.1794464111328]
ws:[11.2345081448555, 19.770562744140626]
memory len:10000
memory used:2892.0
now epsilon is 0.10739099761739515, the reward is 247.25 with loss [30.1507625579834, 25.68983507156372] in episode 1591
Report: 
rewardSum:247.25
loss:[30.1507625579834, 25.68983507156372]
policies:[2, 2, 0]
qAverage:[0.0, 136.24548848470053]
ws:[17.308494567871094, 25.060338338216145]
memory len:10000
memory used:2891.0
now epsilon is 0.10723001176647595, the reward is 245.25 with loss [42.98373556137085, 21.854851484298706] in episode 1592
Report: 
rewardSum:245.25
loss:[42.98373556137085, 21.854851484298706]
policies:[0, 5, 1]
qAverage:[0.0, 153.70526733398438]
ws:[11.883837807178498, 19.52630558013916]
memory len:10000
memory used:2891.0
now epsilon is 0.10712282195926244, the reward is 247.25 with loss [27.58357810974121, 32.821330070495605] in episode 1593
Report: 
rewardSum:247.25
loss:[27.58357810974121, 32.821330070495605]
policies:[0, 4, 0]
qAverage:[0.0, 149.51060180664064]
ws:[10.246401795744896, 16.668845701217652]
memory len:10000
memory used:2890.0
now epsilon is 0.10701573930166666, the reward is 247.25 with loss [22.50535821914673, 24.128326892852783] in episode 1594
Report: 
rewardSum:247.25
loss:[22.50535821914673, 24.128326892852783]
policies:[1, 3, 0]
qAverage:[0.0, 149.69865798950195]
ws:[9.694038227200508, 15.18251621723175]
memory len:10000
memory used:2890.0
now epsilon is 0.10690876368657917, the reward is 247.25 with loss [31.857973098754883, 30.758164644241333] in episode 1595
Report: 
rewardSum:247.25
loss:[31.857973098754883, 30.758164644241333]
policies:[0, 4, 0]
qAverage:[0.0, 150.15241394042968]
ws:[9.706795823574065, 15.713408803939819]
memory len:10000
memory used:2890.0
now epsilon is 0.10680189500699759, the reward is 247.25 with loss [24.770312309265137, 28.987378120422363] in episode 1596
Report: 
rewardSum:247.25
loss:[24.770312309265137, 28.987378120422363]
policies:[1, 3, 0]
qAverage:[0.0, 112.77970886230469]
ws:[2.776012188910196, 7.2533566157023115]
memory len:10000
memory used:2890.0
now epsilon is 0.10669513315602652, the reward is 247.25 with loss [26.002034425735474, 32.5779447555542] in episode 1597
Report: 
rewardSum:247.25
loss:[26.002034425735474, 32.5779447555542]
policies:[0, 4, 0]
qAverage:[0.0, 150.64380493164063]
ws:[9.735319173336029, 16.686149501800536]
memory len:10000
memory used:2890.0
now epsilon is 0.10664179225789433, the reward is -1.0 with loss [11.191719055175781, 16.770318508148193] in episode 1598
Report: 
rewardSum:-1.0
loss:[11.191719055175781, 16.770318508148193]
policies:[0, 1, 1]
qAverage:[0.0, 79.29912567138672]
ws:[0.2685289680957794, 3.0366053581237793]
memory len:10000
memory used:2891.0
now epsilon is 0.10648192951286845, the reward is 245.25 with loss [42.59596014022827, 33.56107711791992] in episode 1599
Report: 
rewardSum:245.25
loss:[42.59596014022827, 33.56107711791992]
policies:[1, 4, 1]
qAverage:[0.0, 155.83344421386718]
ws:[13.188726949691773, 21.035499000549315]
memory len:10000
memory used:2891.0
now epsilon is 0.10632230641213872, the reward is 245.25 with loss [46.18087387084961, 36.47843599319458] in episode 1600
Report: 
rewardSum:245.25
loss:[46.18087387084961, 36.47843599319458]
policies:[0, 2, 4]
qAverage:[0.0, 90.00697326660156]
ws:[-0.04188661649823189, 2.2317864894866943]
memory len:10000
memory used:2891.0
now epsilon is 0.10621602396994675, the reward is 247.25 with loss [26.797126293182373, 24.064740657806396] in episode 1601
Report: 
rewardSum:247.25
loss:[26.797126293182373, 24.064740657806396]
policies:[0, 4, 0]
qAverage:[0.0, 148.65606689453125]
ws:[10.88091299533844, 17.008781051635744]
memory len:10000
memory used:2891.0
now epsilon is 0.10610984777034772, the reward is 247.25 with loss [20.61703896522522, 32.3402624130249] in episode 1602
Report: 
rewardSum:247.25
loss:[20.61703896522522, 32.3402624130249]
policies:[0, 3, 1]
qAverage:[0.0, 136.11053848266602]
ws:[13.74654807522893, 18.376415252685547]
memory len:10000
memory used:2891.0
now epsilon is 0.10600377770713885, the reward is 247.25 with loss [23.657710552215576, 26.35533571243286] in episode 1603
Report: 
rewardSum:247.25
loss:[23.657710552215576, 26.35533571243286]
policies:[0, 4, 0]
qAverage:[0.0, 149.23381652832032]
ws:[10.736620101332665, 14.484986877441406]
memory len:10000
memory used:2890.0
now epsilon is 0.10589781367422355, the reward is 247.25 with loss [29.885774612426758, 29.082096099853516] in episode 1604
Report: 
rewardSum:247.25
loss:[29.885774612426758, 29.082096099853516]
policies:[1, 3, 0]
qAverage:[0.0, 143.45757675170898]
ws:[12.99188381433487, 16.226207494735718]
memory len:10000
memory used:2890.0
now epsilon is 0.10579195556561126, the reward is 247.25 with loss [24.396061897277832, 27.831607818603516] in episode 1605
Report: 
rewardSum:247.25
loss:[24.396061897277832, 27.831607818603516]
policies:[0, 4, 0]
qAverage:[0.0, 148.91624145507814]
ws:[10.622907695174217, 15.23722701072693]
memory len:10000
memory used:2890.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25*		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.10568620327541742, the reward is 247.25 with loss [29.139206409454346, 19.698985815048218] in episode 1606
Report: 
rewardSum:247.25
loss:[29.139206409454346, 19.698985815048218]
policies:[1, 1, 2]
qAverage:[0.0, 77.13826751708984]
ws:[0.14783085882663727, 1.996558427810669]
memory len:10000
memory used:2890.0
now epsilon is 0.10558055669786326, the reward is 247.25 with loss [30.21869659423828, 25.8048152923584] in episode 1607
Report: 
rewardSum:247.25
loss:[30.21869659423828, 25.8048152923584]
policies:[1, 3, 0]
qAverage:[0.0, 134.57936477661133]
ws:[1.6705112010240555, 5.134804934263229]
memory len:10000
memory used:2890.0
now epsilon is 0.1054750157272758, the reward is 247.25 with loss [32.785637855529785, 29.76431703567505] in episode 1608
Report: 
rewardSum:247.25
loss:[32.785637855529785, 29.76431703567505]
policies:[0, 4, 0]
qAverage:[0.0, 148.3616455078125]
ws:[8.10572283267975, 11.33811812400818]
memory len:10000
memory used:2891.0
now epsilon is 0.10536958025808767, the reward is 247.25 with loss [23.160829544067383, 26.56613826751709] in episode 1609
Report: 
rewardSum:247.25
loss:[23.160829544067383, 26.56613826751709]
policies:[0, 3, 1]
qAverage:[0.0, 143.71625900268555]
ws:[10.971263885498047, 14.82109808921814]
memory len:10000
memory used:2891.0
now epsilon is 0.105264250184837, the reward is 247.25 with loss [41.14700698852539, 33.69661092758179] in episode 1610
Report: 
rewardSum:247.25
loss:[41.14700698852539, 33.69661092758179]
policies:[0, 4, 0]
qAverage:[0.0, 149.30173034667968]
ws:[10.664258217811584, 15.524556541442871]
memory len:10000
memory used:2891.0
now epsilon is 0.10513273564581686, the reward is 246.25 with loss [37.72913599014282, 31.307055473327637] in episode 1611
Report: 
rewardSum:246.25
loss:[37.72913599014282, 31.307055473327637]
policies:[0, 4, 1]
qAverage:[0.0, 129.2589111328125]
ws:[11.721929669380188, 16.008127689361572]
memory len:10000
memory used:2892.0
now epsilon is 0.10502764232837654, the reward is 247.25 with loss [27.051418781280518, 31.402291774749756] in episode 1612
Report: 
rewardSum:247.25
loss:[27.051418781280518, 31.402291774749756]
policies:[0, 4, 0]
qAverage:[0.0, 141.17079162597656]
ws:[6.8192140579223635, 10.392729568481446]
memory len:10000
memory used:2892.0
now epsilon is 0.10492265406485023, the reward is 247.25 with loss [26.48183798789978, 23.279333114624023] in episode 1613
Report: 
rewardSum:247.25
loss:[26.48183798789978, 23.279333114624023]
policies:[0, 4, 0]
qAverage:[0.0, 144.98375549316407]
ws:[5.792447590827942, 8.954074668884278]
memory len:10000
memory used:2891.0
now epsilon is 0.1048177707502234, the reward is 247.25 with loss [33.7793984413147, 30.29326868057251] in episode 1614
Report: 
rewardSum:247.25
loss:[33.7793984413147, 30.29326868057251]
policies:[0, 4, 0]
qAverage:[0.0, 144.92894897460937]
ws:[6.147031641006469, 9.924383306503296]
memory len:10000
memory used:2891.0
now epsilon is 0.10471299227958651, the reward is 247.25 with loss [22.71259641647339, 22.414461851119995] in episode 1615
Report: 
rewardSum:247.25
loss:[22.71259641647339, 22.414461851119995]
policies:[0, 3, 1]
qAverage:[0.0, 137.76801681518555]
ws:[7.782564759254456, 12.004794120788574]
memory len:10000
memory used:2891.0
now epsilon is 0.10460831854813488, the reward is 247.25 with loss [26.230411291122437, 25.852299690246582] in episode 1616
Report: 
rewardSum:247.25
loss:[26.230411291122437, 25.852299690246582]
policies:[0, 4, 0]
qAverage:[0.0, 134.8299903869629]
ws:[7.950114049017429, 11.78190279006958]
memory len:10000
memory used:2891.0
now epsilon is 0.10450374945116861, the reward is 247.25 with loss [27.679306030273438, 20.151130199432373] in episode 1617
Report: 
rewardSum:247.25
loss:[27.679306030273438, 20.151130199432373]
policies:[0, 4, 0]
qAverage:[0.0, 143.17945251464843]
ws:[7.770519423484802, 12.021461868286133]
memory len:10000
memory used:2891.0
now epsilon is 0.10439928488409242, the reward is 247.25 with loss [28.781779766082764, 26.87877130508423] in episode 1618
Report: 
rewardSum:247.25
loss:[28.781779766082764, 26.87877130508423]
policies:[0, 2, 2]
qAverage:[0.0, 92.59910583496094]
ws:[1.8699657917022705, 4.461095333099365]
memory len:10000
memory used:2891.0
now epsilon is 0.10424278379847722, the reward is 245.25 with loss [38.11152410507202, 42.38168454170227] in episode 1619
Report: 
rewardSum:245.25
loss:[38.11152410507202, 42.38168454170227]
policies:[1, 4, 1]
qAverage:[0.0, 141.66421203613282]
ws:[8.518525975942612, 12.642319202423096]
memory len:10000
memory used:2891.0
now epsilon is 0.10413858009920791, the reward is 247.25 with loss [34.083664894104004, 20.617504835128784] in episode 1620
Report: 
rewardSum:247.25
loss:[34.083664894104004, 20.617504835128784]
policies:[0, 4, 0]
qAverage:[0.0, 137.49249877929688]
ws:[7.554751247167587, 10.838578462600708]
memory len:10000
memory used:2891.0
now epsilon is 0.104034480564568, the reward is 247.25 with loss [36.614065647125244, 26.97582507133484] in episode 1621
Report: 
rewardSum:247.25
loss:[36.614065647125244, 26.97582507133484]
policies:[0, 3, 1]
qAverage:[0.0, 134.77144622802734]
ws:[6.883019387722015, 8.602655917406082]
memory len:10000
memory used:2891.0
now epsilon is 0.10393048509043191, the reward is 247.25 with loss [40.43129205703735, 25.900275707244873] in episode 1622
Report: 
rewardSum:247.25
loss:[40.43129205703735, 25.900275707244873]
policies:[1, 3, 0]
qAverage:[0.0, 129.32070922851562]
ws:[8.597988868132234, 10.750244140625]
memory len:10000
memory used:2891.0
now epsilon is 0.10382659357277814, the reward is 247.25 with loss [42.76391887664795, 34.71788549423218] in episode 1623
Report: 
rewardSum:247.25
loss:[42.76391887664795, 34.71788549423218]
policies:[0, 4, 0]
qAverage:[0.0, 132.43685913085938]
ws:[6.199950310587883, 7.826753675937653]
memory len:10000
memory used:2891.0
now epsilon is 0.10315382548033834, the reward is 225.25 with loss [167.36560821533203, 148.0424815416336] in episode 1624
Report: 
rewardSum:225.25
loss:[167.36560821533203, 148.0424815416336]
policies:[13, 9, 4]
qAverage:[103.03878420875186, 67.45378258114769]
ws:[11.298985770770482, 10.590150834549041]
memory len:10000
memory used:2892.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23*		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.10289623084344286, the reward is 31.159999999999968 with loss [70.0597095489502, 65.71829652786255] in episode 1625
Report: 
rewardSum:31.159999999999968
loss:[70.0597095489502, 65.71829652786255]
policies:[3, 6, 1]
qAverage:[38.86611090766059, 115.37860446506076]
ws:[5.96225090821584, 7.324610349204805]
memory len:10000
memory used:2892.0
now epsilon is 0.10258796624435788, the reward is -11.0 with loss [86.38431167602539, 80.68293857574463] in episode 1626
Report: 
rewardSum:-11.0
loss:[86.38431167602539, 80.68293857574463]
policies:[0, 10, 2]
qAverage:[0.0, 141.2732628716363]
ws:[6.9526085125075445, 11.907281080881754]
memory len:10000
memory used:2891.0
now epsilon is 0.10225505501224562, the reward is 238.25 with loss [84.81847715377808, 75.12690472602844] in episode 1627
Report: 
rewardSum:238.25
loss:[84.81847715377808, 75.12690472602844]
policies:[2, 9, 2]
qAverage:[27.177282969156902, 119.11405944824219]
ws:[5.920575842261314, 7.81635703643163]
memory len:10000
memory used:2892.0
now epsilon is 0.10138946282015675, the reward is 217.25 with loss [220.9174554347992, 205.96788573265076] in episode 1628
Report: 
rewardSum:217.25
loss:[220.9174554347992, 205.96788573265076]
policies:[7, 24, 3]
qAverage:[18.662013127253605, 132.90120873084436]
ws:[4.530712081280608, 6.280357984396128]
memory len:10000
memory used:2891.0
now epsilon is 0.10108571231490014, the reward is 239.25 with loss [83.32331156730652, 84.92734384536743] in episode 1629
Report: 
rewardSum:239.25
loss:[83.32331156730652, 84.92734384536743]
policies:[0, 11, 1]
qAverage:[0.0, 127.34630584716797]
ws:[8.097877848148347, 11.762344646453858]
memory len:10000
memory used:2892.0
now epsilon is 0.10055633697689494, the reward is 230.25 with loss [121.8672423362732, 114.5391309261322] in episode 1630
Report: 
rewardSum:230.25
loss:[121.8672423362732, 114.5391309261322]
policies:[7, 13, 1]
qAverage:[32.59977891710069, 104.40724690755208]
ws:[4.145418350067404, 5.85455236170027]
memory len:10000
memory used:2891.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12*		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.10028015245349965, the reward is 240.25 with loss [72.37888240814209, 69.48599624633789] in episode 1631
Report: 
rewardSum:240.25
loss:[72.37888240814209, 69.48599624633789]
policies:[0, 11, 0]
qAverage:[0.0, 132.823749889027]
ws:[1.7618836611509323, 3.35431890596043]
memory len:10000
memory used:2892.0
now epsilon is 0.10010479374957913, the reward is 244.25 with loss [50.103601932525635, 56.12002730369568] in episode 1632
Report: 
rewardSum:244.25
loss:[50.103601932525635, 56.12002730369568]
policies:[1, 4, 2]
qAverage:[0.0, 110.19134521484375]
ws:[3.442789700627327, 5.6678221940994264]
memory len:10000
memory used:2893.0
now epsilon is 0.10000472648887107, the reward is 247.25 with loss [26.08660125732422, 25.4680814743042] in episode 1633
Report: 
rewardSum:247.25
loss:[26.08660125732422, 25.4680814743042]
policies:[0, 4, 0]
qAverage:[0.0, 103.61210632324219]
ws:[3.7300096988677978, 6.122698426246643]
memory len:10000
memory used:2892.0
now epsilon is 0.09985481312232325, the reward is 245.25 with loss [32.573819160461426, 36.2406439781189] in episode 1634
Report: 
rewardSum:245.25
loss:[32.573819160461426, 36.2406439781189]
policies:[0, 6, 0]
qAverage:[0.0, 114.41311427525112]
ws:[2.5277737847396304, 4.301200713430132]
memory len:10000
memory used:2892.0
now epsilon is 0.09973005699957821, the reward is 246.25 with loss [28.97460389137268, 32.279603004455566] in episode 1635
Report: 
rewardSum:246.25
loss:[28.97460389137268, 32.279603004455566]
policies:[0, 5, 0]
qAverage:[0.0, 108.88047536214192]
ws:[3.7575007552901902, 6.510585337877274]
memory len:10000
memory used:2892.0
now epsilon is 0.09958055537984749, the reward is 245.25 with loss [47.70437002182007, 28.402323961257935] in episode 1636
Report: 
rewardSum:245.25
loss:[47.70437002182007, 28.402323961257935]
policies:[0, 6, 0]
qAverage:[0.0, 113.39192417689732]
ws:[1.8245098612138204, 3.515095089163099]
memory len:10000
memory used:2892.0
now epsilon is 0.09945614190791228, the reward is 246.25 with loss [26.034955501556396, 23.193790197372437] in episode 1637
Report: 
rewardSum:246.25
loss:[26.034955501556396, 23.193790197372437]
policies:[0, 3, 2]
qAverage:[0.0, 98.17849349975586]
ws:[3.3152719140052795, 5.9003050327301025]
memory len:10000
memory used:2892.0
now epsilon is 0.099331883875078, the reward is 246.25 with loss [35.678749561309814, 30.785045623779297] in episode 1638
Report: 
rewardSum:246.25
loss:[35.678749561309814, 30.785045623779297]
policies:[0, 5, 0]
qAverage:[0.0, 107.39495340983073]
ws:[3.968855152527491, 7.421251893043518]
memory len:10000
memory used:2891.0
now epsilon is 0.09915818339708568, the reward is 244.25 with loss [35.68000888824463, 52.33776617050171] in episode 1639
Report: 
rewardSum:244.25
loss:[35.68000888824463, 52.33776617050171]
policies:[0, 7, 0]
qAverage:[0.0, 114.8638858795166]
ws:[4.04952597618103, 6.260983454063535]
memory len:10000
memory used:2891.0
now epsilon is 0.0987622934829433, the reward is 235.25 with loss [111.19606304168701, 111.2919852733612] in episode 1640
Report: 
rewardSum:235.25
loss:[111.19606304168701, 111.2919852733612]
policies:[0, 13, 3]
qAverage:[0.0, 114.77273877461751]
ws:[1.244856832548976, 2.6070773601531982]
memory len:10000
memory used:2891.0
now epsilon is 0.09868824027921796, the reward is -2.0 with loss [26.364103317260742, 23.385453701019287] in episode 1641
Report: 
rewardSum:-2.0
loss:[26.364103317260742, 23.385453701019287]
policies:[0, 2, 1]
qAverage:[0.0, 79.8555399576823]
ws:[0.9466192324956259, 2.6789719462394714]
memory len:10000
memory used:2891.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44*		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.09666130807329701, the reward is -41.84000000000003 with loss [519.3315165042877, 473.46277391910553] in episode 1642
Report: 
rewardSum:-41.84000000000003
loss:[519.3315165042877, 473.46277391910553]
policies:[7, 72, 4]
qAverage:[7.743019318916429, 103.59805308597188]
ws:[1.5419800585844148, 3.42067955686173]
memory len:10000
memory used:2911.0
now epsilon is 0.09632354270684676, the reward is 237.25 with loss [101.44984269142151, 97.93412494659424] in episode 1643
Report: 
rewardSum:237.25
loss:[101.44984269142151, 97.93412494659424]
policies:[0, 13, 1]
qAverage:[0.0, 91.91101426344652]
ws:[2.05938850916349, 3.7246341246825]
memory len:10000
memory used:2911.0
now epsilon is 0.09622725527944859, the reward is 247.25 with loss [26.682334423065186, 26.522098302841187] in episode 1644
Report: 
rewardSum:247.25
loss:[26.682334423065186, 26.522098302841187]
policies:[0, 4, 0]
qAverage:[0.0, 77.64640808105469]
ws:[0.857340669631958, 2.989821720123291]
memory len:10000
memory used:2911.0
now epsilon is 0.09615510287909583, the reward is -2.0 with loss [21.67437171936035, 16.817683219909668] in episode 1645
Report: 
rewardSum:-2.0
loss:[21.67437171936035, 16.817683219909668]
policies:[0, 1, 2]
qAverage:[0.0, 46.517086029052734]
ws:[0.1689065843820572, 1.8690396547317505]
memory len:10000
memory used:2911.0
now epsilon is 0.09584306712144064, the reward is 238.25 with loss [91.84872007369995, 67.49832201004028] in episode 1646
Report: 
rewardSum:238.25
loss:[91.84872007369995, 67.49832201004028]
policies:[1, 11, 1]
qAverage:[0.0, 85.95103801380505]
ws:[2.6499552672559563, 7.374049533497203]
memory len:10000
memory used:2912.0
now epsilon is 0.09567546749560268, the reward is 244.25 with loss [36.360111474990845, 45.94140148162842] in episode 1647
Report: 
rewardSum:244.25
loss:[36.360111474990845, 45.94140148162842]
policies:[0, 6, 1]
qAverage:[0.0, 82.73179626464844]
ws:[-0.32734054752758573, 2.811829311507089]
memory len:10000
memory used:2912.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2*		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.09550816094922703, the reward is 244.25 with loss [35.620410680770874, 30.990575671195984] in episode 1648
Report: 
rewardSum:244.25
loss:[35.620410680770874, 30.990575671195984]
policies:[0, 5, 2]
qAverage:[0.0, 79.43090438842773]
ws:[1.3939855893452961, 5.772423267364502]
memory len:10000
memory used:2911.0
now epsilon is 0.09536498821686339, the reward is 245.25 with loss [45.01635551452637, 33.03145980834961] in episode 1649
Report: 
rewardSum:245.25
loss:[45.01635551452637, 33.03145980834961]
policies:[0, 5, 1]
qAverage:[0.0, 79.24269612630208]
ws:[-0.5612754821777344, 2.4334481358528137]
memory len:10000
memory used:2912.0
now epsilon is 0.09519822460189124, the reward is 244.25 with loss [42.928738594055176, 51.81930065155029] in episode 1650
Report: 
rewardSum:244.25
loss:[42.928738594055176, 51.81930065155029]
policies:[1, 6, 0]
qAverage:[0.0, 72.01239166259765]
ws:[-0.042244446277618405, 2.8727887153625487]
memory len:10000
memory used:2912.0
now epsilon is 0.09510306207067408, the reward is 247.25 with loss [28.841434001922607, 22.355539798736572] in episode 1651
Report: 
rewardSum:247.25
loss:[28.841434001922607, 22.355539798736572]
policies:[0, 4, 0]
qAverage:[0.0, 69.86726531982421]
ws:[-0.5696911692619324, 2.0799032997339966]
memory len:10000
memory used:2912.0
now epsilon is 0.09500799466630812, the reward is 247.25 with loss [22.06300115585327, 22.424960613250732] in episode 1652
Report: 
rewardSum:247.25
loss:[22.06300115585327, 22.424960613250732]
policies:[0, 4, 0]
qAverage:[0.0, 71.465380859375]
ws:[-0.42914984226226804, 2.071979355812073]
memory len:10000
memory used:2912.0
now epsilon is 0.09488929403812878, the reward is 246.25 with loss [25.96647810935974, 31.32028639316559] in episode 1653
Report: 
rewardSum:246.25
loss:[25.96647810935974, 31.32028639316559]
policies:[0, 4, 1]
qAverage:[0.0, 70.85975189208985]
ws:[-0.1615297317504883, 1.9573251843452453]
memory len:10000
memory used:2911.0
now epsilon is 0.09477074171156531, the reward is 246.25 with loss [25.66282033920288, 21.335832118988037] in episode 1654
Report: 
rewardSum:246.25
loss:[25.66282033920288, 21.335832118988037]
policies:[1, 4, 0]
qAverage:[0.0, 70.19397430419922]
ws:[0.0936467856168747, 2.655350959300995]
memory len:10000
memory used:2912.0
now epsilon is 0.09467600650295911, the reward is 247.25 with loss [19.54633140563965, 23.382898807525635] in episode 1655
Report: 
rewardSum:247.25
loss:[19.54633140563965, 23.382898807525635]
policies:[0, 4, 0]
qAverage:[0.0, 71.19277801513672]
ws:[0.22941763550043107, 2.775246524810791]
memory len:10000
memory used:2912.0
now epsilon is 0.09455772065254321, the reward is 246.25 with loss [22.75156879425049, 27.77775239944458] in episode 1656
Report: 
rewardSum:246.25
loss:[22.75156879425049, 27.77775239944458]
policies:[0, 4, 1]
qAverage:[0.0, 70.09739379882812]
ws:[0.0839033305644989, 2.3302671909332275]
memory len:10000
memory used:2911.0
now epsilon is 0.09432159211728301, the reward is 241.25 with loss [58.539753437042236, 65.1699366569519] in episode 1657
Report: 
rewardSum:241.25
loss:[58.539753437042236, 65.1699366569519]
policies:[0, 9, 1]
qAverage:[0.0, 77.93174362182617]
ws:[1.8292023181915282, 4.991247326135635]
memory len:10000
memory used:2912.0
now epsilon is 0.09422730588986807, the reward is 247.25 with loss [13.724833011627197, 21.29644215106964] in episode 1658
Report: 
rewardSum:247.25
loss:[13.724833011627197, 21.29644215106964]
policies:[1, 3, 0]
qAverage:[21.624528884887695, 42.81002426147461]
ws:[-0.34495649486780167, 0.4268486946821213]
memory len:10000
memory used:2912.0
now epsilon is 0.09413311391332907, the reward is 247.25 with loss [22.588365077972412, 23.100961208343506] in episode 1659
Report: 
rewardSum:247.25
loss:[22.588365077972412, 23.100961208343506]
policies:[1, 2, 1]
qAverage:[21.930683135986328, 44.67036437988281]
ws:[-0.01729685254395008, 1.0099747036583722]
memory len:10000
memory used:2912.0
now epsilon is 0.09401550633942715, the reward is 246.25 with loss [27.242998600006104, 34.87026929855347] in episode 1660
Report: 
rewardSum:246.25
loss:[27.242998600006104, 34.87026929855347]
policies:[0, 4, 1]
qAverage:[0.0, 59.612388610839844]
ws:[0.22355513274669647, 1.3628127127885818]
memory len:10000
memory used:2912.0
now epsilon is 0.09366356449952422, the reward is 236.25 with loss [85.28454446792603, 81.51755547523499] in episode 1661
Report: 
rewardSum:236.25
loss:[85.28454446792603, 81.51755547523499]
policies:[1, 13, 1]
qAverage:[5.085254923502604, 69.43632049560547]
ws:[0.3850896179676056, 1.4707234442234038]
memory len:10000
memory used:2911.0
now epsilon is 0.09356993605300779, the reward is 247.25 with loss [22.895068883895874, 24.409900188446045] in episode 1662
Report: 
rewardSum:247.25
loss:[22.895068883895874, 24.409900188446045]
policies:[0, 4, 0]
qAverage:[0.0, 64.59900817871093]
ws:[0.1543561927974224, 1.70036501288414]
memory len:10000
memory used:2912.0
now epsilon is 0.09342966884150822, the reward is 245.25 with loss [31.430627822875977, 36.160192012786865] in episode 1663
Report: 
rewardSum:245.25
loss:[31.430627822875977, 36.160192012786865]
policies:[1, 4, 1]
qAverage:[13.222994486490885, 55.17175038655599]
ws:[-0.3425379504139225, 0.27820848921934765]
memory len:10000
memory used:2912.0
now epsilon is 0.09333627420295354, the reward is 247.25 with loss [27.643461227416992, 26.84697914123535] in episode 1664
Report: 
rewardSum:247.25
loss:[27.643461227416992, 26.84697914123535]
policies:[0, 4, 0]
qAverage:[0.0, 64.05603790283203]
ws:[-0.42000064849853513, 1.1510025292634964]
memory len:10000
memory used:2912.0
now epsilon is 0.09324297292402027, the reward is 37.15999999999997 with loss [19.450993537902832, 20.851377964019775] in episode 1665
Report: 
rewardSum:37.15999999999997
loss:[19.450993537902832, 20.851377964019775]
policies:[0, 3, 1]
qAverage:[0.0, 59.667184829711914]
ws:[-0.9127800464630127, -0.07205580174922943]
memory len:10000
memory used:2912.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21*		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.09314976491138378, the reward is 247.25 with loss [25.747400760650635, 19.013190865516663] in episode 1666
Report: 
rewardSum:247.25
loss:[25.747400760650635, 19.013190865516663]
policies:[0, 4, 0]
qAverage:[0.0, 64.25166168212891]
ws:[-1.4166142106056214, 0.21449186552781613]
memory len:10000
memory used:2912.0
now epsilon is 0.0930101275628175, the reward is 245.25 with loss [20.004316329956055, 31.734326004981995] in episode 1667
Report: 
rewardSum:245.25
loss:[20.004316329956055, 31.734326004981995]
policies:[0, 5, 1]
qAverage:[0.0, 66.38580958048503]
ws:[-0.6284278382857641, 1.5994055361176531]
memory len:10000
memory used:2912.0
now epsilon is 0.09287069953940766, the reward is 245.25 with loss [45.70462620258331, 47.33111763000488] in episode 1668
Report: 
rewardSum:245.25
loss:[45.70462620258331, 47.33111763000488]
policies:[1, 5, 0]
qAverage:[0.0, 62.56925659179687]
ws:[1.2177924156188964, 3.8203265190124513]
memory len:10000
memory used:2912.0
now epsilon is 0.09277786366057653, the reward is 247.25 with loss [14.95075535774231, 21.057552337646484] in episode 1669
Report: 
rewardSum:247.25
loss:[14.95075535774231, 21.057552337646484]
policies:[0, 4, 0]
qAverage:[0.0, 58.19987182617187]
ws:[0.39295375943183897, 3.9753745794296265]
memory len:10000
memory used:2912.0
now epsilon is 0.09268512058281657, the reward is 247.25 with loss [26.531664848327637, 25.831829071044922] in episode 1670
Report: 
rewardSum:247.25
loss:[26.531664848327637, 25.831829071044922]
policies:[0, 4, 0]
qAverage:[0.0, 60.214508056640625]
ws:[0.22200119197368623, 3.977658486366272]
memory len:10000
memory used:2912.0
now epsilon is 0.09259247021336152, the reward is 247.25 with loss [24.454948902130127, 20.68655276298523] in episode 1671
Report: 
rewardSum:247.25
loss:[24.454948902130127, 20.68655276298523]
policies:[0, 4, 0]
qAverage:[0.0, 59.760685729980466]
ws:[0.11715079247951507, 3.8034740924835204]
memory len:10000
memory used:2912.0
now epsilon is 0.09247678748142295, the reward is 246.25 with loss [31.874855518341064, 32.95028233528137] in episode 1672
Report: 
rewardSum:246.25
loss:[31.874855518341064, 32.95028233528137]
policies:[0, 4, 1]
qAverage:[0.0, 57.71157531738281]
ws:[0.03525786399841309, 2.2629658937454225]
memory len:10000
memory used:2912.0
now epsilon is 0.0923843453669574, the reward is 247.25 with loss [26.64901113510132, 19.77528142929077] in episode 1673
Report: 
rewardSum:247.25
loss:[26.64901113510132, 19.77528142929077]
policies:[0, 4, 0]
qAverage:[0.0, 59.628108215332034]
ws:[-0.33824865221977235, 2.111423835158348]
memory len:10000
memory used:2912.0
now epsilon is 0.09229199565994631, the reward is 247.25 with loss [20.34863018989563, 27.01486349105835] in episode 1674
Report: 
rewardSum:247.25
loss:[20.34863018989563, 27.01486349105835]
policies:[0, 4, 0]
qAverage:[0.0, 59.812974548339845]
ws:[-0.9100357055664062, 0.7520874619483948]
memory len:10000
memory used:2912.0
now epsilon is 0.09146499142121246, the reward is 215.25 with loss [215.49711275100708, 196.6301989555359] in episode 1675
Report: 
rewardSum:215.25
loss:[215.49711275100708, 196.6301989555359]
policies:[2, 31, 3]
qAverage:[2.389227098034274, 65.39084539105815]
ws:[2.556718249474802, 7.426431917375134]
memory len:10000
memory used:2912.0
now epsilon is 0.09137356072344682, the reward is 247.25 with loss [22.619075775146484, 26.629760265350342] in episode 1676
Report: 
rewardSum:247.25
loss:[22.619075775146484, 26.629760265350342]
policies:[0, 4, 0]
qAverage:[0.0, 53.941584777832034]
ws:[-1.1951501727104188, 2.243284058570862]
memory len:10000
memory used:2912.0
now epsilon is 0.09128222142209817, the reward is 247.25 with loss [8.526421785354614, 19.246240615844727] in episode 1677
Report: 
rewardSum:247.25
loss:[8.526421785354614, 19.246240615844727]
policies:[0, 4, 0]
qAverage:[0.0, 47.19322204589844]
ws:[-2.928954859574636, -1.0037151376406352]
memory len:10000
memory used:2912.0
now epsilon is 0.0910998166432945, the reward is 243.25 with loss [38.494645833969116, 43.7893590927124] in episode 1678
Report: 
rewardSum:243.25
loss:[38.494645833969116, 43.7893590927124]
policies:[4, 4, 0]
qAverage:[29.796131557888454, 30.711218939887154]
ws:[-1.149550626675288, -0.08234845267401801]
memory len:10000
memory used:2912.0
now epsilon is 0.09100875098338906, the reward is 247.25 with loss [25.367964267730713, 20.773425340652466] in episode 1679
Report: 
rewardSum:247.25
loss:[25.367964267730713, 20.773425340652466]
policies:[1, 3, 0]
qAverage:[16.512968063354492, 35.606658935546875]
ws:[-1.1961900889873505, 0.5685797855257988]
memory len:10000
memory used:2913.0
now epsilon is 0.0909177763549996, the reward is 247.25 with loss [26.137864112854004, 27.632747650146484] in episode 1680
Report: 
rewardSum:247.25
loss:[26.137864112854004, 27.632747650146484]
policies:[1, 3, 0]
qAverage:[13.389834594726562, 42.99466400146484]
ws:[-0.9744873285293579, 0.8214318290352821]
memory len:10000
memory used:2912.0
############# STATE ###############
0-		10-		20-		30-		40-		
1*		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.09078148489747598, the reward is 245.25 with loss [28.322495937347412, 31.84943962097168] in episode 1681
Report: 
rewardSum:245.25
loss:[28.322495937347412, 31.84943962097168]
policies:[2, 4, 0]
qAverage:[13.321231079101562, 42.498190307617186]
ws:[-0.308161187171936, 2.705132508277893]
memory len:10000
memory used:2912.0
now epsilon is 0.09069073744996187, the reward is 247.25 with loss [19.919436931610107, 18.557445764541626] in episode 1682
Report: 
rewardSum:247.25
loss:[19.919436931610107, 18.557445764541626]
policies:[1, 3, 0]
qAverage:[16.676013946533203, 36.00433349609375]
ws:[-1.5185732413083315, 0.12161704897880554]
memory len:10000
memory used:2912.0
now epsilon is 0.09060008071587064, the reward is 247.25 with loss [28.291849613189697, 17.913240551948547] in episode 1683
Report: 
rewardSum:247.25
loss:[28.291849613189697, 17.913240551948547]
policies:[1, 2, 1]
qAverage:[16.32565689086914, 33.762001037597656]
ws:[-1.387889787554741, 0.28233644366264343]
memory len:10000
memory used:2912.0
now epsilon is 0.09050951460452289, the reward is 247.25 with loss [27.108105182647705, 24.352633476257324] in episode 1684
Report: 
rewardSum:247.25
loss:[27.108105182647705, 24.352633476257324]
policies:[0, 4, 0]
qAverage:[0.0, 50.33304290771484]
ws:[-0.18910907506942748, 2.377902839705348]
memory len:10000
memory used:2912.0
now epsilon is 0.09041903902532987, the reward is 247.25 with loss [21.07608914375305, 30.299062728881836] in episode 1685
Report: 
rewardSum:247.25
loss:[21.07608914375305, 30.299062728881836]
policies:[0, 4, 0]
qAverage:[0.0, 40.95246378580729]
ws:[1.4172815481821697, 3.2227999369303384]
memory len:10000
memory used:2912.0
now epsilon is 0.09032865388779335, the reward is 247.25 with loss [21.303470373153687, 20.713265419006348] in episode 1686
Report: 
rewardSum:247.25
loss:[21.303470373153687, 20.713265419006348]
policies:[1, 3, 0]
qAverage:[15.395567893981934, 32.09475231170654]
ws:[0.8994261715561152, 2.218141607940197]
memory len:10000
memory used:2912.0
now epsilon is 0.09019324556185226, the reward is 245.25 with loss [46.09974527359009, 35.69523024559021] in episode 1687
Report: 
rewardSum:245.25
loss:[46.09974527359009, 35.69523024559021]
policies:[0, 6, 0]
qAverage:[0.0, 54.40507997785296]
ws:[-0.3089831184063639, 1.4749469246183122]
memory len:10000
memory used:2912.0
now epsilon is 0.09003552571144173, the reward is 244.25 with loss [42.04530167579651, 44.530967473983765] in episode 1688
Report: 
rewardSum:244.25
loss:[42.04530167579651, 44.530967473983765]
policies:[1, 4, 2]
qAverage:[12.052930450439453, 38.15920181274414]
ws:[-0.6375869035720825, 2.1160278245806694]
memory len:10000
memory used:2914.0
now epsilon is 0.08990055680304912, the reward is 245.25 with loss [30.539039850234985, 42.79866313934326] in episode 1689
Report: 
rewardSum:245.25
loss:[30.539039850234985, 42.79866313934326]
policies:[2, 3, 1]
qAverage:[12.118845367431641, 36.1380729675293]
ws:[-0.4824459314346313, 0.6626595973968505]
memory len:10000
memory used:2912.0
now epsilon is 0.08981068995333645, the reward is 37.15999999999997 with loss [31.01214027404785, 27.39986228942871] in episode 1690
Report: 
rewardSum:37.15999999999997
loss:[31.01214027404785, 27.39986228942871]
policies:[2, 1, 1]
qAverage:[20.127012888590496, 22.625356038411457]
ws:[-0.8425714572270712, -0.5210326587160429]
memory len:10000
memory used:2912.0
now epsilon is 0.08972091293677903, the reward is 247.25 with loss [21.01249885559082, 25.09971284866333] in episode 1691
Report: 
rewardSum:247.25
loss:[21.01249885559082, 25.09971284866333]
policies:[0, 4, 0]
qAverage:[0.0, 51.36733474731445]
ws:[-3.057801938056946, 0.04471135139465332]
memory len:10000
memory used:2912.0
now epsilon is 0.08963122566357742, the reward is 247.25 with loss [19.697803735733032, 30.67348003387451] in episode 1692
Report: 
rewardSum:247.25
loss:[19.697803735733032, 30.67348003387451]
policies:[0, 3, 1]
qAverage:[0.0, 42.845298767089844]
ws:[-0.054417550563812256, 1.2063840826352437]
memory len:10000
memory used:2911.0
now epsilon is 0.08954162804402188, the reward is 247.25 with loss [20.14271116256714, 23.850921869277954] in episode 1693
Report: 
rewardSum:247.25
loss:[20.14271116256714, 23.850921869277954]
policies:[1, 3, 0]
qAverage:[0.0, 46.82630443572998]
ws:[1.0544929578900337, 3.7922003269195557]
memory len:10000
memory used:2911.0
now epsilon is 0.08945211998849238, the reward is 247.25 with loss [22.527082443237305, 20.00268244743347] in episode 1694
Report: 
rewardSum:247.25
loss:[22.527082443237305, 20.00268244743347]
policies:[0, 4, 0]
qAverage:[0.0, 48.65365753173828]
ws:[-2.621529272198677, 1.2181729793548584]
memory len:10000
memory used:2911.0
now epsilon is 0.08936270140745849, the reward is 247.25 with loss [22.008822917938232, 15.06813383102417] in episode 1695
Report: 
rewardSum:247.25
loss:[22.008822917938232, 15.06813383102417]
policies:[0, 4, 0]
qAverage:[0.0, 47.34689025878906]
ws:[-2.7907458007335664, 0.5225350141525269]
memory len:10000
memory used:2911.0
now epsilon is 0.08927337221147923, the reward is 247.25 with loss [28.83890390396118, 27.30972909927368] in episode 1696
Report: 
rewardSum:247.25
loss:[28.83890390396118, 27.30972909927368]
policies:[1, 3, 0]
qAverage:[10.976773834228515, 38.39296417236328]
ws:[-1.5868151023983956, 1.872009414434433]
memory len:10000
memory used:2911.0
now epsilon is 0.08918413231120309, the reward is 247.25 with loss [29.521923542022705, 23.583966732025146] in episode 1697
Report: 
rewardSum:247.25
loss:[29.521923542022705, 23.583966732025146]
policies:[0, 4, 0]
qAverage:[0.0, 48.43760528564453]
ws:[-1.7097018539905549, 1.355373999476433]
memory len:10000
memory used:2911.0
now epsilon is 0.08909498161736785, the reward is 247.25 with loss [27.361627101898193, 19.124154925346375] in episode 1698
Report: 
rewardSum:247.25
loss:[27.361627101898193, 19.124154925346375]
policies:[0, 4, 0]
qAverage:[0.0, 49.0384033203125]
ws:[-1.446561650931835, 1.2788015723228454]
memory len:10000
memory used:2911.0
now epsilon is 0.08900592004080052, the reward is 247.25 with loss [23.964942932128906, 20.8195161819458] in episode 1699
Report: 
rewardSum:247.25
loss:[23.964942932128906, 20.8195161819458]
policies:[1, 3, 0]
qAverage:[10.854730224609375, 37.68941192626953]
ws:[-1.8616211056709289, 0.7462895452976227]
memory len:10000
memory used:2911.0
now epsilon is 0.08891694749241721, the reward is 247.25 with loss [19.913100957870483, 23.951711177825928] in episode 1700
Report: 
rewardSum:247.25
loss:[19.913100957870483, 23.951711177825928]
policies:[0, 4, 0]
qAverage:[0.0, 48.85121231079101]
ws:[-1.716350482404232, 1.6893418312072754]
memory len:10000
memory used:2911.0
now epsilon is 0.08882806388322316, the reward is 247.25 with loss [24.948445320129395, 13.154048204421997] in episode 1701
Report: 
rewardSum:247.25
loss:[24.948445320129395, 13.154048204421997]
policies:[1, 3, 0]
qAverage:[0.0, 47.48932933807373]
ws:[-1.2518017534166574, 3.7726351022720337]
memory len:10000
memory used:2911.0
now epsilon is 0.08873926912431249, the reward is 247.25 with loss [27.917009830474854, 15.121162414550781] in episode 1702
Report: 
rewardSum:247.25
loss:[27.917009830474854, 15.121162414550781]
policies:[0, 4, 0]
qAverage:[0.0, 49.025089263916016]
ws:[-1.6242179661989211, 2.5799443244934084]
memory len:10000
memory used:2911.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22*		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.08860624338596503, the reward is 245.25 with loss [45.1160306930542, 34.59349250793457] in episode 1703
Report: 
rewardSum:245.25
loss:[45.1160306930542, 34.59349250793457]
policies:[0, 5, 1]
qAverage:[0.0, 50.98706881205241]
ws:[-0.33888648822903633, 3.710989644130071]
memory len:10000
memory used:2912.0
now epsilon is 0.0885176703643828, the reward is 247.25 with loss [22.045859694480896, 21.015806198120117] in episode 1704
Report: 
rewardSum:247.25
loss:[22.045859694480896, 21.015806198120117]
policies:[0, 3, 1]
qAverage:[0.0, 47.583889961242676]
ws:[-3.2381443977355957, 0.3078695237636566]
memory len:10000
memory used:2912.0
now epsilon is 0.0884291858826128, the reward is 247.25 with loss [22.74857807159424, 24.30465793609619] in episode 1705
Report: 
rewardSum:247.25
loss:[22.74857807159424, 24.30465793609619]
policies:[0, 4, 0]
qAverage:[0.0, 47.46049880981445]
ws:[-2.3575673550367355, 1.3169123008847237]
memory len:10000
memory used:2911.0
now epsilon is 0.08834078985214841, the reward is 247.25 with loss [20.75964593887329, 26.103943586349487] in episode 1706
Report: 
rewardSum:247.25
loss:[20.75964593887329, 26.103943586349487]
policies:[0, 4, 0]
qAverage:[0.0, 42.72632884979248]
ws:[-2.8270581513643265, -1.0922954678535461]
memory len:10000
memory used:2911.0
now epsilon is 0.0882524821845715, the reward is 247.25 with loss [25.354461193084717, 24.528841495513916] in episode 1707
Report: 
rewardSum:247.25
loss:[25.354461193084717, 24.528841495513916]
policies:[2, 2, 0]
qAverage:[26.095036506652832, 16.697107315063477]
ws:[-2.4710542261600494, -1.1498506851494312]
memory len:10000
memory used:2912.0
now epsilon is 0.08812018617042297, the reward is 245.25 with loss [49.806084632873535, 40.568747997283936] in episode 1708
Report: 
rewardSum:245.25
loss:[49.806084632873535, 40.568747997283936]
policies:[1, 4, 1]
qAverage:[8.508631388346354, 39.659366607666016]
ws:[-1.5094328299164772, 1.4281675120194752]
memory len:10000
memory used:2911.0
now epsilon is 0.0879660914541904, the reward is 244.25 with loss [46.79047632217407, 27.478027820587158] in episode 1709
Report: 
rewardSum:244.25
loss:[46.79047632217407, 27.478027820587158]
policies:[1, 5, 1]
qAverage:[7.197383335658482, 42.83164978027344]
ws:[-2.431318630065237, -0.39929717992033276]
memory len:10000
memory used:2910.0
now epsilon is 0.08785618880493687, the reward is 246.25 with loss [29.444239377975464, 33.20902156829834] in episode 1710
Report: 
rewardSum:246.25
loss:[29.444239377975464, 33.20902156829834]
policies:[0, 4, 1]
qAverage:[0.0, 47.417945861816406]
ws:[-2.3528814911842346, 2.2360779643058777]
memory len:10000
memory used:2910.0
now epsilon is 0.08776836555671208, the reward is 247.25 with loss [21.191993236541748, 21.2238929271698] in episode 1711
Report: 
rewardSum:247.25
loss:[21.191993236541748, 21.2238929271698]
policies:[0, 4, 0]
qAverage:[0.0, 42.40004920959473]
ws:[1.2022089920938015, 4.000327050685883]
memory len:10000
memory used:2912.0
now epsilon is 0.08768063009880728, the reward is 247.25 with loss [29.825589656829834, 23.62142324447632] in episode 1712
Report: 
rewardSum:247.25
loss:[29.825589656829834, 23.62142324447632]
policies:[0, 4, 0]
qAverage:[0.0, 42.68610954284668]
ws:[0.8652139771729708, 3.898197650909424]
memory len:10000
memory used:2912.0
now epsilon is 0.08759298234346508, the reward is 247.25 with loss [23.170926332473755, 27.429261684417725] in episode 1713
Report: 
rewardSum:247.25
loss:[23.170926332473755, 27.429261684417725]
policies:[0, 3, 1]
qAverage:[0.0, 45.824604988098145]
ws:[-4.194306492805481, 0.810489147901535]
memory len:10000
memory used:2912.0
now epsilon is 0.08750542220301578, the reward is 247.25 with loss [26.174667835235596, 22.666799545288086] in episode 1714
Report: 
rewardSum:247.25
loss:[26.174667835235596, 22.666799545288086]
policies:[0, 4, 0]
qAverage:[0.0, 47.296060943603514]
ws:[-3.3724690318107604, 1.0570276975631714]
memory len:10000
memory used:2911.0
now epsilon is 0.08741794958987736, the reward is 247.25 with loss [26.666995525360107, 20.201699256896973] in episode 1715
Report: 
rewardSum:247.25
loss:[26.666995525360107, 20.201699256896973]
policies:[0, 2, 2]
qAverage:[0.0, 38.02421569824219]
ws:[0.2862919208904107, 1.4462103446324666]
memory len:10000
memory used:2911.0
now epsilon is 0.08733056441655532, the reward is 247.25 with loss [19.03959035873413, 20.25422763824463] in episode 1716
Report: 
rewardSum:247.25
loss:[19.03959035873413, 20.25422763824463]
policies:[1, 3, 0]
qAverage:[0.0, 45.60367298126221]
ws:[-2.5061141550540924, 2.664583444595337]
memory len:10000
memory used:2911.0
now epsilon is 0.08724326659564262, the reward is 247.25 with loss [21.44242525100708, 22.06153130531311] in episode 1717
Report: 
rewardSum:247.25
loss:[21.44242525100708, 22.06153130531311]
policies:[0, 3, 1]
qAverage:[0.0, 42.355133056640625]
ws:[-4.0450904332101345, 0.5371879935264587]
memory len:10000
memory used:2911.0
now epsilon is 0.08713426702580963, the reward is 246.25 with loss [34.65078401565552, 39.35067963600159] in episode 1718
Report: 
rewardSum:246.25
loss:[34.65078401565552, 39.35067963600159]
policies:[0, 4, 1]
qAverage:[0.0, 41.354817390441895]
ws:[0.23791101574897766, 2.2031928934156895]
memory len:10000
memory used:2911.0
now epsilon is 0.08704716542868841, the reward is 247.25 with loss [18.909213066101074, 31.99636709690094] in episode 1719
Report: 
rewardSum:247.25
loss:[18.909213066101074, 31.99636709690094]
policies:[1, 3, 0]
qAverage:[12.120365142822266, 30.556633949279785]
ws:[-4.311053603887558, -2.331650786101818]
memory len:10000
memory used:2911.0
now epsilon is 0.08696015090050667, the reward is 247.25 with loss [19.37834882736206, 23.788410902023315] in episode 1720
Report: 
rewardSum:247.25
loss:[19.37834882736206, 23.788410902023315]
policies:[1, 3, 0]
qAverage:[8.930918884277343, 35.58779602050781]
ws:[-2.454191321134567, 0.3154921293258667]
memory len:10000
memory used:2912.0
now epsilon is 0.08691667626006586, the reward is -1.0 with loss [12.124897480010986, 6.203542709350586] in episode 1721
Report: 
rewardSum:-1.0
loss:[12.124897480010986, 6.203542709350586]
policies:[1, 0, 1]
qAverage:[23.713193893432617, 0.0]
ws:[0.12429175525903702, 0.021356919780373573]
memory len:10000
memory used:2912.0
now epsilon is 0.08682979217212745, the reward is 247.25 with loss [20.242456912994385, 23.4691903591156] in episode 1722
Report: 
rewardSum:247.25
loss:[20.242456912994385, 23.4691903591156]
policies:[0, 4, 0]
qAverage:[0.0, 44.85178070068359]
ws:[-3.6770628094673157, -0.14679480195045472]
memory len:10000
memory used:2912.0
now epsilon is 0.08674299493570088, the reward is 247.25 with loss [20.856554985046387, 16.596706867218018] in episode 1723
Report: 
rewardSum:247.25
loss:[20.856554985046387, 16.596706867218018]
policies:[1, 3, 0]
qAverage:[9.0543701171875, 36.12602081298828]
ws:[-2.572479563951492, 1.1179638504981995]
memory len:10000
memory used:2913.0
now epsilon is 0.08665628446396718, the reward is 247.25 with loss [25.52615261077881, 22.45043706893921] in episode 1724
Report: 
rewardSum:247.25
loss:[25.52615261077881, 22.45043706893921]
policies:[1, 3, 0]
qAverage:[9.323699951171875, 35.249166870117186]
ws:[-1.809329891204834, 1.7849895358085632]
memory len:10000
memory used:2912.0
now epsilon is 0.08656966067019423, the reward is 247.25 with loss [23.066750288009644, 28.873794555664062] in episode 1725
Report: 
rewardSum:247.25
loss:[23.066750288009644, 28.873794555664062]
policies:[2, 2, 0]
qAverage:[19.60379409790039, 24.05488739013672]
ws:[-2.0946871399879456, 0.6388860821723938]
memory len:10000
memory used:2912.0
now epsilon is 0.08648312346773653, the reward is 247.25 with loss [28.439489126205444, 26.93967342376709] in episode 1726
Report: 
rewardSum:247.25
loss:[28.439489126205444, 26.93967342376709]
policies:[1, 2, 1]
qAverage:[0.0, 39.818485260009766]
ws:[-5.604437232017517, -1.926857630411784]
memory len:10000
memory used:2912.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12*		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.08639667277003525, the reward is 247.25 with loss [28.489522457122803, 17.356450080871582] in episode 1727
Report: 
rewardSum:247.25
loss:[28.489522457122803, 17.356450080871582]
policies:[2, 2, 0]
qAverage:[24.369683265686035, 16.692028045654297]
ws:[-4.413772027939558, -3.168943203985691]
memory len:10000
memory used:2912.0
now epsilon is 0.08631030849061806, the reward is 247.25 with loss [29.311092376708984, 20.511976718902588] in episode 1728
Report: 
rewardSum:247.25
loss:[29.311092376708984, 20.511976718902588]
policies:[2, 2, 0]
qAverage:[19.305358123779296, 24.017198944091795]
ws:[-3.1602957487106322, -0.43891283571720124]
memory len:10000
memory used:2912.0
now epsilon is 0.08622403054309907, the reward is 247.25 with loss [21.162972450256348, 17.740700721740723] in episode 1729
Report: 
rewardSum:247.25
loss:[21.162972450256348, 17.740700721740723]
policies:[1, 3, 0]
qAverage:[9.118221282958984, 35.55934371948242]
ws:[-1.9882946729660034, 2.4779786467552185]
memory len:10000
memory used:2912.0
now epsilon is 0.08613783884117877, the reward is 247.25 with loss [22.166396617889404, 24.569336414337158] in episode 1730
Report: 
rewardSum:247.25
loss:[22.166396617889404, 24.569336414337158]
policies:[1, 3, 0]
qAverage:[8.859402465820313, 34.67320175170899]
ws:[-0.3752124309539795, 3.8984195709228517]
memory len:10000
memory used:2912.0
now epsilon is 0.0860517332986439, the reward is 247.25 with loss [26.366092681884766, 19.705385208129883] in episode 1731
Report: 
rewardSum:247.25
loss:[26.366092681884766, 19.705385208129883]
policies:[1, 3, 0]
qAverage:[0.0, 44.80417251586914]
ws:[-2.1918529868125916, 2.6341097950935364]
memory len:10000
memory used:2912.0
now epsilon is 0.08592273634530981, the reward is 245.25 with loss [22.357882261276245, 30.319435238838196] in episode 1732
Report: 
rewardSum:245.25
loss:[22.357882261276245, 30.319435238838196]
policies:[1, 4, 1]
qAverage:[7.338382720947266, 38.21230125427246]
ws:[-0.9820215702056885, 1.8288176854451497]
memory len:10000
memory used:2913.0
now epsilon is 0.08583684582462081, the reward is 247.25 with loss [15.85324501991272, 19.666806936264038] in episode 1733
Report: 
rewardSum:247.25
loss:[15.85324501991272, 19.666806936264038]
policies:[2, 2, 0]
qAverage:[19.18844985961914, 23.40679931640625]
ws:[-1.8688796043395997, 0.8854422330856323]
memory len:10000
memory used:2913.0
now epsilon is 0.08575104116224891, the reward is 247.25 with loss [26.21534776687622, 19.66386866569519] in episode 1734
Report: 
rewardSum:247.25
loss:[26.21534776687622, 19.66386866569519]
policies:[1, 3, 0]
qAverage:[0.0, 38.86294809977213]
ws:[-5.185643911361694, 0.3239876429239909]
memory len:10000
memory used:2913.0
now epsilon is 0.08566532227236798, the reward is 247.25 with loss [22.66334867477417, 28.99267864227295] in episode 1735
Report: 
rewardSum:247.25
loss:[22.66334867477417, 28.99267864227295]
policies:[1, 3, 0]
qAverage:[8.75817642211914, 35.34391784667969]
ws:[-2.5478089094161986, 0.8975512742996216]
memory len:10000
memory used:2913.0
now epsilon is 0.08557968906923773, the reward is 247.25 with loss [24.12598705291748, 21.88514518737793] in episode 1736
Report: 
rewardSum:247.25
loss:[24.12598705291748, 21.88514518737793]
policies:[2, 2, 0]
qAverage:[23.39742660522461, 16.34760284423828]
ws:[-3.8235714435577393, -1.9742806255817413]
memory len:10000
memory used:2912.0
now epsilon is 0.08545139973985375, the reward is 245.25 with loss [38.773510456085205, 33.97785925865173] in episode 1737
Report: 
rewardSum:245.25
loss:[38.773510456085205, 33.97785925865173]
policies:[2, 3, 1]
qAverage:[8.771221923828126, 34.25471572875976]
ws:[1.078238958120346, 2.107164907455444]
memory len:10000
memory used:2913.0
now epsilon is 0.08536598037904841, the reward is 247.25 with loss [20.92066979408264, 19.873809814453125] in episode 1738
Report: 
rewardSum:247.25
loss:[20.92066979408264, 19.873809814453125]
policies:[1, 3, 0]
qAverage:[8.538497161865234, 35.34636688232422]
ws:[-2.9698928356170655, 0.035812211036682126]
memory len:10000
memory used:2913.0
now epsilon is 0.08528064640557698, the reward is 247.25 with loss [30.605387687683105, 22.983332633972168] in episode 1739
Report: 
rewardSum:247.25
loss:[30.605387687683105, 22.983332633972168]
policies:[3, 1, 0]
qAverage:[27.816045379638673, 12.819468688964843]
ws:[-2.6313443422317504, -1.4495651483535767]
memory len:10000
memory used:2913.0
now epsilon is 0.08515280535992943, the reward is 245.25 with loss [36.25370979309082, 30.456156969070435] in episode 1740
Report: 
rewardSum:245.25
loss:[36.25370979309082, 30.456156969070435]
policies:[3, 2, 1]
qAverage:[24.459426244099934, 19.405691146850586]
ws:[0.5032531420389811, 1.0902243455251057]
memory len:10000
memory used:2912.0
now epsilon is 0.0850676844815498, the reward is 247.25 with loss [23.012828826904297, 20.933884143829346] in episode 1741
Report: 
rewardSum:247.25
loss:[23.012828826904297, 20.933884143829346]
policies:[3, 1, 0]
qAverage:[26.772737884521483, 12.514209747314453]
ws:[-2.4340206384658813, -1.9245355546474456]
memory len:10000
memory used:2912.0
now epsilon is 0.08496140302996052, the reward is 246.25 with loss [28.34284543991089, 27.28445726633072] in episode 1742
Report: 
rewardSum:246.25
loss:[28.34284543991089, 27.28445726633072]
policies:[1, 3, 1]
qAverage:[8.45347900390625, 34.940791320800784]
ws:[-3.0533103585243224, -0.8504119969904422]
memory len:10000
memory used:2912.0
now epsilon is 0.08487647348214697, the reward is 247.25 with loss [21.808444023132324, 18.186033964157104] in episode 1743
Report: 
rewardSum:247.25
loss:[21.808444023132324, 18.186033964157104]
policies:[1, 3, 0]
qAverage:[8.237468719482422, 34.54550018310547]
ws:[-1.4803419589996338, 1.4892228603363038]
memory len:10000
memory used:2912.0
now epsilon is 0.08479162883203795, the reward is 247.25 with loss [22.129223346710205, 29.25390386581421] in episode 1744
Report: 
rewardSum:247.25
loss:[22.129223346710205, 29.25390386581421]
policies:[0, 4, 0]
qAverage:[0.0, 43.95060806274414]
ws:[-2.328111100196838, 0.5424123048782349]
memory len:10000
memory used:2912.0
now epsilon is 0.08470686899476759, the reward is 247.25 with loss [26.974234580993652, 19.816685914993286] in episode 1745
Report: 
rewardSum:247.25
loss:[26.974234580993652, 19.816685914993286]
policies:[1, 3, 0]
qAverage:[10.419711112976074, 27.817699432373047]
ws:[1.1203166469931602, 2.861809805035591]
memory len:10000
memory used:2911.0
now epsilon is 0.08462219388555486, the reward is 247.25 with loss [26.928258895874023, 18.516109585762024] in episode 1746
Report: 
rewardSum:247.25
loss:[26.928258895874023, 18.516109585762024]
policies:[1, 3, 0]
qAverage:[8.21170883178711, 35.283853149414064]
ws:[-3.425214046239853, -0.3986616849899292]
memory len:10000
memory used:2911.0
now epsilon is 0.08453760341970347, the reward is 247.25 with loss [23.755444526672363, 21.263623476028442] in episode 1747
Report: 
rewardSum:247.25
loss:[23.755444526672363, 21.263623476028442]
policies:[0, 3, 1]
qAverage:[0.0, 43.57375621795654]
ws:[-5.4997571893036366, -1.912093480117619]
memory len:10000
memory used:2912.0
now epsilon is 0.08445309751260179, the reward is 247.25 with loss [25.767804384231567, 23.808820247650146] in episode 1748
Report: 
rewardSum:247.25
loss:[25.767804384231567, 23.808820247650146]
policies:[1, 3, 0]
qAverage:[0.0, 41.301472663879395]
ws:[-4.623095374554396, -0.9904155135154724]
memory len:10000
memory used:2912.0
now epsilon is 0.08436867607972277, the reward is 247.25 with loss [19.119603157043457, 24.469618797302246] in episode 1749
Report: 
rewardSum:247.25
loss:[19.119603157043457, 24.469618797302246]
policies:[0, 4, 0]
qAverage:[0.0, 44.329289245605466]
ws:[-1.4511050224304198, 2.86322340965271]
memory len:10000
memory used:2912.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21*		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.08426326795186473, the reward is 246.25 with loss [33.14245891571045, 21.74827265739441] in episode 1750
Report: 
rewardSum:246.25
loss:[33.14245891571045, 21.74827265739441]
policies:[0, 4, 1]
qAverage:[0.0, 39.885355949401855]
ws:[0.607428215444088, 2.254353702068329]
memory len:10000
memory used:2912.0
now epsilon is 0.08417903627737222, the reward is 247.25 with loss [29.7821946144104, 23.637210607528687] in episode 1751
Report: 
rewardSum:247.25
loss:[29.7821946144104, 23.637210607528687]
policies:[0, 4, 0]
qAverage:[0.0, 44.36910705566406]
ws:[-3.54943727850914, 2.0970751523971556]
memory len:10000
memory used:2912.0
now epsilon is 0.0841369520204233, the reward is -1.0 with loss [19.57501792907715, 20.832785606384277] in episode 1752
Report: 
rewardSum:-1.0
loss:[19.57501792907715, 20.832785606384277]
policies:[0, 1, 1]
qAverage:[0.0, 23.46249771118164]
ws:[-0.17315734922885895, 0.974897563457489]
memory len:10000
memory used:2912.0
now epsilon is 0.08398982273813621, the reward is 244.25 with loss [47.38869595527649, 37.67670273780823] in episode 1753
Report: 
rewardSum:244.25
loss:[47.38869595527649, 37.67670273780823]
policies:[0, 5, 2]
qAverage:[0.0, 46.78805923461914]
ws:[-1.2666700184345245, 3.5880613327026367]
memory len:10000
memory used:2912.0
now epsilon is 0.08390586440633258, the reward is 247.25 with loss [33.26776933670044, 17.40194034576416] in episode 1754
Report: 
rewardSum:247.25
loss:[33.26776933670044, 17.40194034576416]
policies:[0, 4, 0]
qAverage:[0.0, 44.46060791015625]
ws:[-2.6401445120573044, 2.951691544055939]
memory len:10000
memory used:2912.0
now epsilon is 0.08382199000138163, the reward is 247.25 with loss [21.531563758850098, 12.813892245292664] in episode 1755
Report: 
rewardSum:247.25
loss:[21.531563758850098, 12.813892245292664]
policies:[1, 3, 0]
qAverage:[8.26453857421875, 35.44186782836914]
ws:[-3.6179621636867525, 0.8775926887989044]
memory len:10000
memory used:2912.0
now epsilon is 0.08373819943938794, the reward is 247.25 with loss [26.797202587127686, 29.745269775390625] in episode 1756
Report: 
rewardSum:247.25
loss:[26.797202587127686, 29.745269775390625]
policies:[1, 1, 2]
qAverage:[13.845392862955729, 17.321224212646484]
ws:[0.966720461845398, 2.215834252536297]
memory len:10000
memory used:2912.0
now epsilon is 0.08365449263654004, the reward is 247.25 with loss [18.124858260154724, 10.411651492118835] in episode 1757
Report: 
rewardSum:247.25
loss:[18.124858260154724, 10.411651492118835]
policies:[1, 3, 0]
qAverage:[8.287560272216798, 35.67700729370117]
ws:[-3.9826352149248123, 0.2477926254272461]
memory len:10000
memory used:2912.0
now epsilon is 0.08352908929753496, the reward is 245.25 with loss [32.62130630016327, 35.5977988243103] in episode 1758
Report: 
rewardSum:245.25
loss:[32.62130630016327, 35.5977988243103]
policies:[1, 4, 1]
qAverage:[6.8404490152994795, 38.41734250386556]
ws:[0.32245934009552, 3.1334662213921547]
memory len:10000
memory used:2912.0
now epsilon is 0.08344559152642568, the reward is 247.25 with loss [29.78561544418335, 26.30123257637024] in episode 1759
Report: 
rewardSum:247.25
loss:[29.78561544418335, 26.30123257637024]
policies:[1, 3, 0]
qAverage:[8.441226959228516, 35.03561553955078]
ws:[-2.4547840118408204, 1.9134809017181396]
memory len:10000
memory used:2911.0
now epsilon is 0.08336217722178108, the reward is 247.25 with loss [26.00347089767456, 26.564873218536377] in episode 1760
Report: 
rewardSum:247.25
loss:[26.00347089767456, 26.564873218536377]
policies:[1, 2, 1]
qAverage:[10.373940467834473, 31.35165309906006]
ws:[-3.9903758764266968, -0.5489545464515686]
memory len:10000
memory used:2911.0
now epsilon is 0.08327884630016595, the reward is 247.25 with loss [24.87993359565735, 15.8391432762146] in episode 1761
Report: 
rewardSum:247.25
loss:[24.87993359565735, 15.8391432762146]
policies:[2, 2, 0]
qAverage:[18.826436614990236, 23.586295318603515]
ws:[-3.8960814401507378, -1.159345443546772]
memory len:10000
memory used:2911.0
now epsilon is 0.08319559867822855, the reward is 247.25 with loss [35.45280599594116, 20.663336753845215] in episode 1762
Report: 
rewardSum:247.25
loss:[35.45280599594116, 20.663336753845215]
policies:[1, 3, 0]
qAverage:[8.315318298339843, 35.96986083984375]
ws:[-4.03585629761219, -1.1219244003295898]
memory len:10000
memory used:2911.0
now epsilon is 0.08307088325009125, the reward is 245.25 with loss [27.90547239780426, 37.556499004364014] in episode 1763
Report: 
rewardSum:245.25
loss:[27.90547239780426, 37.556499004364014]
policies:[1, 4, 1]
qAverage:[7.003609975179036, 38.35472615559896]
ws:[-0.9970578104257584, 1.2527274837096531]
memory len:10000
memory used:2912.0
now epsilon is 0.08298784351323078, the reward is 247.25 with loss [28.973861694335938, 26.939074516296387] in episode 1764
Report: 
rewardSum:247.25
loss:[28.973861694335938, 26.939074516296387]
policies:[0, 4, 0]
qAverage:[0.0, 40.49683856964111]
ws:[2.1346218287944794, 4.557354807853699]
memory len:10000
memory used:2911.0
now epsilon is 0.08290488678497246, the reward is 247.25 with loss [17.46594727039337, 22.490683555603027] in episode 1765
Report: 
rewardSum:247.25
loss:[17.46594727039337, 22.490683555603027]
policies:[0, 3, 1]
qAverage:[0.0, 44.53334331512451]
ws:[-2.374380499124527, 0.3309158682823181]
memory len:10000
memory used:2911.0
now epsilon is 0.0828220129823388, the reward is 247.25 with loss [26.170294284820557, 25.045669555664062] in episode 1766
Report: 
rewardSum:247.25
loss:[26.170294284820557, 25.045669555664062]
policies:[0, 4, 0]
qAverage:[0.0, 45.69951477050781]
ws:[-1.4251517415046693, 1.4399832248687745]
memory len:10000
memory used:2911.0
now epsilon is 0.08273922202243529, the reward is 37.15999999999997 with loss [28.254787921905518, 19.319496154785156] in episode 1767
Report: 
rewardSum:37.15999999999997
loss:[28.254787921905518, 19.319496154785156]
policies:[0, 3, 1]
qAverage:[0.0, 40.06263446807861]
ws:[0.5587422349490225, 2.1452175974845886]
memory len:10000
memory used:2911.0
now epsilon is 0.08265651382245025, the reward is 247.25 with loss [16.108771324157715, 31.16546869277954] in episode 1768
Report: 
rewardSum:247.25
loss:[16.108771324157715, 31.16546869277954]
policies:[0, 4, 0]
qAverage:[0.0, 45.558142852783206]
ws:[-1.8950291596353055, 2.0609251618385316]
memory len:10000
memory used:2911.0
now epsilon is 0.08257388829965479, the reward is 247.25 with loss [23.530176401138306, 19.294294953346252] in episode 1769
Report: 
rewardSum:247.25
loss:[23.530176401138306, 19.294294953346252]
policies:[0, 4, 0]
qAverage:[0.0, 45.99675369262695]
ws:[-1.9240440458059311, 0.8624101877212524]
memory len:10000
memory used:2912.0
now epsilon is 0.0824501048544261, the reward is 245.25 with loss [38.354180574417114, 28.93490743637085] in episode 1770
Report: 
rewardSum:245.25
loss:[38.354180574417114, 28.93490743637085]
policies:[0, 3, 3]
qAverage:[0.0, 42.466081619262695]
ws:[-2.1787121295928955, 0.14606933295726776]
memory len:10000
memory used:2913.0
now epsilon is 0.08236768566320818, the reward is 247.25 with loss [13.75220775604248, 25.800742626190186] in episode 1771
Report: 
rewardSum:247.25
loss:[13.75220775604248, 25.800742626190186]
policies:[1, 3, 0]
qAverage:[8.368972778320312, 35.93252792358398]
ws:[-1.6210590243339538, 1.7412746906280518]
memory len:10000
memory used:2913.0
now epsilon is 0.08232650696835693, the reward is -1.0 with loss [12.384256839752197, 10.5021071434021] in episode 1772
Report: 
rewardSum:-1.0
loss:[12.384256839752197, 10.5021071434021]
policies:[1, 0, 1]
qAverage:[20.075138092041016, 0.0]
ws:[0.7020975351333618, 0.06030908226966858]
memory len:10000
memory used:2912.0
now epsilon is 0.08224421132868362, the reward is 247.25 with loss [22.046875, 25.638946056365967] in episode 1773
Report: 
rewardSum:247.25
loss:[22.046875, 25.638946056365967]
policies:[1, 3, 0]
qAverage:[8.307928466796875, 36.03732757568359]
ws:[-2.5980571463704107, 0.020761561393737794]
memory len:10000
memory used:2912.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22*		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.08216199795379425, the reward is 247.25 with loss [22.35551881790161, 21.18501353263855] in episode 1774
Report: 
rewardSum:247.25
loss:[22.35551881790161, 21.18501353263855]
policies:[1, 3, 0]
qAverage:[7.967028045654297, 36.193170166015626]
ws:[-1.7200366973876953, -0.16880321502685547]
memory len:10000
memory used:2912.0
now epsilon is 0.0820798667614549, the reward is 247.25 with loss [22.156986713409424, 20.6937837600708] in episode 1775
Report: 
rewardSum:247.25
loss:[22.156986713409424, 20.6937837600708]
policies:[2, 2, 0]
qAverage:[10.37724494934082, 29.67277431488037]
ws:[-1.7664804458618164, 1.136753797531128]
memory len:10000
memory used:2912.0
now epsilon is 0.08197731821509643, the reward is 246.25 with loss [29.959108114242554, 41.793729305267334] in episode 1776
Report: 
rewardSum:246.25
loss:[29.959108114242554, 41.793729305267334]
policies:[1, 3, 1]
qAverage:[0.0, 43.46427536010742]
ws:[-3.1476882891729474, -0.30264151096343994]
memory len:10000
memory used:2912.0
now epsilon is 0.08185442906589653, the reward is 35.15999999999997 with loss [35.304237604141235, 34.649418354034424] in episode 1777
Report: 
rewardSum:35.15999999999997
loss:[35.304237604141235, 34.649418354034424]
policies:[1, 2, 3]
qAverage:[10.260892868041992, 27.626681327819824]
ws:[-0.11788387596607208, 0.5460567250847816]
memory len:10000
memory used:2912.0
now epsilon is 0.08177260532712596, the reward is 247.25 with loss [30.369749546051025, 24.237610340118408] in episode 1778
Report: 
rewardSum:247.25
loss:[30.369749546051025, 24.237610340118408]
policies:[0, 4, 0]
qAverage:[0.0, 45.74237747192383]
ws:[-4.68694394826889, -0.8185972779989242]
memory len:10000
memory used:2912.0
now epsilon is 0.08169086338141539, the reward is 247.25 with loss [32.05427074432373, 16.92615818977356] in episode 1779
Report: 
rewardSum:247.25
loss:[32.05427074432373, 16.92615818977356]
policies:[0, 4, 0]
qAverage:[0.0, 45.285223388671874]
ws:[-4.012752342224121, 1.965702509880066]
memory len:10000
memory used:2912.0
now epsilon is 0.0816092031470024, the reward is 247.25 with loss [19.100273489952087, 34.532188415527344] in episode 1780
Report: 
rewardSum:247.25
loss:[19.100273489952087, 34.532188415527344]
policies:[0, 4, 0]
qAverage:[0.0, 45.27387924194336]
ws:[-3.5869898915290834, 2.331981897354126]
memory len:10000
memory used:2912.0
now epsilon is 0.08152762454220634, the reward is 247.25 with loss [22.398531436920166, 25.935242176055908] in episode 1781
Report: 
rewardSum:247.25
loss:[22.398531436920166, 25.935242176055908]
policies:[0, 4, 0]
qAverage:[0.0, 45.75320205688477]
ws:[-3.302270698547363, 1.2388975620269775]
memory len:10000
memory used:2913.0
now epsilon is 0.08144612748542819, the reward is 247.25 with loss [15.537916898727417, 22.458133459091187] in episode 1782
Report: 
rewardSum:247.25
loss:[15.537916898727417, 22.458133459091187]
policies:[1, 3, 0]
qAverage:[8.064985656738282, 36.095279693603516]
ws:[-3.132699006795883, 1.027724379301071]
memory len:10000
memory used:2913.0
now epsilon is 0.08136471189515052, the reward is 247.25 with loss [23.53036093711853, 20.246848583221436] in episode 1783
Report: 
rewardSum:247.25
loss:[23.53036093711853, 20.246848583221436]
policies:[1, 3, 0]
qAverage:[8.164167022705078, 36.3295036315918]
ws:[-3.405151903629303, -0.2533962786197662]
memory len:10000
memory used:2912.0
now epsilon is 0.08128337768993737, the reward is 247.25 with loss [23.970431327819824, 26.847864627838135] in episode 1784
Report: 
rewardSum:247.25
loss:[23.970431327819824, 26.847864627838135]
policies:[1, 3, 0]
qAverage:[0.0, 43.53939437866211]
ws:[-3.3297324925661087, 1.3216719031333923]
memory len:10000
memory used:2912.0
now epsilon is 0.08116152880117278, the reward is 245.25 with loss [41.401702880859375, 36.62542510032654] in episode 1785
Report: 
rewardSum:245.25
loss:[41.401702880859375, 36.62542510032654]
policies:[0, 5, 1]
qAverage:[0.0, 47.32138188680013]
ws:[0.5179108182589213, 5.575974067052205]
memory len:10000
memory used:2912.0
now epsilon is 0.08108039770287263, the reward is 247.25 with loss [18.987318754196167, 20.952084064483643] in episode 1786
Report: 
rewardSum:247.25
loss:[18.987318754196167, 20.952084064483643]
policies:[0, 4, 0]
qAverage:[0.0, 45.72629241943359]
ws:[-2.524874296784401, 3.3025882124900816]
memory len:10000
memory used:2912.0
now epsilon is 0.0809993477052517, the reward is 247.25 with loss [34.1137638092041, 28.591012477874756] in episode 1787
Report: 
rewardSum:247.25
loss:[34.1137638092041, 28.591012477874756]
policies:[1, 3, 0]
qAverage:[8.012264251708984, 36.889501953125]
ws:[-3.98118057847023, -0.12179405689239502]
memory len:10000
memory used:2912.0
now epsilon is 0.08091837872723971, the reward is 247.25 with loss [17.650805950164795, 34.17508792877197] in episode 1788
Report: 
rewardSum:247.25
loss:[17.650805950164795, 34.17508792877197]
policies:[1, 3, 0]
qAverage:[8.192009735107423, 38.05348663330078]
ws:[-4.623631471395493, -1.571018236875534]
memory len:10000
memory used:2912.0
now epsilon is 0.08083749068784742, the reward is 247.25 with loss [27.67323875427246, 22.87494158744812] in episode 1789
Report: 
rewardSum:247.25
loss:[27.67323875427246, 22.87494158744812]
policies:[1, 3, 0]
qAverage:[0.0, 46.14129161834717]
ws:[-5.058946333825588, -0.8515734672546387]
memory len:10000
memory used:2913.0
now epsilon is 0.08075668350616658, the reward is 247.25 with loss [25.040742874145508, 26.241281747817993] in episode 1790
Report: 
rewardSum:247.25
loss:[25.040742874145508, 26.241281747817993]
policies:[1, 3, 0]
qAverage:[8.0835693359375, 37.50837097167969]
ws:[-3.2996243327856063, 0.3062965631484985]
memory len:10000
memory used:2912.0
now epsilon is 0.08065578811209442, the reward is 246.25 with loss [29.428493976593018, 27.63335657119751] in episode 1791
Report: 
rewardSum:246.25
loss:[29.428493976593018, 27.63335657119751]
policies:[0, 4, 1]
qAverage:[0.0, 45.952537536621094]
ws:[-1.8078077673912047, 1.2992852210998536]
memory len:10000
memory used:2913.0
now epsilon is 0.08057516256486222, the reward is 247.25 with loss [21.270065546035767, 30.877403736114502] in episode 1792
Report: 
rewardSum:247.25
loss:[21.270065546035767, 30.877403736114502]
policies:[2, 2, 0]
qAverage:[10.237168312072754, 29.15653896331787]
ws:[0.9857718050479889, 2.0281604528427124]
memory len:10000
memory used:2913.0
now epsilon is 0.0804946176129477, the reward is 247.25 with loss [22.33624029159546, 31.196310997009277] in episode 1793
Report: 
rewardSum:247.25
loss:[22.33624029159546, 31.196310997009277]
policies:[1, 3, 0]
qAverage:[0.0, 40.77069854736328]
ws:[-3.67914350827535, 0.9099098841349283]
memory len:10000
memory used:2913.0
now epsilon is 0.08039404963749182, the reward is 246.25 with loss [29.259926319122314, 30.62148356437683] in episode 1794
Report: 
rewardSum:246.25
loss:[29.259926319122314, 30.62148356437683]
policies:[0, 4, 1]
qAverage:[0.0, 46.90430221557617]
ws:[-2.9122096240520476, 1.521820068359375]
memory len:10000
memory used:2912.0
now epsilon is 0.08031368573059863, the reward is 247.25 with loss [24.109700679779053, 17.08226776123047] in episode 1795
Report: 
rewardSum:247.25
loss:[24.109700679779053, 17.08226776123047]
policies:[0, 4, 0]
qAverage:[0.0, 40.23823261260986]
ws:[-0.41840045154094696, 1.637480229139328]
memory len:10000
memory used:2912.0
now epsilon is 0.0802334021574809, the reward is 247.25 with loss [23.3374342918396, 30.605998992919922] in episode 1796
Report: 
rewardSum:247.25
loss:[23.3374342918396, 30.605998992919922]
policies:[0, 4, 0]
qAverage:[0.0, 47.08366470336914]
ws:[-3.8878825187683104, 1.4807352781295777]
memory len:10000
memory used:2912.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25*		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.08015319883783496, the reward is 247.25 with loss [32.14051389694214, 28.607673168182373] in episode 1797
Report: 
rewardSum:247.25
loss:[32.14051389694214, 28.607673168182373]
policies:[0, 4, 0]
qAverage:[0.0, 46.845706939697266]
ws:[-4.167290884256363, 0.7197846412658692]
memory len:10000
memory used:2912.0
now epsilon is 0.08007307569143744, the reward is 247.25 with loss [30.16992712020874, 23.509453773498535] in episode 1798
Report: 
rewardSum:247.25
loss:[30.16992712020874, 23.509453773498535]
policies:[0, 4, 0]
qAverage:[0.0, 47.155856132507324]
ws:[-4.971214607357979, 1.0024091005325317]
memory len:10000
memory used:2913.0
now epsilon is 0.07999303263814515, the reward is 247.25 with loss [25.681495904922485, 16.917689085006714] in episode 1799
Report: 
rewardSum:247.25
loss:[25.681495904922485, 16.917689085006714]
policies:[1, 3, 0]
qAverage:[0.0, 40.63228225708008]
ws:[1.635072961449623, 4.44197404384613]
memory len:10000
memory used:2913.0
now epsilon is 0.07991306959789501, the reward is 247.25 with loss [23.072570323944092, 31.997167110443115] in episode 1800
Report: 
rewardSum:247.25
loss:[23.072570323944092, 31.997167110443115]
policies:[0, 4, 0]
qAverage:[0.0, 49.59971237182617]
ws:[-2.9260475993156434, 3.206936073303223]
memory len:10000
memory used:2912.0
now epsilon is 0.07983318649070398, the reward is 247.25 with loss [19.021996021270752, 25.838304042816162] in episode 1801
Report: 
rewardSum:247.25
loss:[19.021996021270752, 25.838304042816162]
policies:[0, 4, 0]
qAverage:[0.0, 48.970654296875]
ws:[-2.4638752579689025, 3.554538917541504]
memory len:10000
memory used:2911.0
now epsilon is 0.07975338323666897, the reward is 247.25 with loss [23.77353811264038, 21.943903923034668] in episode 1802
Report: 
rewardSum:247.25
loss:[23.77353811264038, 21.943903923034668]
policies:[0, 4, 0]
qAverage:[0.0, 49.7162368774414]
ws:[-3.762938469648361, 2.05037522315979]
memory len:10000
memory used:2911.0
now epsilon is 0.07967365975596674, the reward is 247.25 with loss [22.854238033294678, 21.424412965774536] in episode 1803
Report: 
rewardSum:247.25
loss:[22.854238033294678, 21.424412965774536]
policies:[0, 4, 0]
qAverage:[0.0, 49.55731201171875]
ws:[-3.8163689851760862, 1.6123553276062013]
memory len:10000
memory used:2911.0
now epsilon is 0.0795940159688539, the reward is 247.25 with loss [24.231310844421387, 27.74742341041565] in episode 1804
Report: 
rewardSum:247.25
loss:[24.231310844421387, 27.74742341041565]
policies:[0, 4, 0]
qAverage:[0.0, 49.83716506958008]
ws:[-3.2598539710044863, 2.1548937946557998]
memory len:10000
memory used:2912.0
now epsilon is 0.07945483086453729, the reward is 244.25 with loss [42.017887115478516, 33.366402983665466] in episode 1805
Report: 
rewardSum:244.25
loss:[42.017887115478516, 33.366402983665466]
policies:[0, 5, 2]
qAverage:[0.0, 49.21612548828125]
ws:[-0.16299778819084168, 2.9743699550628664]
memory len:10000
memory used:2912.0
now epsilon is 0.07937540582426873, the reward is 247.25 with loss [29.787760972976685, 23.75941491127014] in episode 1806
Report: 
rewardSum:247.25
loss:[29.787760972976685, 23.75941491127014]
policies:[1, 3, 0]
qAverage:[8.073665618896484, 40.81497650146484]
ws:[-0.7237143516540527, 3.5876482248306276]
memory len:10000
memory used:2912.0
now epsilon is 0.07925641710517513, the reward is 245.25 with loss [36.10716959089041, 29.287270307540894] in episode 1807
Report: 
rewardSum:245.25
loss:[36.10716959089041, 29.287270307540894]
policies:[0, 5, 1]
qAverage:[0.0, 52.59004020690918]
ws:[0.5557125806808472, 4.371641635894775]
memory len:10000
memory used:2912.0
now epsilon is 0.07917719040427315, the reward is 247.25 with loss [20.784855127334595, 16.51511836051941] in episode 1808
Report: 
rewardSum:247.25
loss:[20.784855127334595, 16.51511836051941]
policies:[1, 3, 0]
qAverage:[8.321444702148437, 39.95113830566406]
ws:[-1.4383486926555633, 3.1277172565460205]
memory len:10000
memory used:2912.0
now epsilon is 0.07909804290036702, the reward is 247.25 with loss [29.970478057861328, 18.65396499633789] in episode 1809
Report: 
rewardSum:247.25
loss:[29.970478057861328, 18.65396499633789]
policies:[0, 4, 0]
qAverage:[0.0, 49.829976654052736]
ws:[-2.013636219501495, 2.8199225187301638]
memory len:10000
memory used:2912.0
now epsilon is 0.07901897451428942, the reward is 247.25 with loss [23.145618438720703, 15.626405715942383] in episode 1810
Report: 
rewardSum:247.25
loss:[23.145618438720703, 15.626405715942383]
policies:[0, 4, 0]
qAverage:[0.0, 48.67445831298828]
ws:[-1.2512310862541198, 4.409413528442383]
memory len:10000
memory used:2912.0
now epsilon is 0.0789399851669522, the reward is 247.25 with loss [18.428282976150513, 36.724422454833984] in episode 1811
Report: 
rewardSum:247.25
loss:[18.428282976150513, 36.724422454833984]
policies:[0, 4, 0]
qAverage:[0.0, 52.683109283447266]
ws:[-1.0454355478286743, 4.501471948623657]
memory len:10000
memory used:2912.0
now epsilon is 0.07886107477934626, the reward is 247.25 with loss [25.5798282623291, 28.915493965148926] in episode 1812
Report: 
rewardSum:247.25
loss:[25.5798282623291, 28.915493965148926]
policies:[0, 4, 0]
qAverage:[0.0, 49.84334468841553]
ws:[-1.5502265393733978, 2.9375093579292297]
memory len:10000
memory used:2912.0
now epsilon is 0.07878224327254145, the reward is 247.25 with loss [21.503923892974854, 24.5508770942688] in episode 1813
Report: 
rewardSum:247.25
loss:[21.503923892974854, 24.5508770942688]
policies:[1, 3, 0]
qAverage:[8.2583740234375, 42.04527740478515]
ws:[-1.7194674372673036, 1.9105739712715148]
memory len:10000
memory used:2912.0
now epsilon is 0.07870349056768657, the reward is 247.25 with loss [28.367450714111328, 25.100815773010254] in episode 1814
Report: 
rewardSum:247.25
loss:[28.367450714111328, 25.100815773010254]
policies:[1, 3, 0]
qAverage:[8.681941986083984, 41.733253479003906]
ws:[-3.087367856502533, -0.5986263871192932]
memory len:10000
memory used:2912.0
now epsilon is 0.07862481658600919, the reward is 247.25 with loss [24.442763090133667, 24.42688488960266] in episode 1815
Report: 
rewardSum:247.25
loss:[24.442763090133667, 24.42688488960266]
policies:[1, 3, 0]
qAverage:[8.487083435058594, 42.16498947143555]
ws:[-2.5295878767967226, 0.1946386441588402]
memory len:10000
memory used:2912.0
now epsilon is 0.07858550909176723, the reward is -1.0 with loss [18.768457412719727, 15.391057968139648] in episode 1816
Report: 
rewardSum:-1.0
loss:[18.768457412719727, 15.391057968139648]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:10000
memory used:2912.0
now epsilon is 0.0785069530473301, the reward is 247.25 with loss [22.001776337623596, 21.17236852645874] in episode 1817
Report: 
rewardSum:247.25
loss:[22.001776337623596, 21.17236852645874]
policies:[0, 4, 0]
qAverage:[0.0, 51.90439071655273]
ws:[-2.5411048233509064, 0.4189989328384399]
memory len:10000
memory used:2912.0
now epsilon is 0.07842847552948379, the reward is 247.25 with loss [16.718932628631592, 21.3338406085968] in episode 1818
Report: 
rewardSum:247.25
loss:[16.718932628631592, 21.3338406085968]
policies:[1, 3, 0]
qAverage:[8.358325958251953, 42.729835510253906]
ws:[-2.0726716041564943, 1.4419848442077636]
memory len:10000
memory used:2912.0
now epsilon is 0.07835007645973117, the reward is 247.25 with loss [22.425610303878784, 22.857759714126587] in episode 1819
Report: 
rewardSum:247.25
loss:[22.425610303878784, 22.857759714126587]
policies:[0, 4, 0]
qAverage:[0.0, 52.107763671875]
ws:[-1.7467560291290283, 2.5878243446350098]
memory len:10000
memory used:2912.0
now epsilon is 0.07827175575965355, the reward is 247.25 with loss [34.9296817779541, 29.98085641860962] in episode 1820
Report: 
rewardSum:247.25
loss:[34.9296817779541, 29.98085641860962]
policies:[0, 4, 0]
qAverage:[0.0, 52.28498840332031]
ws:[-1.439770269393921, 3.4424288272857666]
memory len:10000
memory used:2912.0
now epsilon is 0.07819351335091064, the reward is 247.25 with loss [19.43550753593445, 32.43447279930115] in episode 1821
Report: 
rewardSum:247.25
loss:[19.43550753593445, 32.43447279930115]
policies:[0, 4, 0]
qAverage:[0.0, 52.21782913208008]
ws:[-2.0113752841949464, 3.3182151556015014]
memory len:10000
memory used:2912.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2*		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.07811534915524045, the reward is 247.25 with loss [23.512934684753418, 21.36886739730835] in episode 1822
Report: 
rewardSum:247.25
loss:[23.512934684753418, 21.36886739730835]
policies:[0, 4, 0]
qAverage:[0.0, 50.9924840927124]
ws:[-3.4565876126289368, 1.4945050477981567]
memory len:10000
memory used:2912.0
now epsilon is 0.07803726309445924, the reward is 247.25 with loss [31.535988807678223, 23.76436996459961] in episode 1823
Report: 
rewardSum:247.25
loss:[31.535988807678223, 23.76436996459961]
policies:[1, 3, 0]
qAverage:[8.586101531982422, 42.76940231323242]
ws:[-2.6443278431892394, 2.1780029773712157]
memory len:10000
memory used:2912.0
now epsilon is 0.07792028033536963, the reward is 245.25 with loss [28.404990911483765, 44.66996669769287] in episode 1824
Report: 
rewardSum:245.25
loss:[28.404990911483765, 44.66996669769287]
policies:[1, 4, 1]
qAverage:[8.799618530273438, 44.872266387939455]
ws:[-1.430396431684494, 1.8984419345855712]
memory len:10000
memory used:2913.0
now epsilon is 0.07784238927026968, the reward is 247.25 with loss [21.748955845832825, 27.870851039886475] in episode 1825
Report: 
rewardSum:247.25
loss:[21.748955845832825, 27.870851039886475]
policies:[1, 3, 0]
qAverage:[9.96559829711914, 46.17998046875]
ws:[-1.8059579372406005, 3.1238263964653017]
memory len:10000
memory used:2912.0
now epsilon is 0.07776457606703055, the reward is 247.25 with loss [17.126226902008057, 26.958162546157837] in episode 1826
Report: 
rewardSum:247.25
loss:[17.126226902008057, 26.958162546157837]
policies:[0, 4, 0]
qAverage:[0.0, 56.95723648071289]
ws:[-0.515360975265503, 4.285088109970093]
memory len:10000
memory used:2912.0
now epsilon is 0.07768684064781958, the reward is 247.25 with loss [26.49446964263916, 18.779261589050293] in episode 1827
Report: 
rewardSum:247.25
loss:[26.49446964263916, 18.779261589050293]
policies:[0, 4, 0]
qAverage:[0.0, 56.3818717956543]
ws:[-0.49386940002441404, 4.127067899703979]
memory len:10000
memory used:2912.0
now epsilon is 0.07760918293488189, the reward is 247.25 with loss [27.602774143218994, 28.022825241088867] in episode 1828
Report: 
rewardSum:247.25
loss:[27.602774143218994, 28.022825241088867]
policies:[1, 3, 0]
qAverage:[8.746407318115235, 48.253561401367186]
ws:[-0.5341822862625122, 2.5753777027130127]
memory len:10000
memory used:2912.0
now epsilon is 0.07753160285054034, the reward is 247.25 with loss [37.734710693359375, 20.897151947021484] in episode 1829
Report: 
rewardSum:247.25
loss:[37.734710693359375, 20.897151947021484]
policies:[1, 3, 0]
qAverage:[9.160613250732421, 46.519961547851565]
ws:[-0.8404173851013184, 2.3614534080028533]
memory len:10000
memory used:2912.0
now epsilon is 0.07745410031719545, the reward is 247.25 with loss [24.635088443756104, 24.86556577682495] in episode 1830
Report: 
rewardSum:247.25
loss:[24.635088443756104, 24.86556577682495]
policies:[2, 2, 0]
qAverage:[11.047993659973145, 44.05904579162598]
ws:[-1.5914645493030548, 2.6277563720941544]
memory len:10000
memory used:2912.0
now epsilon is 0.0773766752573253, the reward is 247.25 with loss [20.70495843887329, 20.386250019073486] in episode 1831
Report: 
rewardSum:247.25
loss:[20.70495843887329, 20.386250019073486]
policies:[1, 3, 0]
qAverage:[9.073655700683593, 46.707970428466794]
ws:[-1.136596256494522, 2.4768614053726195]
memory len:10000
memory used:2913.0
now epsilon is 0.07729932759348547, the reward is 247.25 with loss [33.69460868835449, 21.6277813911438] in episode 1832
Report: 
rewardSum:247.25
loss:[33.69460868835449, 21.6277813911438]
policies:[1, 3, 0]
qAverage:[9.113272094726563, 46.91075592041015]
ws:[-0.5267725467681885, 2.8610554337501526]
memory len:10000
memory used:2913.0
now epsilon is 0.07722205724830894, the reward is 247.25 with loss [24.531368255615234, 26.308998107910156] in episode 1833
Report: 
rewardSum:247.25
loss:[24.531368255615234, 26.308998107910156]
policies:[1, 3, 0]
qAverage:[8.948013305664062, 47.43298492431641]
ws:[-1.7437803387641906, 0.9204145669937134]
memory len:10000
memory used:2913.0
now epsilon is 0.07714486414450603, the reward is 247.25 with loss [20.780866146087646, 20.833544969558716] in episode 1834
Report: 
rewardSum:247.25
loss:[20.780866146087646, 20.833544969558716]
policies:[1, 3, 0]
qAverage:[9.124329376220704, 46.86919021606445]
ws:[-1.4896536350250245, 1.6007303714752197]
memory len:10000
memory used:2912.0
now epsilon is 0.07706774820486434, the reward is 37.15999999999997 with loss [26.205875158309937, 22.030418276786804] in episode 1835
Report: 
rewardSum:37.15999999999997
loss:[26.205875158309937, 22.030418276786804]
policies:[1, 2, 1]
qAverage:[11.348210334777832, 35.00489807128906]
ws:[0.7834481820464134, 2.7870189994573593]
memory len:10000
memory used:2912.0
now epsilon is 0.07699070935224862, the reward is 247.25 with loss [30.554901123046875, 35.99395275115967] in episode 1836
Report: 
rewardSum:247.25
loss:[30.554901123046875, 35.99395275115967]
policies:[1, 3, 0]
qAverage:[0.0, 63.65820598602295]
ws:[-2.9907722175121307, 2.619450718164444]
memory len:10000
memory used:2913.0
now epsilon is 0.07691374750960077, the reward is 247.25 with loss [29.132060050964355, 22.33948016166687] in episode 1837
Report: 
rewardSum:247.25
loss:[29.132060050964355, 22.33948016166687]
policies:[0, 4, 0]
qAverage:[0.0, 65.10854873657226]
ws:[-1.6266146689653396, 4.226499366760254]
memory len:10000
memory used:2913.0
now epsilon is 0.0768368625999397, the reward is 247.25 with loss [24.65177083015442, 22.96385908126831] in episode 1838
Report: 
rewardSum:247.25
loss:[24.65177083015442, 22.96385908126831]
policies:[0, 4, 0]
qAverage:[0.0, 62.3126220703125]
ws:[-0.09080708026885986, 6.0155130386352536]
memory len:10000
memory used:2913.0
now epsilon is 0.07676005454636124, the reward is 247.25 with loss [17.128360748291016, 20.49867081642151] in episode 1839
Report: 
rewardSum:247.25
loss:[17.128360748291016, 20.49867081642151]
policies:[0, 4, 0]
qAverage:[0.0, 65.65783996582032]
ws:[0.17684557437896728, 5.934715270996094]
memory len:10000
memory used:2912.0
now epsilon is 0.07668332327203813, the reward is 247.25 with loss [25.877576112747192, 22.99129891395569] in episode 1840
Report: 
rewardSum:247.25
loss:[25.877576112747192, 22.99129891395569]
policies:[0, 4, 0]
qAverage:[0.0, 63.68879852294922]
ws:[0.037443137168884276, 5.361978578567505]
memory len:10000
memory used:2912.0
now epsilon is 0.0765683701537866, the reward is 35.15999999999997 with loss [36.65869331359863, 42.90946817398071] in episode 1841
Report: 
rewardSum:35.15999999999997
loss:[36.65869331359863, 42.90946817398071]
policies:[1, 3, 2]
qAverage:[9.997385406494141, 44.953355407714845]
ws:[1.167369358241558, 3.1388168334960938]
memory len:10000
memory used:2912.0
now epsilon is 0.07649183049198642, the reward is 247.25 with loss [23.743240356445312, 26.247681617736816] in episode 1842
Report: 
rewardSum:247.25
loss:[23.743240356445312, 26.247681617736816]
policies:[0, 4, 0]
qAverage:[0.0, 64.91128005981446]
ws:[0.15793519616127014, 4.337426471710205]
memory len:10000
memory used:2924.0
now epsilon is 0.07641536734115044, the reward is 247.25 with loss [24.070072174072266, 36.239315032958984] in episode 1843
Report: 
rewardSum:247.25
loss:[24.070072174072266, 36.239315032958984]
policies:[0, 3, 1]
qAverage:[0.0, 65.42723083496094]
ws:[0.24208682775497437, 5.603819251060486]
memory len:10000
memory used:2924.0
now epsilon is 0.07633898062479638, the reward is 247.25 with loss [27.282301664352417, 37.58703947067261] in episode 1844
Report: 
rewardSum:247.25
loss:[27.282301664352417, 37.58703947067261]
policies:[0, 4, 0]
qAverage:[0.0, 64.95394515991211]
ws:[0.5223931550979615, 5.6147969484329225]
memory len:10000
memory used:2924.0
now epsilon is 0.07626267026651844, the reward is 247.25 with loss [34.13442039489746, 32.57285022735596] in episode 1845
Report: 
rewardSum:247.25
loss:[34.13442039489746, 32.57285022735596]
policies:[0, 4, 0]
qAverage:[0.0, 63.516858673095705]
ws:[0.8842080593109131, 8.29912257194519]
memory len:10000
memory used:2924.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22*		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.0761102783189494, the reward is 243.25 with loss [44.878142952919006, 61.09577512741089] in episode 1846
Report: 
rewardSum:243.25
loss:[44.878142952919006, 61.09577512741089]
policies:[0, 6, 2]
qAverage:[0.0, 68.72253254481724]
ws:[2.4999536105564664, 6.974605424063546]
memory len:10000
memory used:2912.0
now epsilon is 0.07603419657722824, the reward is 247.25 with loss [18.21511721611023, 26.933254718780518] in episode 1847
Report: 
rewardSum:247.25
loss:[18.21511721611023, 26.933254718780518]
policies:[1, 3, 0]
qAverage:[10.38447265625, 60.3290283203125]
ws:[-0.6082217156887054, 4.683902633190155]
memory len:10000
memory used:2912.0
now epsilon is 0.07595819088872291, the reward is 247.25 with loss [17.423523545265198, 22.690412044525146] in episode 1848
Report: 
rewardSum:247.25
loss:[17.423523545265198, 22.690412044525146]
policies:[1, 3, 0]
qAverage:[12.165840148925781, 58.7217529296875]
ws:[0.2886189639568329, 4.533250188827514]
memory len:10000
memory used:2912.0
now epsilon is 0.07588226117740868, the reward is 247.25 with loss [22.927878856658936, 18.642733573913574] in episode 1849
Report: 
rewardSum:247.25
loss:[22.927878856658936, 18.642733573913574]
policies:[1, 3, 0]
qAverage:[10.674505615234375, 58.58333282470703]
ws:[0.3652068629860878, 4.972876298427582]
memory len:10000
memory used:2912.0
now epsilon is 0.07580640736733688, the reward is 247.25 with loss [31.51393222808838, 20.23702096939087] in episode 1850
Report: 
rewardSum:247.25
loss:[31.51393222808838, 20.23702096939087]
policies:[0, 4, 0]
qAverage:[0.0, 72.8593963623047]
ws:[2.431789493560791, 9.456188011169434]
memory len:10000
memory used:2913.0
now epsilon is 0.07573062938263471, the reward is 247.25 with loss [22.11454176902771, 22.6335711479187] in episode 1851
Report: 
rewardSum:247.25
loss:[22.11454176902771, 22.6335711479187]
policies:[0, 4, 0]
qAverage:[0.0, 70.36364974975587]
ws:[3.608566999435425, 11.945340538024903]
memory len:10000
memory used:2913.0
now epsilon is 0.07565492714750523, the reward is 247.25 with loss [26.39206075668335, 19.433351755142212] in episode 1852
Report: 
rewardSum:247.25
loss:[26.39206075668335, 19.433351755142212]
policies:[0, 4, 0]
qAverage:[0.0, 65.96842575073242]
ws:[1.6608210802078247, 7.834477424621582]
memory len:10000
memory used:2913.0
now epsilon is 0.07554151565964046, the reward is 245.25 with loss [38.94342279434204, 43.53772020339966] in episode 1853
Report: 
rewardSum:245.25
loss:[38.94342279434204, 43.53772020339966]
policies:[1, 3, 2]
qAverage:[0.0, 57.379902839660645]
ws:[1.3606703728437424, 4.87833845615387]
memory len:10000
memory used:2913.0
now epsilon is 0.07546600246732814, the reward is 247.25 with loss [24.040581703186035, 21.80376434326172] in episode 1854
Report: 
rewardSum:247.25
loss:[24.040581703186035, 21.80376434326172]
policies:[1, 3, 0]
qAverage:[14.706414222717285, 53.18292999267578]
ws:[0.2139018028974533, 5.08784493803978]
memory len:10000
memory used:2913.0
now epsilon is 0.07539056475989542, the reward is 247.25 with loss [31.496002674102783, 24.836142778396606] in episode 1855
Report: 
rewardSum:247.25
loss:[31.496002674102783, 24.836142778396606]
policies:[1, 2, 1]
qAverage:[18.299889882405598, 25.521260579427082]
ws:[0.6926718552907308, 2.427063465118408]
memory len:10000
memory used:2913.0
now epsilon is 0.0753152024618857, the reward is 247.25 with loss [33.2406222820282, 25.102303504943848] in episode 1856
Report: 
rewardSum:247.25
loss:[33.2406222820282, 25.102303504943848]
policies:[0, 3, 1]
qAverage:[0.0, 64.98236274719238]
ws:[-0.5833770334720612, 3.5609328746795654]
memory len:10000
memory used:2913.0
now epsilon is 0.07523991549791784, the reward is 247.25 with loss [22.73054552078247, 16.694271087646484] in episode 1857
Report: 
rewardSum:247.25
loss:[22.73054552078247, 16.694271087646484]
policies:[0, 4, 0]
qAverage:[0.0, 73.3458755493164]
ws:[-0.6777152717113495, 3.7264124393463134]
memory len:10000
memory used:2913.0
now epsilon is 0.07516470379268603, the reward is 247.25 with loss [27.099079608917236, 39.69646120071411] in episode 1858
Report: 
rewardSum:247.25
loss:[27.099079608917236, 39.69646120071411]
policies:[0, 4, 0]
qAverage:[0.0, 71.24848403930665]
ws:[-0.24927659034729005, 5.435752725601196]
memory len:10000
memory used:2912.0
now epsilon is 0.07508956727095979, the reward is 247.25 with loss [26.94554615020752, 30.783198833465576] in episode 1859
Report: 
rewardSum:247.25
loss:[26.94554615020752, 30.783198833465576]
policies:[0, 4, 0]
qAverage:[0.0, 74.8050750732422]
ws:[-0.47872922420501707, 4.538615655899048]
memory len:10000
memory used:2912.0
now epsilon is 0.0749770032930616, the reward is 245.25 with loss [39.04757738113403, 40.010921478271484] in episode 1860
Report: 
rewardSum:245.25
loss:[39.04757738113403, 40.010921478271484]
policies:[0, 4, 2]
qAverage:[0.0, 78.03455657958985]
ws:[-0.03273632526397705, 5.5061288356781]
memory len:10000
memory used:2912.0
now epsilon is 0.07490205440145901, the reward is 37.15999999999997 with loss [17.313931941986084, 19.630424737930298] in episode 1861
Report: 
rewardSum:37.15999999999997
loss:[17.313931941986084, 19.630424737930298]
policies:[1, 2, 1]
qAverage:[15.214839935302734, 47.57222366333008]
ws:[1.098861962556839, 3.346600666642189]
memory len:10000
memory used:2912.0
now epsilon is 0.07482718043064689, the reward is 247.25 with loss [29.748104095458984, 27.160741806030273] in episode 1862
Report: 
rewardSum:247.25
loss:[29.748104095458984, 27.160741806030273]
policies:[1, 3, 0]
qAverage:[0.0, 81.2289810180664]
ws:[1.2953698635101318, 7.519804120063782]
memory len:10000
memory used:2912.0
now epsilon is 0.0747523813057325, the reward is 247.25 with loss [26.47157859802246, 22.384897708892822] in episode 1863
Report: 
rewardSum:247.25
loss:[26.47157859802246, 22.384897708892822]
policies:[1, 2, 1]
qAverage:[0.0, 33.834190368652344]
ws:[0.9612458348274231, 1.5055063962936401]
memory len:10000
memory used:2912.0
now epsilon is 0.07467765695189804, the reward is 247.25 with loss [22.295674085617065, 25.39825963973999] in episode 1864
Report: 
rewardSum:247.25
loss:[22.295674085617065, 25.39825963973999]
policies:[0, 4, 0]
qAverage:[0.0, 77.98264923095704]
ws:[2.4487104415893555, 8.533793735504151]
memory len:10000
memory used:2912.0
now epsilon is 0.07460300729440045, the reward is 247.25 with loss [18.7327139377594, 27.773695468902588] in episode 1865
Report: 
rewardSum:247.25
loss:[18.7327139377594, 27.773695468902588]
policies:[0, 4, 0]
qAverage:[0.0, 78.80980987548828]
ws:[2.8131776332855223, 9.63493537902832]
memory len:10000
memory used:2913.0
now epsilon is 0.07452843225857139, the reward is 247.25 with loss [30.48272132873535, 23.97762632369995] in episode 1866
Report: 
rewardSum:247.25
loss:[30.48272132873535, 23.97762632369995]
policies:[0, 4, 0]
qAverage:[0.0, 77.35176239013671]
ws:[1.9032139778137207, 8.278003168106078]
memory len:10000
memory used:2912.0
now epsilon is 0.07445393176981718, the reward is 247.25 with loss [25.995128870010376, 27.62547779083252] in episode 1867
Report: 
rewardSum:247.25
loss:[25.995128870010376, 27.62547779083252]
policies:[0, 4, 0]
qAverage:[0.0, 79.21088562011718]
ws:[2.866193377971649, 8.960850238800049]
memory len:10000
memory used:2912.0
now epsilon is 0.07437950575361871, the reward is 247.25 with loss [27.437798976898193, 23.5042724609375] in episode 1868
Report: 
rewardSum:247.25
loss:[27.437798976898193, 23.5042724609375]
policies:[0, 4, 0]
qAverage:[0.0, 77.0689483642578]
ws:[2.3173934042453768, 7.53716390132904]
memory len:10000
memory used:2912.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23*		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.07430515413553133, the reward is 247.25 with loss [17.572691440582275, 24.411773204803467] in episode 1869
Report: 
rewardSum:247.25
loss:[17.572691440582275, 24.411773204803467]
policies:[0, 4, 0]
qAverage:[0.0, 78.87616271972657]
ws:[1.7230288788676262, 7.208301401138305]
memory len:10000
memory used:2912.0
now epsilon is 0.07423087684118483, the reward is 247.25 with loss [28.946454264223576, 29.188185691833496] in episode 1870
Report: 
rewardSum:247.25
loss:[28.946454264223576, 29.188185691833496]
policies:[0, 4, 0]
qAverage:[0.0, 75.71528778076171]
ws:[2.2719635486602785, 8.82082109451294]
memory len:10000
memory used:2912.0
now epsilon is 0.07415667379628334, the reward is 247.25 with loss [26.26860475540161, 25.626295566558838] in episode 1871
Report: 
rewardSum:247.25
loss:[26.26860475540161, 25.626295566558838]
policies:[0, 4, 0]
qAverage:[0.0, 82.24876251220704]
ws:[2.0409124851226808, 7.3487224102020265]
memory len:10000
memory used:2913.0
now epsilon is 0.074045508284301, the reward is 245.25 with loss [42.784616470336914, 43.761356592178345] in episode 1872
Report: 
rewardSum:245.25
loss:[42.784616470336914, 43.761356592178345]
policies:[0, 5, 1]
qAverage:[0.0, 79.66158676147461]
ws:[0.48400045931339264, 5.153346044321855]
memory len:10000
memory used:2913.0
now epsilon is 0.07397149053845477, the reward is 247.25 with loss [26.519315719604492, 22.4506196975708] in episode 1873
Report: 
rewardSum:247.25
loss:[26.519315719604492, 22.4506196975708]
policies:[0, 4, 0]
qAverage:[0.0, 80.84533386230468]
ws:[1.062174192070961, 6.798663210868836]
memory len:10000
memory used:2912.0
now epsilon is 0.07389754678260234, the reward is 247.25 with loss [32.223851680755615, 22.69437837600708] in episode 1874
Report: 
rewardSum:247.25
loss:[32.223851680755615, 22.69437837600708]
policies:[0, 4, 0]
qAverage:[0.0, 81.641845703125]
ws:[1.4383376002311707, 7.780694007873535]
memory len:10000
memory used:2912.0
now epsilon is 0.07382367694278147, the reward is 247.25 with loss [24.76646661758423, 24.84560203552246] in episode 1875
Report: 
rewardSum:247.25
loss:[24.76646661758423, 24.84560203552246]
policies:[0, 4, 0]
qAverage:[0.0, 82.20650787353516]
ws:[1.3850885391235352, 7.117503881454468]
memory len:10000
memory used:2912.0
now epsilon is 0.07374988094510386, the reward is 247.25 with loss [34.892467975616455, 28.62860107421875] in episode 1876
Report: 
rewardSum:247.25
loss:[34.892467975616455, 28.62860107421875]
policies:[0, 4, 0]
qAverage:[0.0, 81.50911102294921]
ws:[1.1396253943443297, 7.020043754577637]
memory len:10000
memory used:2912.0
now epsilon is 0.07367615871575504, the reward is 247.25 with loss [18.95629596710205, 25.441425323486328] in episode 1877
Report: 
rewardSum:247.25
loss:[18.95629596710205, 25.441425323486328]
policies:[0, 4, 0]
qAverage:[0.0, 82.25902099609375]
ws:[1.034359860420227, 6.234125232696533]
memory len:10000
memory used:2912.0
now epsilon is 0.07360251018099435, the reward is 247.25 with loss [31.12854528427124, 28.671067237854004] in episode 1878
Report: 
rewardSum:247.25
loss:[31.12854528427124, 28.671067237854004]
policies:[1, 3, 0]
qAverage:[0.0, 61.609781901041664]
ws:[0.8335408965746561, 2.5441306829452515]
memory len:10000
memory used:2912.0
now epsilon is 0.07351055303333803, the reward is 246.25 with loss [22.28105342388153, 26.780948638916016] in episode 1879
Report: 
rewardSum:246.25
loss:[22.28105342388153, 26.780948638916016]
policies:[0, 4, 1]
qAverage:[0.0, 82.83329925537109]
ws:[1.1370883285999298, 6.603970623016357]
memory len:10000
memory used:2912.0
now epsilon is 0.07341871077465742, the reward is 246.25 with loss [29.740853548049927, 34.08522367477417] in episode 1880
Report: 
rewardSum:246.25
loss:[29.740853548049927, 34.08522367477417]
policies:[0, 3, 2]
qAverage:[0.0, 85.58599090576172]
ws:[1.9517990350723267, 8.437556862831116]
memory len:10000
memory used:2913.0
now epsilon is 0.07334531959131094, the reward is 247.25 with loss [18.093438625335693, 14.889432728290558] in episode 1881
Report: 
rewardSum:247.25
loss:[18.093438625335693, 14.889432728290558]
policies:[0, 4, 0]
qAverage:[0.0, 82.20107421875]
ws:[2.890450930595398, 8.338260746002197]
memory len:10000
memory used:2912.0
now epsilon is 0.07325368377118778, the reward is 246.25 with loss [32.931589126586914, 37.15115213394165] in episode 1882
Report: 
rewardSum:246.25
loss:[32.931589126586914, 37.15115213394165]
policies:[0, 4, 1]
qAverage:[0.0, 89.59383239746094]
ws:[3.768755865097046, 9.632846355438232]
memory len:10000
memory used:2912.0
now epsilon is 0.07318045755296994, the reward is 247.25 with loss [24.2788827419281, 27.41375160217285] in episode 1883
Report: 
rewardSum:247.25
loss:[24.2788827419281, 27.41375160217285]
policies:[0, 4, 0]
qAverage:[0.0, 90.91367034912109]
ws:[3.1561119556427, 8.643042945861817]
memory len:10000
memory used:2913.0
now epsilon is 0.07310730453351508, the reward is 247.25 with loss [21.534307956695557, 17.131585597991943] in episode 1884
Report: 
rewardSum:247.25
loss:[21.534307956695557, 17.131585597991943]
policies:[0, 4, 0]
qAverage:[0.0, 90.36185150146484]
ws:[3.0562799692153932, 7.681540393829346]
memory len:10000
memory used:2920.0
now epsilon is 0.07303422463965185, the reward is 247.25 with loss [31.465620040893555, 21.85753870010376] in episode 1885
Report: 
rewardSum:247.25
loss:[31.465620040893555, 21.85753870010376]
policies:[0, 4, 0]
qAverage:[0.0, 92.42805480957031]
ws:[1.6745096087455749, 5.654733920097351]
memory len:10000
memory used:2950.0
now epsilon is 0.07296121779828209, the reward is 247.25 with loss [21.905925273895264, 31.2902774810791] in episode 1886
Report: 
rewardSum:247.25
loss:[21.905925273895264, 31.2902774810791]
policies:[0, 4, 0]
qAverage:[0.0, 91.35050964355469]
ws:[1.8546080827713012, 6.619289922714233]
memory len:10000
memory used:2914.0
now epsilon is 0.0728882839363807, the reward is 247.25 with loss [22.72109818458557, 21.255013942718506] in episode 1887
Report: 
rewardSum:247.25
loss:[22.72109818458557, 21.255013942718506]
policies:[0, 4, 0]
qAverage:[0.0, 92.07742462158203]
ws:[2.4407346844673157, 8.098727130889893]
memory len:10000
memory used:2914.0
now epsilon is 0.07281542298099557, the reward is 247.25 with loss [22.75211811065674, 27.962493419647217] in episode 1888
Report: 
rewardSum:247.25
loss:[22.75211811065674, 27.962493419647217]
policies:[0, 4, 0]
qAverage:[0.0, 92.08327178955078]
ws:[3.394218373298645, 9.610886383056641]
memory len:10000
memory used:2913.0
now epsilon is 0.07270626808823258, the reward is 245.25 with loss [47.74800777435303, 41.18390917778015] in episode 1889
Report: 
rewardSum:245.25
loss:[47.74800777435303, 41.18390917778015]
policies:[0, 5, 1]
qAverage:[0.0, 95.32349904378255]
ws:[4.885152260462443, 12.64756186803182]
memory len:10000
memory used:2914.0
now epsilon is 0.07263358908045102, the reward is 247.25 with loss [25.430898189544678, 20.773723602294922] in episode 1890
Report: 
rewardSum:247.25
loss:[25.430898189544678, 20.773723602294922]
policies:[1, 3, 0]
qAverage:[0.0, 76.0787582397461]
ws:[1.9774632453918457, 8.791520436604818]
memory len:10000
memory used:2913.0
now epsilon is 0.07254284247874607, the reward is 246.25 with loss [25.003644704818726, 32.37091398239136] in episode 1891
Report: 
rewardSum:246.25
loss:[25.003644704818726, 32.37091398239136]
policies:[0, 4, 1]
qAverage:[0.0, 91.41004333496093]
ws:[3.562337788939476, 10.209153747558593]
memory len:10000
memory used:2913.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23*		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.07247032683529961, the reward is 247.25 with loss [22.79845142364502, 23.41713571548462] in episode 1892
Report: 
rewardSum:247.25
loss:[22.79845142364502, 23.41713571548462]
policies:[1, 3, 0]
qAverage:[0.0, 75.77602577209473]
ws:[0.2386016696691513, 5.235906481742859]
memory len:10000
memory used:2914.0
now epsilon is 0.07236168926333536, the reward is 245.25 with loss [50.67267417907715, 49.56167697906494] in episode 1893
Report: 
rewardSum:245.25
loss:[50.67267417907715, 49.56167697906494]
policies:[1, 4, 1]
qAverage:[0.0, 97.59542694091797]
ws:[3.621794182062149, 10.662282371520996]
memory len:10000
memory used:2913.0
now epsilon is 0.07228935470518319, the reward is 247.25 with loss [22.26463270187378, 15.683326959609985] in episode 1894
Report: 
rewardSum:247.25
loss:[22.26463270187378, 15.683326959609985]
policies:[1, 3, 0]
qAverage:[0.0, 99.4441967010498]
ws:[3.5821444988250732, 10.52679455280304]
memory len:10000
memory used:2914.0
now epsilon is 0.07221709245446824, the reward is 247.25 with loss [28.03723645210266, 20.169825315475464] in episode 1895
Report: 
rewardSum:247.25
loss:[28.03723645210266, 20.169825315475464]
policies:[0, 4, 0]
qAverage:[0.0, 99.30131683349609]
ws:[3.5802512526512147, 11.328133487701416]
memory len:10000
memory used:2914.0
now epsilon is 0.07214490243891016, the reward is 247.25 with loss [17.403088808059692, 26.102314949035645] in episode 1896
Report: 
rewardSum:247.25
loss:[17.403088808059692, 26.102314949035645]
policies:[0, 4, 0]
qAverage:[0.0, 104.26151428222656]
ws:[3.862339460849762, 10.684725952148437]
memory len:10000
memory used:2913.0
now epsilon is 0.0720727845863009, the reward is 247.25 with loss [28.61723804473877, 22.912087440490723] in episode 1897
Report: 
rewardSum:247.25
loss:[28.61723804473877, 22.912087440490723]
policies:[0, 4, 0]
qAverage:[0.0, 99.46290588378906]
ws:[3.7668617270886897, 10.693681144714356]
memory len:10000
memory used:2913.0
now epsilon is 0.07200073882450454, the reward is 247.25 with loss [18.63081979751587, 17.525365591049194] in episode 1898
Report: 
rewardSum:247.25
loss:[18.63081979751587, 17.525365591049194]
policies:[1, 3, 0]
qAverage:[0.0, 101.0149917602539]
ws:[5.062680296599865, 12.927924156188965]
memory len:10000
memory used:2913.0
now epsilon is 0.07192876508145733, the reward is 247.25 with loss [25.535592555999756, 30.613059043884277] in episode 1899
Report: 
rewardSum:247.25
loss:[25.535592555999756, 30.613059043884277]
policies:[0, 4, 0]
qAverage:[0.0, 100.10307159423829]
ws:[5.810093641281128, 14.59044532775879]
memory len:10000
memory used:2913.0
now epsilon is 0.07185686328516752, the reward is 247.25 with loss [25.974464416503906, 18.465567350387573] in episode 1900
Report: 
rewardSum:247.25
loss:[25.974464416503906, 18.465567350387573]
policies:[0, 4, 0]
qAverage:[0.0, 101.53554992675781]
ws:[4.876401877403259, 12.080416488647462]
memory len:10000
memory used:2913.0
now epsilon is 0.07178503336371532, the reward is 247.25 with loss [22.42638111114502, 24.217024326324463] in episode 1901
Report: 
rewardSum:247.25
loss:[22.42638111114502, 24.217024326324463]
policies:[0, 4, 0]
qAverage:[0.0, 101.561474609375]
ws:[3.588747489452362, 9.18866548538208]
memory len:10000
memory used:2913.0
now epsilon is 0.07171327524525284, the reward is 247.25 with loss [22.59042263031006, 25.249958515167236] in episode 1902
Report: 
rewardSum:247.25
loss:[22.59042263031006, 25.249958515167236]
policies:[0, 4, 0]
qAverage:[0.0, 100.46890716552734]
ws:[3.1376779198646547, 8.955344581604004]
memory len:10000
memory used:2913.0
now epsilon is 0.07164158885800401, the reward is 247.25 with loss [25.15030813217163, 18.252781629562378] in episode 1903
Report: 
rewardSum:247.25
loss:[25.15030813217163, 18.252781629562378]
policies:[0, 4, 0]
qAverage:[0.0, 103.09085693359376]
ws:[2.6255776941776277, 7.395037412643433]
memory len:10000
memory used:2913.0
now epsilon is 0.07156997413026452, the reward is 247.25 with loss [27.135204315185547, 30.13106346130371] in episode 1904
Report: 
rewardSum:247.25
loss:[27.135204315185547, 30.13106346130371]
policies:[0, 3, 1]
qAverage:[0.0, 98.83362007141113]
ws:[3.5358828604221344, 7.717167556285858]
memory len:10000
memory used:2913.0
now epsilon is 0.0714984309904017, the reward is 247.25 with loss [28.998279571533203, 33.488080978393555] in episode 1905
Report: 
rewardSum:247.25
loss:[28.998279571533203, 33.488080978393555]
policies:[0, 3, 1]
qAverage:[0.0, 106.86243438720703]
ws:[5.2867297530174255, 11.519053220748901]
memory len:10000
memory used:2913.0
now epsilon is 0.07142695936685457, the reward is 247.25 with loss [31.465768337249756, 30.0596022605896] in episode 1906
Report: 
rewardSum:247.25
loss:[31.465768337249756, 30.0596022605896]
policies:[0, 4, 0]
qAverage:[0.0, 114.53191680908203]
ws:[3.8215000152587892, 9.871685218811034]
memory len:10000
memory used:2913.0
now epsilon is 0.07135555918813359, the reward is 247.25 with loss [29.38564920425415, 22.0812246799469] in episode 1907
Report: 
rewardSum:247.25
loss:[29.38564920425415, 22.0812246799469]
policies:[1, 3, 0]
qAverage:[0.0, 122.16803359985352]
ws:[4.767364323139191, 10.97319483757019]
memory len:10000
memory used:2913.0
now epsilon is 0.0712485927228937, the reward is 245.25 with loss [38.556291580200195, 27.404533624649048] in episode 1908
Report: 
rewardSum:245.25
loss:[38.556291580200195, 27.404533624649048]
policies:[0, 5, 1]
qAverage:[0.0, 121.09811401367188]
ws:[4.899501999219258, 10.612454175949097]
memory len:10000
memory used:2913.0
now epsilon is 0.07114178660710405, the reward is 245.25 with loss [36.134174823760986, 37.958768367767334] in episode 1909
Report: 
rewardSum:245.25
loss:[36.134174823760986, 37.958768367767334]
policies:[0, 5, 1]
qAverage:[0.0, 114.33972803751628]
ws:[4.365948801239331, 10.181326270103455]
memory len:10000
memory used:2913.0
now epsilon is 0.07107067149422085, the reward is 247.25 with loss [24.638826847076416, 25.485238075256348] in episode 1910
Report: 
rewardSum:247.25
loss:[24.638826847076416, 25.485238075256348]
policies:[0, 4, 0]
qAverage:[0.0, 109.2679557800293]
ws:[4.483419544994831, 8.217180639505386]
memory len:10000
memory used:2913.0
now epsilon is 0.0709996274697868, the reward is 247.25 with loss [29.71890687942505, 22.6216299533844] in episode 1911
Report: 
rewardSum:247.25
loss:[29.71890687942505, 22.6216299533844]
policies:[0, 4, 0]
qAverage:[0.0, 115.55516662597657]
ws:[4.764919447898865, 9.068097758293153]
memory len:10000
memory used:2913.0
now epsilon is 0.07092865446274013, the reward is 247.25 with loss [24.28386640548706, 26.220850944519043] in episode 1912
Report: 
rewardSum:247.25
loss:[24.28386640548706, 26.220850944519043]
policies:[0, 4, 0]
qAverage:[0.0, 116.47204132080078]
ws:[6.133310922980309, 11.90049638748169]
memory len:10000
memory used:2913.0
now epsilon is 0.07085775240209007, the reward is 247.25 with loss [22.88722538948059, 23.795363664627075] in episode 1913
Report: 
rewardSum:247.25
loss:[22.88722538948059, 23.795363664627075]
policies:[0, 3, 1]
qAverage:[0.0, 95.51364135742188]
ws:[4.0218751430511475, 10.953717947006226]
memory len:10000
memory used:2913.0
now epsilon is 0.0707869212169168, the reward is 247.25 with loss [33.94413471221924, 31.009396076202393] in episode 1914
Report: 
rewardSum:247.25
loss:[33.94413471221924, 31.009396076202393]
policies:[0, 4, 0]
qAverage:[0.0, 114.33506317138672]
ws:[9.917565441131591, 19.386978149414062]
memory len:10000
memory used:2913.0
now epsilon is 0.07071616083637143, the reward is 247.25 with loss [27.321911334991455, 25.934762239456177] in episode 1915
Report: 
rewardSum:247.25
loss:[27.321911334991455, 25.934762239456177]
policies:[0, 4, 0]
qAverage:[0.0, 117.11888427734375]
ws:[11.814591979980468, 22.064995956420898]
memory len:10000
memory used:2913.0
############# STATE ###############
0-		10*		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.0706454711896759, the reward is 247.25 with loss [23.923494338989258, 30.861032962799072] in episode 1916
Report: 
rewardSum:247.25
loss:[23.923494338989258, 30.861032962799072]
policies:[0, 4, 0]
qAverage:[0.0, 104.94264729817708]
ws:[10.947324752807617, 17.945059458414715]
memory len:10000
memory used:2913.0
now epsilon is 0.07057485220612286, the reward is 247.25 with loss [18.33842182159424, 38.147582054138184] in episode 1917
Report: 
rewardSum:247.25
loss:[18.33842182159424, 38.147582054138184]
policies:[0, 4, 0]
qAverage:[0.0, 124.78081665039062]
ws:[4.756336298584938, 10.034628023207187]
memory len:10000
memory used:2913.0
now epsilon is 0.07050430381507566, the reward is 247.25 with loss [23.95728874206543, 23.495906829833984] in episode 1918
Report: 
rewardSum:247.25
loss:[23.95728874206543, 23.495906829833984]
policies:[0, 4, 0]
qAverage:[0.0, 119.6170482635498]
ws:[5.743723303079605, 11.481101751327515]
memory len:10000
memory used:2912.0
now epsilon is 0.07039861343510942, the reward is 245.25 with loss [42.01783633232117, 40.79739689826965] in episode 1919
Report: 
rewardSum:245.25
loss:[42.01783633232117, 40.79739689826965]
policies:[0, 5, 1]
qAverage:[0.0, 126.48567352294921]
ws:[6.2460976362228395, 12.778429508209229]
memory len:10000
memory used:2912.0
now epsilon is 0.07032824121675471, the reward is 247.25 with loss [35.608932971954346, 21.052764892578125] in episode 1920
Report: 
rewardSum:247.25
loss:[35.608932971954346, 21.052764892578125]
policies:[0, 4, 0]
qAverage:[0.0, 116.6050910949707]
ws:[8.113525301218033, 14.63199269771576]
memory len:10000
memory used:2913.0
now epsilon is 0.07025793934423318, the reward is 247.25 with loss [28.962058544158936, 27.57703161239624] in episode 1921
Report: 
rewardSum:247.25
loss:[28.962058544158936, 27.57703161239624]
policies:[0, 4, 0]
qAverage:[0.0, 127.24996948242188]
ws:[8.727275323867797, 15.382459163665771]
memory len:10000
memory used:2913.0
now epsilon is 0.07018770774722537, the reward is 247.25 with loss [28.73586130142212, 30.254947185516357] in episode 1922
Report: 
rewardSum:247.25
loss:[28.73586130142212, 30.254947185516357]
policies:[0, 4, 0]
qAverage:[0.0, 126.63385314941407]
ws:[9.28400797843933, 17.234387016296388]
memory len:10000
memory used:2912.0
now epsilon is 0.07008249196465101, the reward is 245.25 with loss [50.66883134841919, 41.60537362098694] in episode 1923
Report: 
rewardSum:245.25
loss:[50.66883134841919, 41.60537362098694]
policies:[0, 5, 1]
qAverage:[0.0, 126.1564712524414]
ws:[9.442852934201559, 17.70332686106364]
memory len:10000
memory used:2912.0
now epsilon is 0.06997743390714359, the reward is 245.25 with loss [44.555113792419434, 31.57253623008728] in episode 1924
Report: 
rewardSum:245.25
loss:[44.555113792419434, 31.57253623008728]
policies:[1, 4, 1]
qAverage:[0.0, 121.79633178710938]
ws:[6.691155669093132, 12.153993034362793]
memory len:10000
memory used:2913.0
now epsilon is 0.06990748271040086, the reward is 247.25 with loss [25.149697303771973, 16.138038158416748] in episode 1925
Report: 
rewardSum:247.25
loss:[25.149697303771973, 16.138038158416748]
policies:[0, 4, 0]
qAverage:[0.0, 127.42377471923828]
ws:[5.0452164053916935, 9.975353454053401]
memory len:10000
memory used:2913.0
now epsilon is 0.06983760143862754, the reward is 247.25 with loss [24.941449642181396, 37.32251262664795] in episode 1926
Report: 
rewardSum:247.25
loss:[24.941449642181396, 37.32251262664795]
policies:[0, 4, 0]
qAverage:[0.0, 128.6000213623047]
ws:[6.642646157741547, 12.571807622909546]
memory len:10000
memory used:2913.0
now epsilon is 0.06976779002192487, the reward is 247.25 with loss [20.526286840438843, 18.814093112945557] in episode 1927
Report: 
rewardSum:247.25
loss:[20.526286840438843, 18.814093112945557]
policies:[0, 4, 0]
qAverage:[0.0, 126.03787078857422]
ws:[10.008238077163696, 18.071134662628175]
memory len:10000
memory used:2913.0
now epsilon is 0.06969804839046399, the reward is 247.25 with loss [35.23188781738281, 34.47795867919922] in episode 1928
Report: 
rewardSum:247.25
loss:[35.23188781738281, 34.47795867919922]
policies:[2, 2, 0]
qAverage:[0.0, 128.61775716145834]
ws:[15.058751424153646, 21.822340965270996]
memory len:10000
memory used:2913.0
now epsilon is 0.06962837647448583, the reward is 247.25 with loss [29.152448177337646, 21.807151794433594] in episode 1929
Report: 
rewardSum:247.25
loss:[29.152448177337646, 21.807151794433594]
policies:[0, 4, 0]
qAverage:[0.0, 149.25654296875]
ws:[10.116183853149414, 17.358171272277833]
memory len:10000
memory used:2913.0
now epsilon is 0.06955877420430104, the reward is 247.25 with loss [29.440412521362305, 24.71267080307007] in episode 1930
Report: 
rewardSum:247.25
loss:[29.440412521362305, 24.71267080307007]
policies:[0, 4, 0]
qAverage:[0.0, 137.34881286621095]
ws:[11.856201553344727, 19.5320143699646]
memory len:10000
memory used:2913.0
now epsilon is 0.06945450123261236, the reward is 245.25 with loss [37.06517148017883, 38.78470754623413] in episode 1931
Report: 
rewardSum:245.25
loss:[37.06517148017883, 38.78470754623413]
policies:[0, 5, 1]
qAverage:[0.0, 152.5042266845703]
ws:[10.717968980471293, 17.46100616455078]
memory len:10000
memory used:2913.0
now epsilon is 0.06938507277247707, the reward is 247.25 with loss [18.575978875160217, 44.58799362182617] in episode 1932
Report: 
rewardSum:247.25
loss:[18.575978875160217, 44.58799362182617]
policies:[0, 4, 0]
qAverage:[0.0, 137.30923309326172]
ws:[10.801909744739532, 18.353170680999757]
memory len:10000
memory used:2913.0
now epsilon is 0.06931571371477059, the reward is 247.25 with loss [21.65449857711792, 23.114487171173096] in episode 1933
Report: 
rewardSum:247.25
loss:[21.65449857711792, 23.114487171173096]
policies:[0, 4, 0]
qAverage:[0.0, 141.73511962890626]
ws:[8.812697052955627, 15.232788896560669]
memory len:10000
memory used:2913.0
now epsilon is 0.0692464239901165, the reward is 247.25 with loss [22.515278816223145, 23.0850887298584] in episode 1934
Report: 
rewardSum:247.25
loss:[22.515278816223145, 23.0850887298584]
policies:[0, 4, 0]
qAverage:[0.0, 139.84141845703124]
ws:[8.674183344841003, 15.353988361358642]
memory len:10000
memory used:2913.0
now epsilon is 0.06917720352920777, the reward is 247.25 with loss [9.601037919521332, 26.61152696609497] in episode 1935
Report: 
rewardSum:247.25
loss:[9.601037919521332, 26.61152696609497]
policies:[0, 4, 0]
qAverage:[0.0, 122.08464813232422]
ws:[0.028090178966522217, 4.276627838611603]
memory len:10000
memory used:2913.0
now epsilon is 0.06910805226280657, the reward is 247.25 with loss [19.198665857315063, 29.369609832763672] in episode 1936
Report: 
rewardSum:247.25
loss:[19.198665857315063, 29.369609832763672]
policies:[1, 3, 0]
qAverage:[0.0, 122.54272079467773]
ws:[0.6059486865997314, 5.455742716789246]
memory len:10000
memory used:2913.0
now epsilon is 0.06903897012174438, the reward is 247.25 with loss [17.42361569404602, 33.105780601501465] in episode 1937
Report: 
rewardSum:247.25
loss:[17.42361569404602, 33.105780601501465]
policies:[1, 3, 0]
qAverage:[0.0, 138.2594223022461]
ws:[12.64965283870697, 22.300786018371582]
memory len:10000
memory used:2913.0
now epsilon is 0.06896995703692178, the reward is 247.25 with loss [23.781479358673096, 27.686755657196045] in episode 1938
Report: 
rewardSum:247.25
loss:[23.781479358673096, 27.686755657196045]
policies:[0, 4, 0]
qAverage:[0.0, 141.5393798828125]
ws:[10.339630508422852, 18.59489040374756]
memory len:10000
memory used:2913.0
############# STATE ###############
0-		10*		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.0689010129393084, the reward is 247.25 with loss [28.21684741973877, 26.73393750190735] in episode 1939
Report: 
rewardSum:247.25
loss:[28.21684741973877, 26.73393750190735]
policies:[0, 4, 0]
qAverage:[0.0, 137.87413330078124]
ws:[9.457722401618957, 16.059132385253907]
memory len:10000
memory used:2913.0
now epsilon is 0.06879772599307156, the reward is 35.15999999999997 with loss [40.652827739715576, 46.63553237915039] in episode 1940
Report: 
rewardSum:35.15999999999997
loss:[40.652827739715576, 46.63553237915039]
policies:[0, 4, 2]
qAverage:[0.0, 143.57567138671874]
ws:[1.6235945284366609, 6.2248687744140625]
memory len:10000
memory used:2913.0
now epsilon is 0.06867742023198474, the reward is 244.25 with loss [47.49857020378113, 55.04609513282776] in episode 1941
Report: 
rewardSum:244.25
loss:[47.49857020378113, 55.04609513282776]
policies:[0, 5, 2]
qAverage:[0.0, 145.79315490722655]
ws:[10.287965726852416, 17.583001136779785]
memory len:10000
memory used:2913.0
now epsilon is 0.06864308581420751, the reward is -1.0 with loss [19.530795097351074, 10.15135669708252] in episode 1942
Report: 
rewardSum:-1.0
loss:[19.530795097351074, 10.15135669708252]
policies:[0, 1, 1]
qAverage:[0.0, 76.60319519042969]
ws:[0.2444966584444046, 1.8922871351242065]
memory len:10000
memory used:2913.0
now epsilon is 0.06857446846526057, the reward is 247.25 with loss [16.873648166656494, 29.823519229888916] in episode 1943
Report: 
rewardSum:247.25
loss:[16.873648166656494, 29.823519229888916]
policies:[1, 3, 0]
qAverage:[0.0, 118.70339457194011]
ws:[4.074419339497884, 10.647053400675455]
memory len:10000
memory used:2913.0
now epsilon is 0.06847167102970136, the reward is 245.25 with loss [40.24488925933838, 36.66183114051819] in episode 1944
Report: 
rewardSum:245.25
loss:[40.24488925933838, 36.66183114051819]
policies:[0, 5, 1]
qAverage:[0.0, 145.5963134765625]
ws:[8.078123847643534, 14.990807135899862]
memory len:10000
memory used:2912.0
now epsilon is 0.0684032250312691, the reward is 247.25 with loss [30.927284240722656, 19.45304822921753] in episode 1945
Report: 
rewardSum:247.25
loss:[30.927284240722656, 19.45304822921753]
policies:[0, 4, 0]
qAverage:[0.0, 148.21332397460938]
ws:[7.044708411768079, 13.636899948120117]
memory len:10000
memory used:2913.0
now epsilon is 0.06833484745317228, the reward is 247.25 with loss [27.907854080200195, 30.54218816757202] in episode 1946
Report: 
rewardSum:247.25
loss:[27.907854080200195, 30.54218816757202]
policies:[0, 4, 0]
qAverage:[0.0, 143.67841491699218]
ws:[6.2349564790725704, 12.630317687988281]
memory len:10000
memory used:2913.0
now epsilon is 0.06826653822701625, the reward is 247.25 with loss [25.605097770690918, 28.000054359436035] in episode 1947
Report: 
rewardSum:247.25
loss:[25.605097770690918, 28.000054359436035]
policies:[0, 4, 0]
qAverage:[0.0, 146.40717468261718]
ws:[7.050929379463196, 14.31177897453308]
memory len:10000
memory used:2913.0
now epsilon is 0.0681982972844747, the reward is 247.25 with loss [22.4481418132782, 32.052531719207764] in episode 1948
Report: 
rewardSum:247.25
loss:[22.4481418132782, 32.052531719207764]
policies:[0, 4, 0]
qAverage:[0.0, 143.82505493164064]
ws:[7.369090378284454, 14.434120988845825]
memory len:10000
memory used:2913.0
now epsilon is 0.06813012455728959, the reward is 247.25 with loss [25.12652325630188, 33.18405079841614] in episode 1949
Report: 
rewardSum:247.25
loss:[25.12652325630188, 33.18405079841614]
policies:[0, 4, 0]
qAverage:[0.0, 147.01332397460936]
ws:[7.576407864689827, 15.040008735656738]
memory len:10000
memory used:2913.0
now epsilon is 0.06806201997727114, the reward is 247.25 with loss [27.78554916381836, 19.562100887298584] in episode 1950
Report: 
rewardSum:247.25
loss:[27.78554916381836, 19.562100887298584]
policies:[1, 3, 0]
qAverage:[0.0, 139.06143188476562]
ws:[8.178634207695723, 14.394851922988892]
memory len:10000
memory used:2912.0
now epsilon is 0.06799398347629776, the reward is 247.25 with loss [29.75431251525879, 39.33618211746216] in episode 1951
Report: 
rewardSum:247.25
loss:[29.75431251525879, 39.33618211746216]
policies:[0, 4, 0]
qAverage:[0.0, 148.74443664550782]
ws:[7.253527861833573, 14.42341890335083]
memory len:10000
memory used:2912.0
now epsilon is 0.06792601498631592, the reward is 247.25 with loss [28.47851037979126, 29.02390766143799] in episode 1952
Report: 
rewardSum:247.25
loss:[28.47851037979126, 29.02390766143799]
policies:[0, 4, 0]
qAverage:[0.0, 147.54054260253906]
ws:[8.137518246471881, 15.778878498077393]
memory len:10000
memory used:2912.0
now epsilon is 0.06785811443934012, the reward is 247.25 with loss [27.882352352142334, 29.17178964614868] in episode 1953
Report: 
rewardSum:247.25
loss:[27.882352352142334, 29.17178964614868]
policies:[0, 4, 0]
qAverage:[0.0, 147.48404541015626]
ws:[9.66576321721077, 17.469790363311766]
memory len:10000
memory used:2912.0
now epsilon is 0.06779028176745283, the reward is 247.25 with loss [26.528401374816895, 40.067030906677246] in episode 1954
Report: 
rewardSum:247.25
loss:[26.528401374816895, 40.067030906677246]
policies:[0, 3, 1]
qAverage:[0.0, 143.00729751586914]
ws:[13.634470760822296, 21.646251678466797]
memory len:10000
memory used:2912.0
now epsilon is 0.06772251690280442, the reward is 247.25 with loss [20.685145378112793, 24.212263107299805] in episode 1955
Report: 
rewardSum:247.25
loss:[20.685145378112793, 24.212263107299805]
policies:[0, 4, 0]
qAverage:[0.0, 143.6448760986328]
ws:[8.648853421211243, 14.023822593688966]
memory len:10000
memory used:2912.0
now epsilon is 0.06765481977761308, the reward is 247.25 with loss [24.645501136779785, 33.79393720626831] in episode 1956
Report: 
rewardSum:247.25
loss:[24.645501136779785, 33.79393720626831]
policies:[0, 3, 1]
qAverage:[0.0, 135.70923233032227]
ws:[8.552309155464172, 12.132835775613785]
memory len:10000
memory used:2912.0
now epsilon is 0.06758719032416473, the reward is 247.25 with loss [27.119722366333008, 26.095584869384766] in episode 1957
Report: 
rewardSum:247.25
loss:[27.119722366333008, 26.095584869384766]
policies:[0, 4, 0]
qAverage:[0.0, 140.98086547851562]
ws:[4.0312271296977995, 7.638343095779419]
memory len:10000
memory used:2912.0
now epsilon is 0.067519628474813, the reward is 247.25 with loss [21.400269031524658, 26.23193645477295] in episode 1958
Report: 
rewardSum:247.25
loss:[21.400269031524658, 26.23193645477295]
policies:[1, 3, 0]
qAverage:[0.0, 138.71693420410156]
ws:[4.295597285032272, 7.233414238318801]
memory len:10000
memory used:2912.0
now epsilon is 0.06745213416197918, the reward is 247.25 with loss [32.84503126144409, 24.86485528945923] in episode 1959
Report: 
rewardSum:247.25
loss:[32.84503126144409, 24.86485528945923]
policies:[0, 3, 1]
qAverage:[0.0, 140.3825340270996]
ws:[4.270230762660503, 7.532850235700607]
memory len:10000
memory used:2912.0
now epsilon is 0.067384707318152, the reward is 247.25 with loss [29.929322957992554, 30.110296964645386] in episode 1960
Report: 
rewardSum:247.25
loss:[29.929322957992554, 30.110296964645386]
policies:[0, 4, 0]
qAverage:[0.0, 147.66773071289063]
ws:[6.201730227470398, 11.479941892623902]
memory len:10000
memory used:2912.0
now epsilon is 0.06731734787588782, the reward is 247.25 with loss [24.452207803726196, 18.80950117111206] in episode 1961
Report: 
rewardSum:247.25
loss:[24.452207803726196, 18.80950117111206]
policies:[0, 4, 0]
qAverage:[0.0, 147.08380432128905]
ws:[8.579872989654541, 13.772323608398438]
memory len:10000
memory used:2912.0
now epsilon is 0.0672500557678103, the reward is 247.25 with loss [27.189563751220703, 27.454172611236572] in episode 1962
Report: 
rewardSum:247.25
loss:[27.189563751220703, 27.454172611236572]
policies:[0, 4, 0]
qAverage:[0.0, 148.4158935546875]
ws:[7.310181570053101, 13.216884803771972]
memory len:10000
memory used:2912.0
############# STATE ###############
0-		10*		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.06718283092661055, the reward is 247.25 with loss [25.88437795639038, 30.831408500671387] in episode 1963
Report: 
rewardSum:247.25
loss:[25.88437795639038, 30.831408500671387]
policies:[0, 4, 0]
qAverage:[0.0, 146.14757080078124]
ws:[6.861427187919617, 13.07232837677002]
memory len:10000
memory used:2912.0
now epsilon is 0.06711567328504688, the reward is 247.25 with loss [23.660279512405396, 19.672285556793213] in episode 1964
Report: 
rewardSum:247.25
loss:[23.660279512405396, 19.672285556793213]
policies:[0, 4, 0]
qAverage:[0.0, 149.34014587402345]
ws:[8.392500686645509, 15.86249599456787]
memory len:10000
memory used:2912.0
now epsilon is 0.06704858277594485, the reward is 247.25 with loss [27.281853199005127, 30.90692710876465] in episode 1965
Report: 
rewardSum:247.25
loss:[27.281853199005127, 30.90692710876465]
policies:[0, 3, 1]
qAverage:[0.0, 139.91646575927734]
ws:[10.753318309783936, 19.25710678100586]
memory len:10000
memory used:2912.0
now epsilon is 0.06698155933219717, the reward is 247.25 with loss [24.170828819274902, 26.975871324539185] in episode 1966
Report: 
rewardSum:247.25
loss:[24.170828819274902, 26.975871324539185]
policies:[0, 4, 0]
qAverage:[0.0, 153.62049865722656]
ws:[7.955790185928345, 15.799790382385254]
memory len:10000
memory used:2912.0
now epsilon is 0.06691460288676364, the reward is 247.25 with loss [28.743098497390747, 23.411536693572998] in episode 1967
Report: 
rewardSum:247.25
loss:[28.743098497390747, 23.411536693572998]
policies:[0, 4, 0]
qAverage:[0.0, 147.28501586914064]
ws:[7.144147592782974, 15.036876010894776]
memory len:10000
memory used:2912.0
now epsilon is 0.06684771337267106, the reward is 247.25 with loss [26.8667311668396, 25.228630542755127] in episode 1968
Report: 
rewardSum:247.25
loss:[26.8667311668396, 25.228630542755127]
policies:[1, 3, 0]
qAverage:[0.0, 134.26806259155273]
ws:[0.4060790240764618, 7.259248316287994]
memory len:10000
memory used:2912.0
now epsilon is 0.06678089072301319, the reward is 247.25 with loss [24.791786670684814, 23.953936100006104] in episode 1969
Report: 
rewardSum:247.25
loss:[24.791786670684814, 23.953936100006104]
policies:[0, 4, 0]
qAverage:[0.0, 151.39521179199218]
ws:[6.112336754798889, 13.488152408599854]
memory len:10000
memory used:2912.0
now epsilon is 0.06671413487095065, the reward is 247.25 with loss [20.686821222305298, 16.918596029281616] in episode 1970
Report: 
rewardSum:247.25
loss:[20.686821222305298, 16.918596029281616]
policies:[1, 3, 0]
qAverage:[0.0, 133.19884490966797]
ws:[1.9459420102648437, 9.09932017326355]
memory len:10000
memory used:2912.0
now epsilon is 0.0666474457497109, the reward is 247.25 with loss [14.74794489145279, 24.401583671569824] in episode 1971
Report: 
rewardSum:247.25
loss:[14.74794489145279, 24.401583671569824]
policies:[1, 3, 0]
qAverage:[0.0, 147.3469467163086]
ws:[7.986522346735001, 15.444070100784302]
memory len:10000
memory used:2912.0
now epsilon is 0.06658082329258816, the reward is 247.25 with loss [20.799808502197266, 30.450857162475586] in episode 1972
Report: 
rewardSum:247.25
loss:[20.799808502197266, 30.450857162475586]
policies:[0, 4, 0]
qAverage:[0.0, 147.31714477539063]
ws:[7.8986553192138675, 16.222331714630126]
memory len:10000
memory used:2912.0
now epsilon is 0.06651426743294328, the reward is 247.25 with loss [17.006307244300842, 36.01030349731445] in episode 1973
Report: 
rewardSum:247.25
loss:[17.006307244300842, 36.01030349731445]
policies:[0, 4, 0]
qAverage:[0.0, 150.55203857421876]
ws:[7.383329117298127, 15.429186344146729]
memory len:10000
memory used:2912.0
now epsilon is 0.06641455836813777, the reward is 245.25 with loss [37.386088848114014, 24.890106558799744] in episode 1974
Report: 
rewardSum:245.25
loss:[37.386088848114014, 24.890106558799744]
policies:[0, 4, 2]
qAverage:[0.0, 149.75481033325195]
ws:[8.614433281123638, 18.127878189086914]
memory len:10000
memory used:2912.0
now epsilon is 0.06631499877348337, the reward is 245.25 with loss [33.42478823661804, 48.2959942817688] in episode 1975
Report: 
rewardSum:245.25
loss:[33.42478823661804, 48.2959942817688]
policies:[0, 5, 1]
qAverage:[0.0, 153.68555959065756]
ws:[6.222730694959561, 15.999095678329468]
memory len:10000
memory used:2912.0
now epsilon is 0.06624870863869002, the reward is 247.25 with loss [28.842005252838135, 27.654306769371033] in episode 1976
Report: 
rewardSum:247.25
loss:[28.842005252838135, 27.654306769371033]
policies:[0, 3, 1]
qAverage:[0.0, 142.91949081420898]
ws:[6.919292975217104, 13.979050874710083]
memory len:10000
memory used:2913.0
now epsilon is 0.06618248476917679, the reward is 37.15999999999997 with loss [14.39502727985382, 27.231241703033447] in episode 1977
Report: 
rewardSum:37.15999999999997
loss:[14.39502727985382, 27.231241703033447]
policies:[0, 3, 1]
qAverage:[0.0, 129.7607078552246]
ws:[0.02918829768896103, 6.517339050769806]
memory len:10000
memory used:2913.0
now epsilon is 0.06611632709870326, the reward is 247.25 with loss [37.37068319320679, 23.607759475708008] in episode 1978
Report: 
rewardSum:247.25
loss:[37.37068319320679, 23.607759475708008]
policies:[0, 4, 0]
qAverage:[0.0, 146.6993865966797]
ws:[7.70942057967186, 15.813318824768066]
memory len:10000
memory used:2913.0
now epsilon is 0.06605023556109522, the reward is 247.25 with loss [21.18036937713623, 11.52830958366394] in episode 1979
Report: 
rewardSum:247.25
loss:[21.18036937713623, 11.52830958366394]
policies:[0, 4, 0]
qAverage:[0.0, 134.37519454956055]
ws:[10.68630675971508, 16.796186208724976]
memory len:10000
memory used:2913.0
now epsilon is 0.06598421009024459, the reward is 247.25 with loss [27.326662063598633, 33.413190841674805] in episode 1980
Report: 
rewardSum:247.25
loss:[27.326662063598633, 33.413190841674805]
policies:[0, 4, 0]
qAverage:[0.0, 148.5490936279297]
ws:[7.229663336277008, 12.899773359298706]
memory len:10000
memory used:2913.0
now epsilon is 0.06591825062010938, the reward is 247.25 with loss [26.75615406036377, 35.81381034851074] in episode 1981
Report: 
rewardSum:247.25
loss:[26.75615406036377, 35.81381034851074]
policies:[0, 4, 0]
qAverage:[0.0, 146.6032287597656]
ws:[7.894082838296891, 14.658747339248658]
memory len:10000
memory used:2913.0
now epsilon is 0.06585235708471364, the reward is 247.25 with loss [14.651862382888794, 21.496690273284912] in episode 1982
Report: 
rewardSum:247.25
loss:[14.651862382888794, 21.496690273284912]
policies:[1, 3, 0]
qAverage:[0.0, 136.95202255249023]
ws:[9.022133231163025, 13.84538221359253]
memory len:10000
memory used:2913.0
now epsilon is 0.06578652941814732, the reward is 247.25 with loss [34.95299053192139, 32.522367000579834] in episode 1983
Report: 
rewardSum:247.25
loss:[34.95299053192139, 32.522367000579834]
policies:[0, 4, 0]
qAverage:[0.0, 147.48977355957032]
ws:[7.8910633563995365, 13.65054931640625]
memory len:10000
memory used:2913.0
now epsilon is 0.06572076755456631, the reward is 37.15999999999997 with loss [26.385120391845703, 24.643471717834473] in episode 1984
Report: 
rewardSum:37.15999999999997
loss:[26.385120391845703, 24.643471717834473]
policies:[0, 3, 1]
qAverage:[0.0, 129.6516876220703]
ws:[2.6710280179977417, 9.233259081840515]
memory len:10000
memory used:2913.0
now epsilon is 0.06565507142819228, the reward is 247.25 with loss [28.148343324661255, 23.62229347229004] in episode 1985
Report: 
rewardSum:247.25
loss:[28.148343324661255, 23.62229347229004]
policies:[0, 4, 0]
qAverage:[0.0, 148.54360961914062]
ws:[9.518121433258056, 16.470478439331053]
memory len:10000
memory used:2913.0
now epsilon is 0.0655894409733127, the reward is 247.25 with loss [33.51911115646362, 23.939230918884277] in episode 1986
Report: 
rewardSum:247.25
loss:[33.51911115646362, 23.939230918884277]
policies:[0, 4, 0]
qAverage:[0.0, 144.9784393310547]
ws:[8.701718032360077, 15.592709255218505]
memory len:10000
memory used:2913.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12*		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.06552387612428068, the reward is 247.25 with loss [27.268218517303467, 31.99538278579712] in episode 1987
Report: 
rewardSum:247.25
loss:[27.268218517303467, 31.99538278579712]
policies:[0, 4, 0]
qAverage:[0.0, 134.51257705688477]
ws:[11.237300500273705, 17.84747052192688]
memory len:10000
memory used:2913.0
now epsilon is 0.06545837681551496, the reward is 247.25 with loss [25.66660511493683, 35.75493621826172] in episode 1988
Report: 
rewardSum:247.25
loss:[25.66660511493683, 35.75493621826172]
policies:[0, 4, 0]
qAverage:[0.0, 140.0793212890625]
ws:[9.17190716266632, 16.188264751434325]
memory len:10000
memory used:2913.0
now epsilon is 0.06539294298149988, the reward is 247.25 with loss [27.69530439376831, 22.396140575408936] in episode 1989
Report: 
rewardSum:247.25
loss:[27.69530439376831, 22.396140575408936]
policies:[1, 3, 0]
qAverage:[0.0, 133.1601448059082]
ws:[10.050998240709305, 16.775815725326538]
memory len:10000
memory used:2913.0
now epsilon is 0.0653275745567852, the reward is 247.25 with loss [35.78064012527466, 27.836947441101074] in episode 1990
Report: 
rewardSum:247.25
loss:[35.78064012527466, 27.836947441101074]
policies:[0, 4, 0]
qAverage:[0.0, 139.71754455566406]
ws:[9.051813113689423, 17.76555290222168]
memory len:10000
memory used:2913.0
now epsilon is 0.06526227147598616, the reward is 247.25 with loss [23.47776699066162, 30.47965669631958] in episode 1991
Report: 
rewardSum:247.25
loss:[23.47776699066162, 30.47965669631958]
policies:[0, 4, 0]
qAverage:[0.0, 139.94356689453124]
ws:[9.139289212226867, 17.400563144683836]
memory len:10000
memory used:2913.0
now epsilon is 0.06519703367378335, the reward is 247.25 with loss [21.75155782699585, 24.246468544006348] in episode 1992
Report: 
rewardSum:247.25
loss:[21.75155782699585, 24.246468544006348]
policies:[0, 4, 0]
qAverage:[0.0, 139.50601501464843]
ws:[7.3045470237731935, 14.355684041976929]
memory len:10000
memory used:2913.0
now epsilon is 0.06513186108492264, the reward is 247.25 with loss [25.173003673553467, 27.044524669647217] in episode 1993
Report: 
rewardSum:247.25
loss:[25.173003673553467, 27.044524669647217]
policies:[0, 3, 1]
qAverage:[0.0, 126.38971328735352]
ws:[-1.176696389913559, 3.1451283544301987]
memory len:10000
memory used:2913.0
now epsilon is 0.06506675364421514, the reward is 37.15999999999997 with loss [25.3241925239563, 19.295679569244385] in episode 1994
Report: 
rewardSum:37.15999999999997
loss:[25.3241925239563, 19.295679569244385]
policies:[0, 3, 1]
qAverage:[0.0, 126.47955703735352]
ws:[0.017826169729232788, 5.137984037399292]
memory len:10000
memory used:2913.0
now epsilon is 0.06500171128653713, the reward is 247.25 with loss [22.72160840034485, 36.02739906311035] in episode 1995
Report: 
rewardSum:247.25
loss:[22.72160840034485, 36.02739906311035]
policies:[1, 3, 0]
qAverage:[0.0, 134.41822814941406]
ws:[7.564886853098869, 12.994524478912354]
memory len:10000
memory used:2913.0
now epsilon is 0.06492049976334327, the reward is 246.25 with loss [32.82524609565735, 34.843416690826416] in episode 1996
Report: 
rewardSum:246.25
loss:[32.82524609565735, 34.843416690826416]
policies:[0, 4, 1]
qAverage:[0.0, 137.53659057617188]
ws:[7.365857112407684, 13.521883296966553]
memory len:10000
memory used:2913.0
now epsilon is 0.06485560360471007, the reward is 247.25 with loss [30.415112495422363, 33.289093017578125] in episode 1997
Report: 
rewardSum:247.25
loss:[30.415112495422363, 33.289093017578125]
policies:[0, 4, 0]
qAverage:[0.0, 138.40771484375]
ws:[7.196710902452469, 11.918491697311401]
memory len:10000
memory used:2914.0
now epsilon is 0.06479077231790349, the reward is 247.25 with loss [22.714076042175293, 23.069420337677002] in episode 1998
Report: 
rewardSum:247.25
loss:[22.714076042175293, 23.069420337677002]
policies:[0, 4, 0]
qAverage:[0.0, 136.66903076171874]
ws:[9.257829451560974, 15.653874254226684]
memory len:10000
memory used:2914.0
now epsilon is 0.06472600583807604, the reward is 247.25 with loss [19.66040849685669, 20.836246848106384] in episode 1999
Report: 
rewardSum:247.25
loss:[19.66040849685669, 20.836246848106384]
policies:[0, 4, 0]
qAverage:[0.0, 137.85149841308595]
ws:[8.646046471595763, 14.083384275436401]
memory len:10000
memory used:2914.0
now epsilon is 0.06459666704029257, the reward is 243.25 with loss [48.22036170959473, 60.35004281997681] in episode 2000
Report: 
rewardSum:243.25
loss:[48.22036170959473, 60.35004281997681]
policies:[1, 5, 2]
qAverage:[0.0, 139.34959411621094]
ws:[6.523770958185196, 10.971866130828857]
memory len:10000
memory used:2913.0
now epsilon is 0.06449983257892483, the reward is 245.25 with loss [45.455366373062134, 36.960219383239746] in episode 2001
Report: 
rewardSum:245.25
loss:[45.455366373062134, 36.960219383239746]
policies:[0, 6, 0]
qAverage:[0.0, 139.97491455078125]
ws:[8.512263178825378, 11.79143227849688]
memory len:10000
memory used:2913.0
now epsilon is 0.0642262558543299, the reward is 234.25 with loss [108.00805997848511, 123.35705232620239] in episode 2002
Report: 
rewardSum:234.25
loss:[108.00805997848511, 123.35705232620239]
policies:[2, 14, 1]
qAverage:[10.43705546061198, 144.02810465494792]
ws:[7.774694165587425, 10.24587128162384]
memory len:10000
memory used:2913.0
now epsilon is 0.06416205367930762, the reward is 37.15999999999997 with loss [27.640342712402344, 23.24700403213501] in episode 2003
Report: 
rewardSum:37.15999999999997
loss:[27.640342712402344, 23.24700403213501]
policies:[0, 3, 1]
qAverage:[0.0, 118.03937911987305]
ws:[1.745524562895298, 6.721713423728943]
memory len:10000
memory used:2913.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34*		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.06311193582312978, the reward is 185.25 with loss [439.12095189094543, 432.4969747066498] in episode 2004
Report: 
rewardSum:185.25
loss:[439.12095189094543, 432.4969747066498]
policies:[15, 49, 2]
qAverage:[30.62103711144399, 128.46746412374205]
ws:[6.333753534292771, 7.70328684075404]
memory len:10000
memory used:2915.0
now epsilon is 0.06295433336760835, the reward is 241.25 with loss [87.05503225326538, 72.83236360549927] in episode 2005
Report: 
rewardSum:241.25
loss:[87.05503225326538, 72.83236360549927]
policies:[0, 8, 2]
qAverage:[0.0, 119.00083065032959]
ws:[7.702930010855198, 10.707509204745293]
memory len:10000
memory used:2914.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44*		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44*		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.06063697818713314, the reward is 101.25 with loss [953.266232252121, 952.6056559085846] in episode 2006
Report: 
rewardSum:101.25
loss:[953.266232252121, 952.6056559085846]
policies:[48, 91, 11]
qAverage:[40.740456159724744, 85.3845495771068]
ws:[2.7344347383076135, 4.400424656191076]
memory len:10000
memory used:2916.0
now epsilon is 0.06036469085399781, the reward is 233.25 with loss [119.07834124565125, 122.07683515548706] in episode 2007
Report: 
rewardSum:233.25
loss:[119.07834124565125, 122.07683515548706]
policies:[0, 16, 2]
qAverage:[0.0, 95.89695024490356]
ws:[1.1378190442919731, 5.2304397970438]
memory len:10000
memory used:2915.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43*		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.059258073516057753, the reward is 566.1600000000001 with loss [418.2214649319649, 443.56758546829224] in episode 2008
Report: 
rewardSum:566.1600000000001
loss:[418.2214649319649, 443.56758546829224]
policies:[5, 65, 4]
qAverage:[1.4242699146270752, 94.53596687316895]
ws:[1.2228326235199347, 3.05921773891896]
memory len:10000
memory used:2916.0
now epsilon is 0.05909531736081795, the reward is 240.25 with loss [63.458876609802246, 66.76168084144592] in episode 2009
Report: 
rewardSum:240.25
loss:[63.458876609802246, 66.76168084144592]
policies:[0, 10, 1]
qAverage:[0.0, 78.22009963989258]
ws:[1.0007455384358763, 4.971596598625183]
memory len:10000
memory used:2916.0
now epsilon is 0.05894774516224074, the reward is 241.25 with loss [63.763997077941895, 58.8232364654541] in episode 2010
Report: 
rewardSum:241.25
loss:[63.763997077941895, 58.8232364654541]
policies:[0, 10, 0]
qAverage:[0.0, 78.12351088090377]
ws:[0.3211121775887229, 4.888952894813635]
memory len:10000
memory used:2916.0
now epsilon is 0.05885937878959076, the reward is 245.25 with loss [41.10184049606323, 30.16368293762207] in episode 2011
Report: 
rewardSum:245.25
loss:[41.10184049606323, 30.16368293762207]
policies:[1, 5, 0]
qAverage:[0.0, 71.90518315633138]
ws:[-1.0141882350047429, 2.4245118548472724]
memory len:10000
memory used:2916.0
now epsilon is 0.058668372485132694, the reward is 238.25 with loss [78.2148597240448, 81.85954856872559] in episode 2012
Report: 
rewardSum:238.25
loss:[78.2148597240448, 81.85954856872559]
policies:[2, 9, 2]
qAverage:[8.303969643332742, 70.38461997292258]
ws:[-0.2682012908838012, 3.35095351934433]
memory len:10000
memory used:2915.0
now epsilon is 0.05856577980344636, the reward is 244.25 with loss [36.15936994552612, 39.31828212738037] in episode 2013
Report: 
rewardSum:244.25
loss:[36.15936994552612, 39.31828212738037]
policies:[1, 6, 0]
qAverage:[11.437206268310547, 64.57108402252197]
ws:[-0.7645190209150314, 0.08220450580120087]
memory len:10000
memory used:2915.0
now epsilon is 0.05837572626570941, the reward is 238.25 with loss [96.48210382461548, 93.29087924957275] in episode 2014
Report: 
rewardSum:238.25
loss:[96.48210382461548, 93.29087924957275]
policies:[5, 7, 1]
qAverage:[33.88383014385517, 42.39809828538161]
ws:[2.458430913778452, 3.267124666617467]
memory len:10000
memory used:2915.0
now epsilon is 0.05812812500305369, the reward is 234.25 with loss [95.74987185001373, 98.68330526351929] in episode 2015
Report: 
rewardSum:234.25
loss:[95.74987185001373, 98.68330526351929]
policies:[2, 15, 0]
qAverage:[9.029029422336155, 66.48998938666449]
ws:[-3.672807678580284, -1.6195367210441165]
memory len:10000
memory used:2915.0
now epsilon is 0.057982968066955076, the reward is 241.25 with loss [56.41644334793091, 53.922720432281494] in episode 2016
Report: 
rewardSum:241.25
loss:[56.41644334793091, 53.922720432281494]
policies:[2, 8, 0]
qAverage:[8.310716247558593, 63.272650146484374]
ws:[-0.8930398613214493, 1.6599160462617875]
memory len:10000
memory used:2916.0
now epsilon is 0.05789604795577094, the reward is 245.25 with loss [32.764485359191895, 30.531834721565247] in episode 2017
Report: 
rewardSum:245.25
loss:[32.764485359191895, 30.531834721565247]
policies:[0, 6, 0]
qAverage:[0.0, 65.37091445922852]
ws:[-1.8031229674816132, 0.2651117940743764]
memory len:10000
memory used:2915.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22*		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.05776591203801829, the reward is 242.25 with loss [50.552278995513916, 65.1364483833313] in episode 2018
Report: 
rewardSum:242.25
loss:[50.552278995513916, 65.1364483833313]
policies:[2, 7, 0]
qAverage:[9.179034762912327, 59.25663587782118]
ws:[-0.04951000213623047, 1.0727118584844801]
memory len:10000
memory used:2916.0
now epsilon is 0.05767931730745535, the reward is 245.25 with loss [35.16148257255554, 41.49538326263428] in episode 2019
Report: 
rewardSum:245.25
loss:[35.16148257255554, 41.49538326263428]
policies:[1, 5, 0]
qAverage:[10.285345894949776, 49.90718732561384]
ws:[-0.7701781147292682, 0.6682755351066589]
memory len:10000
memory used:2916.0
now epsilon is 0.05750651708455573, the reward is 239.25 with loss [72.01346206665039, 54.104137659072876] in episode 2020
Report: 
rewardSum:239.25
loss:[72.01346206665039, 54.104137659072876]
policies:[1, 11, 0]
qAverage:[6.203451156616211, 58.95428021748861]
ws:[-3.59126732374231, 0.49897966037193936]
memory len:10000
memory used:2916.0
now epsilon is 0.05742031120332126, the reward is 245.25 with loss [40.68277597427368, 27.72318661212921] in episode 2021
Report: 
rewardSum:245.25
loss:[40.68277597427368, 27.72318661212921]
policies:[0, 6, 0]
qAverage:[0.0, 57.86594009399414]
ws:[-0.7349260797103246, 1.5106066068013508]
memory len:10000
memory used:2916.0
now epsilon is 0.057334234550117553, the reward is 245.25 with loss [26.775124073028564, 27.7965886592865] in episode 2022
Report: 
rewardSum:245.25
loss:[26.775124073028564, 27.7965886592865]
policies:[0, 6, 0]
qAverage:[0.0, 58.262011210123696]
ws:[-0.9318011601765951, 1.2254770398139954]
memory len:10000
memory used:2916.0
now epsilon is 0.057248286931223684, the reward is 245.25 with loss [26.531712293624878, 33.938321352005005] in episode 2023
Report: 
rewardSum:245.25
loss:[26.531712293624878, 33.938321352005005]
policies:[0, 6, 0]
qAverage:[0.0, 60.16688210623605]
ws:[-1.8313507011958532, 2.4031273807798113]
memory len:10000
memory used:2915.0
now epsilon is 0.05716246815320912, the reward is 245.25 with loss [36.72275519371033, 37.025989055633545] in episode 2024
Report: 
rewardSum:245.25
loss:[36.72275519371033, 37.025989055633545]
policies:[1, 5, 0]
qAverage:[10.309116908482142, 50.971505301339285]
ws:[0.27513870171138216, 2.2173299278531755]
memory len:10000
memory used:2915.0
now epsilon is 0.05699121634754514, the reward is 239.25 with loss [66.66866743564606, 70.17196130752563] in episode 2025
Report: 
rewardSum:239.25
loss:[66.66866743564606, 70.17196130752563]
policies:[0, 12, 0]
qAverage:[0.0, 60.75576224693885]
ws:[1.0690557291874518, 3.841172401721661]
memory len:10000
memory used:2916.0
now epsilon is 0.056806272472074947, the reward is 238.25 with loss [70.67801362276077, 72.8167941570282] in episode 2026
Report: 
rewardSum:238.25
loss:[70.67801362276077, 72.8167941570282]
policies:[0, 13, 0]
qAverage:[0.0, 57.78671029897836]
ws:[-0.42266540229320526, 1.8231588425831153]
memory len:10000
memory used:2916.0
now epsilon is 0.05659362133791342, the reward is 236.25 with loss [89.79010093212128, 93.69005155563354] in episode 2027
Report: 
rewardSum:236.25
loss:[89.79010093212128, 93.69005155563354]
policies:[2, 13, 0]
qAverage:[8.267319202423096, 51.180368185043335]
ws:[-1.4466056459350511, -0.7414959648158401]
memory len:10000
memory used:2915.0
now epsilon is 0.056438183273975026, the reward is 240.25 with loss [69.28634905815125, 62.6198793053627] in episode 2028
Report: 
rewardSum:240.25
loss:[69.28634905815125, 62.6198793053627]
policies:[5, 3, 3]
qAverage:[33.198720932006836, 23.437968730926514]
ws:[-3.850663734599948, -2.60526917129755]
memory len:10000
memory used:2915.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22*		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.05635357889222726, the reward is 245.25 with loss [39.73477840423584, 31.11886215209961] in episode 2029
Report: 
rewardSum:245.25
loss:[39.73477840423584, 31.11886215209961]
policies:[2, 4, 0]
qAverage:[21.98681386311849, 30.8881778717041]
ws:[-2.874303619066874, 0.41564588869611424]
memory len:10000
memory used:2916.0
now epsilon is 0.056269101337761945, the reward is 245.25 with loss [44.34849309921265, 40.69728326797485] in episode 2030
Report: 
rewardSum:245.25
loss:[44.34849309921265, 40.69728326797485]
policies:[4, 2, 0]
qAverage:[33.591518947056365, 16.20693097795759]
ws:[0.454901967729841, 1.1460694095918111]
memory len:10000
memory used:2915.0
now epsilon is 0.0561145545891227, the reward is 240.25 with loss [53.837960600852966, 66.6451210975647] in episode 2031
Report: 
rewardSum:240.25
loss:[53.837960600852966, 66.6451210975647]
policies:[3, 8, 0]
qAverage:[10.90221474387429, 41.87853934548118]
ws:[-1.3936672088774769, -0.8845797929574143]
memory len:10000
memory used:2916.0
now epsilon is 0.05601642773826466, the reward is 244.25 with loss [43.9428346157074, 55.49960660934448] in episode 2032
Report: 
rewardSum:244.25
loss:[43.9428346157074, 55.49960660934448]
policies:[3, 3, 1]
qAverage:[25.98872974940709, 24.71102796282087]
ws:[-3.8941207272665843, -0.26766733612333027]
memory len:10000
memory used:2918.0
now epsilon is 0.055932455594556424, the reward is 245.25 with loss [46.34609007835388, 32.41757595539093] in episode 2033
Report: 
rewardSum:245.25
loss:[46.34609007835388, 32.41757595539093]
policies:[4, 2, 0]
qAverage:[35.21446772984096, 16.53714152744838]
ws:[-1.0504468594278609, -2.0394733463014876]
memory len:10000
memory used:2917.0
now epsilon is 0.0558486093303661, the reward is 245.25 with loss [38.16943025588989, 28.4824857711792] in episode 2034
Report: 
rewardSum:245.25
loss:[38.16943025588989, 28.4824857711792]
policies:[4, 2, 0]
qAverage:[34.84828513009207, 16.553316388811385]
ws:[-1.9402260897415025, -3.114610994500773]
memory len:10000
memory used:2917.0
now epsilon is 0.055695217490389456, the reward is 240.25 with loss [65.39623498916626, 73.69943904876709] in episode 2035
Report: 
rewardSum:240.25
loss:[65.39623498916626, 73.69943904876709]
policies:[9, 1, 1]
qAverage:[50.00376788052645, 5.309628573330966]
ws:[-1.061626198616895, -2.3947962239723313]
memory len:10000
memory used:2917.0
now epsilon is 0.05559782392930353, the reward is 633.1600000000001 with loss [35.44095754623413, 51.00199913978577] in episode 2036
Report: 
rewardSum:633.1600000000001
loss:[35.44095754623413, 51.00199913978577]
policies:[5, 1, 1]
qAverage:[39.12505667550223, 7.689467293875558]
ws:[-0.5712840557098389, -4.759840735367367]
memory len:10000
memory used:2917.0
now epsilon is 0.05544512088775123, the reward is 240.25 with loss [66.2635840177536, 59.66849446296692] in episode 2037
Report: 
rewardSum:240.25
loss:[66.2635840177536, 59.66849446296692]
policies:[10, 1, 0]
qAverage:[45.727064768473305, 4.342948913574219]
ws:[-0.7685146940251192, -2.2232940370837846]
memory len:10000
memory used:2917.0
now epsilon is 0.05538969655531871, the reward is 247.25 with loss [33.4834942817688, 22.961381435394287] in episode 2038
Report: 
rewardSum:247.25
loss:[33.4834942817688, 22.961381435394287]
policies:[2, 2, 0]
qAverage:[21.78155517578125, 20.96806945800781]
ws:[-5.5363701522350315, -3.8981067180633544]
memory len:10000
memory used:2917.0
now epsilon is 0.05533432762643797, the reward is 247.25 with loss [28.86652708053589, 33.07545757293701] in episode 2039
Report: 
rewardSum:247.25
loss:[28.86652708053589, 33.07545757293701]
policies:[2, 2, 0]
qAverage:[21.65670700073242, 20.796759033203124]
ws:[-4.692099791765213, -3.008597803115845]
memory len:10000
memory used:2917.0
now epsilon is 0.055209949818916575, the reward is 242.25 with loss [48.741809129714966, 53.022756576538086] in episode 2040
Report: 
rewardSum:242.25
loss:[48.741809129714966, 53.022756576538086]
policies:[8, 1, 0]
qAverage:[48.18468348185221, 0.0]
ws:[-0.30830277336968315, -2.5089168465799756]
memory len:10000
memory used:2917.0
now epsilon is 0.05505831209885007, the reward is 240.25 with loss [60.12648320198059, 69.44169980287552] in episode 2041
Report: 
rewardSum:240.25
loss:[60.12648320198059, 69.44169980287552]
policies:[9, 1, 1]
qAverage:[42.833697424994575, 5.582712809244792]
ws:[-0.7970630261633131, -1.472866306702296]
memory len:10000
memory used:2917.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42*		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.05496203228660924, the reward is 244.25 with loss [42.574474573135376, 40.66248798370361] in episode 2042
Report: 
rewardSum:244.25
loss:[42.574474573135376, 40.66248798370361]
policies:[3, 4, 0]
qAverage:[20.354924201965332, 25.791985988616943]
ws:[-4.211982334963977, -4.221144032664597]
memory len:10000
memory used:2917.0
now epsilon is 0.054811075488178894, the reward is 240.25 with loss [61.15410506725311, 75.1634111404419] in episode 2043
Report: 
rewardSum:240.25
loss:[61.15410506725311, 75.1634111404419]
policies:[8, 1, 2]
qAverage:[39.99917030334473, 4.4659873962402346]
ws:[-0.6415760040283203, -2.218610402941704]
memory len:10000
memory used:2916.0
now epsilon is 0.05467420185288247, the reward is 241.25 with loss [51.73271806538105, 71.68287658691406] in episode 2044
Report: 
rewardSum:241.25
loss:[51.73271806538105, 71.68287658691406]
policies:[7, 2, 1]
qAverage:[34.53615226745605, 10.187577056884766]
ws:[1.4020320653915406, 1.2749767184257508]
memory len:10000
memory used:2917.0
now epsilon is 0.054592241790084914, the reward is 245.25 with loss [30.116315364837646, 38.25757956504822] in episode 2045
Report: 
rewardSum:245.25
loss:[30.116315364837646, 38.25757956504822]
policies:[5, 1, 0]
qAverage:[32.66650390625, 8.81659189860026]
ws:[-1.5968869874874752, -1.271967351436615]
memory len:10000
memory used:2917.0
now epsilon is 0.05451040459056959, the reward is 245.25 with loss [29.316292762756348, 39.55773329734802] in episode 2046
Report: 
rewardSum:245.25
loss:[29.316292762756348, 39.55773329734802]
policies:[3, 3, 0]
qAverage:[21.256450653076172, 20.875494275774276]
ws:[-2.2312520337956294, -1.8455885819026403]
memory len:10000
memory used:2916.0
now epsilon is 0.05445591462397407, the reward is 247.25 with loss [19.050476551055908, 25.245542764663696] in episode 2047
Report: 
rewardSum:247.25
loss:[19.050476551055908, 25.245542764663696]
policies:[2, 2, 0]
qAverage:[19.617301177978515, 19.68940887451172]
ws:[-5.3927148342132565, -3.1334183633327486]
memory len:10000
memory used:2916.0
now epsilon is 0.0544014791269148, the reward is 247.25 with loss [13.253406524658203, 20.94034731388092] in episode 2048
Report: 
rewardSum:247.25
loss:[13.253406524658203, 20.94034731388092]
policies:[1, 3, 0]
qAverage:[9.698583984375, 29.011398315429688]
ws:[-4.198700904846191, -1.1192481696605683]
memory len:10000
memory used:2916.0
now epsilon is 0.054319927892613834, the reward is 245.25 with loss [41.66884446144104, 30.247007608413696] in episode 2049
Report: 
rewardSum:245.25
loss:[41.66884446144104, 30.247007608413696]
policies:[1, 5, 0]
qAverage:[0.0, 40.35537846883138]
ws:[0.443440318107605, 3.7227261066436768]
memory len:10000
memory used:2916.0
now epsilon is 0.05423849890873552, the reward is 245.25 with loss [37.61143636703491, 24.82148051261902] in episode 2050
Report: 
rewardSum:245.25
loss:[37.61143636703491, 24.82148051261902]
policies:[0, 6, 0]
qAverage:[0.0, 38.23519243512835]
ws:[-1.7478466204234533, 2.058093113558633]
memory len:10000
memory used:2916.0
now epsilon is 0.054170734675687726, the reward is 246.25 with loss [32.43524694442749, 23.307523012161255] in episode 2051
Report: 
rewardSum:246.25
loss:[32.43524694442749, 23.307523012161255]
policies:[0, 4, 1]
qAverage:[0.0, 34.4507438659668]
ws:[-4.352692383527756, 2.4014542520046236]
memory len:10000
memory used:2916.0
now epsilon is 0.05410305510558918, the reward is 246.25 with loss [33.13000202178955, 24.412095308303833] in episode 2052
Report: 
rewardSum:246.25
loss:[33.13000202178955, 24.412095308303833]
policies:[1, 4, 0]
qAverage:[9.302435302734375, 26.192916107177734]
ws:[0.24623088240623475, 1.7701202273368835]
memory len:10000
memory used:2916.0
now epsilon is 0.054048972335748034, the reward is 247.25 with loss [23.44611120223999, 25.3088800907135] in episode 2053
Report: 
rewardSum:247.25
loss:[23.44611120223999, 25.3088800907135]
policies:[1, 3, 0]
qAverage:[10.925312042236328, 22.22702121734619]
ws:[-5.135417357087135, -1.5668157190084457]
memory len:10000
memory used:2916.0
now epsilon is 0.053994943628399066, the reward is 247.25 with loss [18.22675085067749, 22.643399477005005] in episode 2054
Report: 
rewardSum:247.25
loss:[18.22675085067749, 22.643399477005005]
policies:[2, 2, 0]
qAverage:[17.928392791748045, 18.179814910888673]
ws:[-5.082140207290649, -3.592496168613434]
memory len:10000
memory used:2916.0
now epsilon is 0.05390052331589178, the reward is 633.1600000000001 with loss [43.02299642562866, 41.599064111709595] in episode 2055
Report: 
rewardSum:633.1600000000001
loss:[43.02299642562866, 41.599064111709595]
policies:[5, 0, 2]
qAverage:[36.56930414835612, 0.0]
ws:[-3.3356267511844635, -7.370657041668892]
memory len:10000
memory used:2916.0
now epsilon is 0.05379281654802759, the reward is 243.25 with loss [45.641185998916626, 53.192949295043945] in episode 2056
Report: 
rewardSum:243.25
loss:[45.641185998916626, 53.192949295043945]
policies:[5, 2, 1]
qAverage:[28.42119312286377, 11.838311672210693]
ws:[-0.38248889264650643, -0.4538048915565014]
memory len:10000
memory used:2916.0
now epsilon is 0.05373904390042394, the reward is 247.25 with loss [28.45128583908081, 22.137073040008545] in episode 2057
Report: 
rewardSum:247.25
loss:[28.45128583908081, 22.137073040008545]
policies:[0, 4, 0]
qAverage:[0.0, 34.47550106048584]
ws:[-6.4042357206344604, -3.2707283794879913]
memory len:10000
memory used:2916.0
now epsilon is 0.053685325005306504, the reward is 247.25 with loss [24.528100967407227, 32.29622030258179] in episode 2058
Report: 
rewardSum:247.25
loss:[24.528100967407227, 32.29622030258179]
policies:[0, 4, 0]
qAverage:[0.0, 35.585628509521484]
ws:[-6.490163493156433, -2.8565014004707336]
memory len:10000
memory used:2916.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24*		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.05363165980894295, the reward is 247.25 with loss [27.53852367401123, 24.109030723571777] in episode 2059
Report: 
rewardSum:247.25
loss:[27.53852367401123, 24.109030723571777]
policies:[0, 4, 0]
qAverage:[0.0, 34.133023262023926]
ws:[-8.935237526893616, -5.070184882730246]
memory len:10000
memory used:2916.0
now epsilon is 0.05357804825765467, the reward is 247.25 with loss [18.458554446697235, 21.967397212982178] in episode 2060
Report: 
rewardSum:247.25
loss:[18.458554446697235, 21.967397212982178]
policies:[0, 4, 0]
qAverage:[0.0, 33.28149642944336]
ws:[-3.5042643189430236, -0.41944968700408936]
memory len:10000
memory used:2916.0
now epsilon is 0.0535244902978167, the reward is 247.25 with loss [17.18666434288025, 27.871324062347412] in episode 2061
Report: 
rewardSum:247.25
loss:[17.18666434288025, 27.871324062347412]
policies:[0, 3, 1]
qAverage:[0.0, 31.787175178527832]
ws:[-4.244756698608398, -1.7179605960845947]
memory len:10000
memory used:2916.0
now epsilon is 0.053470985875857684, the reward is 247.25 with loss [24.804086685180664, 23.051235675811768] in episode 2062
Report: 
rewardSum:247.25
loss:[24.804086685180664, 23.051235675811768]
policies:[2, 2, 0]
qAverage:[17.139810180664064, 17.922325134277344]
ws:[-3.199747180938721, -1.4426772832870483]
memory len:10000
memory used:2916.0
now epsilon is 0.053417534938259806, the reward is 247.25 with loss [18.19739532470703, 23.578355312347412] in episode 2063
Report: 
rewardSum:247.25
loss:[18.19739532470703, 23.578355312347412]
policies:[3, 1, 0]
qAverage:[13.53805669148763, 13.143653869628906]
ws:[0.6960310935974121, 2.3353793223698935]
memory len:10000
memory used:2916.0
now epsilon is 0.05336413743155876, the reward is 247.25 with loss [19.36135196685791, 30.107481002807617] in episode 2064
Report: 
rewardSum:247.25
loss:[19.36135196685791, 30.107481002807617]
policies:[1, 3, 0]
qAverage:[8.622161102294921, 25.301197052001953]
ws:[-2.818022131919861, -0.6194875717163086]
memory len:10000
memory used:2917.0
now epsilon is 0.05325750249725712, the reward is 243.25 with loss [49.482566356658936, 40.43072438240051] in episode 2065
Report: 
rewardSum:243.25
loss:[49.482566356658936, 40.43072438240051]
policies:[5, 1, 2]
qAverage:[27.046929041544598, 8.221478144327799]
ws:[-0.8451759020487467, -1.089268684387207]
memory len:10000
memory used:2916.0
now epsilon is 0.05320426496299492, the reward is 247.25 with loss [13.472534537315369, 27.896594285964966] in episode 2066
Report: 
rewardSum:247.25
loss:[13.472534537315369, 27.896594285964966]
policies:[2, 2, 0]
qAverage:[16.715839385986328, 17.827163696289062]
ws:[-5.700889097154141, -2.1110718846321106]
memory len:10000
memory used:2916.0
now epsilon is 0.05315108064630623, the reward is 247.25 with loss [27.293701887130737, 15.435671091079712] in episode 2067
Report: 
rewardSum:247.25
loss:[27.293701887130737, 15.435671091079712]
policies:[1, 3, 0]
qAverage:[7.910914611816406, 26.354766845703125]
ws:[-4.120293235778808, 0.45272537469863894]
memory len:10000
memory used:2916.0
now epsilon is 0.053097949493993435, the reward is 247.25 with loss [25.220720529556274, 26.712565422058105] in episode 2068
Report: 
rewardSum:247.25
loss:[25.220720529556274, 26.712565422058105]
policies:[1, 3, 0]
qAverage:[7.962562561035156, 26.84383773803711]
ws:[-4.625213845074176, -0.29808793067932127]
memory len:10000
memory used:2916.0
now epsilon is 0.05304487145291209, the reward is 247.25 with loss [28.039610862731934, 14.448957324028015] in episode 2069
Report: 
rewardSum:247.25
loss:[28.039610862731934, 14.448957324028015]
policies:[1, 3, 0]
qAverage:[8.014604949951172, 26.02676773071289]
ws:[-4.160181713104248, 1.7186525106430053]
memory len:10000
memory used:2916.0
now epsilon is 0.052991846469970884, the reward is 247.25 with loss [20.827475786209106, 19.571839809417725] in episode 2070
Report: 
rewardSum:247.25
loss:[20.827475786209106, 19.571839809417725]
policies:[1, 3, 0]
qAverage:[7.911643981933594, 26.674024200439455]
ws:[-4.371202778816223, 1.442276155948639]
memory len:10000
memory used:2916.0
now epsilon is 0.05296535385872631, the reward is -1.0 with loss [12.149219512939453, 10.259077787399292] in episode 2071
Report: 
rewardSum:-1.0
loss:[12.149219512939453, 10.259077787399292]
policies:[1, 0, 1]
qAverage:[20.237565994262695, 0.0]
ws:[0.13494041562080383, -0.46676871180534363]
memory len:10000
memory used:2916.0
now epsilon is 0.052912408363565165, the reward is 247.25 with loss [22.159455060958862, 30.009681224822998] in episode 2072
Report: 
rewardSum:247.25
loss:[22.159455060958862, 30.009681224822998]
policies:[2, 2, 0]
qAverage:[0.0, 20.966575622558594]
ws:[-0.024305766448378563, 0.05138188600540161]
memory len:10000
memory used:2916.0
now epsilon is 0.05285951579404793, the reward is 247.25 with loss [24.125614881515503, 21.00033736228943] in episode 2073
Report: 
rewardSum:247.25
loss:[24.125614881515503, 21.00033736228943]
policies:[2, 2, 0]
qAverage:[15.889241027832032, 17.591791534423827]
ws:[-5.796958243101836, -2.00493483543396]
memory len:10000
memory used:2916.0
now epsilon is 0.052806676097268794, the reward is 247.25 with loss [21.186551570892334, 29.249759674072266] in episode 2074
Report: 
rewardSum:247.25
loss:[21.186551570892334, 29.249759674072266]
policies:[2, 2, 0]
qAverage:[16.258700561523437, 17.797315216064455]
ws:[-5.4705753900110725, -1.9601235389709473]
memory len:10000
memory used:2916.0
now epsilon is 0.05275388922037486, the reward is 247.25 with loss [24.507874488830566, 15.995017230510712] in episode 2075
Report: 
rewardSum:247.25
loss:[24.507874488830566, 15.995017230510712]
policies:[1, 3, 0]
qAverage:[9.153039932250977, 21.73455047607422]
ws:[-5.222286760807037, -2.887667927891016]
memory len:10000
memory used:2916.0
now epsilon is 0.05270115511056604, the reward is 247.25 with loss [26.298693418502808, 25.875999212265015] in episode 2076
Report: 
rewardSum:247.25
loss:[26.298693418502808, 25.875999212265015]
policies:[3, 1, 0]
qAverage:[23.545736694335936, 10.254523468017577]
ws:[-2.761585307121277, -1.6466091215610503]
memory len:10000
memory used:2916.0
now epsilon is 0.05264847371509503, the reward is 247.25 with loss [23.048052668571472, 30.24214220046997] in episode 2077
Report: 
rewardSum:247.25
loss:[23.048052668571472, 30.24214220046997]
policies:[2, 1, 1]
qAverage:[19.6820125579834, 12.683392524719238]
ws:[-4.3922035694122314, -2.8267738968133926]
memory len:10000
memory used:2916.0
now epsilon is 0.052595844981267265, the reward is 247.25 with loss [21.53420925140381, 20.76932716369629] in episode 2078
Report: 
rewardSum:247.25
loss:[21.53420925140381, 20.76932716369629]
policies:[2, 2, 0]
qAverage:[15.720207214355469, 18.119692230224608]
ws:[-4.476962131261826, -2.5194738119840623]
memory len:10000
memory used:2916.0
now epsilon is 0.05254326885644083, the reward is 247.25 with loss [25.728548765182495, 26.034053325653076] in episode 2079
Report: 
rewardSum:247.25
loss:[25.728548765182495, 26.034053325653076]
policies:[3, 1, 0]
qAverage:[23.27717514038086, 10.155430603027344]
ws:[-4.763534295558929, -3.408648633956909]
memory len:10000
memory used:2916.0
now epsilon is 0.052490745288026476, the reward is 247.25 with loss [21.09635329246521, 21.9819393157959] in episode 2080
Report: 
rewardSum:247.25
loss:[21.09635329246521, 21.9819393157959]
policies:[2, 2, 0]
qAverage:[15.782233428955077, 18.246725463867186]
ws:[-4.476243317127228, -1.9658709198236466]
memory len:10000
memory used:2915.0
now epsilon is 0.05243827422348748, the reward is 247.25 with loss [26.663716316223145, 25.75294017791748] in episode 2081
Report: 
rewardSum:247.25
loss:[26.663716316223145, 25.75294017791748]
policies:[2, 2, 0]
qAverage:[15.800714874267578, 18.09906921386719]
ws:[-4.71246612071991, -1.5573861122131347]
memory len:10000
memory used:2915.0
now epsilon is 0.05238585561033964, the reward is 247.25 with loss [29.321258544921875, 19.837153911590576] in episode 2082
Report: 
rewardSum:247.25
loss:[29.321258544921875, 19.837153911590576]
policies:[1, 3, 0]
qAverage:[7.2489166259765625, 26.72248077392578]
ws:[-4.295394682884217, -1.3631352663040162]
memory len:10000
memory used:2915.0
now epsilon is 0.05233348939615125, the reward is 247.25 with loss [20.541335105895996, 20.496609687805176] in episode 2083
Report: 
rewardSum:247.25
loss:[20.541335105895996, 20.496609687805176]
policies:[1, 3, 0]
qAverage:[7.263558959960937, 26.845330810546876]
ws:[-3.20600882768631, -0.26677956581115725]
memory len:10000
memory used:2915.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12*		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.052281175528542986, the reward is 247.25 with loss [21.18622589111328, 20.246397495269775] in episode 2084
Report: 
rewardSum:247.25
loss:[21.18622589111328, 20.246397495269775]
policies:[1, 3, 0]
qAverage:[7.2302490234375, 26.880784606933595]
ws:[-2.472417879104614, 1.076853132247925]
memory len:10000
memory used:2916.0
now epsilon is 0.0522289139551879, the reward is 247.25 with loss [21.661794662475586, 24.166961669921875] in episode 2085
Report: 
rewardSum:247.25
loss:[21.661794662475586, 24.166961669921875]
policies:[1, 2, 1]
qAverage:[8.783604621887207, 22.40860939025879]
ws:[-4.256238579750061, 0.12016093730926514]
memory len:10000
memory used:2917.0
now epsilon is 0.052176704623811354, the reward is 247.25 with loss [32.51642894744873, 24.579275369644165] in episode 2086
Report: 
rewardSum:247.25
loss:[32.51642894744873, 24.579275369644165]
policies:[1, 3, 0]
qAverage:[6.869701385498047, 27.24808578491211]
ws:[-4.064479947090149, -0.05008111000061035]
memory len:10000
memory used:2917.0
now epsilon is 0.05212454748219093, the reward is 247.25 with loss [33.112504959106445, 18.61044692993164] in episode 2087
Report: 
rewardSum:247.25
loss:[33.112504959106445, 18.61044692993164]
policies:[1, 3, 0]
qAverage:[8.692968368530273, 24.63645362854004]
ws:[-6.986674711108208, -3.1864136084914207]
memory len:10000
memory used:2917.0
now epsilon is 0.052020389559589916, the reward is 243.25 with loss [56.52834939956665, 59.207783222198486] in episode 2088
Report: 
rewardSum:243.25
loss:[56.52834939956665, 59.207783222198486]
policies:[2, 3, 3]
qAverage:[12.089277267456055, 21.177875518798828]
ws:[-0.424737811088562, -0.18173529704411825]
memory len:10000
memory used:2916.0
now epsilon is 0.05196838867442534, the reward is 247.25 with loss [28.38923978805542, 22.993173241615295] in episode 2089
Report: 
rewardSum:247.25
loss:[28.38923978805542, 22.993173241615295]
policies:[0, 3, 1]
qAverage:[0.0, 33.43894672393799]
ws:[-6.500327870249748, -0.19191119074821472]
memory len:10000
memory used:2916.0
now epsilon is 0.05191643977064885, the reward is 247.25 with loss [16.428144693374634, 19.425737380981445] in episode 2090
Report: 
rewardSum:247.25
loss:[16.428144693374634, 19.425737380981445]
policies:[0, 4, 0]
qAverage:[0.0, 34.5897331237793]
ws:[-3.9838566064834593, 3.080234098434448]
memory len:10000
memory used:2917.0
now epsilon is 0.05186454279629855, the reward is 247.25 with loss [25.8797607421875, 20.62320327758789] in episode 2091
Report: 
rewardSum:247.25
loss:[25.8797607421875, 20.62320327758789]
policies:[0, 4, 0]
qAverage:[0.0, 34.83998413085938]
ws:[-2.7886343002319336, 3.657191276550293]
memory len:10000
memory used:2917.0
now epsilon is 0.05181269769946447, the reward is 247.25 with loss [18.69659447669983, 25.94058609008789] in episode 2092
Report: 
rewardSum:247.25
loss:[18.69659447669983, 25.94058609008789]
policies:[0, 4, 0]
qAverage:[0.0, 34.64168090820313]
ws:[-3.970845651626587, 2.772036981582642]
memory len:10000
memory used:2917.0
now epsilon is 0.051760904428288554, the reward is 247.25 with loss [21.912920475006104, 22.83034610748291] in episode 2093
Report: 
rewardSum:247.25
loss:[21.912920475006104, 22.83034610748291]
policies:[1, 2, 1]
qAverage:[0.0, 24.640626271565754]
ws:[0.7937576373418173, 3.8043357133865356]
memory len:10000
memory used:2917.0
now epsilon is 0.05170916293096457, the reward is 247.25 with loss [22.310786724090576, 17.332903265953064] in episode 2094
Report: 
rewardSum:247.25
loss:[22.310786724090576, 17.332903265953064]
policies:[1, 3, 0]
qAverage:[6.793051147460938, 27.9355827331543]
ws:[-3.9474381685256956, 2.2004340410232546]
memory len:10000
memory used:2917.0
now epsilon is 0.05165747315573809, the reward is 247.25 with loss [15.114803671836853, 19.954349100589752] in episode 2095
Report: 
rewardSum:247.25
loss:[15.114803671836853, 19.954349100589752]
policies:[1, 3, 0]
qAverage:[6.883230590820313, 27.66492156982422]
ws:[-4.553775310516357, 0.5410898804664612]
memory len:10000
memory used:2917.0
now epsilon is 0.05158003535874565, the reward is 245.25 with loss [33.680628657341, 37.14425611495972] in episode 2096
Report: 
rewardSum:245.25
loss:[33.680628657341, 37.14425611495972]
policies:[1, 3, 2]
qAverage:[8.413203239440918, 21.78995704650879]
ws:[0.9560093283653259, 2.7909518415108323]
memory len:10000
memory used:2916.0
now epsilon is 0.05152847466267661, the reward is 247.25 with loss [27.355703592300415, 19.66979694366455] in episode 2097
Report: 
rewardSum:247.25
loss:[27.355703592300415, 19.66979694366455]
policies:[1, 3, 0]
qAverage:[6.880286407470703, 28.847293853759766]
ws:[-4.3966592669487, -0.1945994973182678]
memory len:10000
memory used:2916.0
now epsilon is 0.05147696550797161, the reward is 247.25 with loss [18.787112951278687, 31.656470775604248] in episode 2098
Report: 
rewardSum:247.25
loss:[18.787112951278687, 31.656470775604248]
policies:[1, 3, 0]
qAverage:[6.725177001953125, 29.171549987792968]
ws:[-4.487468034029007, -0.025959694385528566]
memory len:10000
memory used:2916.0
now epsilon is 0.0514255078431086, the reward is 247.25 with loss [24.970438480377197, 17.393813133239746] in episode 2099
Report: 
rewardSum:247.25
loss:[24.970438480377197, 17.393813133239746]
policies:[1, 3, 0]
qAverage:[6.636532592773437, 28.73464126586914]
ws:[-4.334457159042358, -0.6488147735595703]
memory len:10000
memory used:2916.0
now epsilon is 0.051374101616617054, the reward is 247.25 with loss [30.183104038238525, 16.657222986221313] in episode 2100
Report: 
rewardSum:247.25
loss:[30.183104038238525, 16.657222986221313]
policies:[1, 3, 0]
qAverage:[6.525677490234375, 29.43024597167969]
ws:[-3.831359124183655, -0.45915062427520753]
memory len:10000
memory used:2916.0
now epsilon is 0.05132274677707786, the reward is 247.25 with loss [24.279726266860962, 24.783682584762573] in episode 2101
Report: 
rewardSum:247.25
loss:[24.279726266860962, 24.783682584762573]
policies:[1, 3, 0]
qAverage:[6.718315124511719, 28.848043060302736]
ws:[-2.8544652700424193, 1.9069761224091053]
memory len:10000
memory used:2916.0
now epsilon is 0.051271443273123364, the reward is 247.25 with loss [25.96929258108139, 21.63269317150116] in episode 2102
Report: 
rewardSum:247.25
loss:[25.96929258108139, 21.63269317150116]
policies:[1, 3, 0]
qAverage:[8.177183151245117, 26.340166091918945]
ws:[-3.674782156944275, 0.7002207860350609]
memory len:10000
memory used:2916.0
now epsilon is 0.05122019105343721, the reward is 247.25 with loss [36.31670141220093, 26.617892742156982] in episode 2103
Report: 
rewardSum:247.25
loss:[36.31670141220093, 26.617892742156982]
policies:[1, 3, 0]
qAverage:[6.619408416748047, 28.95276336669922]
ws:[-3.0554216146469115, 2.399069905281067]
memory len:10000
memory used:2915.0
now epsilon is 0.051168990066754357, the reward is 247.25 with loss [19.545854330062866, 29.03299331665039] in episode 2104
Report: 
rewardSum:247.25
loss:[19.545854330062866, 29.03299331665039]
policies:[1, 3, 0]
qAverage:[6.723970794677735, 28.919937896728516]
ws:[-3.066759467124939, 2.3762287855148316]
memory len:10000
memory used:2915.0
now epsilon is 0.051117840261861024, the reward is 247.25 with loss [24.439780235290527, 17.985909461975098] in episode 2105
Report: 
rewardSum:247.25
loss:[24.439780235290527, 17.985909461975098]
policies:[1, 3, 0]
qAverage:[6.613560485839844, 29.14108810424805]
ws:[-3.0407673835754396, 1.9337544441223145]
memory len:10000
memory used:2915.0
now epsilon is 0.0510667415875946, the reward is 247.25 with loss [23.435909271240234, 19.668824315071106] in episode 2106
Report: 
rewardSum:247.25
loss:[23.435909271240234, 19.668824315071106]
policies:[2, 2, 0]
qAverage:[8.343646049499512, 24.156461715698242]
ws:[-4.101282745599747, 1.839353859424591]
memory len:10000
memory used:2916.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25*		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.051015693992843635, the reward is 247.25 with loss [18.221204042434692, 30.8890061378479] in episode 2107
Report: 
rewardSum:247.25
loss:[18.221204042434692, 30.8890061378479]
policies:[1, 3, 0]
qAverage:[6.696379852294922, 29.210002899169922]
ws:[-2.8630043506622314, 2.1187073230743407]
memory len:10000
memory used:2915.0
now epsilon is 0.05096469742654776, the reward is 247.25 with loss [34.637720346450806, 19.723575592041016] in episode 2108
Report: 
rewardSum:247.25
loss:[34.637720346450806, 19.723575592041016]
policies:[1, 3, 0]
qAverage:[6.769209289550782, 30.96414260864258]
ws:[-3.180881345272064, 1.57587788105011]
memory len:10000
memory used:2915.0
now epsilon is 0.050913751837697654, the reward is 247.25 with loss [32.121582984924316, 29.47449493408203] in episode 2109
Report: 
rewardSum:247.25
loss:[32.121582984924316, 29.47449493408203]
policies:[1, 3, 0]
qAverage:[9.108646392822266, 23.264893531799316]
ws:[0.6877146940678358, 2.592440366744995]
memory len:10000
memory used:2915.0
now epsilon is 0.050862857175334994, the reward is 247.25 with loss [28.03945827484131, 18.90197515487671] in episode 2110
Report: 
rewardSum:247.25
loss:[28.03945827484131, 18.90197515487671]
policies:[1, 3, 0]
qAverage:[6.817732238769532, 30.85731201171875]
ws:[-4.610397267341614, -0.9922059059143067]
memory len:10000
memory used:2915.0
now epsilon is 0.05079931038520524, the reward is 246.25 with loss [16.9537615776062, 29.91104745864868] in episode 2111
Report: 
rewardSum:246.25
loss:[16.9537615776062, 29.91104745864868]
policies:[1, 3, 1]
qAverage:[7.060169982910156, 30.36319808959961]
ws:[-4.1359079599380495, 1.4094496011734008]
memory len:10000
memory used:2914.0
now epsilon is 0.05074853012138668, the reward is 247.25 with loss [20.567416191101074, 20.532973051071167] in episode 2112
Report: 
rewardSum:247.25
loss:[20.567416191101074, 20.532973051071167]
policies:[1, 3, 0]
qAverage:[0.0, 39.05275535583496]
ws:[-5.132035747170448, 0.908897876739502]
memory len:10000
memory used:2915.0
now epsilon is 0.050685126168637816, the reward is 246.25 with loss [33.44927978515625, 29.39229679107666] in episode 2113
Report: 
rewardSum:246.25
loss:[33.44927978515625, 29.39229679107666]
policies:[0, 4, 1]
qAverage:[0.0, 38.22320556640625]
ws:[-4.108765971660614, 2.2685570120811462]
memory len:10000
memory used:2915.0
now epsilon is 0.05063446004622387, the reward is 247.25 with loss [25.954350471496582, 18.65714740753174] in episode 2114
Report: 
rewardSum:247.25
loss:[25.954350471496582, 18.65714740753174]
policies:[1, 3, 0]
qAverage:[6.954185485839844, 31.111588287353516]
ws:[-3.958320438861847, 2.345172715187073]
memory len:10000
memory used:2915.0
now epsilon is 0.05058384457093572, the reward is 247.25 with loss [26.57383108139038, 20.66978645324707] in episode 2115
Report: 
rewardSum:247.25
loss:[26.57383108139038, 20.66978645324707]
policies:[0, 4, 0]
qAverage:[0.0, 38.00892868041992]
ws:[-4.506657779216766, 0.9217257022857666]
memory len:10000
memory used:2915.0
now epsilon is 0.05053327969214521, the reward is 247.25 with loss [32.8061203956604, 27.137023210525513] in episode 2116
Report: 
rewardSum:247.25
loss:[32.8061203956604, 27.137023210525513]
policies:[1, 3, 0]
qAverage:[6.890235900878906, 31.17337646484375]
ws:[-3.6672375440597533, 2.816414451599121]
memory len:10000
memory used:2915.0
now epsilon is 0.050482765359274824, the reward is 247.25 with loss [29.778063774108887, 29.04857063293457] in episode 2117
Report: 
rewardSum:247.25
loss:[29.778063774108887, 29.04857063293457]
policies:[1, 3, 0]
qAverage:[6.760867309570313, 31.50496826171875]
ws:[-4.1341876745223995, 1.414898157119751]
memory len:10000
memory used:2915.0
now epsilon is 0.05043230152179759, the reward is 247.25 with loss [25.109501600265503, 23.387660026550293] in episode 2118
Report: 
rewardSum:247.25
loss:[25.109501600265503, 23.387660026550293]
policies:[1, 3, 0]
qAverage:[6.836964416503906, 31.09810791015625]
ws:[-4.156945308297873, 1.0714182615280152]
memory len:10000
memory used:2915.0
now epsilon is 0.05036929265720474, the reward is 246.25 with loss [28.369369506835938, 23.40610957145691] in episode 2119
Report: 
rewardSum:246.25
loss:[28.369369506835938, 23.40610957145691]
policies:[0, 4, 1]
qAverage:[0.0, 39.346483612060545]
ws:[-4.2692051321268085, -0.11807377338409424]
memory len:10000
memory used:2915.0
now epsilon is 0.050318942249884396, the reward is 247.25 with loss [20.978139400482178, 26.847145557403564] in episode 2120
Report: 
rewardSum:247.25
loss:[20.978139400482178, 26.847145557403564]
policies:[0, 3, 1]
qAverage:[0.0, 43.594515800476074]
ws:[-4.735377460718155, 0.42590218782424927]
memory len:10000
memory used:2915.0
now epsilon is 0.050256075013549596, the reward is 246.25 with loss [27.06290578842163, 29.604877948760986] in episode 2121
Report: 
rewardSum:246.25
loss:[27.06290578842163, 29.604877948760986]
policies:[1, 3, 1]
qAverage:[7.8291473388671875, 33.12098846435547]
ws:[-3.6865153715014456, 0.41521828174591063]
memory len:10000
memory used:2915.0
now epsilon is 0.050205837781423374, the reward is 247.25 with loss [25.964855670928955, 22.32645320892334] in episode 2122
Report: 
rewardSum:247.25
loss:[25.964855670928955, 22.32645320892334]
policies:[1, 3, 0]
qAverage:[7.304750061035156, 34.71383819580078]
ws:[-4.116751027107239, -0.018909525871276856]
memory len:10000
memory used:2914.0
now epsilon is 0.050155650767693453, the reward is 247.25 with loss [29.95943021774292, 21.542691946029663] in episode 2123
Report: 
rewardSum:247.25
loss:[29.95943021774292, 21.542691946029663]
policies:[1, 3, 0]
qAverage:[7.508990478515625, 33.70311279296875]
ws:[-4.678545647859574, 0.062214231491088866]
memory len:10000
memory used:2914.0
now epsilon is 0.05010551392216027, the reward is 247.25 with loss [20.041536808013916, 17.984983444213867] in episode 2124
Report: 
rewardSum:247.25
loss:[20.041536808013916, 17.984983444213867]
policies:[1, 3, 0]
qAverage:[7.191707611083984, 34.89650192260742]
ws:[-4.206220078468323, 1.0061212539672852]
memory len:10000
memory used:2914.0
now epsilon is 0.050055427194674436, the reward is 247.25 with loss [25.278654098510742, 14.039782494306564] in episode 2125
Report: 
rewardSum:247.25
loss:[25.278654098510742, 14.039782494306564]
policies:[0, 3, 1]
qAverage:[0.0, 42.66211414337158]
ws:[-4.0691874623298645, 2.583046078681946]
memory len:10000
memory used:2914.0
now epsilon is 0.050005390535136696, the reward is 247.25 with loss [17.02639889717102, 32.020713567733765] in episode 2126
Report: 
rewardSum:247.25
loss:[17.02639889717102, 32.020713567733765]
policies:[1, 3, 0]
qAverage:[7.259183502197265, 34.56277008056641]
ws:[-1.96056227684021, 2.7455134868621824]
memory len:10000
memory used:2914.0
now epsilon is 0.049930429313763874, the reward is 245.25 with loss [30.104215145111084, 32.07005596160889] in episode 2127
Report: 
rewardSum:245.25
loss:[30.104215145111084, 32.07005596160889]
policies:[2, 3, 1]
qAverage:[18.015946197509766, 22.92324752807617]
ws:[0.49741082191467284, 2.763638949394226]
memory len:10000
memory used:2914.0
now epsilon is 0.049855580463970386, the reward is 245.25 with loss [43.76559257507324, 42.809035301208496] in episode 2128
Report: 
rewardSum:245.25
loss:[43.76559257507324, 42.809035301208496]
policies:[3, 2, 1]
qAverage:[16.016114044189454, 25.437422943115234]
ws:[-3.6513310194015505, -0.18525910377502441]
memory len:10000
memory used:2914.0
now epsilon is 0.04980574357623331, the reward is 247.25 with loss [21.35251021385193, 18.605425536632538] in episode 2129
Report: 
rewardSum:247.25
loss:[21.35251021385193, 18.605425536632538]
policies:[2, 2, 0]
qAverage:[9.376802444458008, 29.092390060424805]
ws:[-5.338462173938751, 0.3864363729953766]
memory len:10000
memory used:2915.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25*		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.049755956506698264, the reward is 247.25 with loss [28.536632537841797, 21.81054973602295] in episode 2130
Report: 
rewardSum:247.25
loss:[28.536632537841797, 21.81054973602295]
policies:[0, 4, 0]
qAverage:[0.0, 42.49538116455078]
ws:[-3.6683895111083986, 2.0676876306533813]
memory len:10000
memory used:2915.0
now epsilon is 0.0497310816381922, the reward is -1.0 with loss [15.390876770019531, 10.918710708618164] in episode 2131
Report: 
rewardSum:-1.0
loss:[15.390876770019531, 10.918710708618164]
policies:[0, 1, 1]
qAverage:[0.0, 20.75164222717285]
ws:[1.0040452480316162, 1.036341667175293]
memory len:10000
memory used:2915.0
now epsilon is 0.049681369202601626, the reward is 247.25 with loss [20.462445735931396, 20.686721801757812] in episode 2132
Report: 
rewardSum:247.25
loss:[20.462445735931396, 20.686721801757812]
policies:[0, 4, 0]
qAverage:[0.0, 47.95740966796875]
ws:[-3.701016592979431, 2.3916909217834474]
memory len:10000
memory used:2915.0
now epsilon is 0.049631706460807584, the reward is 247.25 with loss [26.689827919006348, 24.26870059967041] in episode 2133
Report: 
rewardSum:247.25
loss:[26.689827919006348, 24.26870059967041]
policies:[1, 3, 0]
qAverage:[8.383312225341797, 37.28354034423828]
ws:[-2.4350475311279296, 3.1860478043556215]
memory len:10000
memory used:2915.0
now epsilon is 0.04958209336313491, the reward is 247.25 with loss [23.465553283691406, 24.54800820350647] in episode 2134
Report: 
rewardSum:247.25
loss:[23.465553283691406, 24.54800820350647]
policies:[1, 3, 0]
qAverage:[9.73300552368164, 36.302345275878906]
ws:[-2.939898908138275, 2.870257318019867]
memory len:10000
memory used:2915.0
now epsilon is 0.04950776669081124, the reward is 245.25 with loss [35.30644774436951, 39.91681432723999] in episode 2135
Report: 
rewardSum:245.25
loss:[35.30644774436951, 39.91681432723999]
policies:[1, 4, 1]
qAverage:[9.818352381388346, 38.91680081685384]
ws:[1.2650146981080372, 4.112578233083089]
memory len:10000
memory used:2915.0
now epsilon is 0.04945827748643891, the reward is 247.25 with loss [26.70869493484497, 30.591954231262207] in episode 2136
Report: 
rewardSum:247.25
loss:[26.70869493484497, 30.591954231262207]
policies:[1, 3, 0]
qAverage:[9.915542602539062, 29.3944673538208]
ws:[0.7610550001263618, 2.356484889984131]
memory len:10000
memory used:2915.0
now epsilon is 0.04940883775271559, the reward is 247.25 with loss [27.6433687210083, 19.37848424911499] in episode 2137
Report: 
rewardSum:247.25
loss:[27.6433687210083, 19.37848424911499]
policies:[1, 3, 0]
qAverage:[7.7718650817871096, 39.04462661743164]
ws:[-3.4215627193450926, 1.807886815071106]
memory len:10000
memory used:2915.0
now epsilon is 0.04935944744018918, the reward is 247.25 with loss [26.868536472320557, 31.543758392333984] in episode 2138
Report: 
rewardSum:247.25
loss:[26.868536472320557, 31.543758392333984]
policies:[1, 3, 0]
qAverage:[7.992831420898438, 39.272761535644534]
ws:[-3.662648781388998, 2.561228892207146]
memory len:10000
memory used:2915.0
now epsilon is 0.04931010649945701, the reward is 247.25 with loss [22.26012349128723, 16.25857710838318] in episode 2139
Report: 
rewardSum:247.25
loss:[22.26012349128723, 16.25857710838318]
policies:[0, 4, 0]
qAverage:[0.0, 46.936547088623044]
ws:[-4.374337849020958, 1.287103166617453]
memory len:10000
memory used:2915.0
now epsilon is 0.04926081488116581, the reward is 247.25 with loss [22.39849281311035, 13.881976962089539] in episode 2140
Report: 
rewardSum:247.25
loss:[22.39849281311035, 13.881976962089539]
policies:[1, 3, 0]
qAverage:[0.0, 48.96798610687256]
ws:[-4.244402885437012, 2.7010806798934937]
memory len:10000
memory used:2915.0
now epsilon is 0.04921157253601162, the reward is 247.25 with loss [27.68019390106201, 21.742303609848022] in episode 2141
Report: 
rewardSum:247.25
loss:[27.68019390106201, 21.742303609848022]
policies:[0, 4, 0]
qAverage:[0.0, 48.80075931549072]
ws:[-2.7590395025908947, 2.6363500356674194]
memory len:10000
memory used:2915.0
now epsilon is 0.04916237941473978, the reward is 247.25 with loss [26.375117301940918, 20.993639945983887] in episode 2142
Report: 
rewardSum:247.25
loss:[26.375117301940918, 20.993639945983887]
policies:[1, 3, 0]
qAverage:[10.109130859375, 35.56134510040283]
ws:[-2.712165415287018, 1.4672932922840118]
memory len:10000
memory used:2915.0
now epsilon is 0.04911323546814487, the reward is 247.25 with loss [19.772297382354736, 23.309539318084717] in episode 2143
Report: 
rewardSum:247.25
loss:[19.772297382354736, 23.309539318084717]
policies:[1, 3, 0]
qAverage:[7.66990966796875, 40.01221466064453]
ws:[-2.8243784487247465, 1.0840296983718871]
memory len:10000
memory used:2915.0
now epsilon is 0.049064140647070646, the reward is 247.25 with loss [25.740378856658936, 29.888442993164062] in episode 2144
Report: 
rewardSum:247.25
loss:[25.740378856658936, 29.888442993164062]
policies:[1, 3, 0]
qAverage:[8.688467407226563, 44.762236785888675]
ws:[-2.9911558747291567, 1.5431712567806244]
memory len:10000
memory used:2915.0
now epsilon is 0.04901509490241001, the reward is 247.25 with loss [30.526506900787354, 18.230395078659058] in episode 2145
Report: 
rewardSum:247.25
loss:[30.526506900787354, 18.230395078659058]
policies:[1, 3, 0]
qAverage:[9.511774444580078, 42.392817687988284]
ws:[-3.464963948726654, 1.1707981169223785]
memory len:10000
memory used:2915.0
now epsilon is 0.04896609818510494, the reward is 247.25 with loss [27.913805961608887, 18.79747200012207] in episode 2146
Report: 
rewardSum:247.25
loss:[27.913805961608887, 18.79747200012207]
policies:[1, 3, 0]
qAverage:[0.0, 51.92878723144531]
ws:[-4.653959572315216, 1.5962937474250793]
memory len:10000
memory used:2915.0
now epsilon is 0.048917150446146467, the reward is 247.25 with loss [22.80408477783203, 25.71443462371826] in episode 2147
Report: 
rewardSum:247.25
loss:[22.80408477783203, 25.71443462371826]
policies:[0, 4, 0]
qAverage:[0.0, 52.29729995727539]
ws:[-3.5487302422523497, 0.9536417484283447]
memory len:10000
memory used:2915.0
now epsilon is 0.048868251636574614, the reward is 247.25 with loss [23.467130661010742, 23.05528473854065] in episode 2148
Report: 
rewardSum:247.25
loss:[23.467130661010742, 23.05528473854065]
policies:[0, 4, 0]
qAverage:[0.0, 51.22626724243164]
ws:[-2.186776340007782, 2.4334659576416016]
memory len:10000
memory used:2915.0
now epsilon is 0.04881940170747833, the reward is 247.25 with loss [19.57174062728882, 24.90682554244995] in episode 2149
Report: 
rewardSum:247.25
loss:[19.57174062728882, 24.90682554244995]
policies:[0, 4, 0]
qAverage:[0.0, 52.237395477294925]
ws:[-1.0816704034805298, 3.4325870752334593]
memory len:10000
memory used:2915.0
now epsilon is 0.048770600609995475, the reward is 247.25 with loss [25.797955751419067, 15.131460428237915] in episode 2150
Report: 
rewardSum:247.25
loss:[25.797955751419067, 15.131460428237915]
policies:[0, 4, 0]
qAverage:[0.0, 51.81639404296875]
ws:[-1.6371176958084106, 3.376988449692726]
memory len:10000
memory used:2915.0
now epsilon is 0.048721848295312745, the reward is 247.25 with loss [24.80920720100403, 33.138476848602295] in episode 2151
Report: 
rewardSum:247.25
loss:[24.80920720100403, 33.138476848602295]
policies:[0, 4, 0]
qAverage:[0.0, 52.18660049438476]
ws:[-2.2699934959411623, 4.324502301216126]
memory len:10000
memory used:2928.0
now epsilon is 0.04867314471466562, the reward is 247.25 with loss [20.930908679962158, 21.861878156661987] in episode 2152
Report: 
rewardSum:247.25
loss:[20.930908679962158, 21.861878156661987]
policies:[0, 4, 0]
qAverage:[0.0, 51.66166534423828]
ws:[-2.824043369293213, 2.3748479008674623]
memory len:10000
memory used:2928.0
now epsilon is 0.04862448981933835, the reward is 247.25 with loss [18.397460460662842, 16.98967170715332] in episode 2153
Report: 
rewardSum:247.25
loss:[18.397460460662842, 16.98967170715332]
policies:[0, 4, 0]
qAverage:[0.0, 52.49388046264649]
ws:[-1.586380958557129, 2.6301504015922545]
memory len:10000
memory used:2929.0
now epsilon is 0.048575883560663856, the reward is 247.25 with loss [25.562525749206543, 23.05280351638794] in episode 2154
Report: 
rewardSum:247.25
loss:[25.562525749206543, 23.05280351638794]
policies:[2, 2, 0]
qAverage:[0.0, 33.06314214070638]
ws:[1.7965235710144043, 4.499290068944295]
memory len:10000
memory used:2929.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26*		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.048527325890023724, the reward is 247.25 with loss [29.343467235565186, 25.785297870635986] in episode 2155
Report: 
rewardSum:247.25
loss:[29.343467235565186, 25.785297870635986]
policies:[0, 4, 0]
qAverage:[0.0, 52.33966979980469]
ws:[-0.5351948976516724, 3.5264972686767577]
memory len:10000
memory used:2916.0
############# STATE ###############
0*		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.048478816758848145, the reward is 247.25 with loss [27.60888147354126, 21.87104368209839] in episode 2156
Report: 
rewardSum:247.25
loss:[27.60888147354126, 21.87104368209839]
policies:[0, 3, 1]
qAverage:[0.0, 41.472679138183594]
ws:[1.3096399704615276, 2.8102943897247314]
memory len:10000
memory used:2916.0
now epsilon is 0.04843035611861585, the reward is 247.25 with loss [26.70761013031006, 22.487255096435547] in episode 2157
Report: 
rewardSum:247.25
loss:[26.70761013031006, 22.487255096435547]
policies:[1, 3, 0]
qAverage:[0.0, 57.60500717163086]
ws:[-1.742668628692627, 2.3548831045627594]
memory len:10000
memory used:2916.0
now epsilon is 0.04838194392085407, the reward is 247.25 with loss [21.624624490737915, 25.952382564544678] in episode 2158
Report: 
rewardSum:247.25
loss:[21.624624490737915, 25.952382564544678]
policies:[0, 4, 0]
qAverage:[0.0, 56.47063217163086]
ws:[-0.8040420055389405, 3.948724055290222]
memory len:10000
memory used:2916.0
now epsilon is 0.04833358011713851, the reward is 247.25 with loss [20.834320306777954, 18.583146572113037] in episode 2159
Report: 
rewardSum:247.25
loss:[20.834320306777954, 18.583146572113037]
policies:[0, 4, 0]
qAverage:[0.0, 58.77791290283203]
ws:[-0.6039856195449829, 4.210739469528198]
memory len:10000
memory used:2916.0
now epsilon is 0.04828526465909326, the reward is 247.25 with loss [18.62500524520874, 21.72732639312744] in episode 2160
Report: 
rewardSum:247.25
loss:[18.62500524520874, 21.72732639312744]
policies:[0, 4, 0]
qAverage:[0.0, 57.165975189208986]
ws:[-0.41998131275177003, 3.837009048461914]
memory len:10000
memory used:2916.0
now epsilon is 0.048236997498390784, the reward is 247.25 with loss [17.654649317264557, 25.665603160858154] in episode 2161
Report: 
rewardSum:247.25
loss:[17.654649317264557, 25.665603160858154]
policies:[0, 4, 0]
qAverage:[0.0, 61.28250312805176]
ws:[-0.3612523674964905, 4.393514633178711]
memory len:10000
memory used:2916.0
now epsilon is 0.04818877858675183, the reward is 247.25 with loss [29.789564609527588, 12.414064168930054] in episode 2162
Report: 
rewardSum:247.25
loss:[29.789564609527588, 12.414064168930054]
policies:[1, 3, 0]
qAverage:[9.786946105957032, 47.619692993164065]
ws:[0.2928394556045532, 3.8458945989608764]
memory len:10000
memory used:2916.0
now epsilon is 0.04814060787594544, the reward is 247.25 with loss [29.591715812683105, 27.266809463500977] in episode 2163
Report: 
rewardSum:247.25
loss:[29.591715812683105, 27.266809463500977]
policies:[1, 3, 0]
qAverage:[9.88637466430664, 48.63186798095703]
ws:[0.7270458102226257, 4.154879188537597]
memory len:10000
memory used:2916.0
now epsilon is 0.048092485317788855, the reward is 247.25 with loss [25.14082384109497, 22.950310707092285] in episode 2164
Report: 
rewardSum:247.25
loss:[25.14082384109497, 22.950310707092285]
policies:[2, 2, 0]
qAverage:[12.25741958618164, 43.983585357666016]
ws:[0.9296949505805969, 4.766734719276428]
memory len:10000
memory used:2916.0
now epsilon is 0.04804441086414747, the reward is 247.25 with loss [23.54173707962036, 20.3913254737854] in episode 2165
Report: 
rewardSum:247.25
loss:[23.54173707962036, 20.3913254737854]
policies:[1, 3, 0]
qAverage:[10.008737945556641, 47.67482986450195]
ws:[1.5025980472564697, 6.353563714027405]
memory len:10000
memory used:2916.0
now epsilon is 0.04799638446693481, the reward is 247.25 with loss [18.351003885269165, 23.533857345581055] in episode 2166
Report: 
rewardSum:247.25
loss:[18.351003885269165, 23.533857345581055]
policies:[1, 3, 0]
qAverage:[9.768722534179688, 47.4169319152832]
ws:[1.6506548762321471, 6.424678468704224]
memory len:10000
memory used:2916.0
now epsilon is 0.047924434871848796, the reward is 245.25 with loss [41.37097501754761, 45.25568628311157] in episode 2167
Report: 
rewardSum:245.25
loss:[41.37097501754761, 45.25568628311157]
policies:[1, 4, 1]
qAverage:[8.413824081420898, 47.29988924662272]
ws:[0.9295321404933929, 4.614322900772095]
memory len:10000
memory used:2916.0
now epsilon is 0.04787652840564494, the reward is 247.25 with loss [27.141537189483643, 29.13035488128662] in episode 2168
Report: 
rewardSum:247.25
loss:[27.141537189483643, 29.13035488128662]
policies:[1, 3, 0]
qAverage:[10.127671051025391, 50.42385559082031]
ws:[-0.3858227252960205, 3.5986462116241453]
memory len:10000
memory used:2916.0
now epsilon is 0.04782866982794536, the reward is 247.25 with loss [15.506000280380249, 25.564640522003174] in episode 2169
Report: 
rewardSum:247.25
loss:[15.506000280380249, 25.564640522003174]
policies:[1, 3, 0]
qAverage:[11.75533218383789, 48.88291473388672]
ws:[-0.9910233184695244, 3.0067863345146177]
memory len:10000
memory used:2916.0
now epsilon is 0.0477808590908795, the reward is 247.25 with loss [22.355549812316895, 21.934574127197266] in episode 2170
Report: 
rewardSum:247.25
loss:[22.355549812316895, 21.934574127197266]
policies:[0, 4, 0]
qAverage:[0.0, 50.00816249847412]
ws:[0.6280221790075302, 3.4139728248119354]
memory len:10000
memory used:2916.0
now epsilon is 0.047733096146624666, the reward is 247.25 with loss [31.876220226287842, 25.255414843559265] in episode 2171
Report: 
rewardSum:247.25
loss:[31.876220226287842, 25.255414843559265]
policies:[0, 4, 0]
qAverage:[0.0, 62.968224334716794]
ws:[-0.7748985648155212, 4.6894042015075685]
memory len:10000
memory used:2916.0
now epsilon is 0.04768538094740597, the reward is 247.25 with loss [11.515297889709473, 28.92634677886963] in episode 2172
Report: 
rewardSum:247.25
loss:[11.515297889709473, 28.92634677886963]
policies:[0, 4, 0]
qAverage:[0.0, 59.695413970947264]
ws:[-1.3989671409130096, 3.900163006782532]
memory len:10000
memory used:2916.0
now epsilon is 0.04763771344549628, the reward is 37.15999999999997 with loss [20.64247477054596, 17.613064289093018] in episode 2173
Report: 
rewardSum:37.15999999999997
loss:[20.64247477054596, 17.613064289093018]
policies:[0, 3, 1]
qAverage:[0.0, 51.52991008758545]
ws:[0.8313236013054848, 4.12380838394165]
memory len:10000
memory used:2916.0
now epsilon is 0.04759009359321616, the reward is 247.25 with loss [25.22653341293335, 18.542038679122925] in episode 2174
Report: 
rewardSum:247.25
loss:[25.22653341293335, 18.542038679122925]
policies:[0, 4, 0]
qAverage:[0.0, 61.26151657104492]
ws:[0.2611034631729126, 7.885512924194336]
memory len:10000
memory used:2916.0
now epsilon is 0.047542521342933845, the reward is 247.25 with loss [21.20435070991516, 22.39421820640564] in episode 2175
Report: 
rewardSum:247.25
loss:[21.20435070991516, 22.39421820640564]
policies:[0, 4, 0]
qAverage:[0.0, 61.050236511230466]
ws:[0.5436005115509033, 7.457472133636474]
memory len:10000
memory used:2916.0
now epsilon is 0.0474949966470652, the reward is 247.25 with loss [22.06340456008911, 20.113107919692993] in episode 2176
Report: 
rewardSum:247.25
loss:[22.06340456008911, 20.113107919692993]
policies:[1, 3, 0]
qAverage:[0.0, 55.62043476104736]
ws:[-0.4877822697162628, 4.956294238567352]
memory len:10000
memory used:2916.0
now epsilon is 0.04742379866381456, the reward is 245.25 with loss [33.953959703445435, 31.409661531448364] in episode 2177
Report: 
rewardSum:245.25
loss:[33.953959703445435, 31.409661531448364]
policies:[1, 4, 1]
qAverage:[9.064175923665365, 51.019692738850914]
ws:[-0.1432659775018692, 4.886143604914348]
memory len:10000
memory used:2917.0
now epsilon is 0.04737639264611144, the reward is 247.25 with loss [20.423014402389526, 23.178064823150635] in episode 2178
Report: 
rewardSum:247.25
loss:[20.423014402389526, 23.178064823150635]
policies:[1, 3, 0]
qAverage:[10.905329132080078, 50.48701629638672]
ws:[-0.31359039545059203, 3.7313249826431276]
memory len:10000
memory used:2917.0
now epsilon is 0.04732903401665174, the reward is 247.25 with loss [29.74205732345581, 19.8841655254364] in episode 2179
Report: 
rewardSum:247.25
loss:[29.74205732345581, 19.8841655254364]
policies:[1, 3, 0]
qAverage:[11.259683990478516, 49.423884582519534]
ws:[-0.5798864364624023, 4.082483494281769]
memory len:10000
memory used:2917.0
############# STATE ###############
0-		10*		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.047281722728064975, the reward is 247.25 with loss [27.140265464782715, 23.494070529937744] in episode 2180
Report: 
rewardSum:247.25
loss:[27.140265464782715, 23.494070529937744]
policies:[0, 4, 0]
qAverage:[0.0, 66.18569374084473]
ws:[-1.6895528882741928, 3.254947543144226]
memory len:10000
memory used:2917.0
now epsilon is 0.04723445873302802, the reward is 247.25 with loss [24.526368618011475, 28.88627290725708] in episode 2181
Report: 
rewardSum:247.25
loss:[24.526368618011475, 28.88627290725708]
policies:[0, 4, 0]
qAverage:[0.0, 61.59764862060547]
ws:[-3.8256086657444635, 2.1991043090820312]
memory len:10000
memory used:2917.0
now epsilon is 0.047187241984265056, the reward is 247.25 with loss [18.88116216659546, 35.28623008728027] in episode 2182
Report: 
rewardSum:247.25
loss:[18.88116216659546, 35.28623008728027]
policies:[1, 3, 0]
qAverage:[12.314225006103516, 50.739208984375]
ws:[-1.4357712030410767, 4.147336459159851]
memory len:10000
memory used:2917.0
now epsilon is 0.04712828741643889, the reward is 246.25 with loss [31.88521146774292, 28.08206272125244] in episode 2183
Report: 
rewardSum:246.25
loss:[31.88521146774292, 28.08206272125244]
policies:[2, 2, 1]
qAverage:[14.404581069946289, 40.86766052246094]
ws:[1.4766764342784882, 4.343884527683258]
memory len:10000
memory used:2916.0
now epsilon is 0.0470811767991849, the reward is 247.25 with loss [14.71095323562622, 23.01476764678955] in episode 2184
Report: 
rewardSum:247.25
loss:[14.71095323562622, 23.01476764678955]
policies:[1, 3, 0]
qAverage:[12.54819564819336, 51.7495834350586]
ws:[-0.17033782005310058, 5.523921775817871]
memory len:10000
memory used:2916.0
now epsilon is 0.047034113274884624, the reward is 247.25 with loss [31.043920516967773, 22.45084857940674] in episode 2185
Report: 
rewardSum:247.25
loss:[31.043920516967773, 22.45084857940674]
policies:[0, 4, 0]
qAverage:[0.0, 65.19095458984376]
ws:[0.007421469688415528, 6.051708984375]
memory len:10000
memory used:2917.0
now epsilon is 0.04698709679646277, the reward is 247.25 with loss [20.481229305267334, 26.5913565158844] in episode 2186
Report: 
rewardSum:247.25
loss:[20.481229305267334, 26.5913565158844]
policies:[0, 4, 0]
qAverage:[0.0, 64.29661636352539]
ws:[-0.06656782627105713, 6.278912591934204]
memory len:10000
memory used:2917.0
now epsilon is 0.0469401273168911, the reward is 247.25 with loss [23.978420734405518, 23.96958589553833] in episode 2187
Report: 
rewardSum:247.25
loss:[23.978420734405518, 23.96958589553833]
policies:[0, 4, 0]
qAverage:[0.0, 63.0543327331543]
ws:[-0.3799367189407349, 5.122540712356567]
memory len:10000
memory used:2917.0
now epsilon is 0.04689320478918838, the reward is 247.25 with loss [17.89343762397766, 28.650543928146362] in episode 2188
Report: 
rewardSum:247.25
loss:[17.89343762397766, 28.650543928146362]
policies:[0, 4, 0]
qAverage:[0.0, 63.194905281066895]
ws:[-1.4896365255117416, 3.8522847294807434]
memory len:10000
memory used:2917.0
now epsilon is 0.046846329166420356, the reward is 247.25 with loss [20.58987808227539, 26.136568069458008] in episode 2189
Report: 
rewardSum:247.25
loss:[20.58987808227539, 26.136568069458008]
policies:[0, 4, 0]
qAverage:[0.0, 64.17453384399414]
ws:[-1.6483162924647332, 3.2989024758338927]
memory len:10000
memory used:2917.0
now epsilon is 0.04679950040169967, the reward is 247.25 with loss [22.871315240859985, 25.96979284286499] in episode 2190
Report: 
rewardSum:247.25
loss:[22.871315240859985, 25.96979284286499]
policies:[1, 3, 0]
qAverage:[12.519075775146485, 51.76612854003906]
ws:[-2.0766760770231487, 2.394453336298466]
memory len:10000
memory used:2917.0
now epsilon is 0.04675271844818584, the reward is 247.25 with loss [29.274993658065796, 24.284363269805908] in episode 2191
Report: 
rewardSum:247.25
loss:[29.274993658065796, 24.284363269805908]
policies:[1, 3, 0]
qAverage:[12.295437622070313, 51.64207153320312]
ws:[-1.3046701669692993, 3.5032768875360487]
memory len:10000
memory used:2917.0
now epsilon is 0.04670598325908521, the reward is 247.25 with loss [28.24310541152954, 21.13212537765503] in episode 2192
Report: 
rewardSum:247.25
loss:[28.24310541152954, 21.13212537765503]
policies:[1, 3, 0]
qAverage:[0.0, 67.4907054901123]
ws:[-1.079030692577362, 4.068251937627792]
memory len:10000
memory used:2916.0
now epsilon is 0.04665929478765091, the reward is 247.25 with loss [21.18709897994995, 26.202348232269287] in episode 2193
Report: 
rewardSum:247.25
loss:[21.18709897994995, 26.202348232269287]
policies:[1, 3, 0]
qAverage:[13.947769165039062, 54.490499877929686]
ws:[-0.5083335638046265, 4.15743328332901]
memory len:10000
memory used:2916.0
now epsilon is 0.046612652987182784, the reward is 247.25 with loss [19.474797010421753, 22.925452709197998] in episode 2194
Report: 
rewardSum:247.25
loss:[19.474797010421753, 22.925452709197998]
policies:[1, 3, 0]
qAverage:[13.123532104492188, 55.49330596923828]
ws:[-0.4268938899040222, 4.2191307783126835]
memory len:10000
memory used:2916.0
now epsilon is 0.04656605781102737, the reward is 247.25 with loss [35.41169834136963, 27.251529216766357] in episode 2195
Report: 
rewardSum:247.25
loss:[35.41169834136963, 27.251529216766357]
policies:[1, 3, 0]
qAverage:[13.693405151367188, 54.4734390258789]
ws:[-0.6865996599197388, 4.079003763198853]
memory len:10000
memory used:2916.0
now epsilon is 0.04651950921257783, the reward is 247.25 with loss [22.83675527572632, 32.106236696243286] in episode 2196
Report: 
rewardSum:247.25
loss:[22.83675527572632, 32.106236696243286]
policies:[1, 3, 0]
qAverage:[13.111705017089843, 55.0328125]
ws:[-0.7209651947021485, 4.239016151428222]
memory len:10000
memory used:2916.0
now epsilon is 0.04647300714527392, the reward is 247.25 with loss [24.802520751953125, 26.36437177658081] in episode 2197
Report: 
rewardSum:247.25
loss:[24.802520751953125, 26.36437177658081]
policies:[1, 3, 0]
qAverage:[0.0, 68.93190574645996]
ws:[-1.2048503160476685, 5.798265337944031]
memory len:10000
memory used:2916.0
now epsilon is 0.04641494492471131, the reward is 246.25 with loss [20.21101713180542, 28.470123767852783] in episode 2198
Report: 
rewardSum:246.25
loss:[20.21101713180542, 28.470123767852783]
policies:[0, 4, 1]
qAverage:[0.0, 69.25319976806641]
ws:[-1.0583621740341187, 4.560218763351441]
memory len:10000
memory used:2916.0
now epsilon is 0.046368547382490195, the reward is 247.25 with loss [25.56065320968628, 25.020413875579834] in episode 2199
Report: 
rewardSum:247.25
loss:[25.56065320968628, 25.020413875579834]
policies:[2, 2, 0]
qAverage:[16.906003952026367, 41.47976303100586]
ws:[1.4120388329029083, 4.517442882061005]
memory len:10000
memory used:2916.0
now epsilon is 0.04629903801744219, the reward is 245.25 with loss [42.63340950012207, 30.765729904174805] in episode 2200
Report: 
rewardSum:245.25
loss:[42.63340950012207, 30.765729904174805]
policies:[0, 5, 1]
qAverage:[0.0, 72.9816385904948]
ws:[1.1277020201086998, 5.43803612391154]
memory len:10000
memory used:2916.0
now epsilon is 0.046229632851298434, the reward is 245.25 with loss [38.383358001708984, 38.965461015701294] in episode 2201
Report: 
rewardSum:245.25
loss:[38.383358001708984, 38.965461015701294]
policies:[1, 4, 1]
qAverage:[10.762587229410807, 58.224212646484375]
ws:[-0.15676998595396677, 5.840809623400371]
memory len:10000
memory used:2915.0
now epsilon is 0.046183420551670296, the reward is 247.25 with loss [20.819780588150024, 22.62924814224243] in episode 2202
Report: 
rewardSum:247.25
loss:[20.819780588150024, 22.62924814224243]
policies:[1, 3, 0]
qAverage:[13.103823852539062, 55.22706909179688]
ws:[0.1810682773590088, 6.325124645233155]
memory len:10000
memory used:2915.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23*		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.04613725444701506, the reward is 247.25 with loss [26.88353204727173, 20.69574499130249] in episode 2203
Report: 
rewardSum:247.25
loss:[26.88353204727173, 20.69574499130249]
policies:[0, 4, 0]
qAverage:[0.0, 74.15056762695312]
ws:[-0.27845990657806396, 4.8794461488723755]
memory len:10000
memory used:2915.0
now epsilon is 0.04609113449115506, the reward is 247.25 with loss [27.656936407089233, 26.53103494644165] in episode 2204
Report: 
rewardSum:247.25
loss:[27.656936407089233, 26.53103494644165]
policies:[0, 4, 0]
qAverage:[0.0, 68.5744514465332]
ws:[-0.08866466581821442, 5.7572140246629715]
memory len:10000
memory used:2915.0
now epsilon is 0.04604506063795883, the reward is 247.25 with loss [24.11578130722046, 29.711101531982422] in episode 2205
Report: 
rewardSum:247.25
loss:[24.11578130722046, 29.711101531982422]
policies:[0, 4, 0]
qAverage:[0.0, 72.00578002929687]
ws:[1.2185595750808715, 6.553753995895386]
memory len:10000
memory used:2915.0
now epsilon is 0.04599903284134098, the reward is 247.25 with loss [23.280250072479248, 24.347206592559814] in episode 2206
Report: 
rewardSum:247.25
loss:[23.280250072479248, 24.347206592559814]
policies:[0, 4, 0]
qAverage:[0.0, 76.41669616699218]
ws:[1.1123043060302735, 6.219819211959839]
memory len:10000
memory used:2915.0
now epsilon is 0.0459530510552622, the reward is 247.25 with loss [29.89412784576416, 28.09363317489624] in episode 2207
Report: 
rewardSum:247.25
loss:[29.89412784576416, 28.09363317489624]
policies:[0, 4, 0]
qAverage:[0.0, 72.69469757080078]
ws:[1.2606136560440064, 6.356233215332031]
memory len:10000
memory used:2915.0
now epsilon is 0.045907115233729194, the reward is 247.25 with loss [25.32844829559326, 18.909769296646118] in episode 2208
Report: 
rewardSum:247.25
loss:[25.32844829559326, 18.909769296646118]
policies:[0, 4, 0]
qAverage:[0.0, 75.32388000488281]
ws:[1.0740455269813538, 5.879692411422729]
memory len:10000
memory used:2915.0
now epsilon is 0.04586122533079467, the reward is 247.25 with loss [27.45207166671753, 31.22983694076538] in episode 2209
Report: 
rewardSum:247.25
loss:[27.45207166671753, 31.22983694076538]
policies:[0, 4, 0]
qAverage:[0.0, 73.25409545898438]
ws:[1.3415543675422668, 6.066915559768677]
memory len:10000
memory used:2915.0
now epsilon is 0.045803927455232096, the reward is 246.25 with loss [24.819850206375122, 39.67672300338745] in episode 2210
Report: 
rewardSum:246.25
loss:[24.819850206375122, 39.67672300338745]
policies:[1, 3, 1]
qAverage:[0.0, 76.04444313049316]
ws:[1.1892302930355072, 7.439021587371826]
memory len:10000
memory used:2916.0
now epsilon is 0.0457581407013871, the reward is 247.25 with loss [31.300939083099365, 22.909644603729248] in episode 2211
Report: 
rewardSum:247.25
loss:[31.300939083099365, 22.909644603729248]
policies:[1, 3, 0]
qAverage:[14.008302307128906, 60.629006958007814]
ws:[0.1938704490661621, 4.094311916828156]
memory len:10000
memory used:2915.0
now epsilon is 0.045712399717128774, the reward is 247.25 with loss [24.522194385528564, 28.97932243347168] in episode 2212
Report: 
rewardSum:247.25
loss:[24.522194385528564, 28.97932243347168]
policies:[0, 4, 0]
qAverage:[0.0, 74.57637939453124]
ws:[0.5159144997596741, 5.254851198196411]
memory len:10000
memory used:2915.0
now epsilon is 0.0456667044567047, the reward is 247.25 with loss [20.28565740585327, 16.155458688735962] in episode 2213
Report: 
rewardSum:247.25
loss:[20.28565740585327, 16.155458688735962]
policies:[1, 3, 0]
qAverage:[17.53521156311035, 46.49650192260742]
ws:[0.46908800303936005, 2.7837598472833633]
memory len:10000
memory used:2915.0
now epsilon is 0.04562105487440818, the reward is 247.25 with loss [16.34069848060608, 17.49335026741028] in episode 2214
Report: 
rewardSum:247.25
loss:[16.34069848060608, 17.49335026741028]
policies:[1, 3, 0]
qAverage:[14.418299865722656, 59.43875579833984]
ws:[0.7735605239868164, 5.296934676170349]
memory len:10000
memory used:2915.0
now epsilon is 0.04557545092457822, the reward is 247.25 with loss [37.56882953643799, 18.464794874191284] in episode 2215
Report: 
rewardSum:247.25
loss:[37.56882953643799, 18.464794874191284]
policies:[1, 3, 0]
qAverage:[14.265046691894531, 63.07766876220703]
ws:[0.3493713021278381, 4.395040309429168]
memory len:10000
memory used:2915.0
now epsilon is 0.04552989256159946, the reward is 247.25 with loss [18.294893741607666, 23.19665288925171] in episode 2216
Report: 
rewardSum:247.25
loss:[18.294893741607666, 23.19665288925171]
policies:[0, 4, 0]
qAverage:[0.0, 81.96234741210938]
ws:[0.42797303199768066, 5.592194676399231]
memory len:10000
memory used:2915.0
now epsilon is 0.04548437973990213, the reward is 247.25 with loss [28.34697914123535, 24.42788338661194] in episode 2217
Report: 
rewardSum:247.25
loss:[28.34697914123535, 24.42788338661194]
policies:[0, 4, 0]
qAverage:[0.0, 78.34535064697266]
ws:[0.7490637063980102, 5.844513320922852]
memory len:10000
memory used:2915.0
now epsilon is 0.04543891241396204, the reward is 247.25 with loss [36.59842300415039, 15.960935160517693] in episode 2218
Report: 
rewardSum:247.25
loss:[36.59842300415039, 15.960935160517693]
policies:[0, 4, 0]
qAverage:[0.0, 78.75460433959961]
ws:[1.447156798094511, 6.732848942279816]
memory len:10000
memory used:2916.0
now epsilon is 0.04539349053830049, the reward is 247.25 with loss [23.705158710479736, 30.190998554229736] in episode 2219
Report: 
rewardSum:247.25
loss:[23.705158710479736, 30.190998554229736]
policies:[0, 4, 0]
qAverage:[0.0, 78.6226318359375]
ws:[2.112116074562073, 8.324883604049683]
memory len:10000
memory used:2916.0
now epsilon is 0.04534811406748423, the reward is 247.25 with loss [28.372695446014404, 41.529483795166016] in episode 2220
Report: 
rewardSum:247.25
loss:[28.372695446014404, 41.529483795166016]
policies:[0, 4, 0]
qAverage:[0.0, 81.01878509521484]
ws:[2.2011959075927736, 8.212642478942872]
memory len:10000
memory used:2916.0
now epsilon is 0.04530278295612545, the reward is 247.25 with loss [25.910574436187744, 25.66987657546997] in episode 2221
Report: 
rewardSum:247.25
loss:[25.910574436187744, 25.66987657546997]
policies:[0, 4, 0]
qAverage:[0.0, 79.22379150390626]
ws:[1.7923244476318358, 8.560348796844483]
memory len:10000
memory used:2916.0
now epsilon is 0.0452574971588817, the reward is 247.25 with loss [32.92577934265137, 26.688557624816895] in episode 2222
Report: 
rewardSum:247.25
loss:[32.92577934265137, 26.688557624816895]
policies:[0, 4, 0]
qAverage:[0.0, 77.73866081237793]
ws:[1.6563876569271088, 8.58713686466217]
memory len:10000
memory used:2916.0
now epsilon is 0.04521225663045584, the reward is 247.25 with loss [31.26950168609619, 26.590818405151367] in episode 2223
Report: 
rewardSum:247.25
loss:[31.26950168609619, 26.590818405151367]
policies:[0, 4, 0]
qAverage:[0.0, 78.6285400390625]
ws:[1.577640426158905, 8.557554721832275]
memory len:10000
memory used:2916.0
now epsilon is 0.04516706132559603, the reward is 247.25 with loss [22.312504291534424, 31.31570053100586] in episode 2224
Report: 
rewardSum:247.25
loss:[22.312504291534424, 31.31570053100586]
policies:[0, 4, 0]
qAverage:[0.0, 81.06509857177734]
ws:[1.5230698108673095, 9.563878774642944]
memory len:10000
memory used:2916.0
now epsilon is 0.045121911199095664, the reward is 247.25 with loss [22.942896366119385, 21.684500217437744] in episode 2225
Report: 
rewardSum:247.25
loss:[22.942896366119385, 21.684500217437744]
policies:[0, 4, 0]
qAverage:[0.0, 78.58988189697266]
ws:[1.1963922441005708, 8.52525062561035]
memory len:10000
memory used:2916.0
now epsilon is 0.04507680620579333, the reward is 247.25 with loss [31.91088056564331, 27.626335620880127] in episode 2226
Report: 
rewardSum:247.25
loss:[31.91088056564331, 27.626335620880127]
policies:[1, 3, 0]
qAverage:[0.0, 81.6171875]
ws:[1.0033958032727242, 9.484713315963745]
memory len:10000
memory used:2916.0
now epsilon is 0.045031746300572745, the reward is 247.25 with loss [26.810926914215088, 23.137449741363525] in episode 2227
Report: 
rewardSum:247.25
loss:[26.810926914215088, 23.137449741363525]
policies:[0, 4, 0]
qAverage:[0.0, 78.8613265991211]
ws:[0.5025386422872543, 7.262952756881714]
memory len:10000
memory used:2916.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21*		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.04498673143836272, the reward is 247.25 with loss [22.0857834815979, 31.758281230926514] in episode 2228
Report: 
rewardSum:247.25
loss:[22.0857834815979, 31.758281230926514]
policies:[0, 4, 0]
qAverage:[0.0, 86.25907897949219]
ws:[0.42372823804616927, 7.547505283355713]
memory len:10000
memory used:2916.0
now epsilon is 0.04494176157413716, the reward is 247.25 with loss [30.03606939315796, 31.996203422546387] in episode 2229
Report: 
rewardSum:247.25
loss:[30.03606939315796, 31.996203422546387]
policies:[1, 3, 0]
qAverage:[0.0, 77.11973762512207]
ws:[1.112045556306839, 8.098274230957031]
memory len:10000
memory used:2916.0
now epsilon is 0.044896836662914934, the reward is 247.25 with loss [21.32423162460327, 30.706536769866943] in episode 2230
Report: 
rewardSum:247.25
loss:[21.32423162460327, 30.706536769866943]
policies:[0, 4, 0]
qAverage:[0.0, 83.57059478759766]
ws:[2.3084993720054627, 11.18121829032898]
memory len:10000
memory used:2916.0
now epsilon is 0.04485195665975989, the reward is 247.25 with loss [39.57006502151489, 25.97130274772644] in episode 2231
Report: 
rewardSum:247.25
loss:[39.57006502151489, 25.97130274772644]
policies:[0, 4, 0]
qAverage:[0.0, 83.17440948486328]
ws:[2.0011298179626467, 10.803147125244141]
memory len:10000
memory used:2916.0
now epsilon is 0.04480712151978081, the reward is 247.25 with loss [21.78541135787964, 22.483190059661865] in episode 2232
Report: 
rewardSum:247.25
loss:[21.78541135787964, 22.483190059661865]
policies:[0, 4, 0]
qAverage:[0.0, 84.94283676147461]
ws:[1.7420299351215363, 11.257160425186157]
memory len:10000
memory used:2915.0
now epsilon is 0.04476233119813133, the reward is 247.25 with loss [25.1710262298584, 27.3813419342041] in episode 2233
Report: 
rewardSum:247.25
loss:[25.1710262298584, 27.3813419342041]
policies:[0, 4, 0]
qAverage:[0.0, 83.76289825439453]
ws:[1.671121859550476, 9.229902958869934]
memory len:10000
memory used:2915.0
now epsilon is 0.04471758565000993, the reward is 247.25 with loss [27.05852460861206, 30.412217617034912] in episode 2234
Report: 
rewardSum:247.25
loss:[27.05852460861206, 30.412217617034912]
policies:[1, 3, 0]
qAverage:[16.486846923828125, 68.23627471923828]
ws:[1.0204046010971068, 7.894385686516761]
memory len:10000
memory used:2915.0
now epsilon is 0.04467288483065987, the reward is 247.25 with loss [26.11013126373291, 24.463032722473145] in episode 2235
Report: 
rewardSum:247.25
loss:[26.11013126373291, 24.463032722473145]
policies:[1, 3, 0]
qAverage:[16.96577606201172, 66.41327209472657]
ws:[0.18915721774101257, 6.710801005363464]
memory len:10000
memory used:2915.0
now epsilon is 0.04462822869536915, the reward is 247.25 with loss [35.235429763793945, 27.377620697021484] in episode 2236
Report: 
rewardSum:247.25
loss:[35.235429763793945, 27.377620697021484]
policies:[1, 3, 0]
qAverage:[16.30816650390625, 69.32547454833984]
ws:[0.27663769125938414, 6.30782125890255]
memory len:10000
memory used:2915.0
now epsilon is 0.04458361719947045, the reward is 247.25 with loss [24.333854913711548, 29.01637029647827] in episode 2237
Report: 
rewardSum:247.25
loss:[24.333854913711548, 29.01637029647827]
policies:[1, 3, 0]
qAverage:[17.20734405517578, 66.3683074951172]
ws:[0.5460560083389282, 6.553546988964081]
memory len:10000
memory used:2915.0
now epsilon is 0.04453905029834113, the reward is 247.25 with loss [22.224348068237305, 19.329832553863525] in episode 2238
Report: 
rewardSum:247.25
loss:[22.224348068237305, 19.329832553863525]
policies:[0, 3, 1]
qAverage:[0.0, 82.43539047241211]
ws:[0.20656287670135498, 5.381020724773407]
memory len:10000
memory used:2915.0
now epsilon is 0.04449452794740314, the reward is 247.25 with loss [24.209067821502686, 25.293989181518555] in episode 2239
Report: 
rewardSum:247.25
loss:[24.209067821502686, 25.293989181518555]
policies:[1, 3, 0]
qAverage:[16.887432861328126, 67.19417877197266]
ws:[0.35528987646102905, 5.707613372802735]
memory len:10000
memory used:2915.0
now epsilon is 0.044450050102122995, the reward is 247.25 with loss [31.774338245391846, 17.8607896566391] in episode 2240
Report: 
rewardSum:247.25
loss:[31.774338245391846, 17.8607896566391]
policies:[1, 3, 0]
qAverage:[16.619830322265624, 72.23350982666015]
ws:[0.2697774648666382, 5.24050914645195]
memory len:10000
memory used:2915.0
now epsilon is 0.044405616718011715, the reward is 247.25 with loss [19.663001537322998, 21.663211345672607] in episode 2241
Report: 
rewardSum:247.25
loss:[19.663001537322998, 21.663211345672607]
policies:[1, 3, 0]
qAverage:[18.448902893066407, 74.05740661621094]
ws:[0.5922426760196686, 5.479337164759636]
memory len:10000
memory used:2915.0
now epsilon is 0.044339049909326224, the reward is 245.25 with loss [34.89662981033325, 41.37984228134155] in episode 2242
Report: 
rewardSum:245.25
loss:[34.89662981033325, 41.37984228134155]
policies:[0, 5, 1]
qAverage:[0.0, 96.12635803222656]
ws:[2.4621396561463675, 6.544923543930054]
memory len:10000
memory used:2915.0
now epsilon is 0.0442947274837896, the reward is 247.25 with loss [24.20503854751587, 23.093926906585693] in episode 2243
Report: 
rewardSum:247.25
loss:[24.20503854751587, 23.093926906585693]
policies:[1, 3, 0]
qAverage:[0.0, 91.5667896270752]
ws:[2.531632959842682, 9.602378845214844]
memory len:10000
memory used:2915.0
now epsilon is 0.04425044936406037, the reward is 247.25 with loss [34.426161766052246, 19.56020498275757] in episode 2244
Report: 
rewardSum:247.25
loss:[34.426161766052246, 19.56020498275757]
policies:[0, 4, 0]
qAverage:[0.0, 91.6982406616211]
ws:[2.8770877599716185, 9.09346046447754]
memory len:10000
memory used:2916.0
now epsilon is 0.04420621550584935, the reward is 247.25 with loss [22.101871013641357, 30.478357791900635] in episode 2245
Report: 
rewardSum:247.25
loss:[22.101871013641357, 30.478357791900635]
policies:[0, 4, 0]
qAverage:[0.0, 91.30603332519532]
ws:[2.547133684158325, 7.959194803237915]
memory len:10000
memory used:2916.0
now epsilon is 0.04416202586491161, the reward is 247.25 with loss [24.826987266540527, 24.04263162612915] in episode 2246
Report: 
rewardSum:247.25
loss:[24.826987266540527, 24.04263162612915]
policies:[0, 4, 0]
qAverage:[0.0, 90.49923248291016]
ws:[2.6610570430755613, 8.06005027294159]
memory len:10000
memory used:2916.0
now epsilon is 0.04411788039704644, the reward is 247.25 with loss [22.750678539276123, 17.688233375549316] in episode 2247
Report: 
rewardSum:247.25
loss:[22.750678539276123, 17.688233375549316]
policies:[0, 4, 0]
qAverage:[0.0, 91.1409408569336]
ws:[1.9599680811166764, 6.275986182689667]
memory len:10000
memory used:2916.0
now epsilon is 0.04407377905809735, the reward is 247.25 with loss [26.51756262779236, 21.864425659179688] in episode 2248
Report: 
rewardSum:247.25
loss:[26.51756262779236, 21.864425659179688]
policies:[0, 4, 0]
qAverage:[0.0, 91.05967559814454]
ws:[1.798137938976288, 5.5074318766593935]
memory len:10000
memory used:2916.0
now epsilon is 0.04402972180395196, the reward is 247.25 with loss [32.57991361618042, 27.290736198425293] in episode 2249
Report: 
rewardSum:247.25
loss:[32.57991361618042, 27.290736198425293]
policies:[1, 3, 0]
qAverage:[0.0, 82.50765991210938]
ws:[2.197028636932373, 5.032098866999149]
memory len:10000
memory used:2916.0
now epsilon is 0.04398570859054201, the reward is 247.25 with loss [20.66853380203247, 23.359504461288452] in episode 2250
Report: 
rewardSum:247.25
loss:[20.66853380203247, 23.359504461288452]
policies:[0, 4, 0]
qAverage:[0.0, 75.95682271321614]
ws:[3.0567121505737305, 5.080413977305095]
memory len:10000
memory used:2916.0
now epsilon is 0.043941739373843255, the reward is 247.25 with loss [29.83951759338379, 18.22255229949951] in episode 2251
Report: 
rewardSum:247.25
loss:[29.83951759338379, 18.22255229949951]
policies:[0, 4, 0]
qAverage:[0.0, 91.2053207397461]
ws:[1.933378279209137, 4.498873007297516]
memory len:10000
memory used:2916.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25*		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.0438978141098755, the reward is 247.25 with loss [25.832037448883057, 29.861059188842773] in episode 2252
Report: 
rewardSum:247.25
loss:[25.832037448883057, 29.861059188842773]
policies:[0, 4, 0]
qAverage:[0.0, 92.1394546508789]
ws:[1.5897818446159362, 4.866201639175415]
memory len:10000
memory used:2916.0
now epsilon is 0.04385393275470247, the reward is 247.25 with loss [38.39611339569092, 32.50495529174805] in episode 2253
Report: 
rewardSum:247.25
loss:[38.39611339569092, 32.50495529174805]
policies:[0, 4, 0]
qAverage:[0.0, 98.7470916748047]
ws:[1.451077950000763, 4.915101335942746]
memory len:10000
memory used:2916.0
now epsilon is 0.04381009526443186, the reward is 247.25 with loss [24.92341661453247, 23.467182159423828] in episode 2254
Report: 
rewardSum:247.25
loss:[24.92341661453247, 23.467182159423828]
policies:[0, 4, 0]
qAverage:[0.0, 93.84904632568359]
ws:[2.7889486238360406, 6.663039684295654]
memory len:10000
memory used:2916.0
now epsilon is 0.0437663015952152, the reward is 247.25 with loss [24.774463653564453, 24.597886562347412] in episode 2255
Report: 
rewardSum:247.25
loss:[24.774463653564453, 24.597886562347412]
policies:[0, 4, 0]
qAverage:[0.0, 96.60870513916015]
ws:[3.8217627882957457, 7.803338861465454]
memory len:10000
memory used:2916.0
now epsilon is 0.04372255170324787, the reward is 247.25 with loss [29.828969478607178, 28.754064559936523] in episode 2256
Report: 
rewardSum:247.25
loss:[29.828969478607178, 28.754064559936523]
policies:[0, 4, 0]
qAverage:[0.0, 95.27876434326171]
ws:[4.382715916633606, 8.744325876235962]
memory len:10000
memory used:2916.0
now epsilon is 0.04367884554476902, the reward is 247.25 with loss [19.6458683013916, 26.844359397888184] in episode 2257
Report: 
rewardSum:247.25
loss:[19.6458683013916, 26.844359397888184]
policies:[0, 4, 0]
qAverage:[0.0, 97.26843719482422]
ws:[4.077290678024292, 8.568877172470092]
memory len:10000
memory used:2916.0
now epsilon is 0.04363518307606158, the reward is 247.25 with loss [21.03007471561432, 26.738874912261963] in episode 2258
Report: 
rewardSum:247.25
loss:[21.03007471561432, 26.738874912261963]
policies:[0, 4, 0]
qAverage:[0.0, 96.29599609375]
ws:[3.078495979309082, 7.111254692077637]
memory len:10000
memory used:2916.0
now epsilon is 0.04359156425345215, the reward is 247.25 with loss [26.81447196006775, 25.97735023498535] in episode 2259
Report: 
rewardSum:247.25
loss:[26.81447196006775, 25.97735023498535]
policies:[0, 4, 0]
qAverage:[0.0, 98.36860809326171]
ws:[3.2296218454837797, 7.636044907569885]
memory len:10000
memory used:2916.0
now epsilon is 0.04354798903331099, the reward is 247.25 with loss [25.187349796295166, 25.47419238090515] in episode 2260
Report: 
rewardSum:247.25
loss:[25.187349796295166, 25.47419238090515]
policies:[0, 4, 0]
qAverage:[0.0, 95.38675842285156]
ws:[3.1965776920318603, 7.484355163574219]
memory len:10000
memory used:2916.0
now epsilon is 0.04350445737205198, the reward is 247.25 with loss [29.886333465576172, 33.168925285339355] in episode 2261
Report: 
rewardSum:247.25
loss:[29.886333465576172, 33.168925285339355]
policies:[0, 4, 0]
qAverage:[0.0, 97.8753677368164]
ws:[3.737258279323578, 8.800071287155152]
memory len:10000
memory used:2916.0
now epsilon is 0.04346096922613259, the reward is 247.25 with loss [16.686252117156982, 33.446425914764404] in episode 2262
Report: 
rewardSum:247.25
loss:[16.686252117156982, 33.446425914764404]
policies:[0, 4, 0]
qAverage:[0.0, 94.00775299072265]
ws:[3.937911093235016, 8.730394220352172]
memory len:10000
memory used:2916.0
now epsilon is 0.04341752455205378, the reward is 247.25 with loss [24.830045461654663, 17.91425633430481] in episode 2263
Report: 
rewardSum:247.25
loss:[24.830045461654663, 17.91425633430481]
policies:[0, 4, 0]
qAverage:[0.0, 97.67125701904297]
ws:[3.7226046532392503, 8.787216567993164]
memory len:10000
memory used:2916.0
now epsilon is 0.04337412330636, the reward is 247.25 with loss [22.49654769897461, 23.579660415649414] in episode 2264
Report: 
rewardSum:247.25
loss:[22.49654769897461, 23.579660415649414]
policies:[0, 4, 0]
qAverage:[0.0, 95.1217529296875]
ws:[4.353448659181595, 10.643940210342407]
memory len:10000
memory used:2916.0
now epsilon is 0.04333076544563917, the reward is 247.25 with loss [22.912474155426025, 29.538095951080322] in episode 2265
Report: 
rewardSum:247.25
loss:[22.912474155426025, 29.538095951080322]
policies:[0, 4, 0]
qAverage:[0.0, 104.28396301269531]
ws:[3.5631667137145997, 9.746680641174317]
memory len:10000
memory used:2915.0
now epsilon is 0.04328745092652257, the reward is 247.25 with loss [18.608967065811157, 32.162954330444336] in episode 2266
Report: 
rewardSum:247.25
loss:[18.608967065811157, 32.162954330444336]
policies:[0, 4, 0]
qAverage:[0.0, 101.93854370117188]
ws:[3.6951117277145387, 10.295316886901855]
memory len:10000
memory used:2916.0
now epsilon is 0.04324417970568486, the reward is 247.25 with loss [27.041879177093506, 24.258805751800537] in episode 2267
Report: 
rewardSum:247.25
loss:[27.041879177093506, 24.258805751800537]
policies:[0, 4, 0]
qAverage:[0.0, 102.17377319335938]
ws:[4.014300188422203, 10.992613220214844]
memory len:10000
memory used:2915.0
now epsilon is 0.04320095173984398, the reward is 247.25 with loss [18.736314058303833, 22.05310869216919] in episode 2268
Report: 
rewardSum:247.25
loss:[18.736314058303833, 22.05310869216919]
policies:[0, 4, 0]
qAverage:[0.0, 102.6812026977539]
ws:[3.731537085771561, 10.078757953643798]
memory len:10000
memory used:2915.0
now epsilon is 0.04315776698576115, the reward is 247.25 with loss [22.40781021118164, 22.881657123565674] in episode 2269
Report: 
rewardSum:247.25
loss:[22.40781021118164, 22.881657123565674]
policies:[0, 4, 0]
qAverage:[0.0, 100.99087715148926]
ws:[4.166447751224041, 9.64320707321167]
memory len:10000
memory used:2915.0
now epsilon is 0.04311462540024082, the reward is 247.25 with loss [20.197935581207275, 33.67907428741455] in episode 2270
Report: 
rewardSum:247.25
loss:[20.197935581207275, 33.67907428741455]
policies:[0, 4, 0]
qAverage:[0.0, 102.01864166259766]
ws:[3.846253734827042, 10.045819425582886]
memory len:10000
memory used:2915.0
now epsilon is 0.04306075905839558, the reward is 246.25 with loss [35.949607133865356, 32.38206481933594] in episode 2271
Report: 
rewardSum:246.25
loss:[35.949607133865356, 32.38206481933594]
policies:[0, 4, 1]
qAverage:[0.0, 103.9328399658203]
ws:[3.6747151613235474, 10.77994899749756]
memory len:10000
memory used:2915.0
now epsilon is 0.043017714444430706, the reward is 247.25 with loss [29.700329303741455, 20.616095304489136] in episode 2272
Report: 
rewardSum:247.25
loss:[29.700329303741455, 20.616095304489136]
policies:[0, 4, 0]
qAverage:[0.0, 100.63668823242188]
ws:[4.04978529214859, 11.027344989776612]
memory len:10000
memory used:2916.0
now epsilon is 0.042974712858940764, the reward is 247.25 with loss [27.817716121673584, 25.70314311981201] in episode 2273
Report: 
rewardSum:247.25
loss:[27.817716121673584, 25.70314311981201]
policies:[0, 4, 0]
qAverage:[0.0, 103.63910064697265]
ws:[3.761384117603302, 10.372805213928222]
memory len:10000
memory used:2915.0
now epsilon is 0.0429317542589134, the reward is 247.25 with loss [21.798242568969727, 21.566648960113525] in episode 2274
Report: 
rewardSum:247.25
loss:[21.798242568969727, 21.566648960113525]
policies:[0, 4, 0]
qAverage:[0.0, 102.21729125976563]
ws:[3.6627363860607147, 10.487319087982177]
memory len:10000
memory used:2915.0
now epsilon is 0.04288883860137928, the reward is 247.25 with loss [35.046441078186035, 21.45012879371643] in episode 2275
Report: 
rewardSum:247.25
loss:[35.046441078186035, 21.45012879371643]
policies:[0, 4, 0]
qAverage:[0.0, 101.5905258178711]
ws:[3.7494650930166245, 10.07865629196167]
memory len:10000
memory used:2915.0
now epsilon is 0.04284596584341199, the reward is 247.25 with loss [27.582653522491455, 18.47293710708618] in episode 2276
Report: 
rewardSum:247.25
loss:[27.582653522491455, 18.47293710708618]
policies:[0, 4, 0]
qAverage:[0.0, 102.87894592285156]
ws:[4.030011260509491, 9.721066951751709]
memory len:10000
memory used:2915.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23*		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.04280313594212807, the reward is 247.25 with loss [30.126444339752197, 34.932406425476074] in episode 2277
Report: 
rewardSum:247.25
loss:[30.126444339752197, 34.932406425476074]
policies:[0, 4, 0]
qAverage:[0.0, 104.46944885253906]
ws:[4.414847314357758, 10.195493125915528]
memory len:10000
memory used:2915.0
now epsilon is 0.04276034885468689, the reward is 247.25 with loss [27.345749139785767, 31.250558376312256] in episode 2278
Report: 
rewardSum:247.25
loss:[27.345749139785767, 31.250558376312256]
policies:[0, 4, 0]
qAverage:[0.0, 95.67720603942871]
ws:[0.7582727558910847, 5.661329925060272]
memory len:10000
memory used:2915.0
now epsilon is 0.04271760453829067, the reward is 247.25 with loss [26.407326698303223, 29.565168857574463] in episode 2279
Report: 
rewardSum:247.25
loss:[26.407326698303223, 29.565168857574463]
policies:[0, 4, 0]
qAverage:[0.0, 105.38190307617188]
ws:[3.296520559489727, 8.117729210853577]
memory len:10000
memory used:2916.0
now epsilon is 0.042674902950184404, the reward is 247.25 with loss [26.754347324371338, 25.419981241226196] in episode 2280
Report: 
rewardSum:247.25
loss:[26.754347324371338, 25.419981241226196]
policies:[0, 4, 0]
qAverage:[0.0, 110.22538452148437]
ws:[3.78542275428772, 8.702926588058471]
memory len:10000
memory used:2916.0
now epsilon is 0.042632244047655816, the reward is 247.25 with loss [29.700188159942627, 36.607370376586914] in episode 2281
Report: 
rewardSum:247.25
loss:[29.700188159942627, 36.607370376586914]
policies:[0, 4, 0]
qAverage:[0.0, 107.70292663574219]
ws:[4.003188791871071, 8.572686624526977]
memory len:10000
memory used:2916.0
now epsilon is 0.042589627788035336, the reward is 247.25 with loss [28.285458087921143, 25.679298400878906] in episode 2282
Report: 
rewardSum:247.25
loss:[28.285458087921143, 25.679298400878906]
policies:[0, 4, 0]
qAverage:[0.0, 110.16021270751953]
ws:[4.392234802246094, 8.548069381713868]
memory len:10000
memory used:2916.0
now epsilon is 0.04254705412869604, the reward is 247.25 with loss [26.630492687225342, 27.96897315979004] in episode 2283
Report: 
rewardSum:247.25
loss:[26.630492687225342, 27.96897315979004]
policies:[0, 4, 0]
qAverage:[0.0, 105.80940399169921]
ws:[4.396603107452393, 8.179267930984498]
memory len:10000
memory used:2916.0
now epsilon is 0.04250452302705363, the reward is 247.25 with loss [23.210768580436707, 20.36858057975769] in episode 2284
Report: 
rewardSum:247.25
loss:[23.210768580436707, 20.36858057975769]
policies:[0, 4, 0]
qAverage:[0.0, 109.56131591796876]
ws:[4.168561699986458, 7.7001642942428585]
memory len:10000
memory used:2916.0
now epsilon is 0.04246203444056636, the reward is 247.25 with loss [17.71316933631897, 28.82418203353882] in episode 2285
Report: 
rewardSum:247.25
loss:[17.71316933631897, 28.82418203353882]
policies:[0, 4, 0]
qAverage:[0.0, 106.69400482177734]
ws:[3.8558496847748756, 7.30175371170044]
memory len:10000
memory used:2916.0
now epsilon is 0.04241958832673501, the reward is 247.25 with loss [22.340226650238037, 20.487426280975342] in episode 2286
Report: 
rewardSum:247.25
loss:[22.340226650238037, 20.487426280975342]
policies:[0, 4, 0]
qAverage:[0.0, 109.43111572265624]
ws:[3.916797485947609, 7.624100303649902]
memory len:10000
memory used:2916.0
now epsilon is 0.04237718464310285, the reward is 247.25 with loss [34.40748739242554, 19.436318397521973] in episode 2287
Report: 
rewardSum:247.25
loss:[34.40748739242554, 19.436318397521973]
policies:[0, 4, 0]
qAverage:[0.0, 93.0457763671875]
ws:[0.8622019737958908, 4.33759480714798]
memory len:10000
memory used:2916.0
now epsilon is 0.04233482334725559, the reward is 247.25 with loss [26.75559425354004, 26.78747844696045] in episode 2288
Report: 
rewardSum:247.25
loss:[26.75559425354004, 26.78747844696045]
policies:[0, 4, 0]
qAverage:[0.0, 109.19903106689453]
ws:[4.602568531036377, 8.35387954711914]
memory len:10000
memory used:2916.0
now epsilon is 0.042292504396821334, the reward is 247.25 with loss [34.041218280792236, 30.001672744750977] in episode 2289
Report: 
rewardSum:247.25
loss:[34.041218280792236, 30.001672744750977]
policies:[0, 4, 0]
qAverage:[0.0, 110.273974609375]
ws:[4.273708975315094, 7.64385986328125]
memory len:10000
memory used:2916.0
now epsilon is 0.04225022774947055, the reward is 247.25 with loss [27.314773559570312, 23.54834222793579] in episode 2290
Report: 
rewardSum:247.25
loss:[27.314773559570312, 23.54834222793579]
policies:[0, 4, 0]
qAverage:[0.0, 121.52290344238281]
ws:[4.31868354678154, 7.986419701576233]
memory len:10000
memory used:2916.0
now epsilon is 0.04220799336291601, the reward is 247.25 with loss [20.03129768371582, 16.03405499458313] in episode 2291
Report: 
rewardSum:247.25
loss:[20.03129768371582, 16.03405499458313]
policies:[0, 4, 0]
qAverage:[0.0, 114.52653350830079]
ws:[5.5426335513591765, 10.140490102767945]
memory len:10000
memory used:2916.0
now epsilon is 0.042165801194912776, the reward is 247.25 with loss [24.176642179489136, 23.724786281585693] in episode 2292
Report: 
rewardSum:247.25
loss:[24.176642179489136, 23.724786281585693]
policies:[1, 3, 0]
qAverage:[0.0, 106.93212381998698]
ws:[8.774556954701742, 13.543957074483236]
memory len:10000
memory used:2916.0
now epsilon is 0.042123651203258124, the reward is 247.25 with loss [26.233011722564697, 28.942692041397095] in episode 2293
Report: 
rewardSum:247.25
loss:[26.233011722564697, 28.942692041397095]
policies:[0, 4, 0]
qAverage:[0.0, 115.49387054443359]
ws:[7.2778390645980835, 12.792377853393555]
memory len:10000
memory used:2916.0
now epsilon is 0.042081543345791506, the reward is 247.25 with loss [29.352022171020508, 33.15011119842529] in episode 2294
Report: 
rewardSum:247.25
loss:[29.352022171020508, 33.15011119842529]
policies:[0, 4, 0]
qAverage:[0.0, 117.7540069580078]
ws:[7.3916332721710205, 12.273964214324952]
memory len:10000
memory used:2916.0
now epsilon is 0.042039477580394544, the reward is 247.25 with loss [32.21522521972656, 22.025718688964844] in episode 2295
Report: 
rewardSum:247.25
loss:[32.21522521972656, 22.025718688964844]
policies:[0, 4, 0]
qAverage:[0.0, 115.7730712890625]
ws:[7.860317134857178, 12.971068477630615]
memory len:10000
memory used:2916.0
now epsilon is 0.041997453864990944, the reward is 247.25 with loss [17.673916816711426, 22.124945878982544] in episode 2296
Report: 
rewardSum:247.25
loss:[17.673916816711426, 22.124945878982544]
policies:[0, 4, 0]
qAverage:[0.0, 119.17031402587891]
ws:[8.201465940475464, 12.713634490966797]
memory len:10000
memory used:2916.0
now epsilon is 0.041934497043684724, the reward is 245.25 with loss [42.09425067901611, 39.83611583709717] in episode 2297
Report: 
rewardSum:245.25
loss:[42.09425067901611, 39.83611583709717]
policies:[0, 5, 1]
qAverage:[0.0, 124.39516576131184]
ws:[6.768841465314229, 11.273828228314718]
memory len:10000
memory used:2916.0
now epsilon is 0.04189257826945669, the reward is 247.25 with loss [26.96258306503296, 30.183494091033936] in episode 2298
Report: 
rewardSum:247.25
loss:[26.96258306503296, 30.183494091033936]
policies:[0, 4, 0]
qAverage:[0.0, 116.69491119384766]
ws:[6.670471858978272, 10.682624292373657]
memory len:10000
memory used:2916.0
now epsilon is 0.04185070139828598, the reward is 247.25 with loss [16.360576152801514, 28.510191917419434] in episode 2299
Report: 
rewardSum:247.25
loss:[16.360576152801514, 28.510191917419434]
policies:[0, 4, 0]
qAverage:[0.0, 118.37642974853516]
ws:[6.6856970429420475, 11.003527855873108]
memory len:10000
memory used:2916.0
now epsilon is 0.04180886638828522, the reward is 247.25 with loss [37.19220042228699, 25.01870632171631] in episode 2300
Report: 
rewardSum:247.25
loss:[37.19220042228699, 25.01870632171631]
policies:[0, 4, 0]
qAverage:[0.0, 115.96053009033203]
ws:[6.984953904151917, 11.781419944763183]
memory len:10000
memory used:2916.0
now epsilon is 0.041767073197608945, the reward is 247.25 with loss [32.40085458755493, 27.037379264831543] in episode 2301
Report: 
rewardSum:247.25
loss:[32.40085458755493, 27.037379264831543]
policies:[0, 4, 0]
qAverage:[0.0, 119.42805480957031]
ws:[7.598140168190002, 13.761445450782777]
memory len:10000
memory used:2916.0
############# STATE ###############
0-		10-		20-		30-		40-		
1*		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.041725321784453505, the reward is 247.25 with loss [35.22890329360962, 33.114501953125] in episode 2302
Report: 
rewardSum:247.25
loss:[35.22890329360962, 33.114501953125]
policies:[0, 3, 1]
qAverage:[0.0, 124.42404747009277]
ws:[8.229913480579853, 13.55445110797882]
memory len:10000
memory used:2917.0
now epsilon is 0.04168361210705705, the reward is 247.25 with loss [25.71349573135376, 31.576011657714844] in episode 2303
Report: 
rewardSum:247.25
loss:[25.71349573135376, 31.576011657714844]
policies:[0, 4, 0]
qAverage:[0.0, 127.55300445556641]
ws:[7.6543213605880736, 14.217611980438232]
memory len:10000
memory used:2917.0
now epsilon is 0.04164194412369948, the reward is 247.25 with loss [23.56136703491211, 35.345632553100586] in episode 2304
Report: 
rewardSum:247.25
loss:[23.56136703491211, 35.345632553100586]
policies:[0, 4, 0]
qAverage:[0.0, 125.38523712158204]
ws:[7.425058454275131, 13.293723869323731]
memory len:10000
memory used:2917.0
now epsilon is 0.04160031779270237, the reward is 247.25 with loss [29.241585731506348, 27.502548217773438] in episode 2305
Report: 
rewardSum:247.25
loss:[29.241585731506348, 27.502548217773438]
policies:[0, 4, 0]
qAverage:[0.0, 125.80126647949218]
ws:[6.97417533993721, 12.666545820236205]
memory len:10000
memory used:2917.0
now epsilon is 0.04155873307242898, the reward is 247.25 with loss [34.82539463043213, 37.54498529434204] in episode 2306
Report: 
rewardSum:247.25
loss:[34.82539463043213, 37.54498529434204]
policies:[0, 4, 0]
qAverage:[0.0, 125.20657043457031]
ws:[6.85495274066925, 12.613036489486694]
memory len:10000
memory used:2917.0
now epsilon is 0.0415171899212842, the reward is 247.25 with loss [19.345831394195557, 27.607871532440186] in episode 2307
Report: 
rewardSum:247.25
loss:[19.345831394195557, 27.607871532440186]
policies:[0, 3, 1]
qAverage:[0.0, 120.00387382507324]
ws:[10.01584655046463, 15.399648070335388]
memory len:10000
memory used:2917.0
now epsilon is 0.041475688297714475, the reward is 247.25 with loss [22.778573989868164, 28.156283378601074] in episode 2308
Report: 
rewardSum:247.25
loss:[22.778573989868164, 28.156283378601074]
policies:[0, 4, 0]
qAverage:[0.0, 126.64192657470703]
ws:[7.1330548286437985, 12.911479711532593]
memory len:10000
memory used:2917.0
now epsilon is 0.04143422816020781, the reward is 247.25 with loss [30.204206943511963, 27.133633613586426] in episode 2309
Report: 
rewardSum:247.25
loss:[30.204206943511963, 27.133633613586426]
policies:[0, 4, 0]
qAverage:[0.0, 122.76437530517578]
ws:[7.765712022781372, 14.405484294891357]
memory len:10000
memory used:2917.0
now epsilon is 0.04139280946729369, the reward is 247.25 with loss [27.18821620941162, 31.082417964935303] in episode 2310
Report: 
rewardSum:247.25
loss:[27.18821620941162, 31.082417964935303]
policies:[0, 4, 0]
qAverage:[0.0, 126.73722991943359]
ws:[9.443803218007087, 16.38834218978882]
memory len:10000
memory used:2917.0
now epsilon is 0.04135143217754306, the reward is 247.25 with loss [32.48953437805176, 21.503554344177246] in episode 2311
Report: 
rewardSum:247.25
loss:[32.48953437805176, 21.503554344177246]
policies:[0, 4, 0]
qAverage:[0.0, 123.11708068847656]
ws:[8.207388722896576, 12.966933345794677]
memory len:10000
memory used:2917.0
now epsilon is 0.04128944378332452, the reward is 245.25 with loss [43.589855670928955, 46.94695234298706] in episode 2312
Report: 
rewardSum:245.25
loss:[43.589855670928955, 46.94695234298706]
policies:[0, 5, 1]
qAverage:[0.0, 132.16905466715494]
ws:[5.313230673472087, 8.896608094374338]
memory len:10000
memory used:2917.0
now epsilon is 0.04124816982050219, the reward is 247.25 with loss [38.531813621520996, 26.31268882751465] in episode 2313
Report: 
rewardSum:247.25
loss:[38.531813621520996, 26.31268882751465]
policies:[0, 4, 0]
qAverage:[0.0, 127.2967544555664]
ws:[7.568044102191925, 11.767054986953735]
memory len:10000
memory used:2917.0
now epsilon is 0.041206937116167526, the reward is 247.25 with loss [34.04378604888916, 31.625061511993408] in episode 2314
Report: 
rewardSum:247.25
loss:[34.04378604888916, 31.625061511993408]
policies:[0, 4, 0]
qAverage:[0.0, 131.34624938964845]
ws:[8.835634469985962, 14.555895709991455]
memory len:10000
memory used:2917.0
now epsilon is 0.04116574562907751, the reward is 247.25 with loss [28.834562301635742, 17.776403665542603] in episode 2315
Report: 
rewardSum:247.25
loss:[28.834562301635742, 17.776403665542603]
policies:[0, 4, 0]
qAverage:[0.0, 130.07366333007812]
ws:[9.42532730102539, 14.180427360534669]
memory len:10000
memory used:2917.0
now epsilon is 0.04112459531803035, the reward is 247.25 with loss [28.453115463256836, 32.33310556411743] in episode 2316
Report: 
rewardSum:247.25
loss:[28.453115463256836, 32.33310556411743]
policies:[0, 4, 0]
qAverage:[0.0, 133.77492980957032]
ws:[8.408352601528168, 13.258879804611206]
memory len:10000
memory used:2916.0
now epsilon is 0.041083486141865445, the reward is 247.25 with loss [19.897379398345947, 21.351666927337646] in episode 2317
Report: 
rewardSum:247.25
loss:[19.897379398345947, 21.351666927337646]
policies:[0, 4, 0]
qAverage:[0.0, 128.08724975585938]
ws:[7.412448865175247, 12.17996301651001]
memory len:10000
memory used:2916.0
now epsilon is 0.04104241805946333, the reward is 247.25 with loss [27.722087383270264, 33.60678148269653] in episode 2318
Report: 
rewardSum:247.25
loss:[27.722087383270264, 33.60678148269653]
policies:[1, 3, 0]
qAverage:[0.0, 122.38229370117188]
ws:[7.603786796331406, 11.528276890516281]
memory len:10000
memory used:2916.0
now epsilon is 0.04099114068198822, the reward is 246.25 with loss [28.82836627960205, 42.5243239402771] in episode 2319
Report: 
rewardSum:246.25
loss:[28.82836627960205, 42.5243239402771]
policies:[0, 4, 1]
qAverage:[0.0, 131.23844299316406]
ws:[5.464836908131838, 11.058472156524658]
memory len:10000
memory used:2917.0
now epsilon is 0.04095016491042221, the reward is 247.25 with loss [19.307076334953308, 26.188747584819794] in episode 2320
Report: 
rewardSum:247.25
loss:[19.307076334953308, 26.188747584819794]
policies:[1, 3, 0]
qAverage:[0.0, 120.464111328125]
ws:[5.632715098559856, 9.18612441048026]
memory len:10000
memory used:2917.0
now epsilon is 0.0409092300992644, the reward is 247.25 with loss [25.209755420684814, 26.94886064529419] in episode 2321
Report: 
rewardSum:247.25
loss:[25.209755420684814, 26.94886064529419]
policies:[0, 4, 0]
qAverage:[0.0, 131.3434265136719]
ws:[6.1031009435653685, 11.71268618106842]
memory len:10000
memory used:2917.0
now epsilon is 0.040868336207569765, the reward is 247.25 with loss [25.938974857330322, 21.72331476211548] in episode 2322
Report: 
rewardSum:247.25
loss:[25.938974857330322, 21.72331476211548]
policies:[0, 4, 0]
qAverage:[0.0, 130.68709716796874]
ws:[7.295046266913414, 12.229303026199341]
memory len:10000
memory used:2916.0
now epsilon is 0.04082748319443416, the reward is 247.25 with loss [34.294766426086426, 18.22287654876709] in episode 2323
Report: 
rewardSum:247.25
loss:[34.294766426086426, 18.22287654876709]
policies:[0, 4, 0]
qAverage:[0.0, 130.06792297363282]
ws:[8.881224083900452, 14.698059940338135]
memory len:10000
memory used:2916.0
now epsilon is 0.04078667101899437, the reward is 247.25 with loss [21.864256143569946, 26.218380451202393] in episode 2324
Report: 
rewardSum:247.25
loss:[21.864256143569946, 26.218380451202393]
policies:[0, 4, 0]
qAverage:[0.0, 131.75514221191406]
ws:[8.36138322353363, 12.9748610496521]
memory len:10000
memory used:2916.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25*		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.040725529237226527, the reward is 245.25 with loss [48.03888511657715, 42.612489223480225] in episode 2325
Report: 
rewardSum:245.25
loss:[48.03888511657715, 42.612489223480225]
policies:[0, 5, 1]
qAverage:[0.0, 137.62858072916666]
ws:[7.464001615842183, 12.917192260424295]
memory len:10000
memory used:2916.0
now epsilon is 0.040684818977517585, the reward is 247.25 with loss [34.13508224487305, 28.483951568603516] in episode 2326
Report: 
rewardSum:247.25
loss:[34.13508224487305, 28.483951568603516]
policies:[0, 4, 0]
qAverage:[0.0, 135.53754272460938]
ws:[7.686807382106781, 11.964172267913819]
memory len:10000
memory used:2916.0
now epsilon is 0.040644149412804545, the reward is 37.15999999999997 with loss [30.939021825790405, 28.951252222061157] in episode 2327
Report: 
rewardSum:37.15999999999997
loss:[30.939021825790405, 28.951252222061157]
policies:[0, 3, 1]
qAverage:[0.0, 119.3424072265625]
ws:[0.5972235091030598, 4.763103157281876]
memory len:10000
memory used:2916.0
now epsilon is 0.040603520502407675, the reward is 247.25 with loss [24.808943271636963, 36.50058174133301] in episode 2328
Report: 
rewardSum:247.25
loss:[24.808943271636963, 36.50058174133301]
policies:[0, 4, 0]
qAverage:[0.0, 134.51834106445312]
ws:[7.68095850944519, 14.137125873565674]
memory len:10000
memory used:2916.0
now epsilon is 0.040552791472636476, the reward is 246.25 with loss [35.964895248413086, 39.60845327377319] in episode 2329
Report: 
rewardSum:246.25
loss:[35.964895248413086, 39.60845327377319]
policies:[0, 4, 1]
qAverage:[0.0, 135.109326171875]
ws:[7.339890038967132, 12.943830299377442]
memory len:10000
memory used:2916.0
now epsilon is 0.040512253885926254, the reward is 247.25 with loss [22.975284814834595, 33.252962589263916] in episode 2330
Report: 
rewardSum:247.25
loss:[22.975284814834595, 33.252962589263916]
policies:[0, 3, 1]
qAverage:[0.0, 133.19263076782227]
ws:[8.645455434918404, 13.954936027526855]
memory len:10000
memory used:2916.0
now epsilon is 0.04047175682160368, the reward is 247.25 with loss [31.306663990020752, 35.364896297454834] in episode 2331
Report: 
rewardSum:247.25
loss:[31.306663990020752, 35.364896297454834]
policies:[0, 4, 0]
qAverage:[0.0, 123.42112350463867]
ws:[6.7456925958395, 8.596743628382683]
memory len:10000
memory used:2916.0
now epsilon is 0.04043130023916156, the reward is 247.25 with loss [19.920421361923218, 22.557727336883545] in episode 2332
Report: 
rewardSum:247.25
loss:[19.920421361923218, 22.557727336883545]
policies:[0, 4, 0]
qAverage:[0.0, 120.33212661743164]
ws:[-0.03151523321866989, 2.1721051931381226]
memory len:10000
memory used:2916.0
now epsilon is 0.0403908840981332, the reward is 247.25 with loss [21.253196239471436, 29.912246227264404] in episode 2333
Report: 
rewardSum:247.25
loss:[21.253196239471436, 29.912246227264404]
policies:[0, 4, 0]
qAverage:[0.0, 133.10404052734376]
ws:[6.480679208040238, 10.440219694375992]
memory len:10000
memory used:2916.0
now epsilon is 0.040350508358092334, the reward is 247.25 with loss [22.520885944366455, 30.894922733306885] in episode 2334
Report: 
rewardSum:247.25
loss:[22.520885944366455, 30.894922733306885]
policies:[0, 4, 0]
qAverage:[0.0, 133.67462158203125]
ws:[7.427038013935089, 11.349605417251587]
memory len:10000
memory used:2916.0
now epsilon is 0.04031017297865313, the reward is 247.25 with loss [22.546468496322632, 27.75317144393921] in episode 2335
Report: 
rewardSum:247.25
loss:[22.546468496322632, 27.75317144393921]
policies:[0, 4, 0]
qAverage:[0.0, 135.8690979003906]
ws:[8.101853561401366, 12.291582775115966]
memory len:10000
memory used:2916.0
now epsilon is 0.04026987791947012, the reward is 247.25 with loss [23.68150281906128, 26.48632025718689] in episode 2336
Report: 
rewardSum:247.25
loss:[23.68150281906128, 26.48632025718689]
policies:[0, 4, 0]
qAverage:[0.0, 131.59251403808594]
ws:[8.00164155960083, 12.710544013977051]
memory len:10000
memory used:2916.0
now epsilon is 0.04022962314023816, the reward is 247.25 with loss [23.59475326538086, 24.97283935546875] in episode 2337
Report: 
rewardSum:247.25
loss:[23.59475326538086, 24.97283935546875]
policies:[0, 4, 0]
qAverage:[0.0, 135.40331420898437]
ws:[7.197002601623535, 10.783540487289429]
memory len:10000
memory used:2916.0
now epsilon is 0.04018940860069241, the reward is 247.25 with loss [27.097110271453857, 31.973137855529785] in episode 2338
Report: 
rewardSum:247.25
loss:[27.097110271453857, 31.973137855529785]
policies:[0, 4, 0]
qAverage:[0.0, 122.92750549316406]
ws:[7.752761751413345, 10.683623731136322]
memory len:10000
memory used:2916.0
now epsilon is 0.04014923426060826, the reward is 247.25 with loss [25.773643016815186, 32.04210186004639] in episode 2339
Report: 
rewardSum:247.25
loss:[25.773643016815186, 32.04210186004639]
policies:[0, 4, 0]
qAverage:[0.0, 133.6173858642578]
ws:[6.397168898582459, 10.1856538772583]
memory len:10000
memory used:2916.0
now epsilon is 0.04010910007980134, the reward is 247.25 with loss [25.284174919128418, 23.21695375442505] in episode 2340
Report: 
rewardSum:247.25
loss:[25.284174919128418, 23.21695375442505]
policies:[0, 4, 0]
qAverage:[0.0, 131.9454315185547]
ws:[6.912859082221985, 11.502097702026367]
memory len:10000
memory used:2916.0
now epsilon is 0.040069006018127414, the reward is 247.25 with loss [25.79178512096405, 24.9323787689209] in episode 2341
Report: 
rewardSum:247.25
loss:[25.79178512096405, 24.9323787689209]
policies:[0, 3, 1]
qAverage:[0.0, 121.02154159545898]
ws:[1.0203486382961273, 4.141763925552368]
memory len:10000
memory used:2916.0
now epsilon is 0.04002895203548239, the reward is 247.25 with loss [23.79738712310791, 27.579918384552002] in episode 2342
Report: 
rewardSum:247.25
loss:[23.79738712310791, 27.579918384552002]
policies:[0, 4, 0]
qAverage:[0.0, 133.17992248535157]
ws:[5.788497197628021, 9.379151272773743]
memory len:10000
memory used:2916.0
now epsilon is 0.039988938091802276, the reward is 247.25 with loss [23.770784616470337, 28.499823093414307] in episode 2343
Report: 
rewardSum:247.25
loss:[23.770784616470337, 28.499823093414307]
policies:[1, 3, 0]
qAverage:[0.0, 130.84546661376953]
ws:[6.303560599684715, 9.088403582572937]
memory len:10000
memory used:2916.0
now epsilon is 0.039938976906026345, the reward is 246.25 with loss [37.9409556388855, 35.91179609298706] in episode 2344
Report: 
rewardSum:246.25
loss:[37.9409556388855, 35.91179609298706]
policies:[0, 4, 1]
qAverage:[0.0, 132.3437713623047]
ws:[6.562654292583465, 10.864962577819824]
memory len:10000
memory used:2916.0
now epsilon is 0.039899052903740635, the reward is 247.25 with loss [18.060424327850342, 20.493074893951416] in episode 2345
Report: 
rewardSum:247.25
loss:[18.060424327850342, 20.493074893951416]
policies:[0, 3, 1]
qAverage:[0.0, 132.49053955078125]
ws:[7.789262846112251, 11.780959844589233]
memory len:10000
memory used:2916.0
now epsilon is 0.039859168810488205, the reward is 247.25 with loss [25.35374879837036, 21.7863347530365] in episode 2346
Report: 
rewardSum:247.25
loss:[25.35374879837036, 21.7863347530365]
policies:[0, 4, 0]
qAverage:[0.0, 133.0128601074219]
ws:[6.019219422340393, 9.534667778015137]
memory len:10000
memory used:2916.0
now epsilon is 0.03981932458637498, the reward is 247.25 with loss [25.803283214569092, 25.31527853012085] in episode 2347
Report: 
rewardSum:247.25
loss:[25.803283214569092, 25.31527853012085]
policies:[1, 3, 0]
qAverage:[0.0, 126.63389587402344]
ws:[6.287186570465565, 8.336910367012024]
memory len:10000
memory used:2916.0
now epsilon is 0.03977952019154678, the reward is 247.25 with loss [33.65465068817139, 23.66028082370758] in episode 2348
Report: 
rewardSum:247.25
loss:[33.65465068817139, 23.66028082370758]
policies:[0, 4, 0]
qAverage:[0.0, 132.93189086914063]
ws:[6.7571511507034305, 10.52628426551819]
memory len:10000
memory used:2916.0
now epsilon is 0.039739755586189245, the reward is 247.25 with loss [23.221371173858643, 32.22431564331055] in episode 2349
Report: 
rewardSum:247.25
loss:[23.221371173858643, 32.22431564331055]
policies:[0, 4, 0]
qAverage:[0.0, 130.6960235595703]
ws:[7.070883560180664, 9.839179515838623]
memory len:10000
memory used:2916.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22*		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.03970003073052782, the reward is 247.25 with loss [32.07005310058594, 27.599490642547607] in episode 2350
Report: 
rewardSum:247.25
loss:[32.07005310058594, 27.599490642547607]
policies:[1, 3, 0]
qAverage:[34.20063171386719, 92.99046630859375]
ws:[6.774863910675049, 9.337911081314086]
memory len:10000
memory used:2916.0
now epsilon is 0.03966034558482773, the reward is 247.25 with loss [28.865796089172363, 23.43637752532959] in episode 2351
Report: 
rewardSum:247.25
loss:[28.865796089172363, 23.43637752532959]
policies:[1, 3, 0]
qAverage:[34.48182983398438, 89.22652893066406]
ws:[6.2342761874198915, 9.782147789001465]
memory len:10000
memory used:2916.0
now epsilon is 0.039620700109393885, the reward is 247.25 with loss [31.54890727996826, 28.47593879699707] in episode 2352
Report: 
rewardSum:247.25
loss:[31.54890727996826, 28.47593879699707]
policies:[1, 3, 0]
qAverage:[40.7210578918457, 75.51922988891602]
ws:[7.048322230577469, 9.592622756958008]
memory len:10000
memory used:2916.0
now epsilon is 0.039581094264570904, the reward is 247.25 with loss [36.22644901275635, 33.86870813369751] in episode 2353
Report: 
rewardSum:247.25
loss:[36.22644901275635, 33.86870813369751]
policies:[2, 2, 0]
qAverage:[42.300045013427734, 77.35207748413086]
ws:[6.46078166924417, 10.10743772983551]
memory len:10000
memory used:2916.0
now epsilon is 0.03954152801074302, the reward is 247.25 with loss [36.23653984069824, 34.78162479400635] in episode 2354
Report: 
rewardSum:247.25
loss:[36.23653984069824, 34.78162479400635]
policies:[0, 4, 0]
qAverage:[0.0, 123.91189880371094]
ws:[4.706624595820903, 8.324022912979126]
memory len:10000
memory used:2916.0
now epsilon is 0.03950200130833409, the reward is 247.25 with loss [23.44452714920044, 21.177128553390503] in episode 2355
Report: 
rewardSum:247.25
loss:[23.44452714920044, 21.177128553390503]
policies:[1, 3, 0]
qAverage:[34.602056884765624, 91.02835998535156]
ws:[4.421899975836277, 7.819290494918823]
memory len:10000
memory used:2917.0
now epsilon is 0.039462514117807525, the reward is 247.25 with loss [33.55117845535278, 22.807781219482422] in episode 2356
Report: 
rewardSum:247.25
loss:[33.55117845535278, 22.807781219482422]
policies:[1, 3, 0]
qAverage:[31.860260009765625, 89.61788635253906]
ws:[3.397977185249329, 5.875446438789368]
memory len:10000
memory used:2917.0
now epsilon is 0.03942306639966626, the reward is 247.25 with loss [26.509446620941162, 18.572489261627197] in episode 2357
Report: 
rewardSum:247.25
loss:[26.509446620941162, 18.572489261627197]
policies:[0, 4, 0]
qAverage:[0.0, 127.46773376464844]
ws:[3.0251188345253466, 5.35421233177185]
memory len:10000
memory used:2917.0
now epsilon is 0.039383658114452706, the reward is 247.25 with loss [29.59816837310791, 34.779544830322266] in episode 2358
Report: 
rewardSum:247.25
loss:[29.59816837310791, 34.779544830322266]
policies:[0, 4, 0]
qAverage:[0.0, 124.51264953613281]
ws:[2.911811280250549, 5.905453670024872]
memory len:10000
memory used:2917.0
now epsilon is 0.03934428922274872, the reward is 247.25 with loss [28.55001163482666, 33.50514554977417] in episode 2359
Report: 
rewardSum:247.25
loss:[28.55001163482666, 33.50514554977417]
policies:[0, 4, 0]
qAverage:[0.0, 127.70556945800782]
ws:[2.772833061218262, 5.862159681320191]
memory len:10000
memory used:2918.0
now epsilon is 0.039295133445254274, the reward is 246.25 with loss [23.448854088783264, 33.87049341201782] in episode 2360
Report: 
rewardSum:246.25
loss:[23.448854088783264, 33.87049341201782]
policies:[0, 4, 1]
qAverage:[0.0, 127.40103454589844]
ws:[3.139170694351196, 6.7626323223114015]
memory len:10000
memory used:2917.0
now epsilon is 0.03925585304502828, the reward is 247.25 with loss [29.395851135253906, 26.51704502105713] in episode 2361
Report: 
rewardSum:247.25
loss:[29.395851135253906, 26.51704502105713]
policies:[0, 4, 0]
qAverage:[0.0, 112.11495971679688]
ws:[0.10528289526700974, 3.589585691690445]
memory len:10000
memory used:2918.0
now epsilon is 0.03921661191047482, the reward is 247.25 with loss [34.50929069519043, 21.351726531982422] in episode 2362
Report: 
rewardSum:247.25
loss:[34.50929069519043, 21.351726531982422]
policies:[1, 3, 0]
qAverage:[34.348358154296875, 91.53762512207031]
ws:[3.6665117025375364, 6.294613981246949]
memory len:10000
memory used:2918.0
now epsilon is 0.03917741000234293, the reward is 247.25 with loss [16.37612795829773, 16.67997670173645] in episode 2363
Report: 
rewardSum:247.25
loss:[16.37612795829773, 16.67997670173645]
policies:[0, 4, 0]
qAverage:[0.0, 120.42479248046875]
ws:[4.3296399608254434, 7.26565043926239]
memory len:10000
memory used:2918.0
now epsilon is 0.03913824728142091, the reward is 247.25 with loss [35.64062738418579, 26.754199266433716] in episode 2364
Report: 
rewardSum:247.25
loss:[35.64062738418579, 26.754199266433716]
policies:[0, 3, 1]
qAverage:[0.0, 111.13412857055664]
ws:[0.9428899735212326, 4.674284666776657]
memory len:10000
memory used:2918.0
now epsilon is 0.03909912370853624, the reward is 247.25 with loss [27.150960445404053, 33.443970680236816] in episode 2365
Report: 
rewardSum:247.25
loss:[27.150960445404053, 33.443970680236816]
policies:[1, 3, 0]
qAverage:[0.0, 119.44295883178711]
ws:[5.706793650984764, 7.901265501976013]
memory len:10000
memory used:2918.0
now epsilon is 0.03906003924455556, the reward is 37.15999999999997 with loss [27.44630718231201, 31.611769676208496] in episode 2366
Report: 
rewardSum:37.15999999999997
loss:[27.44630718231201, 31.611769676208496]
policies:[0, 3, 1]
qAverage:[0.0, 110.90575408935547]
ws:[0.46074600517749786, 3.806789755821228]
memory len:10000
memory used:2918.0
now epsilon is 0.03902099385038463, the reward is 247.25 with loss [22.51280689239502, 32.334893226623535] in episode 2367
Report: 
rewardSum:247.25
loss:[22.51280689239502, 32.334893226623535]
policies:[0, 4, 0]
qAverage:[0.0, 116.70045852661133]
ws:[4.180247083306313, 6.398188292980194]
memory len:10000
memory used:2918.0
now epsilon is 0.038981987486968274, the reward is 247.25 with loss [29.63097047805786, 23.936256408691406] in episode 2368
Report: 
rewardSum:247.25
loss:[29.63097047805786, 23.936256408691406]
policies:[0, 3, 1]
qAverage:[0.0, 113.11087036132812]
ws:[4.637876816093922, 6.39566570520401]
memory len:10000
memory used:2917.0
now epsilon is 0.03894302011529039, the reward is 247.25 with loss [27.348824501037598, 26.55867648124695] in episode 2369
Report: 
rewardSum:247.25
loss:[27.348824501037598, 26.55867648124695]
policies:[0, 4, 0]
qAverage:[0.0, 122.10717468261718]
ws:[4.150173045694828, 6.650592947006226]
memory len:10000
memory used:2917.0
now epsilon is 0.03890409169637386, the reward is 247.25 with loss [22.671237468719482, 31.457345724105835] in episode 2370
Report: 
rewardSum:247.25
loss:[22.671237468719482, 31.457345724105835]
policies:[0, 4, 0]
qAverage:[0.0, 120.02301940917968]
ws:[4.5871041178703305, 8.160023140907288]
memory len:10000
memory used:2917.0
now epsilon is 0.03886520219128053, the reward is 247.25 with loss [21.53755807876587, 30.793182849884033] in episode 2371
Report: 
rewardSum:247.25
loss:[21.53755807876587, 30.793182849884033]
policies:[0, 4, 0]
qAverage:[0.0, 121.98228759765625]
ws:[4.522652792930603, 7.583239936828614]
memory len:10000
memory used:2917.0
now epsilon is 0.03882635156111115, the reward is 247.25 with loss [26.86623525619507, 22.05781126022339] in episode 2372
Report: 
rewardSum:247.25
loss:[26.86623525619507, 22.05781126022339]
policies:[1, 3, 0]
qAverage:[0.0, 108.47018814086914]
ws:[0.8093938678503036, 4.510040640830994]
memory len:10000
memory used:2917.0
now epsilon is 0.038787539767005386, the reward is 247.25 with loss [24.40279722213745, 31.48402738571167] in episode 2373
Report: 
rewardSum:247.25
loss:[24.40279722213745, 31.48402738571167]
policies:[1, 3, 0]
qAverage:[0.0, 116.58485412597656]
ws:[5.737726077437401, 10.325232148170471]
memory len:10000
memory used:2917.0
now epsilon is 0.03874876677014173, the reward is 247.25 with loss [32.65290594100952, 29.569056034088135] in episode 2374
Report: 
rewardSum:247.25
loss:[32.65290594100952, 29.569056034088135]
policies:[0, 4, 0]
qAverage:[0.0, 121.69782409667968]
ws:[4.611468052864074, 8.178755474090575]
memory len:10000
memory used:2917.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12*		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.038710032531737486, the reward is 247.25 with loss [31.721491813659668, 30.64116382598877] in episode 2375
Report: 
rewardSum:247.25
loss:[31.721491813659668, 30.64116382598877]
policies:[0, 4, 0]
qAverage:[0.0, 118.2172119140625]
ws:[4.587150716781617, 8.449406003952026]
memory len:10000
memory used:2917.0
now epsilon is 0.03867133701304873, the reward is 247.25 with loss [29.62234592437744, 32.98688268661499] in episode 2376
Report: 
rewardSum:247.25
loss:[29.62234592437744, 32.98688268661499]
policies:[0, 4, 0]
qAverage:[0.0, 117.93833618164062]
ws:[3.5511112213134766, 6.285643148422241]
memory len:10000
memory used:2917.0
now epsilon is 0.038632680175370256, the reward is 247.25 with loss [33.07227325439453, 35.41470909118652] in episode 2377
Report: 
rewardSum:247.25
loss:[33.07227325439453, 35.41470909118652]
policies:[1, 3, 0]
qAverage:[0.0, 106.47613143920898]
ws:[0.14338638726621866, 2.5322541296482086]
memory len:10000
memory used:2917.0
now epsilon is 0.03859406198003556, the reward is 247.25 with loss [26.543224096298218, 28.133045196533203] in episode 2378
Report: 
rewardSum:247.25
loss:[26.543224096298218, 28.133045196533203]
policies:[0, 4, 0]
qAverage:[0.0, 118.06651916503907]
ws:[4.352255457639695, 6.881860613822937]
memory len:10000
memory used:2918.0
now epsilon is 0.03855548238841679, the reward is 247.25 with loss [32.09265613555908, 21.771026611328125] in episode 2379
Report: 
rewardSum:247.25
loss:[32.09265613555908, 21.771026611328125]
policies:[0, 4, 0]
qAverage:[0.0, 117.56634216308593]
ws:[3.8427408754825594, 6.250964164733887]
memory len:10000
memory used:2918.0
now epsilon is 0.038516941361924704, the reward is 247.25 with loss [34.27188539505005, 26.012603282928467] in episode 2380
Report: 
rewardSum:247.25
loss:[34.27188539505005, 26.012603282928467]
policies:[0, 4, 0]
qAverage:[0.0, 118.84710083007812]
ws:[2.842442286014557, 5.4114880681037905]
memory len:10000
memory used:2918.0
now epsilon is 0.03847843886200864, the reward is 247.25 with loss [30.59164047241211, 26.29965353012085] in episode 2381
Report: 
rewardSum:247.25
loss:[30.59164047241211, 26.29965353012085]
policies:[0, 4, 0]
qAverage:[0.0, 119.725341796875]
ws:[2.428147333860397, 4.96577912569046]
memory len:10000
memory used:2918.0
now epsilon is 0.038439974850156454, the reward is 247.25 with loss [23.316787242889404, 34.351277351379395] in episode 2382
Report: 
rewardSum:247.25
loss:[23.316787242889404, 34.351277351379395]
policies:[0, 4, 0]
qAverage:[0.0, 115.70116806030273]
ws:[3.174920402467251, 6.022793650627136]
memory len:10000
memory used:2917.0
now epsilon is 0.03840154928789453, the reward is 247.25 with loss [31.130562782287598, 34.18653440475464] in episode 2383
Report: 
rewardSum:247.25
loss:[31.130562782287598, 34.18653440475464]
policies:[0, 4, 0]
qAverage:[0.0, 119.39511413574219]
ws:[2.784970301389694, 5.418802917003632]
memory len:10000
memory used:2917.0
now epsilon is 0.03836316213678768, the reward is 247.25 with loss [27.878844738006592, 33.11270809173584] in episode 2384
Report: 
rewardSum:247.25
loss:[27.878844738006592, 33.11270809173584]
policies:[0, 4, 0]
qAverage:[0.0, 118.84943237304688]
ws:[3.2366925939917564, 6.321410751342773]
memory len:10000
memory used:2917.0
now epsilon is 0.03832481335843915, the reward is 37.15999999999997 with loss [15.721075296401978, 20.462644815444946] in episode 2385
Report: 
rewardSum:37.15999999999997
loss:[15.721075296401978, 20.462644815444946]
policies:[0, 3, 1]
qAverage:[0.0, 105.75774383544922]
ws:[0.22836551070213318, 2.9839926958084106]
memory len:10000
memory used:2917.0
now epsilon is 0.038286502914490574, the reward is 247.25 with loss [27.523700714111328, 26.485952854156494] in episode 2386
Report: 
rewardSum:247.25
loss:[27.523700714111328, 26.485952854156494]
policies:[0, 4, 0]
qAverage:[0.0, 119.23018493652344]
ws:[4.06935338973999, 8.236871814727783]
memory len:10000
memory used:2917.0
now epsilon is 0.03824823076662192, the reward is 247.25 with loss [27.960313320159912, 26.282021045684814] in episode 2387
Report: 
rewardSum:247.25
loss:[27.960313320159912, 26.282021045684814]
policies:[0, 4, 0]
qAverage:[0.0, 117.23181762695313]
ws:[4.178796637058258, 7.8785850524902346]
memory len:10000
memory used:2918.0
now epsilon is 0.03820999687655147, the reward is 247.25 with loss [19.387129545211792, 33.061625957489014] in episode 2388
Report: 
rewardSum:247.25
loss:[19.387129545211792, 33.061625957489014]
policies:[0, 4, 0]
qAverage:[0.0, 116.51353149414062]
ws:[4.058765757083893, 8.19588327407837]
memory len:10000
memory used:2918.0
now epsilon is 0.038171801206035784, the reward is 247.25 with loss [21.598237991333008, 28.089687824249268] in episode 2389
Report: 
rewardSum:247.25
loss:[21.598237991333008, 28.089687824249268]
policies:[1, 3, 0]
qAverage:[0.0, 74.87548828125]
ws:[0.35477542877197266, 3.42226505279541]
memory len:10000
memory used:2917.0
now epsilon is 0.038133643716869614, the reward is 247.25 with loss [30.88497304916382, 25.935118198394775] in episode 2390
Report: 
rewardSum:247.25
loss:[30.88497304916382, 25.935118198394775]
policies:[0, 4, 0]
qAverage:[0.0, 116.10061798095703]
ws:[3.7172490894794463, 7.624286270141601]
memory len:10000
memory used:2917.0
now epsilon is 0.038095524370885946, the reward is 247.25 with loss [27.87892484664917, 24.30222797393799] in episode 2391
Report: 
rewardSum:247.25
loss:[27.87892484664917, 24.30222797393799]
policies:[0, 4, 0]
qAverage:[0.0, 115.2522705078125]
ws:[3.933763098716736, 8.323179054260255]
memory len:10000
memory used:2917.0
now epsilon is 0.038038416786981104, the reward is 245.25 with loss [38.41987752914429, 32.99974179267883] in episode 2392
Report: 
rewardSum:245.25
loss:[38.41987752914429, 32.99974179267883]
policies:[0, 5, 1]
qAverage:[0.0, 116.78346557617188]
ws:[3.470007783174515, 7.702789402008056]
memory len:10000
memory used:2917.0
now epsilon is 0.0379813948109316, the reward is 245.25 with loss [46.569425106048584, 31.201684951782227] in episode 2393
Report: 
rewardSum:245.25
loss:[46.569425106048584, 31.201684951782227]
policies:[0, 5, 1]
qAverage:[0.0, 123.73247782389323]
ws:[2.67638298869133, 6.935853064060211]
memory len:10000
memory used:2917.0
now epsilon is 0.03794342765677004, the reward is 247.25 with loss [24.9176025390625, 34.60510730743408] in episode 2394
Report: 
rewardSum:247.25
loss:[24.9176025390625, 34.60510730743408]
policies:[0, 4, 0]
qAverage:[0.0, 116.40484771728515]
ws:[3.6449271202087403, 8.354920196533204]
memory len:10000
memory used:2917.0
now epsilon is 0.037905498455527334, the reward is 247.25 with loss [25.17866849899292, 27.266133308410645] in episode 2395
Report: 
rewardSum:247.25
loss:[25.17866849899292, 27.266133308410645]
policies:[0, 4, 0]
qAverage:[0.0, 116.23653564453124]
ws:[3.8635478973388673, 8.887737655639649]
memory len:10000
memory used:2917.0
now epsilon is 0.03786760716926479, the reward is 247.25 with loss [31.65579319000244, 37.10776662826538] in episode 2396
Report: 
rewardSum:247.25
loss:[31.65579319000244, 37.10776662826538]
policies:[1, 3, 0]
qAverage:[0.0, 103.6434440612793]
ws:[0.7293732613325119, 4.846107661724091]
memory len:10000
memory used:2917.0
now epsilon is 0.037829753760081644, the reward is 247.25 with loss [33.87473487854004, 26.537688732147217] in episode 2397
Report: 
rewardSum:247.25
loss:[33.87473487854004, 26.537688732147217]
policies:[0, 4, 0]
qAverage:[0.0, 117.40689086914062]
ws:[3.147164762020111, 8.285344171524049]
memory len:10000
memory used:2917.0
now epsilon is 0.03779193819011502, the reward is 247.25 with loss [30.61742115020752, 21.687598705291748] in episode 2398
Report: 
rewardSum:247.25
loss:[30.61742115020752, 21.687598705291748]
policies:[0, 4, 0]
qAverage:[0.0, 114.847119140625]
ws:[2.9200418502092362, 7.990680837631226]
memory len:10000
memory used:2917.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22*		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.037754160421539874, the reward is 247.25 with loss [31.852481365203857, 21.219041109085083] in episode 2399
Report: 
rewardSum:247.25
loss:[31.852481365203857, 21.219041109085083]
policies:[0, 4, 0]
qAverage:[0.0, 114.41515655517578]
ws:[2.602275540679693, 7.220691347122193]
memory len:10000
memory used:2918.0
now epsilon is 0.03771642041656901, the reward is 247.25 with loss [22.417261600494385, 27.474562168121338] in episode 2400
Report: 
rewardSum:247.25
loss:[22.417261600494385, 27.474562168121338]
policies:[0, 4, 0]
qAverage:[0.0, 115.0376968383789]
ws:[2.757815881073475, 7.839106273651123]
memory len:10000
memory used:2918.0
now epsilon is 0.03767871813745298, the reward is 247.25 with loss [27.37275981903076, 42.030141830444336] in episode 2401
Report: 
rewardSum:247.25
loss:[27.37275981903076, 42.030141830444336]
policies:[0, 4, 0]
qAverage:[0.0, 114.9652603149414]
ws:[1.9722510453313589, 6.665358066558838]
memory len:10000
memory used:2918.0
now epsilon is 0.037641053546480056, the reward is 247.25 with loss [21.089028120040894, 21.548728227615356] in episode 2402
Report: 
rewardSum:247.25
loss:[21.089028120040894, 21.548728227615356]
policies:[0, 4, 0]
qAverage:[0.0, 114.49558715820312]
ws:[1.928291031718254, 6.429073762893677]
memory len:10000
memory used:2918.0
now epsilon is 0.037603426605976244, the reward is 247.25 with loss [24.625284671783447, 26.258834838867188] in episode 2403
Report: 
rewardSum:247.25
loss:[24.625284671783447, 26.258834838867188]
policies:[0, 4, 0]
qAverage:[0.0, 115.08832244873047]
ws:[2.074214994907379, 6.651004695892334]
memory len:10000
memory used:2918.0
now epsilon is 0.03756583727830518, the reward is 247.25 with loss [31.608474493026733, 22.30113410949707] in episode 2404
Report: 
rewardSum:247.25
loss:[31.608474493026733, 22.30113410949707]
policies:[0, 4, 0]
qAverage:[0.0, 115.91071929931641]
ws:[2.6628201603889465, 7.459162950515747]
memory len:10000
memory used:2918.0
now epsilon is 0.03752828552586814, the reward is 247.25 with loss [28.57991123199463, 19.937909603118896] in episode 2405
Report: 
rewardSum:247.25
loss:[28.57991123199463, 19.937909603118896]
policies:[0, 4, 0]
qAverage:[0.0, 114.05013275146484]
ws:[2.394539105892181, 6.875910425186158]
memory len:10000
memory used:2917.0
now epsilon is 0.03749077131110398, the reward is 247.25 with loss [37.343140602111816, 34.80259418487549] in episode 2406
Report: 
rewardSum:247.25
loss:[37.343140602111816, 34.80259418487549]
policies:[0, 4, 0]
qAverage:[0.0, 115.59340057373046]
ws:[2.7771296977996824, 7.381486463546753]
memory len:10000
memory used:2917.0
now epsilon is 0.03745329459648909, the reward is 247.25 with loss [30.335505962371826, 22.065110445022583] in episode 2407
Report: 
rewardSum:247.25
loss:[30.335505962371826, 22.065110445022583]
policies:[0, 4, 0]
qAverage:[0.0, 114.20707855224609]
ws:[3.1558161437511445, 7.797933626174927]
memory len:10000
memory used:2917.0
now epsilon is 0.037415855344537396, the reward is 247.25 with loss [29.62485098838806, 26.148801803588867] in episode 2408
Report: 
rewardSum:247.25
loss:[29.62485098838806, 26.148801803588867]
policies:[0, 4, 0]
qAverage:[0.0, 114.84299163818359]
ws:[3.63842414021492, 8.58421688079834]
memory len:10000
memory used:2917.0
now epsilon is 0.037378453517800274, the reward is 247.25 with loss [24.51088857650757, 27.07209348678589] in episode 2409
Report: 
rewardSum:247.25
loss:[24.51088857650757, 27.07209348678589]
policies:[0, 4, 0]
qAverage:[0.0, 113.80819091796874]
ws:[3.6355940461158753, 8.0801495552063]
memory len:10000
memory used:2917.0
now epsilon is 0.03734108907886654, the reward is 247.25 with loss [37.8264799118042, 32.419424057006836] in episode 2410
Report: 
rewardSum:247.25
loss:[37.8264799118042, 32.419424057006836]
policies:[0, 4, 0]
qAverage:[0.0, 108.65279579162598]
ws:[4.615522921085358, 8.833471715450287]
memory len:10000
memory used:2917.0
now epsilon is 0.03730376199036241, the reward is 247.25 with loss [17.00574779510498, 22.669854164123535] in episode 2411
Report: 
rewardSum:247.25
loss:[17.00574779510498, 22.669854164123535]
policies:[0, 4, 0]
qAverage:[0.0, 114.14336395263672]
ws:[4.1598584175109865, 8.653402614593507]
memory len:10000
memory used:2917.0
now epsilon is 0.037266472214951454, the reward is 247.25 with loss [20.798027515411377, 22.323482990264893] in episode 2412
Report: 
rewardSum:247.25
loss:[20.798027515411377, 22.323482990264893]
policies:[0, 4, 0]
qAverage:[0.0, 114.07665252685547]
ws:[3.376896917819977, 7.213073682785034]
memory len:10000
memory used:2917.0
now epsilon is 0.037229219715334584, the reward is 247.25 with loss [19.622040033340454, 19.878257989883423] in episode 2413
Report: 
rewardSum:247.25
loss:[19.622040033340454, 19.878257989883423]
policies:[1, 3, 0]
qAverage:[0.0, 106.1357364654541]
ws:[3.199533373117447, 6.468115866184235]
memory len:10000
memory used:2917.0
now epsilon is 0.037192004454249965, the reward is 247.25 with loss [29.528175354003906, 28.753993034362793] in episode 2414
Report: 
rewardSum:247.25
loss:[29.528175354003906, 28.753993034362793]
policies:[0, 4, 0]
qAverage:[0.0, 115.3959976196289]
ws:[2.588077926635742, 6.155908870697021]
memory len:10000
memory used:2917.0
now epsilon is 0.03715482639447303, the reward is 247.25 with loss [23.357584476470947, 25.626338005065918] in episode 2415
Report: 
rewardSum:247.25
loss:[23.357584476470947, 25.626338005065918]
policies:[0, 4, 0]
qAverage:[0.0, 115.90861358642579]
ws:[2.879833161830902, 6.755470085144043]
memory len:10000
memory used:2917.0
now epsilon is 0.03711768549881643, the reward is 247.25 with loss [26.173956871032715, 32.560516357421875] in episode 2416
Report: 
rewardSum:247.25
loss:[26.173956871032715, 32.560516357421875]
policies:[0, 4, 0]
qAverage:[0.0, 114.06050262451171]
ws:[3.451124316453934, 8.04119119644165]
memory len:10000
memory used:2917.0
now epsilon is 0.03708058173012997, the reward is 247.25 with loss [29.091188430786133, 26.93059754371643] in episode 2417
Report: 
rewardSum:247.25
loss:[29.091188430786133, 26.93059754371643]
policies:[0, 4, 0]
qAverage:[0.0, 116.45254821777344]
ws:[3.4002010226249695, 7.628397798538208]
memory len:10000
memory used:2917.0
now epsilon is 0.037043515051300595, the reward is 247.25 with loss [38.56349182128906, 23.384170532226562] in episode 2418
Report: 
rewardSum:247.25
loss:[38.56349182128906, 23.384170532226562]
policies:[0, 4, 0]
qAverage:[0.0, 114.97703552246094]
ws:[3.347133032977581, 7.3910380601882935]
memory len:10000
memory used:2917.0
now epsilon is 0.037006485425252374, the reward is 247.25 with loss [25.15703248977661, 29.182126998901367] in episode 2419
Report: 
rewardSum:247.25
loss:[25.15703248977661, 29.182126998901367]
policies:[0, 4, 0]
qAverage:[0.0, 116.09071044921875]
ws:[3.7677615970373153, 8.140975618362427]
memory len:10000
memory used:2917.0
now epsilon is 0.036969492814946395, the reward is 247.25 with loss [41.14043998718262, 30.990234851837158] in episode 2420
Report: 
rewardSum:247.25
loss:[41.14043998718262, 30.990234851837158]
policies:[0, 4, 0]
qAverage:[0.0, 115.6944808959961]
ws:[3.9921530455350878, 8.340891218185424]
memory len:10000
memory used:2917.0
now epsilon is 0.03693253718338081, the reward is 247.25 with loss [18.192459106445312, 32.29843091964722] in episode 2421
Report: 
rewardSum:247.25
loss:[18.192459106445312, 32.29843091964722]
policies:[0, 4, 0]
qAverage:[0.0, 115.12163696289062]
ws:[3.740197664499283, 8.054894876480102]
memory len:10000
memory used:2918.0
now epsilon is 0.03689561849359074, the reward is 247.25 with loss [22.65423822402954, 36.17003154754639] in episode 2422
Report: 
rewardSum:247.25
loss:[22.65423822402954, 36.17003154754639]
policies:[0, 4, 0]
qAverage:[0.0, 117.00238494873047]
ws:[3.3602449238300323, 7.692447829246521]
memory len:10000
memory used:2917.0
now epsilon is 0.03685873670864826, the reward is 247.25 with loss [23.370924949645996, 29.447601795196533] in episode 2423
Report: 
rewardSum:247.25
loss:[23.370924949645996, 29.447601795196533]
policies:[1, 3, 0]
qAverage:[0.0, 99.20922660827637]
ws:[0.056114207953214645, 3.4569039940834045]
memory len:10000
memory used:2917.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22*		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.03682189179166236, the reward is 247.25 with loss [24.06237769126892, 36.607882499694824] in episode 2424
Report: 
rewardSum:247.25
loss:[24.06237769126892, 36.607882499694824]
policies:[0, 4, 0]
qAverage:[0.0, 118.06400756835937]
ws:[2.829486484825611, 6.939002108573914]
memory len:10000
memory used:2918.0
now epsilon is 0.0367850837057789, the reward is 247.25 with loss [34.67265510559082, 33.11183786392212] in episode 2425
Report: 
rewardSum:247.25
loss:[34.67265510559082, 33.11183786392212]
policies:[0, 4, 0]
qAverage:[0.0, 118.6373291015625]
ws:[3.3214315116405486, 8.350559401512147]
memory len:10000
memory used:2918.0
now epsilon is 0.036748312414180584, the reward is 247.25 with loss [30.493201732635498, 25.91136884689331] in episode 2426
Report: 
rewardSum:247.25
loss:[30.493201732635498, 25.91136884689331]
policies:[0, 4, 0]
qAverage:[0.0, 119.50640258789062]
ws:[3.780457502603531, 8.811258840560914]
memory len:10000
memory used:2918.0
now epsilon is 0.036711577880086936, the reward is 247.25 with loss [15.220580577850342, 32.256123542785645] in episode 2427
Report: 
rewardSum:247.25
loss:[15.220580577850342, 32.256123542785645]
policies:[0, 4, 0]
qAverage:[0.0, 118.89876937866211]
ws:[6.288464277982712, 12.896555423736572]
memory len:10000
memory used:2925.0
now epsilon is 0.036674880066754234, the reward is 247.25 with loss [21.295475482940674, 22.43463695049286] in episode 2428
Report: 
rewardSum:247.25
loss:[21.295475482940674, 22.43463695049286]
policies:[0, 4, 0]
qAverage:[0.0, 118.99773712158203]
ws:[5.856149971485138, 12.43594732284546]
memory len:10000
memory used:2926.0
now epsilon is 0.03663821893747548, the reward is 247.25 with loss [15.71079421043396, 24.671194076538086] in episode 2429
Report: 
rewardSum:247.25
loss:[15.71079421043396, 24.671194076538086]
policies:[0, 3, 1]
qAverage:[0.0, 102.43342018127441]
ws:[1.9922839999198914, 9.142542243003845]
memory len:10000
memory used:2927.0
now epsilon is 0.03660159445558036, the reward is 247.25 with loss [28.86826181411743, 30.205078125] in episode 2430
Report: 
rewardSum:247.25
loss:[28.86826181411743, 30.205078125]
policies:[0, 4, 0]
qAverage:[0.0, 119.28516693115235]
ws:[5.900543117523194, 14.175494766235351]
memory len:10000
memory used:2926.0
now epsilon is 0.036565006584435254, the reward is 247.25 with loss [22.437269687652588, 35.111942291259766] in episode 2431
Report: 
rewardSum:247.25
loss:[22.437269687652588, 35.111942291259766]
policies:[0, 4, 0]
qAverage:[0.0, 119.37685699462891]
ws:[6.581461310386658, 15.71834955215454]
memory len:10000
memory used:2927.0
now epsilon is 0.03652845528744312, the reward is 247.25 with loss [22.587605237960815, 29.44629716873169] in episode 2432
Report: 
rewardSum:247.25
loss:[22.587605237960815, 29.44629716873169]
policies:[0, 4, 0]
qAverage:[0.0, 118.83731079101562]
ws:[6.474915623664856, 15.533625316619872]
memory len:10000
memory used:2927.0
now epsilon is 0.03649194052804353, the reward is 247.25 with loss [27.337175607681274, 30.277040481567383] in episode 2433
Report: 
rewardSum:247.25
loss:[27.337175607681274, 30.277040481567383]
policies:[0, 4, 0]
qAverage:[0.0, 118.70300750732422]
ws:[5.458921872079372, 14.179084587097169]
memory len:10000
memory used:2926.0
now epsilon is 0.036455462269712594, the reward is 247.25 with loss [24.2976975440979, 28.4795880317688] in episode 2434
Report: 
rewardSum:247.25
loss:[24.2976975440979, 28.4795880317688]
policies:[0, 4, 0]
qAverage:[0.0, 118.47752380371094]
ws:[5.437298166751861, 14.295592784881592]
memory len:10000
memory used:2927.0
now epsilon is 0.03641902047596291, the reward is 247.25 with loss [27.378344535827637, 24.599478721618652] in episode 2435
Report: 
rewardSum:247.25
loss:[27.378344535827637, 24.599478721618652]
policies:[0, 4, 0]
qAverage:[0.0, 119.40330352783204]
ws:[4.680795514583588, 12.517561435699463]
memory len:10000
memory used:2926.0
now epsilon is 0.03638261511034358, the reward is 247.25 with loss [35.5148286819458, 25.272887229919434] in episode 2436
Report: 
rewardSum:247.25
loss:[35.5148286819458, 25.272887229919434]
policies:[0, 4, 0]
qAverage:[0.0, 118.24848175048828]
ws:[3.4897173762321474, 10.305599737167359]
memory len:10000
memory used:2926.0
now epsilon is 0.036346246136440136, the reward is 247.25 with loss [22.09201717376709, 22.096527576446533] in episode 2437
Report: 
rewardSum:247.25
loss:[22.09201717376709, 22.096527576446533]
policies:[0, 4, 0]
qAverage:[0.0, 122.31610565185547]
ws:[2.8451180458068848, 9.095901274681092]
memory len:10000
memory used:2926.0
now epsilon is 0.03629176083048516, the reward is 245.25 with loss [36.591925859451294, 33.91031765937805] in episode 2438
Report: 
rewardSum:245.25
loss:[36.591925859451294, 33.91031765937805]
policies:[0, 5, 1]
qAverage:[0.0, 122.72165807088216]
ws:[2.3004556596279144, 8.657729009787241]
memory len:10000
memory used:2926.0
now epsilon is 0.036255482676796895, the reward is 247.25 with loss [31.386610507965088, 34.58606815338135] in episode 2439
Report: 
rewardSum:247.25
loss:[31.386610507965088, 34.58606815338135]
policies:[0, 4, 0]
qAverage:[0.0, 121.33164978027344]
ws:[3.3210989832878113, 9.690637922286987]
memory len:10000
memory used:2926.0
now epsilon is 0.03621924078766028, the reward is 247.25 with loss [14.139173984527588, 30.849647998809814] in episode 2440
Report: 
rewardSum:247.25
loss:[14.139173984527588, 30.849647998809814]
policies:[0, 3, 1]
qAverage:[0.0, 105.5065689086914]
ws:[0.19416468031704426, 5.422675848007202]
memory len:10000
memory used:2925.0
now epsilon is 0.03618303512682435, the reward is 247.25 with loss [25.649194717407227, 31.63083577156067] in episode 2441
Report: 
rewardSum:247.25
loss:[25.649194717407227, 31.63083577156067]
policies:[0, 3, 1]
qAverage:[0.0, 117.71875]
ws:[5.97567543387413, 13.091987490653992]
memory len:10000
memory used:2925.0
now epsilon is 0.03614686565807441, the reward is 247.25 with loss [34.2480034828186, 23.785473585128784] in episode 2442
Report: 
rewardSum:247.25
loss:[34.2480034828186, 23.785473585128784]
policies:[0, 4, 0]
qAverage:[0.0, 123.74906005859376]
ws:[5.836578035354615, 13.604449462890624]
memory len:10000
memory used:2925.0
now epsilon is 0.036110732345231926, the reward is 247.25 with loss [28.771921157836914, 27.2380313873291] in episode 2443
Report: 
rewardSum:247.25
loss:[28.771921157836914, 27.2380313873291]
policies:[0, 4, 0]
qAverage:[0.0, 123.06557769775391]
ws:[5.563565993309021, 11.982331657409668]
memory len:10000
memory used:2926.0
now epsilon is 0.03607463515215455, the reward is 247.25 with loss [34.66908550262451, 24.261767387390137] in episode 2444
Report: 
rewardSum:247.25
loss:[34.66908550262451, 24.261767387390137]
policies:[0, 4, 0]
qAverage:[0.0, 123.10876770019532]
ws:[5.4330208659172055, 12.448737573623657]
memory len:10000
memory used:2926.0
now epsilon is 0.036038574042736055, the reward is 247.25 with loss [25.6202974319458, 29.524510860443115] in episode 2445
Report: 
rewardSum:247.25
loss:[25.6202974319458, 29.524510860443115]
policies:[0, 4, 0]
qAverage:[0.0, 123.39625396728516]
ws:[5.608834290504456, 12.166075706481934]
memory len:10000
memory used:2926.0
now epsilon is 0.036002548980906324, the reward is 247.25 with loss [25.857060432434082, 28.352662086486816] in episode 2446
Report: 
rewardSum:247.25
loss:[25.857060432434082, 28.352662086486816]
policies:[0, 4, 0]
qAverage:[0.0, 121.73109741210938]
ws:[6.194057309627533, 12.805466365814208]
memory len:10000
memory used:2935.0
now epsilon is 0.03596655993063127, the reward is 247.25 with loss [28.35306167602539, 27.40023946762085] in episode 2447
Report: 
rewardSum:247.25
loss:[28.35306167602539, 27.40023946762085]
policies:[0, 4, 0]
qAverage:[0.0, 123.49419860839843]
ws:[6.105876159667969, 13.049423694610596]
memory len:10000
memory used:2934.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26*		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.035930606855912844, the reward is 247.25 with loss [27.43545627593994, 23.605458736419678] in episode 2448
Report: 
rewardSum:247.25
loss:[27.43545627593994, 23.605458736419678]
policies:[0, 4, 0]
qAverage:[0.0, 121.68504791259765]
ws:[5.300938463211059, 11.259976959228515]
memory len:10000
memory used:2934.0
############# STATE ###############
0*		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.035894689720788985, the reward is 247.25 with loss [19.226378679275513, 23.844772577285767] in episode 2449
Report: 
rewardSum:247.25
loss:[19.226378679275513, 23.844772577285767]
policies:[0, 4, 0]
qAverage:[0.0, 131.50347290039062]
ws:[4.09353035017848, 9.516138601303101]
memory len:10000
memory used:2935.0
now epsilon is 0.035858808489333566, the reward is 247.25 with loss [24.843034505844116, 30.532633781433105] in episode 2450
Report: 
rewardSum:247.25
loss:[24.843034505844116, 30.532633781433105]
policies:[1, 3, 0]
qAverage:[0.0, 128.26651763916016]
ws:[4.678465306758881, 11.24108076095581]
memory len:10000
memory used:2935.0
now epsilon is 0.03582296312565638, the reward is 247.25 with loss [21.846113681793213, 26.296141624450684] in episode 2451
Report: 
rewardSum:247.25
loss:[21.846113681793213, 26.296141624450684]
policies:[0, 4, 0]
qAverage:[0.0, 126.9834228515625]
ws:[3.4402186572551727, 8.93540689945221]
memory len:10000
memory used:2938.0
now epsilon is 0.0357871535939031, the reward is 247.25 with loss [27.30090570449829, 29.73710584640503] in episode 2452
Report: 
rewardSum:247.25
loss:[27.30090570449829, 29.73710584640503]
policies:[0, 4, 0]
qAverage:[0.0, 118.68574905395508]
ws:[4.283854857087135, 9.06172040104866]
memory len:10000
memory used:2937.0
now epsilon is 0.035751379858255244, the reward is 247.25 with loss [27.52412509918213, 28.8094744682312] in episode 2453
Report: 
rewardSum:247.25
loss:[27.52412509918213, 28.8094744682312]
policies:[1, 3, 0]
qAverage:[0.0, 116.37040901184082]
ws:[4.809632800519466, 9.834586590528488]
memory len:10000
memory used:2938.0
now epsilon is 0.03571564188293012, the reward is 247.25 with loss [28.38562536239624, 23.34305238723755] in episode 2454
Report: 
rewardSum:247.25
loss:[28.38562536239624, 23.34305238723755]
policies:[0, 4, 0]
qAverage:[0.0, 123.6407527923584]
ws:[5.785333849489689, 11.704273760318756]
memory len:10000
memory used:2937.0
now epsilon is 0.03567993963218081, the reward is 247.25 with loss [33.04381799697876, 33.02091455459595] in episode 2455
Report: 
rewardSum:247.25
loss:[33.04381799697876, 33.02091455459595]
policies:[0, 4, 0]
qAverage:[0.0, 126.91540832519532]
ws:[6.80229001045227, 14.449236297607422]
memory len:10000
memory used:2937.0
now epsilon is 0.035644273070296134, the reward is 247.25 with loss [22.898197412490845, 29.5452663898468] in episode 2456
Report: 
rewardSum:247.25
loss:[22.898197412490845, 29.5452663898468]
policies:[0, 4, 0]
qAverage:[0.0, 127.28463134765624]
ws:[7.7466460227966305, 15.518629550933838]
memory len:10000
memory used:2937.0
now epsilon is 0.03560864216160062, the reward is 247.25 with loss [29.991736888885498, 27.144068717956543] in episode 2457
Report: 
rewardSum:247.25
loss:[29.991736888885498, 27.144068717956543]
policies:[0, 4, 0]
qAverage:[0.0, 109.25711822509766]
ws:[2.5571632981300354, 9.851967096328735]
memory len:10000
memory used:2937.0
now epsilon is 0.03555526257033464, the reward is 245.25 with loss [53.13796067237854, 64.20279598236084] in episode 2458
Report: 
rewardSum:245.25
loss:[53.13796067237854, 64.20279598236084]
policies:[0, 5, 1]
qAverage:[0.0, 128.9575424194336]
ws:[7.880655984083812, 16.94200563430786]
memory len:10000
memory used:2937.0
now epsilon is 0.035519720638765705, the reward is 247.25 with loss [17.49456751346588, 29.36177682876587] in episode 2459
Report: 
rewardSum:247.25
loss:[17.49456751346588, 29.36177682876587]
policies:[0, 4, 0]
qAverage:[0.0, 127.3776741027832]
ws:[8.68110653758049, 16.743481397628784]
memory len:10000
memory used:2938.0
now epsilon is 0.03548421423580234, the reward is 247.25 with loss [38.825039863586426, 34.45981979370117] in episode 2460
Report: 
rewardSum:247.25
loss:[38.825039863586426, 34.45981979370117]
policies:[0, 4, 0]
qAverage:[0.0, 115.74049758911133]
ws:[7.6505178809165955, 13.05059489607811]
memory len:10000
memory used:2938.0
now epsilon is 0.03544874332592926, the reward is 247.25 with loss [38.97470760345459, 37.05134057998657] in episode 2461
Report: 
rewardSum:247.25
loss:[38.97470760345459, 37.05134057998657]
policies:[1, 3, 0]
qAverage:[0.0, 115.28947448730469]
ws:[-0.24139218591153622, 4.946013107895851]
memory len:10000
memory used:2937.0
now epsilon is 0.03541330787366667, the reward is 247.25 with loss [19.75374436378479, 25.547440767288208] in episode 2462
Report: 
rewardSum:247.25
loss:[19.75374436378479, 25.547440767288208]
policies:[1, 3, 0]
qAverage:[0.0, 116.64802551269531]
ws:[7.250157356262207, 11.277257323265076]
memory len:10000
memory used:2938.0
now epsilon is 0.03537790784357027, the reward is 247.25 with loss [24.60958504676819, 30.355594158172607] in episode 2463
Report: 
rewardSum:247.25
loss:[24.60958504676819, 30.355594158172607]
policies:[0, 4, 0]
qAverage:[0.0, 131.25939483642577]
ws:[6.862027260661125, 11.334168767929077]
memory len:10000
memory used:2938.0
now epsilon is 0.03534254320023117, the reward is 247.25 with loss [26.10044574737549, 19.179949522018433] in episode 2464
Report: 
rewardSum:247.25
loss:[26.10044574737549, 19.179949522018433]
policies:[0, 4, 0]
qAverage:[0.0, 128.53329620361328]
ws:[7.105257666110992, 11.723669958114623]
memory len:10000
memory used:2938.0
now epsilon is 0.03530721390827587, the reward is 247.25 with loss [22.102012634277344, 26.675435304641724] in episode 2465
Report: 
rewardSum:247.25
loss:[22.102012634277344, 26.675435304641724]
policies:[0, 4, 0]
qAverage:[0.0, 130.5506362915039]
ws:[6.8394465208053585, 10.356869077682495]
memory len:10000
memory used:2938.0
now epsilon is 0.03527191993236626, the reward is 247.25 with loss [21.420764923095703, 35.23286437988281] in episode 2466
Report: 
rewardSum:247.25
loss:[21.420764923095703, 35.23286437988281]
policies:[0, 4, 0]
qAverage:[0.0, 129.7867874145508]
ws:[7.470139575004578, 11.900767087936401]
memory len:10000
memory used:2938.0
now epsilon is 0.03523666123719951, the reward is 247.25 with loss [17.931392669677734, 25.961348056793213] in episode 2467
Report: 
rewardSum:247.25
loss:[17.931392669677734, 25.961348056793213]
policies:[0, 4, 0]
qAverage:[0.0, 129.21779174804686]
ws:[7.032198596000671, 10.714172172546387]
memory len:10000
memory used:2938.0
now epsilon is 0.03520143778750813, the reward is 247.25 with loss [25.05342435836792, 27.86495351791382] in episode 2468
Report: 
rewardSum:247.25
loss:[25.05342435836792, 27.86495351791382]
policies:[0, 4, 0]
qAverage:[0.0, 129.77498474121094]
ws:[7.6747581481933596, 12.49705457687378]
memory len:10000
memory used:2938.0
now epsilon is 0.035166249548059844, the reward is 247.25 with loss [20.668482303619385, 17.218073844909668] in episode 2469
Report: 
rewardSum:247.25
loss:[20.668482303619385, 17.218073844909668]
policies:[0, 4, 0]
qAverage:[0.0, 130.03943481445313]
ws:[7.910287523269654, 12.524412298202515]
memory len:10000
memory used:2950.0
now epsilon is 0.03513109648365762, the reward is 247.25 with loss [26.881446361541748, 34.7530951499939] in episode 2470
Report: 
rewardSum:247.25
loss:[26.881446361541748, 34.7530951499939]
policies:[0, 4, 0]
qAverage:[0.0, 128.3979965209961]
ws:[8.06105990409851, 14.136504983901977]
memory len:10000
memory used:2950.0
now epsilon is 0.035095978559139596, the reward is 247.25 with loss [25.691689014434814, 31.303914070129395] in episode 2471
Report: 
rewardSum:247.25
loss:[25.691689014434814, 31.303914070129395]
policies:[0, 4, 0]
qAverage:[0.0, 131.01093292236328]
ws:[7.182577097415924, 12.225912284851074]
memory len:10000
memory used:2950.0
now epsilon is 0.03506089573937906, the reward is 247.25 with loss [32.48753547668457, 27.210384845733643] in episode 2472
Report: 
rewardSum:247.25
loss:[32.48753547668457, 27.210384845733643]
policies:[0, 4, 0]
qAverage:[0.0, 127.83114776611328]
ws:[6.909648209810257, 11.863605546951295]
memory len:10000
memory used:2950.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22*		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.03502584798928442, the reward is 247.25 with loss [34.8853874206543, 42.58104133605957] in episode 2473
Report: 
rewardSum:247.25
loss:[34.8853874206543, 42.58104133605957]
policies:[0, 4, 0]
qAverage:[0.0, 134.7178741455078]
ws:[6.543725621700287, 11.405055904388428]
memory len:10000
memory used:2950.0
now epsilon is 0.034990835273799155, the reward is 247.25 with loss [28.92463731765747, 26.621201515197754] in episode 2474
Report: 
rewardSum:247.25
loss:[28.92463731765747, 26.621201515197754]
policies:[0, 4, 0]
qAverage:[0.0, 131.38392028808593]
ws:[7.6710601329803465, 14.08948369026184]
memory len:10000
memory used:2951.0
now epsilon is 0.034955857557901794, the reward is 247.25 with loss [35.1787805557251, 25.12491512298584] in episode 2475
Report: 
rewardSum:247.25
loss:[35.1787805557251, 25.12491512298584]
policies:[0, 4, 0]
qAverage:[0.0, 131.81878967285155]
ws:[6.976812255382538, 11.811845397949218]
memory len:10000
memory used:2952.0
now epsilon is 0.03492091480660588, the reward is 247.25 with loss [27.95993423461914, 26.39466094970703] in episode 2476
Report: 
rewardSum:247.25
loss:[27.95993423461914, 26.39466094970703]
policies:[0, 4, 0]
qAverage:[0.0, 130.77881774902343]
ws:[6.705249404907226, 12.663480138778686]
memory len:10000
memory used:2952.0
now epsilon is 0.0348860069849599, the reward is 247.25 with loss [24.594526290893555, 28.042073726654053] in episode 2477
Report: 
rewardSum:247.25
loss:[24.594526290893555, 28.042073726654053]
policies:[0, 4, 0]
qAverage:[0.0, 131.41714782714843]
ws:[6.090010553598404, 11.056862187385558]
memory len:10000
memory used:2952.0
now epsilon is 0.03485113405804733, the reward is 247.25 with loss [24.851861000061035, 38.58624505996704] in episode 2478
Report: 
rewardSum:247.25
loss:[24.851861000061035, 38.58624505996704]
policies:[0, 4, 0]
qAverage:[0.0, 129.89164123535156]
ws:[6.149031725525856, 11.368147802352905]
memory len:10000
memory used:2952.0
now epsilon is 0.0348162959909865, the reward is 247.25 with loss [33.82108116149902, 34.570982456207275] in episode 2479
Report: 
rewardSum:247.25
loss:[33.82108116149902, 34.570982456207275]
policies:[0, 3, 1]
qAverage:[0.0, 115.6725082397461]
ws:[0.21071500331163406, 6.699672371149063]
memory len:10000
memory used:2952.0
now epsilon is 0.03478149274893063, the reward is 247.25 with loss [14.323278665542603, 29.377176761627197] in episode 2480
Report: 
rewardSum:247.25
loss:[14.323278665542603, 29.377176761627197]
policies:[0, 4, 0]
qAverage:[0.0, 128.3895690917969]
ws:[7.766202092170715, 14.866864252090455]
memory len:10000
memory used:2952.0
now epsilon is 0.03474672429706778, the reward is 247.25 with loss [24.46930980682373, 27.390268325805664] in episode 2481
Report: 
rewardSum:247.25
loss:[24.46930980682373, 27.390268325805664]
policies:[0, 3, 1]
qAverage:[0.0, 123.93962860107422]
ws:[8.509175062179565, 13.627705454826355]
memory len:10000
memory used:2952.0
now epsilon is 0.03471199060062079, the reward is 247.25 with loss [28.209867000579834, 16.842681169509888] in episode 2482
Report: 
rewardSum:247.25
loss:[28.209867000579834, 16.842681169509888]
policies:[0, 4, 0]
qAverage:[0.0, 130.2425567626953]
ws:[6.531271994113922, 12.607368302345275]
memory len:10000
memory used:2952.0
now epsilon is 0.03467729162484728, the reward is 247.25 with loss [23.066346645355225, 26.809557914733887] in episode 2483
Report: 
rewardSum:247.25
loss:[23.066346645355225, 26.809557914733887]
policies:[0, 4, 0]
qAverage:[0.0, 128.3953826904297]
ws:[6.381117957830429, 12.9462571144104]
memory len:10000
memory used:2953.0
now epsilon is 0.034642627335039596, the reward is 247.25 with loss [26.481730461120605, 33.761592388153076] in episode 2484
Report: 
rewardSum:247.25
loss:[26.481730461120605, 33.761592388153076]
policies:[0, 3, 1]
qAverage:[0.0, 118.64664459228516]
ws:[8.324200510978699, 14.557647198438644]
memory len:10000
memory used:2952.0
now epsilon is 0.03460799769652477, the reward is 247.25 with loss [25.34257936477661, 35.15027952194214] in episode 2485
Report: 
rewardSum:247.25
loss:[25.34257936477661, 35.15027952194214]
policies:[0, 4, 0]
qAverage:[0.0, 124.5850830078125]
ws:[6.506445050239563, 13.978147435188294]
memory len:10000
memory used:2953.0
now epsilon is 0.034573402674664515, the reward is 247.25 with loss [35.86021089553833, 26.64322805404663] in episode 2486
Report: 
rewardSum:247.25
loss:[35.86021089553833, 26.64322805404663]
policies:[0, 3, 1]
qAverage:[0.0, 112.6395378112793]
ws:[-0.31269700545817614, 4.363584667444229]
memory len:10000
memory used:2953.0
now epsilon is 0.034538842234855156, the reward is 247.25 with loss [22.961369514465332, 28.66842484474182] in episode 2487
Report: 
rewardSum:247.25
loss:[22.961369514465332, 28.66842484474182]
policies:[0, 4, 0]
qAverage:[0.0, 127.03211669921875]
ws:[3.722839429974556, 9.461583983898162]
memory len:10000
memory used:2953.0
now epsilon is 0.034504316342527604, the reward is 247.25 with loss [30.30906629562378, 29.354365348815918] in episode 2488
Report: 
rewardSum:247.25
loss:[30.30906629562378, 29.354365348815918]
policies:[0, 4, 0]
qAverage:[0.0, 126.94890747070312]
ws:[3.6501447319984437, 9.323419839143753]
memory len:10000
memory used:2953.0
now epsilon is 0.03446982496314732, the reward is 247.25 with loss [23.006290197372437, 21.373530864715576] in episode 2489
Report: 
rewardSum:247.25
loss:[23.006290197372437, 21.373530864715576]
policies:[0, 4, 0]
qAverage:[0.0, 125.76868896484375]
ws:[3.5230124771595, 8.63321972489357]
memory len:10000
memory used:2953.0
now epsilon is 0.03443536806221431, the reward is 247.25 with loss [20.691781759262085, 33.727227210998535] in episode 2490
Report: 
rewardSum:247.25
loss:[20.691781759262085, 33.727227210998535]
policies:[0, 4, 0]
qAverage:[0.0, 127.37154235839844]
ws:[2.9329217046499254, 7.206046521663666]
memory len:10000
memory used:2953.0
now epsilon is 0.03440094560526305, the reward is 247.25 with loss [22.009212732315063, 28.405181407928467] in episode 2491
Report: 
rewardSum:247.25
loss:[22.009212732315063, 28.405181407928467]
policies:[0, 4, 0]
qAverage:[0.0, 126.08138122558594]
ws:[2.5223109014332294, 6.546260917186737]
memory len:10000
memory used:2953.0
now epsilon is 0.03436655755786247, the reward is 247.25 with loss [28.578320026397705, 24.374621868133545] in episode 2492
Report: 
rewardSum:247.25
loss:[28.578320026397705, 24.374621868133545]
policies:[0, 4, 0]
qAverage:[0.0, 126.35754699707032]
ws:[2.1949400145560505, 6.384555089473724]
memory len:10000
memory used:2953.0
now epsilon is 0.03433220388561592, the reward is 247.25 with loss [33.09597682952881, 28.270888566970825] in episode 2493
Report: 
rewardSum:247.25
loss:[33.09597682952881, 28.270888566970825]
policies:[1, 3, 0]
qAverage:[0.0, 125.66754150390625]
ws:[3.3632992915809155, 8.410494923591614]
memory len:10000
memory used:2953.0
now epsilon is 0.03429788455416114, the reward is 247.25 with loss [25.76789903640747, 32.76721382141113] in episode 2494
Report: 
rewardSum:247.25
loss:[25.76789903640747, 32.76721382141113]
policies:[0, 4, 0]
qAverage:[0.0, 124.32442626953124]
ws:[3.99640234708786, 8.35581021308899]
memory len:10000
memory used:2953.0
now epsilon is 0.034263599529170206, the reward is 247.25 with loss [32.87725210189819, 31.481013774871826] in episode 2495
Report: 
rewardSum:247.25
loss:[32.87725210189819, 31.481013774871826]
policies:[0, 4, 0]
qAverage:[0.0, 126.52520446777343]
ws:[3.764261892437935, 8.183919310569763]
memory len:10000
memory used:2953.0
now epsilon is 0.034229348776349526, the reward is 247.25 with loss [23.644896984100342, 20.61052656173706] in episode 2496
Report: 
rewardSum:247.25
loss:[23.644896984100342, 20.61052656173706]
policies:[0, 4, 0]
qAverage:[0.0, 126.13443603515626]
ws:[3.2504092514514924, 7.868461942672729]
memory len:10000
memory used:2953.0
now epsilon is 0.034195132261439776, the reward is 247.25 with loss [40.64866924285889, 29.101642608642578] in episode 2497
Report: 
rewardSum:247.25
loss:[40.64866924285889, 29.101642608642578]
policies:[0, 4, 0]
qAverage:[0.0, 124.6586700439453]
ws:[2.6937424421310423, 6.953272068500519]
memory len:10000
memory used:2953.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22*		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.034160949950215874, the reward is 247.25 with loss [32.78125286102295, 22.91690683364868] in episode 2498
Report: 
rewardSum:247.25
loss:[32.78125286102295, 22.91690683364868]
policies:[0, 4, 0]
qAverage:[0.0, 116.72791442871093]
ws:[3.2969143271446226, 7.844475817680359]
memory len:10000
memory used:2953.0
now epsilon is 0.03412680180848697, the reward is 247.25 with loss [36.25556945800781, 37.53700923919678] in episode 2499
Report: 
rewardSum:247.25
loss:[36.25556945800781, 37.53700923919678]
policies:[0, 4, 0]
qAverage:[0.0, 120.953076171875]
ws:[3.277716987300664, 7.550333786010742]
memory len:10000
memory used:2953.0
now epsilon is 0.03409268780209638, the reward is 247.25 with loss [26.313018321990967, 30.300231456756592] in episode 2500
Report: 
rewardSum:247.25
loss:[26.313018321990967, 30.300231456756592]
policies:[0, 3, 1]
qAverage:[0.0, 109.0871810913086]
ws:[0.20011698827147484, 3.667283743619919]
memory len:10000
memory used:2954.0
now epsilon is 0.03405860789692155, the reward is 247.25 with loss [33.1673002243042, 35.16947269439697] in episode 2501
Report: 
rewardSum:247.25
loss:[33.1673002243042, 35.16947269439697]
policies:[0, 4, 0]
qAverage:[0.0, 118.82401733398437]
ws:[3.212886479496956, 8.017029809951783]
memory len:10000
memory used:2954.0
now epsilon is 0.03402456205887406, the reward is 247.25 with loss [28.452572345733643, 25.395379066467285] in episode 2502
Report: 
rewardSum:247.25
loss:[28.452572345733643, 25.395379066467285]
policies:[0, 4, 0]
qAverage:[0.0, 117.31397705078125]
ws:[3.480132055282593, 9.048033404350281]
memory len:10000
memory used:2954.0
now epsilon is 0.033982052616336085, the reward is 246.25 with loss [38.94598579406738, 31.799318313598633] in episode 2503
Report: 
rewardSum:246.25
loss:[38.94598579406738, 31.799318313598633]
policies:[0, 4, 1]
qAverage:[0.0, 118.15145263671874]
ws:[2.8471195712685584, 8.234142208099366]
memory len:10000
memory used:2954.0
now epsilon is 0.033948083304865734, the reward is 247.25 with loss [28.041032791137695, 28.781003952026367] in episode 2504
Report: 
rewardSum:247.25
loss:[28.041032791137695, 28.781003952026367]
policies:[0, 4, 0]
qAverage:[0.0, 120.4487060546875]
ws:[2.5042970359325407, 7.352856290340424]
memory len:10000
memory used:2954.0
now epsilon is 0.03391414794997048, the reward is 247.25 with loss [19.53212594985962, 30.439437866210938] in episode 2505
Report: 
rewardSum:247.25
loss:[19.53212594985962, 30.439437866210938]
policies:[0, 4, 0]
qAverage:[0.0, 116.91610107421874]
ws:[2.581163763999939, 7.736767542362213]
memory len:10000
memory used:2954.0
now epsilon is 0.0338802465177065, the reward is 247.25 with loss [23.318106174468994, 26.136853218078613] in episode 2506
Report: 
rewardSum:247.25
loss:[23.318106174468994, 26.136853218078613]
policies:[0, 4, 0]
qAverage:[0.0, 110.30166244506836]
ws:[-0.10488482192158699, 3.670656442642212]
memory len:10000
memory used:2954.0
now epsilon is 0.03384637897416385, the reward is 247.25 with loss [34.26929759979248, 31.49142599105835] in episode 2507
Report: 
rewardSum:247.25
loss:[34.26929759979248, 31.49142599105835]
policies:[0, 4, 0]
qAverage:[0.0, 115.01456909179687]
ws:[3.41773778796196, 9.662682437896729]
memory len:10000
memory used:2954.0
now epsilon is 0.033795641126107896, the reward is 245.25 with loss [47.942891120910645, 40.53113055229187] in episode 2508
Report: 
rewardSum:245.25
loss:[47.942891120910645, 40.53113055229187]
policies:[1, 4, 1]
qAverage:[0.0, 117.55121765136718]
ws:[3.7297672748565676, 10.37577838897705]
memory len:10000
memory used:2985.0
now epsilon is 0.03376185815623512, the reward is 247.25 with loss [37.31420946121216, 35.948431968688965] in episode 2509
Report: 
rewardSum:247.25
loss:[37.31420946121216, 35.948431968688965]
policies:[0, 4, 0]
qAverage:[0.0, 112.59696960449219]
ws:[4.463876023888588, 11.352109670639038]
memory len:10000
memory used:2980.0
now epsilon is 0.03355986833303214, the reward is 227.25 with loss [159.02626132965088, 170.57451963424683] in episode 2510
Report: 
rewardSum:227.25
loss:[159.02626132965088, 170.57451963424683]
policies:[0, 24, 0]
qAverage:[0.0, 132.12899047851562]
ws:[5.463377858400345, 11.980584783554077]
memory len:10000
memory used:2989.0
now epsilon is 0.033342409512281006, the reward is 225.25 with loss [174.6960859298706, 191.897132396698] in episode 2511
Report: 
rewardSum:225.25
loss:[174.6960859298706, 191.897132396698]
policies:[0, 24, 2]
qAverage:[0.0, 133.3065643310547]
ws:[2.3801268669962883, 5.848863419294357]
memory len:10000
memory used:2988.0
############# STATE ###############
0-		10-		20*		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.032813110480280676, the reward is 187.25 with loss [414.89976012706757, 437.8574950695038] in episode 2512
Report: 
rewardSum:187.25
loss:[414.89976012706757, 437.8574950695038]
policies:[0, 62, 2]
qAverage:[0.0, 118.58175043905935]
ws:[2.022821360477997, 4.629501502360067]
memory len:10000
memory used:2989.0
now epsilon is 0.032624953045462195, the reward is 228.25 with loss [140.58888578414917, 153.51743698120117] in episode 2513
Report: 
rewardSum:228.25
loss:[140.58888578414917, 153.51743698120117]
policies:[0, 22, 1]
qAverage:[0.0, 106.53681460293856]
ws:[0.9998519528995861, 5.0974607196721164]
memory len:10000
memory used:2989.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22*		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.03217937461421363, the reward is 196.25 with loss [359.4358503818512, 376.4804813861847] in episode 2514
Report: 
rewardSum:196.25
loss:[359.4358503818512, 376.4804813861847]
policies:[0, 55, 0]
qAverage:[0.0, 100.37077045440674]
ws:[0.21158635975526913, 3.0764171928167343]
memory len:10000
memory used:2990.0
now epsilon is 0.03204288545518057, the reward is -16.0 with loss [97.35306906700134, 120.25054836273193] in episode 2515
Report: 
rewardSum:-16.0
loss:[97.35306906700134, 120.25054836273193]
policies:[0, 16, 1]
qAverage:[0.0, 89.37979619643268]
ws:[-0.627251032520743, 1.7461776277598213]
memory len:10000
memory used:2921.0
now epsilon is 0.03187508020426977, the reward is 230.25 with loss [132.8149938583374, 119.54076933860779] in episode 2516
Report: 
rewardSum:230.25
loss:[132.8149938583374, 119.54076933860779]
policies:[0, 20, 1]
qAverage:[0.0, 86.79032287597656]
ws:[-0.9487730429973453, 2.6133149802684783]
memory len:10000
memory used:2919.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42*		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.031605256667978886, the reward is 217.25 with loss [232.3057942390442, 221.01460337638855] in episode 2517
Report: 
rewardSum:217.25
loss:[232.3057942390442, 221.01460337638855]
policies:[2, 32, 0]
qAverage:[2.2961192411534928, 81.71932422413545]
ws:[-0.034376607933903444, 1.77228313290021]
memory len:10000
memory used:3024.0
now epsilon is 0.03132205029624763, the reward is 215.25 with loss [224.24625313282013, 224.22794198989868] in episode 2518
Report: 
rewardSum:215.25
loss:[224.24625313282013, 224.22794198989868]
policies:[4, 30, 2]
qAverage:[9.30546875, 69.58432856968471]
ws:[-0.41905285652194707, 1.5657368008579526]
memory len:10000
memory used:3156.0
now epsilon is 0.031220406187897065, the reward is 238.25 with loss [79.55946326255798, 82.86616849899292] in episode 2519
Report: 
rewardSum:238.25
loss:[79.55946326255798, 82.86616849899292]
policies:[9, 4, 0]
qAverage:[46.22907257080078, 21.246597290039062]
ws:[0.740037624325071, 0.41366366882409367]
memory len:10000
memory used:3153.0
now epsilon is 0.03116581143678198, the reward is 244.25 with loss [41.346206963062286, 42.09844732284546] in episode 2520
Report: 
rewardSum:244.25
loss:[41.346206963062286, 42.09844732284546]
policies:[5, 2, 0]
qAverage:[44.67270565032959, 18.52975845336914]
ws:[0.9551823437213898, 0.009155955165624619]
memory len:10000
memory used:3148.0
now epsilon is 0.03107244245437158, the reward is 239.25 with loss [78.5863927602768, 99.20438480377197] in episode 2521
Report: 
rewardSum:239.25
loss:[78.5863927602768, 99.20438480377197]
policies:[9, 3, 0]
qAverage:[49.74585313063402, 17.1654293353741]
ws:[0.14847223231425652, -0.18465220297758395]
memory len:10000
memory used:3144.0
now epsilon is 0.031010351919057242, the reward is 243.25 with loss [52.716487407684326, 52.51741576194763] in episode 2522
Report: 
rewardSum:243.25
loss:[52.716487407684326, 52.51741576194763]
policies:[1, 7, 0]
qAverage:[7.875575595431858, 56.774063958062065]
ws:[-3.1167218221558466, -1.8265957534313202]
memory len:10000
memory used:3144.0
now epsilon is 0.030932913197755398, the reward is 241.25 with loss [79.18511629104614, 72.53334736824036] in episode 2523
Report: 
rewardSum:241.25
loss:[79.18511629104614, 72.53334736824036]
policies:[1, 8, 1]
qAverage:[7.302360534667969, 57.85216064453125]
ws:[-0.9686130374670029, -0.2359786331653595]
memory len:10000
memory used:3141.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12*		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.03088654281790017, the reward is 245.25 with loss [44.842127323150635, 38.526885747909546] in episode 2524
Report: 
rewardSum:245.25
loss:[44.842127323150635, 38.526885747909546]
policies:[1, 5, 0]
qAverage:[8.83423560006278, 45.909529549734934]
ws:[-2.7924404367804527, -1.4923241010734014]
memory len:10000
memory used:3140.0
now epsilon is 0.030840241950156985, the reward is 245.25 with loss [46.63387441635132, 40.224204301834106] in episode 2525
Report: 
rewardSum:245.25
loss:[46.63387441635132, 40.224204301834106]
policies:[1, 5, 0]
qAverage:[9.789209638323102, 47.259691510881694]
ws:[-5.172850353377206, -3.1689371722085133]
memory len:10000
memory used:3139.0
now epsilon is 0.030794010490322814, the reward is 245.25 with loss [40.86867952346802, 36.548991203308105] in episode 2526
Report: 
rewardSum:245.25
loss:[40.86867952346802, 36.548991203308105]
policies:[1, 5, 0]
qAverage:[9.215109688895089, 46.60697228567941]
ws:[-1.1138613096305303, -0.5007474252155849]
memory len:10000
memory used:3139.0
now epsilon is 0.030747848334350847, the reward is 245.25 with loss [30.909087657928467, 35.94325876235962] in episode 2527
Report: 
rewardSum:245.25
loss:[30.909087657928467, 35.94325876235962]
policies:[2, 4, 0]
qAverage:[19.01310076032366, 37.6322272164481]
ws:[-1.0699883614267622, -0.7950340610529695]
memory len:10000
memory used:3138.0
now epsilon is 0.03067106513421142, the reward is 241.25 with loss [73.12503957748413, 72.36319875717163] in episode 2528
Report: 
rewardSum:241.25
loss:[73.12503957748413, 72.36319875717163]
policies:[3, 5, 2]
qAverage:[21.947262658013237, 36.85287221272787]
ws:[-2.2287891854842505, -1.636520906454987]
memory len:10000
memory used:3138.0
now epsilon is 0.030625087281050754, the reward is 245.25 with loss [42.48805844783783, 46.16622352600098] in episode 2529
Report: 
rewardSum:245.25
loss:[42.48805844783783, 46.16622352600098]
policies:[3, 3, 0]
qAverage:[28.32988521030971, 28.354408264160156]
ws:[-1.627821724329676, -1.1742491605026382]
memory len:10000
memory used:3138.0
now epsilon is 0.03057917835157996, the reward is 245.25 with loss [35.20429515838623, 31.41741731762886] in episode 2530
Report: 
rewardSum:245.25
loss:[35.20429515838623, 31.41741731762886]
policies:[3, 3, 0]
qAverage:[28.22913360595703, 28.373450142996653]
ws:[-1.8831590243748255, -1.6097189102854048]
memory len:10000
memory used:3138.0
now epsilon is 0.030533338242478104, the reward is 245.25 with loss [47.019097328186035, 38.50006055831909] in episode 2531
Report: 
rewardSum:245.25
loss:[47.019097328186035, 38.50006055831909]
policies:[2, 4, 0]
qAverage:[21.480016072591145, 32.12008921305338]
ws:[-2.4817395402739444, -1.9753554264704387]
memory len:10000
memory used:3138.0
now epsilon is 0.030487566850579115, the reward is 245.25 with loss [40.00019717216492, 40.75473690032959] in episode 2532
Report: 
rewardSum:245.25
loss:[40.00019717216492, 40.75473690032959]
policies:[2, 4, 0]
qAverage:[14.293538093566895, 29.90847873687744]
ws:[-3.572326596826315, -2.703199476003647]
memory len:10000
memory used:3138.0
now epsilon is 0.030441864072871593, the reward is 245.25 with loss [46.15282869338989, 40.66165018081665] in episode 2533
Report: 
rewardSum:245.25
loss:[46.15282869338989, 40.66165018081665]
policies:[0, 6, 0]
qAverage:[0.0, 51.24154717581613]
ws:[-4.735657450876066, -2.10863065293857]
memory len:10000
memory used:3137.0
now epsilon is 0.030335490513699336, the reward is 237.25 with loss [72.1207811832428, 93.24573826789856] in episode 2534
Report: 
rewardSum:237.25
loss:[72.1207811832428, 93.24573826789856]
policies:[5, 8, 1]
qAverage:[15.210473696390787, 39.78239377339681]
ws:[-4.384701336423556, -1.9463559140761693]
memory len:10000
memory used:3137.0
now epsilon is 0.030290015707973086, the reward is 245.25 with loss [34.84967041015625, 27.34685492515564] in episode 2535
Report: 
rewardSum:245.25
loss:[34.84967041015625, 27.34685492515564]
policies:[3, 3, 0]
qAverage:[25.74673788888114, 26.023870740618026]
ws:[-0.92137568124703, -0.4195910521915981]
memory len:10000
memory used:3145.0
now epsilon is 0.030229488657589156, the reward is 243.25 with loss [47.03061103820801, 45.69580137729645] in episode 2536
Report: 
rewardSum:243.25
loss:[47.03061103820801, 45.69580137729645]
policies:[2, 5, 1]
qAverage:[14.77786636352539, 37.61603355407715]
ws:[-2.6716233864426613, -1.3960777595639229]
memory len:10000
memory used:3150.0
now epsilon is 0.03018417275530345, the reward is 245.25 with loss [50.276294469833374, 29.689756870269775] in episode 2537
Report: 
rewardSum:245.25
loss:[50.276294469833374, 29.689756870269775]
policies:[1, 4, 1]
qAverage:[10.012055079142252, 39.920958836873375]
ws:[-2.1095187440514565, -0.8020473221937815]
memory len:10000
memory used:3151.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43*		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.030138924784401677, the reward is 245.25 with loss [34.87271070480347, 38.77252435684204] in episode 2538
Report: 
rewardSum:245.25
loss:[34.87271070480347, 38.77252435684204]
policies:[1, 5, 0]
qAverage:[8.000413077218193, 39.70505360194615]
ws:[-1.5080696429525102, -0.1520859854561942]
memory len:10000
memory used:3152.0
now epsilon is 0.030086221206889655, the reward is 244.25 with loss [44.274492263793945, 51.08181929588318] in episode 2539
Report: 
rewardSum:244.25
loss:[44.274492263793945, 51.08181929588318]
policies:[2, 5, 0]
qAverage:[13.237245559692383, 34.1952600479126]
ws:[-1.3225486502051353, -0.42720669880509377]
memory len:10000
memory used:3151.0
now epsilon is 0.030026101389045774, the reward is 243.25 with loss [38.39647877216339, 56.98229122161865] in episode 2540
Report: 
rewardSum:243.25
loss:[38.39647877216339, 56.98229122161865]
policies:[2, 5, 1]
qAverage:[13.41551923751831, 33.923211097717285]
ws:[-1.4936048164963722, -0.48818061500787735]
memory len:10000
memory used:3152.0
now epsilon is 0.029981090377050865, the reward is 245.25 with loss [29.35789465904236, 39.14426803588867] in episode 2541
Report: 
rewardSum:245.25
loss:[29.35789465904236, 39.14426803588867]
policies:[2, 4, 0]
qAverage:[15.243471418108259, 30.80027171543666]
ws:[-4.979299707072122, -2.801425405911037]
memory len:10000
memory used:3152.0
now epsilon is 0.02993614683939019, the reward is 245.25 with loss [34.81123733520508, 32.62521952390671] in episode 2542
Report: 
rewardSum:245.25
loss:[34.81123733520508, 32.62521952390671]
policies:[0, 6, 0]
qAverage:[0.0, 46.1709360395159]
ws:[-1.9731834552117757, -0.38548746705055237]
memory len:10000
memory used:3152.0
now epsilon is 0.029816626511405354, the reward is 624.1600000000001 with loss [110.26027297973633, 82.50465178489685] in episode 2543
Report: 
rewardSum:624.1600000000001
loss:[110.26027297973633, 82.50465178489685]
policies:[10, 5, 1]
qAverage:[29.33516896565755, 17.74681625366211]
ws:[-8.376365546509623, -10.381462184588115]
memory len:10000
memory used:3152.0
now epsilon is 0.02977192951540966, the reward is 245.25 with loss [42.71368384361267, 40.734904289245605] in episode 2544
Report: 
rewardSum:245.25
loss:[42.71368384361267, 40.734904289245605]
policies:[0, 6, 0]
qAverage:[0.0, 44.50488935198103]
ws:[-3.615024358034134, -1.057696052959987]
memory len:10000
memory used:3151.0
now epsilon is 0.029712437731213202, the reward is 243.25 with loss [50.11704111099243, 61.35123682022095] in episode 2545
Report: 
rewardSum:243.25
loss:[50.11704111099243, 61.35123682022095]
policies:[0, 8, 0]
qAverage:[0.0, 43.02751159667969]
ws:[-5.450630022419824, -1.8475147071811888]
memory len:10000
memory used:3151.0
now epsilon is 0.029660479946513173, the reward is 244.25 with loss [40.50994825363159, 42.80774998664856] in episode 2546
Report: 
rewardSum:244.25
loss:[40.50994825363159, 42.80774998664856]
policies:[0, 6, 1]
qAverage:[0.0, 41.809217180524556]
ws:[-1.866475919527667, 0.5307639752115522]
memory len:10000
memory used:2929.0
now epsilon is 0.029616017024026196, the reward is 245.25 with loss [34.44277763366699, 39.288496017456055] in episode 2547
Report: 
rewardSum:245.25
loss:[34.44277763366699, 39.288496017456055]
policies:[0, 6, 0]
qAverage:[0.0, 42.41528156825474]
ws:[-2.1028123583112444, -0.0775546942438398]
memory len:10000
memory used:2929.0
now epsilon is 0.029564227849064297, the reward is 244.25 with loss [58.251468658447266, 49.56751489639282] in episode 2548
Report: 
rewardSum:244.25
loss:[58.251468658447266, 49.56751489639282]
policies:[1, 5, 1]
qAverage:[7.835252126057942, 33.128449122111]
ws:[0.4424027105172475, 1.235601011974116]
memory len:10000
memory used:2929.0
now epsilon is 0.029512529237213595, the reward is 244.25 with loss [47.234498262405396, 35.579478085041046] in episode 2549
Report: 
rewardSum:244.25
loss:[47.234498262405396, 35.579478085041046]
policies:[1, 5, 1]
qAverage:[0.0, 41.08381271362305]
ws:[-2.9918589492638907, -1.1797691682974498]
memory len:10000
memory used:2929.0
now epsilon is 0.029468288102133002, the reward is 245.25 with loss [27.205313563346863, 38.43198537826538] in episode 2550
Report: 
rewardSum:245.25
loss:[27.205313563346863, 38.43198537826538]
policies:[0, 6, 0]
qAverage:[0.0, 42.47662843976702]
ws:[-3.1297438655580794, -0.9485724334205899]
memory len:10000
memory used:2928.0
now epsilon is 0.02942411328729279, the reward is 245.25 with loss [36.429643869400024, 37.76023316383362] in episode 2551
Report: 
rewardSum:245.25
loss:[36.429643869400024, 37.76023316383362]
policies:[1, 5, 0]
qAverage:[0.0, 40.49551773071289]
ws:[-0.9990519732236862, 1.1671273211638133]
memory len:10000
memory used:2928.0
############# STATE ###############
0*		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.029365316527178408, the reward is 243.25 with loss [49.0777792930603, 55.83133339881897] in episode 2552
Report: 
rewardSum:243.25
loss:[49.0777792930603, 55.83133339881897]
policies:[0, 7, 1]
qAverage:[0.0, 38.82154989242554]
ws:[-3.0272182896733284, 0.6996424198150635]
memory len:10000
memory used:2929.0
now epsilon is 0.02932129607319695, the reward is 245.25 with loss [35.3924058675766, 27.465431690216064] in episode 2553
Report: 
rewardSum:245.25
loss:[35.3924058675766, 27.465431690216064]
policies:[0, 6, 0]
qAverage:[0.0, 38.8846926007952]
ws:[-3.6962268267359053, -0.5919064453669957]
memory len:10000
memory used:2929.0
now epsilon is 0.029277341608641042, the reward is 245.25 with loss [41.53125548362732, 36.48311185836792] in episode 2554
Report: 
rewardSum:245.25
loss:[41.53125548362732, 36.48311185836792]
policies:[0, 6, 0]
qAverage:[0.0, 38.50281797136579]
ws:[-4.630010021584375, -1.1826609000563622]
memory len:10000
memory used:2929.0
now epsilon is 0.02923345303458839, the reward is 245.25 with loss [59.240883350372314, 31.652872562408447] in episode 2555
Report: 
rewardSum:245.25
loss:[59.240883350372314, 31.652872562408447]
policies:[0, 6, 0]
qAverage:[0.0, 38.23155212402344]
ws:[-4.539013162255287, -0.05655356815883091]
memory len:10000
memory used:2929.0
now epsilon is 0.02918963025226499, the reward is 245.25 with loss [32.217732191085815, 47.340704917907715] in episode 2556
Report: 
rewardSum:245.25
loss:[32.217732191085815, 47.340704917907715]
policies:[0, 6, 0]
qAverage:[0.0, 38.538908277239116]
ws:[-7.324549657957895, -1.4670455711228507]
memory len:10000
memory used:2929.0
now epsilon is 0.029145873163044907, the reward is 245.25 with loss [36.25835561752319, 38.45305347442627] in episode 2557
Report: 
rewardSum:245.25
loss:[36.25835561752319, 38.45305347442627]
policies:[0, 6, 0]
qAverage:[0.0, 38.170027596609934]
ws:[-3.741707152553967, -0.48235130310058594]
memory len:10000
memory used:2928.0
now epsilon is 0.02910218166845006, the reward is 245.25 with loss [40.47778367996216, 40.0622820854187] in episode 2558
Report: 
rewardSum:245.25
loss:[40.47778367996216, 40.0622820854187]
policies:[0, 6, 0]
qAverage:[0.0, 37.963706970214844]
ws:[-9.89635259764535, -5.607472368649074]
memory len:10000
memory used:2928.0
now epsilon is 0.029058555670149977, the reward is 245.25 with loss [43.25016641616821, 40.685824394226074] in episode 2559
Report: 
rewardSum:245.25
loss:[43.25016641616821, 40.685824394226074]
policies:[0, 6, 0]
qAverage:[0.0, 38.201062883649556]
ws:[-6.165468309606824, -3.9054918714932034]
memory len:10000
memory used:2928.0
now epsilon is 0.0290149950699616, the reward is 245.25 with loss [37.0107262134552, 38.84165954589844] in episode 2560
Report: 
rewardSum:245.25
loss:[37.0107262134552, 38.84165954589844]
policies:[2, 4, 0]
qAverage:[12.077902657645089, 22.947731018066406]
ws:[-1.831925894532885, -1.0315093738692147]
memory len:10000
memory used:2929.0
now epsilon is 0.02897149976984905, the reward is 245.25 with loss [34.026293992996216, 32.33125579357147] in episode 2561
Report: 
rewardSum:245.25
loss:[34.026293992996216, 32.33125579357147]
policies:[1, 4, 1]
qAverage:[7.39246940612793, 28.256932576497395]
ws:[-4.023309340079625, -3.170335372289022]
memory len:10000
memory used:2928.0
now epsilon is 0.028928069671923424, the reward is 245.25 with loss [37.70677429437637, 37.444021701812744] in episode 2562
Report: 
rewardSum:245.25
loss:[37.70677429437637, 37.444021701812744]
policies:[1, 5, 0]
qAverage:[6.013957977294922, 29.33572496686663]
ws:[-4.1983218150479455, -3.4522717214588607]
memory len:10000
memory used:2928.0
now epsilon is 0.028877483502272927, the reward is 244.25 with loss [48.30972409248352, 35.99814534187317] in episode 2563
Report: 
rewardSum:244.25
loss:[48.30972409248352, 35.99814534187317]
policies:[1, 6, 0]
qAverage:[5.659183979034424, 30.176937580108643]
ws:[-3.7003843784332275, -3.453075371682644]
memory len:10000
memory used:2928.0
now epsilon is 0.02883419434063778, the reward is 245.25 with loss [30.435072422027588, 44.18623399734497] in episode 2564
Report: 
rewardSum:245.25
loss:[30.435072422027588, 44.18623399734497]
policies:[4, 2, 0]
qAverage:[22.200529098510742, 13.054192225138346]
ws:[-3.7371184726556144, -3.7008321235577264]
memory len:10000
memory used:2928.0
now epsilon is 0.028790970072175025, the reward is 245.25 with loss [31.336973190307617, 41.33629631996155] in episode 2565
Report: 
rewardSum:245.25
loss:[31.336973190307617, 41.33629631996155]
policies:[5, 1, 0]
qAverage:[30.30042212350028, 5.641177041190011]
ws:[-4.274663743163858, -5.116395577788353]
memory len:10000
memory used:2928.0
now epsilon is 0.02874062364695582, the reward is 244.25 with loss [41.92163395881653, 47.07862043380737] in episode 2566
Report: 
rewardSum:244.25
loss:[41.92163395881653, 47.07862043380737]
policies:[4, 3, 0]
qAverage:[20.574031829833984, 15.306166648864746]
ws:[-8.14720343053341, -6.319993183016777]
memory len:10000
memory used:2929.0
now epsilon is 0.028697539646840298, the reward is 245.25 with loss [35.749839186668396, 45.323360443115234] in episode 2567
Report: 
rewardSum:245.25
loss:[35.749839186668396, 45.323360443115234]
policies:[4, 2, 0]
qAverage:[24.199784960065568, 11.504576546805245]
ws:[-4.274165379149573, -4.728941606623786]
memory len:10000
memory used:2929.0
############# STATE ###############
0-		10-		20-		30-		40*		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.028640194763138505, the reward is 243.25 with loss [32.51443576812744, 58.400550842285156] in episode 2568
Report: 
rewardSum:243.25
loss:[32.51443576812744, 58.400550842285156]
policies:[7, 0, 1]
qAverage:[33.32271146774292, 0.0]
ws:[-3.190030984580517, -5.213930174708366]
memory len:10000
memory used:2928.0
now epsilon is 0.028597261312228013, the reward is 245.25 with loss [44.487732887268066, 50.489840507507324] in episode 2569
Report: 
rewardSum:245.25
loss:[44.487732887268066, 50.489840507507324]
policies:[5, 1, 0]
qAverage:[27.896934509277344, 5.429376874651227]
ws:[-6.2405358382633755, -6.6763976237603595]
memory len:10000
memory used:2928.0
now epsilon is 0.028554392221257188, the reward is 245.25 with loss [47.73568296432495, 46.03968381881714] in episode 2570
Report: 
rewardSum:245.25
loss:[47.73568296432495, 46.03968381881714]
policies:[3, 3, 0]
qAverage:[18.631494522094727, 12.205703099568685]
ws:[-3.510142743587494, -2.6000793874263763]
memory len:10000
memory used:2928.0
now epsilon is 0.02852584853514848, the reward is 247.25 with loss [23.775989770889282, 30.802624225616455] in episode 2571
Report: 
rewardSum:247.25
loss:[23.775989770889282, 30.802624225616455]
policies:[2, 2, 0]
qAverage:[14.896687316894532, 14.826068878173828]
ws:[-8.973314595222472, -7.2804591178894045]
memory len:10000
memory used:2928.0
now epsilon is 0.02849733338202378, the reward is 247.25 with loss [31.33008122444153, 31.100674629211426] in episode 2572
Report: 
rewardSum:247.25
loss:[31.33008122444153, 31.100674629211426]
policies:[1, 3, 0]
qAverage:[7.129695892333984, 22.4773811340332]
ws:[-7.505973315238952, -5.203599411249161]
memory len:10000
memory used:2929.0
now epsilon is 0.028468846733360805, the reward is 247.25 with loss [32.2208833694458, 19.86093044281006] in episode 2573
Report: 
rewardSum:247.25
loss:[32.2208833694458, 19.86093044281006]
policies:[2, 2, 0]
qAverage:[14.970928192138672, 15.353099822998047]
ws:[-5.080665206909179, -3.330685815215111]
memory len:10000
memory used:2928.0
now epsilon is 0.028426170143909735, the reward is 245.25 with loss [34.297407150268555, 47.31800436973572] in episode 2574
Report: 
rewardSum:245.25
loss:[34.297407150268555, 47.31800436973572]
policies:[3, 2, 1]
qAverage:[19.189476648966473, 12.535451253255209]
ws:[-2.289397358894348, -2.9461851517359414]
memory len:10000
memory used:2929.0
now epsilon is 0.028397754631803104, the reward is 247.25 with loss [24.05157709121704, 26.03598713874817] in episode 2575
Report: 
rewardSum:247.25
loss:[24.05157709121704, 26.03598713874817]
policies:[2, 2, 0]
qAverage:[14.879224395751953, 15.310821533203125]
ws:[-5.5053809642791744, -4.134642231464386]
memory len:10000
memory used:2929.0
now epsilon is 0.028355184613877735, the reward is 245.25 with loss [32.39280152320862, 31.49809169769287] in episode 2576
Report: 
rewardSum:245.25
loss:[32.39280152320862, 31.49809169769287]
policies:[2, 4, 0]
qAverage:[13.483145395914713, 18.875123977661133]
ws:[-3.065408855676651, -3.5104040751854577]
memory len:10000
memory used:2928.0
now epsilon is 0.02831267841108316, the reward is 245.25 with loss [31.01280641555786, 42.35213494300842] in episode 2577
Report: 
rewardSum:245.25
loss:[31.01280641555786, 42.35213494300842]
policies:[3, 3, 0]
qAverage:[15.83896473475865, 15.419640132359095]
ws:[-3.0858907167400633, -3.0771340946001664]
memory len:10000
memory used:2928.0
now epsilon is 0.028284376348157055, the reward is 247.25 with loss [14.660353302955627, 25.88214945793152] in episode 2578
Report: 
rewardSum:247.25
loss:[14.660353302955627, 25.88214945793152]
policies:[0, 4, 0]
qAverage:[0.0, 27.969267654418946]
ws:[-4.935591435432434, -2.2308610677719116]
memory len:10000
memory used:2928.0
now epsilon is 0.028206691467859122, the reward is 240.25 with loss [81.94499069452286, 85.82653713226318] in episode 2579
Report: 
rewardSum:240.25
loss:[81.94499069452286, 85.82653713226318]
policies:[4, 7, 0]
qAverage:[11.515203475952148, 19.95741144816081]
ws:[-4.771864886085193, -3.9099023062735796]
memory len:10000
memory used:2929.0
now epsilon is 0.028178495352137758, the reward is 247.25 with loss [20.76712453365326, 14.87902021408081] in episode 2580
Report: 
rewardSum:247.25
loss:[20.76712453365326, 14.87902021408081]
policies:[1, 3, 0]
qAverage:[0.0, 27.112107276916504]
ws:[-7.341847360134125, -4.997925937175751]
memory len:10000
memory used:2929.0
now epsilon is 0.02813625401764482, the reward is 245.25 with loss [46.035958766937256, 40.534173250198364] in episode 2581
Report: 
rewardSum:245.25
loss:[46.035958766937256, 40.534173250198364]
policies:[2, 4, 0]
qAverage:[11.884654998779297, 17.985313415527344]
ws:[-1.93444179246823, -1.108153559267521]
memory len:10000
memory used:2929.0
now epsilon is 0.028108128312964026, the reward is 247.25 with loss [27.752694606781006, 32.19762897491455] in episode 2582
Report: 
rewardSum:247.25
loss:[27.752694606781006, 32.19762897491455]
policies:[0, 4, 0]
qAverage:[0.0, 28.341550445556642]
ws:[-3.967579650878906, -1.319662857055664]
memory len:10000
memory used:2929.0
now epsilon is 0.02808003072344253, the reward is 247.25 with loss [20.423439025878906, 22.786598205566406] in episode 2583
Report: 
rewardSum:247.25
loss:[20.423439025878906, 22.786598205566406]
policies:[0, 4, 0]
qAverage:[0.0, 28.56249542236328]
ws:[-4.443337869644165, -1.9426413536071778]
memory len:10000
memory used:2929.0
now epsilon is 0.02803793699361281, the reward is 245.25 with loss [33.50457429885864, 44.11449217796326] in episode 2584
Report: 
rewardSum:245.25
loss:[33.50457429885864, 44.11449217796326]
policies:[2, 4, 0]
qAverage:[12.283841451009115, 16.6671085357666]
ws:[0.14427942782640457, 0.443827028075854]
memory len:10000
memory used:2929.0
now epsilon is 0.02800990956909331, the reward is 247.25 with loss [31.243318557739258, 25.107664823532104] in episode 2585
Report: 
rewardSum:247.25
loss:[31.243318557739258, 25.107664823532104]
policies:[0, 4, 0]
qAverage:[0.0, 28.89023361206055]
ws:[-5.727132171392441, -2.884958779811859]
memory len:10000
memory used:2929.0
now epsilon is 0.027981910161489796, the reward is 247.25 with loss [24.755101680755615, 19.571549654006958] in episode 2586
Report: 
rewardSum:247.25
loss:[24.755101680755615, 19.571549654006958]
policies:[1, 3, 0]
qAverage:[6.763399505615235, 21.46720199584961]
ws:[-6.2036794677376745, -3.0475811123847962]
memory len:10000
memory used:2929.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2*		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.02795393874279586, the reward is 247.25 with loss [30.409159660339355, 14.541503190994263] in episode 2587
Report: 
rewardSum:247.25
loss:[30.409159660339355, 14.541503190994263]
policies:[0, 4, 0]
qAverage:[0.0, 27.310939025878906]
ws:[-5.721809053421021, -2.3805742502212524]
memory len:10000
memory used:2929.0
now epsilon is 0.027925995285033083, the reward is 247.25 with loss [29.474733352661133, 22.806161403656006] in episode 2588
Report: 
rewardSum:247.25
loss:[29.474733352661133, 22.806161403656006]
policies:[1, 3, 0]
qAverage:[6.291480255126953, 22.148478698730468]
ws:[-5.159538495540619, -1.854037880897522]
memory len:10000
memory used:2929.0
now epsilon is 0.027898079760251018, the reward is 247.25 with loss [32.82915782928467, 35.3349814414978] in episode 2589
Report: 
rewardSum:247.25
loss:[32.82915782928467, 35.3349814414978]
policies:[1, 3, 0]
qAverage:[0.0, 24.80341148376465]
ws:[0.7898467853665352, 1.4542509317398071]
memory len:10000
memory used:2929.0
now epsilon is 0.027870192140527158, the reward is 247.25 with loss [27.4705171585083, 23.499186515808105] in episode 2590
Report: 
rewardSum:247.25
loss:[27.4705171585083, 23.499186515808105]
policies:[0, 4, 0]
qAverage:[0.0, 27.897163772583006]
ws:[-6.434072554111481, -3.3971687495708465]
memory len:10000
memory used:2929.0
now epsilon is 0.027842332397966907, the reward is 247.25 with loss [31.299413204193115, 30.133655071258545] in episode 2591
Report: 
rewardSum:247.25
loss:[31.299413204193115, 30.133655071258545]
policies:[0, 4, 0]
qAverage:[0.0, 28.401596450805663]
ws:[-6.46430624127388, -2.8083430767059325]
memory len:10000
memory used:2929.0
now epsilon is 0.027814500504703552, the reward is 247.25 with loss [27.458073139190674, 18.568854093551636] in episode 2592
Report: 
rewardSum:247.25
loss:[27.458073139190674, 18.568854093551636]
policies:[0, 4, 0]
qAverage:[0.0, 27.942972564697264]
ws:[-5.782574063539505, -1.6734629154205323]
memory len:10000
memory used:2929.0
now epsilon is 0.027786696432898245, the reward is 247.25 with loss [28.34427309036255, 32.47332954406738] in episode 2593
Report: 
rewardSum:247.25
loss:[28.34427309036255, 32.47332954406738]
policies:[1, 3, 0]
qAverage:[6.125078582763672, 22.288955688476562]
ws:[-4.899051797389984, -0.47638378143310545]
memory len:10000
memory used:2929.0
now epsilon is 0.027758920154739953, the reward is 247.25 with loss [31.101853370666504, 22.493239760398865] in episode 2594
Report: 
rewardSum:247.25
loss:[31.101853370666504, 22.493239760398865]
policies:[1, 3, 0]
qAverage:[6.097023773193359, 22.353622436523438]
ws:[-4.939477908611297, -0.7625721454620361]
memory len:10000
memory used:2929.0
now epsilon is 0.02773117164244545, the reward is 247.25 with loss [26.204723358154297, 33.42106342315674] in episode 2595
Report: 
rewardSum:247.25
loss:[26.204723358154297, 33.42106342315674]
policies:[1, 3, 0]
qAverage:[6.143257141113281, 22.4253288269043]
ws:[-5.962579816579819, -1.6372243285179138]
memory len:10000
memory used:2929.0
now epsilon is 0.027703450868259285, the reward is 247.25 with loss [18.198243141174316, 15.18260109424591] in episode 2596
Report: 
rewardSum:247.25
loss:[18.198243141174316, 15.18260109424591]
policies:[1, 3, 0]
qAverage:[6.0353248596191404, 22.429248809814453]
ws:[-6.269169586896896, -2.4930231526494024]
memory len:10000
memory used:2938.0
now epsilon is 0.02767575780445375, the reward is 247.25 with loss [23.42741048336029, 31.67632484436035] in episode 2597
Report: 
rewardSum:247.25
loss:[23.42741048336029, 31.67632484436035]
policies:[1, 3, 0]
qAverage:[6.093927383422852, 22.362551879882812]
ws:[-6.07155408859253, -2.588657960295677]
memory len:10000
memory used:2937.0
now epsilon is 0.02764809242332885, the reward is 247.25 with loss [30.06669855117798, 31.32314944267273] in episode 2598
Report: 
rewardSum:247.25
loss:[30.06669855117798, 31.32314944267273]
policies:[1, 3, 0]
qAverage:[6.101098251342774, 22.185336303710937]
ws:[-6.7082313060760494, -3.422829395532608]
memory len:10000
memory used:2937.0
now epsilon is 0.027620454697212287, the reward is 247.25 with loss [29.726836919784546, 29.136117458343506] in episode 2599
Report: 
rewardSum:247.25
loss:[29.726836919784546, 29.136117458343506]
policies:[1, 3, 0]
qAverage:[6.0379070281982425, 22.185265731811523]
ws:[-7.9005053520202635, -4.708034968376159]
memory len:10000
memory used:2937.0
now epsilon is 0.02759284459845942, the reward is 247.25 with loss [36.00306558609009, 30.957187175750732] in episode 2600
Report: 
rewardSum:247.25
loss:[36.00306558609009, 30.957187175750732]
policies:[1, 3, 0]
qAverage:[6.081818389892578, 22.809223937988282]
ws:[-8.312784135341644, -5.615576386451721]
memory len:10000
memory used:2937.0
now epsilon is 0.02756526209945324, the reward is 247.25 with loss [27.94915008544922, 30.646662712097168] in episode 2601
Report: 
rewardSum:247.25
loss:[27.94915008544922, 30.646662712097168]
policies:[1, 3, 0]
qAverage:[5.98094596862793, 22.57194595336914]
ws:[-6.635210835933686, -3.842084503173828]
memory len:10000
memory used:2938.0
now epsilon is 0.027537707172604356, the reward is 247.25 with loss [33.695889472961426, 35.525556564331055] in episode 2602
Report: 
rewardSum:247.25
loss:[33.695889472961426, 35.525556564331055]
policies:[1, 3, 0]
qAverage:[5.865905380249023, 23.122571182250976]
ws:[-5.94656810760498, -3.3780499696731567]
memory len:10000
memory used:2938.0
now epsilon is 0.027510179790350944, the reward is 247.25 with loss [23.781806468963623, 21.74090051651001] in episode 2603
Report: 
rewardSum:247.25
loss:[23.781806468963623, 21.74090051651001]
policies:[1, 3, 0]
qAverage:[5.873895645141602, 22.59806594848633]
ws:[-6.589666414260864, -3.512939609773457]
memory len:10000
memory used:2937.0
now epsilon is 0.02748267992515874, the reward is 247.25 with loss [36.33369159698486, 21.23138427734375] in episode 2604
Report: 
rewardSum:247.25
loss:[36.33369159698486, 21.23138427734375]
policies:[1, 3, 0]
qAverage:[5.861996459960937, 23.368880462646484]
ws:[-6.560242801904678, -2.705129700899124]
memory len:10000
memory used:2937.0
now epsilon is 0.027455207549520997, the reward is 247.25 with loss [17.98695468902588, 27.765812873840332] in episode 2605
Report: 
rewardSum:247.25
loss:[17.98695468902588, 27.765812873840332]
policies:[1, 3, 0]
qAverage:[5.908606338500976, 22.331384658813477]
ws:[-5.168584632873535, -1.2848350763320924]
memory len:10000
memory used:2937.0
now epsilon is 0.027427762635958468, the reward is 247.25 with loss [44.73348903656006, 29.708756923675537] in episode 2606
Report: 
rewardSum:247.25
loss:[44.73348903656006, 29.708756923675537]
policies:[1, 3, 0]
qAverage:[5.789414978027343, 23.470050811767578]
ws:[-3.9156470775604246, -0.12199687957763672]
memory len:10000
memory used:2937.0
now epsilon is 0.027400345157019376, the reward is 247.25 with loss [26.81832790374756, 24.9435396194458] in episode 2607
Report: 
rewardSum:247.25
loss:[26.81832790374756, 24.9435396194458]
policies:[1, 3, 0]
qAverage:[6.105566024780273, 22.5931339263916]
ws:[-4.201090478897095, 0.2396867275238037]
memory len:10000
memory used:2937.0
now epsilon is 0.02737295508527938, the reward is 247.25 with loss [39.938899993896484, 21.86610770225525] in episode 2608
Report: 
rewardSum:247.25
loss:[39.938899993896484, 21.86610770225525]
policies:[1, 3, 0]
qAverage:[5.813977432250977, 23.186116409301757]
ws:[-5.379048895835877, -0.4137484788894653]
memory len:10000
memory used:2937.0
now epsilon is 0.02734559239334156, the reward is 247.25 with loss [35.13362765312195, 27.544601917266846] in episode 2609
Report: 
rewardSum:247.25
loss:[35.13362765312195, 27.544601917266846]
policies:[1, 3, 0]
qAverage:[0.0, 27.459722995758057]
ws:[-6.63072082400322, -0.9561414495110512]
memory len:10000
memory used:2937.0
now epsilon is 0.027318257053836375, the reward is 247.25 with loss [30.459163904190063, 24.162315368652344] in episode 2610
Report: 
rewardSum:247.25
loss:[30.459163904190063, 24.162315368652344]
policies:[0, 4, 0]
qAverage:[0.0, 28.673705291748046]
ws:[-4.320033967494965, 0.09955143332481384]
memory len:10000
memory used:2937.0
now epsilon is 0.027290949039421652, the reward is 247.25 with loss [33.21211814880371, 33.890336990356445] in episode 2611
Report: 
rewardSum:247.25
loss:[33.21211814880371, 33.890336990356445]
policies:[0, 4, 0]
qAverage:[0.0, 29.50991897583008]
ws:[-3.411601611971855, 0.6432910561561584]
memory len:10000
memory used:2937.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2*		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.027263668322782545, the reward is 37.15999999999997 with loss [33.71525192260742, 23.073800086975098] in episode 2612
Report: 
rewardSum:37.15999999999997
loss:[33.71525192260742, 23.073800086975098]
policies:[0, 3, 1]
qAverage:[0.0, 23.81519953409831]
ws:[-0.00999488371113936, 1.459693153699239]
memory len:10000
memory used:2937.0
now epsilon is 0.02723641487663151, the reward is 247.25 with loss [21.26489806175232, 25.920405864715576] in episode 2613
Report: 
rewardSum:247.25
loss:[21.26489806175232, 25.920405864715576]
policies:[0, 4, 0]
qAverage:[0.0, 29.72417755126953]
ws:[-5.670332741737366, -0.47059876918792726]
memory len:10000
memory used:2937.0
now epsilon is 0.027209188673708293, the reward is 247.25 with loss [36.358848571777344, 20.436565399169922] in episode 2614
Report: 
rewardSum:247.25
loss:[36.358848571777344, 20.436565399169922]
policies:[0, 4, 0]
qAverage:[0.0, 30.565047836303712]
ws:[-5.590785133838653, -0.7554760694503784]
memory len:10000
memory used:2937.0
now epsilon is 0.027181989686779873, the reward is 247.25 with loss [32.527350425720215, 23.03350067138672] in episode 2615
Report: 
rewardSum:247.25
loss:[32.527350425720215, 23.03350067138672]
policies:[0, 4, 0]
qAverage:[0.0, 30.189081954956055]
ws:[-5.920467790961266, -0.6902927935123444]
memory len:10000
memory used:2965.0
now epsilon is 0.02715481788864046, the reward is 247.25 with loss [25.639987230300903, 33.87780809402466] in episode 2616
Report: 
rewardSum:247.25
loss:[25.639987230300903, 33.87780809402466]
policies:[0, 4, 0]
qAverage:[0.0, 30.547015380859374]
ws:[-6.5976896464824675, -0.9138081967830658]
memory len:10000
memory used:3063.0
now epsilon is 0.027127673252111465, the reward is 247.25 with loss [29.971566677093506, 25.029428958892822] in episode 2617
Report: 
rewardSum:247.25
loss:[29.971566677093506, 25.029428958892822]
policies:[0, 4, 0]
qAverage:[0.0, 30.66824417114258]
ws:[-7.637496709823608, -1.7358899861574173]
memory len:10000
memory used:3144.0
now epsilon is 0.027100555750041454, the reward is 247.25 with loss [25.251991987228394, 24.992084503173828] in episode 2618
Report: 
rewardSum:247.25
loss:[25.251991987228394, 24.992084503173828]
policies:[0, 4, 0]
qAverage:[0.0, 30.635786056518555]
ws:[-6.407517910003662, -1.0135167479515075]
memory len:10000
memory used:3320.0
now epsilon is 0.027073465355306144, the reward is 247.25 with loss [24.927850246429443, 33.903143882751465] in episode 2619
Report: 
rewardSum:247.25
loss:[24.927850246429443, 33.903143882751465]
policies:[0, 4, 0]
qAverage:[0.0, 31.3845157623291]
ws:[-6.418046608567238, -0.6812095642089844]
memory len:10000
memory used:3443.0
now epsilon is 0.027046402040808364, the reward is 247.25 with loss [12.757216095924377, 24.816254377365112] in episode 2620
Report: 
rewardSum:247.25
loss:[12.757216095924377, 24.816254377365112]
policies:[1, 3, 0]
qAverage:[6.011429214477539, 24.71546401977539]
ws:[-4.388612747192383, -0.12007777690887451]
memory len:10000
memory used:3499.0
now epsilon is 0.02701936577947803, the reward is 247.25 with loss [30.535705089569092, 26.394792079925537] in episode 2621
Report: 
rewardSum:247.25
loss:[30.535705089569092, 26.394792079925537]
policies:[1, 3, 0]
qAverage:[5.852331924438476, 25.05945281982422]
ws:[-4.615655326843262, -0.4239897608757019]
memory len:10000
memory used:3485.0
now epsilon is 0.026992356544272116, the reward is 247.25 with loss [35.03231716156006, 15.97350549697876] in episode 2622
Report: 
rewardSum:247.25
loss:[35.03231716156006, 15.97350549697876]
policies:[1, 3, 0]
qAverage:[6.058194732666015, 24.743624114990233]
ws:[-5.38982900083065, -1.0010893583297729]
memory len:10000
memory used:3575.0
now epsilon is 0.026965374308174635, the reward is 247.25 with loss [31.70631718635559, 30.83828067779541] in episode 2623
Report: 
rewardSum:247.25
loss:[31.70631718635559, 30.83828067779541]
policies:[0, 4, 0]
qAverage:[0.0, 30.960748291015626]
ws:[-5.702034986019134, -0.8334983110427856]
memory len:10000
memory used:3656.0
now epsilon is 0.0269384190441966, the reward is 247.25 with loss [27.626990795135498, 28.81159734725952] in episode 2624
Report: 
rewardSum:247.25
loss:[27.626990795135498, 28.81159734725952]
policies:[0, 4, 0]
qAverage:[0.0, 33.24825744628906]
ws:[-4.921056643128395, -0.08606009483337403]
memory len:10000
memory used:3696.0
now epsilon is 0.026911490725376002, the reward is 247.25 with loss [30.18249487876892, 17.436980724334717] in episode 2625
Report: 
rewardSum:247.25
loss:[30.18249487876892, 17.436980724334717]
policies:[0, 4, 0]
qAverage:[0.0, 32.722591400146484]
ws:[-4.882093808054924, -0.306537127494812]
memory len:10000
memory used:3731.0
now epsilon is 0.02688458932477779, the reward is 247.25 with loss [25.55221939086914, 27.035274028778076] in episode 2626
Report: 
rewardSum:247.25
loss:[25.55221939086914, 27.035274028778076]
policies:[0, 4, 0]
qAverage:[0.0, 32.99652862548828]
ws:[-5.306700533628463, -0.5508916020393372]
memory len:10000
memory used:3749.0
now epsilon is 0.02685771481549383, the reward is 247.25 with loss [26.783618927001953, 21.38662576675415] in episode 2627
Report: 
rewardSum:247.25
loss:[26.783618927001953, 21.38662576675415]
policies:[0, 4, 0]
qAverage:[0.0, 32.51683311462402]
ws:[-4.502451148629189, -0.11718476414680482]
memory len:10000
memory used:3785.0
now epsilon is 0.026830867170642893, the reward is 247.25 with loss [17.19174313545227, 26.16906213760376] in episode 2628
Report: 
rewardSum:247.25
loss:[17.19174313545227, 26.16906213760376]
policies:[0, 4, 0]
qAverage:[0.0, 33.02135047912598]
ws:[-2.908936095237732, 1.4478869676589965]
memory len:10000
memory used:3806.0
now epsilon is 0.026804046363370616, the reward is 247.25 with loss [39.16964673995972, 31.329547882080078] in episode 2629
Report: 
rewardSum:247.25
loss:[39.16964673995972, 31.329547882080078]
policies:[0, 4, 0]
qAverage:[0.0, 32.614392852783205]
ws:[-1.4945950031280517, 3.347274971008301]
memory len:10000
memory used:3829.0
now epsilon is 0.02677725236684949, the reward is 247.25 with loss [22.752753257751465, 24.60599446296692] in episode 2630
Report: 
rewardSum:247.25
loss:[22.752753257751465, 24.60599446296692]
policies:[0, 4, 0]
qAverage:[0.0, 32.828498458862306]
ws:[-2.964171528816223, 2.241259527206421]
memory len:10000
memory used:3837.0
now epsilon is 0.02675048515427881, the reward is 247.25 with loss [39.90258264541626, 30.60635781288147] in episode 2631
Report: 
rewardSum:247.25
loss:[39.90258264541626, 30.60635781288147]
policies:[0, 4, 0]
qAverage:[0.0, 33.02035026550293]
ws:[-4.458875864744186, 0.8846534252166748]
memory len:10000
memory used:3860.0
now epsilon is 0.026723744698884666, the reward is 247.25 with loss [25.632582187652588, 36.14023494720459] in episode 2632
Report: 
rewardSum:247.25
loss:[25.632582187652588, 36.14023494720459]
policies:[0, 4, 0]
qAverage:[0.0, 33.44879035949707]
ws:[-5.328273415565491, 0.38334624767303466]
memory len:10000
memory used:3876.0
now epsilon is 0.026697030973919917, the reward is 247.25 with loss [26.93876361846924, 19.57440161705017] in episode 2633
Report: 
rewardSum:247.25
loss:[26.93876361846924, 19.57440161705017]
policies:[0, 4, 0]
qAverage:[0.0, 30.384501934051514]
ws:[-6.780347391963005, -0.5759495235979557]
memory len:10000
memory used:3878.0
now epsilon is 0.026670343952664155, the reward is 247.25 with loss [25.418304443359375, 34.92544651031494] in episode 2634
Report: 
rewardSum:247.25
loss:[25.418304443359375, 34.92544651031494]
policies:[0, 4, 0]
qAverage:[0.0, 33.422455596923825]
ws:[-5.256321541965008, -0.018891429901123045]
memory len:10000
memory used:3889.0
now epsilon is 0.0266303634318497, the reward is 245.25 with loss [44.92165517807007, 43.618412494659424] in episode 2635
Report: 
rewardSum:245.25
loss:[44.92165517807007, 43.618412494659424]
policies:[0, 5, 1]
qAverage:[0.0, 33.80095068613688]
ws:[-3.578138959904512, 0.7509441475073496]
memory len:10000
memory used:3904.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25*		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.02660374305313985, the reward is 247.25 with loss [27.774702072143555, 22.592345237731934] in episode 2636
Report: 
rewardSum:247.25
loss:[27.774702072143555, 22.592345237731934]
policies:[0, 4, 0]
qAverage:[0.0, 36.80839538574219]
ws:[-3.445814108848572, 1.167647123336792]
memory len:10000
memory used:3913.0
now epsilon is 0.02657714928482773, the reward is 247.25 with loss [29.995321035385132, 28.23629879951477] in episode 2637
Report: 
rewardSum:247.25
loss:[29.995321035385132, 28.23629879951477]
policies:[0, 4, 0]
qAverage:[0.0, 35.65749588012695]
ws:[-3.539369022846222, 1.014939022064209]
memory len:10000
memory used:3922.0
now epsilon is 0.026550582100312917, the reward is 247.25 with loss [24.56509518623352, 21.69903039932251] in episode 2638
Report: 
rewardSum:247.25
loss:[24.56509518623352, 21.69903039932251]
policies:[0, 4, 0]
qAverage:[0.0, 35.065223693847656]
ws:[-5.345886155962944, 0.0020186007022857666]
memory len:10000
memory used:3869.0
now epsilon is 0.026524041473021585, the reward is 247.25 with loss [20.14926052093506, 23.770597457885742] in episode 2639
Report: 
rewardSum:247.25
loss:[20.14926052093506, 23.770597457885742]
policies:[0, 4, 0]
qAverage:[0.0, 36.08297882080078]
ws:[-4.77172854244709, -0.01725822687149048]
memory len:10000
memory used:3883.0
now epsilon is 0.02649752737640647, the reward is 37.15999999999997 with loss [28.349918842315674, 32.53317165374756] in episode 2640
Report: 
rewardSum:37.15999999999997
loss:[28.349918842315674, 32.53317165374756]
policies:[0, 3, 1]
qAverage:[0.0, 29.999796867370605]
ws:[0.2280488945543766, 1.74118472635746]
memory len:10000
memory used:3884.0
now epsilon is 0.026471039783946836, the reward is 247.25 with loss [27.040992498397827, 27.024439334869385] in episode 2641
Report: 
rewardSum:247.25
loss:[27.040992498397827, 27.024439334869385]
policies:[0, 4, 0]
qAverage:[0.0, 36.28819580078125]
ws:[-3.9464094638824463, 0.9213805675506592]
memory len:10000
memory used:3885.0
now epsilon is 0.026444578669148476, the reward is 247.25 with loss [28.46398115158081, 28.442094802856445] in episode 2642
Report: 
rewardSum:247.25
loss:[28.46398115158081, 28.442094802856445]
policies:[1, 3, 0]
qAverage:[0.0, 32.705227851867676]
ws:[-4.869562149047852, 0.5531470775604248]
memory len:10000
memory used:3888.0
now epsilon is 0.02641814400554365, the reward is 247.25 with loss [19.050378561019897, 24.325912952423096] in episode 2643
Report: 
rewardSum:247.25
loss:[19.050378561019897, 24.325912952423096]
policies:[0, 4, 0]
qAverage:[0.0, 36.27457046508789]
ws:[-4.643622043728828, 0.5562016725540161]
memory len:10000
memory used:3892.0
now epsilon is 0.026391735766691078, the reward is 247.25 with loss [25.849568605422974, 27.782649040222168] in episode 2644
Report: 
rewardSum:247.25
loss:[25.849568605422974, 27.782649040222168]
policies:[0, 4, 0]
qAverage:[0.0, 36.286817932128905]
ws:[-5.212139125168323, 0.3196830749511719]
memory len:10000
memory used:3894.0
now epsilon is 0.026365353926175922, the reward is 247.25 with loss [23.47719943523407, 38.30402231216431] in episode 2645
Report: 
rewardSum:247.25
loss:[23.47719943523407, 38.30402231216431]
policies:[0, 4, 0]
qAverage:[0.0, 36.45863265991211]
ws:[-5.625152464210987, 0.6199066162109375]
memory len:10000
memory used:3895.0
now epsilon is 0.02633899845760974, the reward is 247.25 with loss [22.319581270217896, 35.81452465057373] in episode 2646
Report: 
rewardSum:247.25
loss:[22.319581270217896, 35.81452465057373]
policies:[0, 4, 0]
qAverage:[0.0, 36.85258255004883]
ws:[-6.637162601947784, -0.6582395792007446]
memory len:10000
memory used:3894.0
now epsilon is 0.026312669334630473, the reward is 247.25 with loss [25.9078049659729, 28.23492407798767] in episode 2647
Report: 
rewardSum:247.25
loss:[25.9078049659729, 28.23492407798767]
policies:[0, 3, 1]
qAverage:[0.0, 35.78651523590088]
ws:[-8.758178234100342, -3.008647821843624]
memory len:10000
memory used:3595.0
now epsilon is 0.026286366530902404, the reward is 247.25 with loss [31.966505527496338, 25.157195568084717] in episode 2648
Report: 
rewardSum:247.25
loss:[31.966505527496338, 25.157195568084717]
policies:[0, 4, 0]
qAverage:[0.0, 38.465397644042966]
ws:[-8.108729314804076, -2.6841617703437803]
memory len:10000
memory used:3595.0
now epsilon is 0.026260090020116155, the reward is 247.25 with loss [32.66685771942139, 27.992316007614136] in episode 2649
Report: 
rewardSum:247.25
loss:[32.66685771942139, 27.992316007614136]
policies:[0, 4, 0]
qAverage:[0.0, 41.60178756713867]
ws:[-7.890293049812317, -1.6721808075904847]
memory len:10000
memory used:3595.0
now epsilon is 0.026233839775988647, the reward is 247.25 with loss [36.94390869140625, 30.096267700195312] in episode 2650
Report: 
rewardSum:247.25
loss:[36.94390869140625, 30.096267700195312]
policies:[0, 4, 0]
qAverage:[0.0, 39.70593109130859]
ws:[-5.9685876846313475, -0.17817697525024415]
memory len:10000
memory used:3595.0
now epsilon is 0.026207615772263066, the reward is 247.25 with loss [26.349602937698364, 20.94331467151642] in episode 2651
Report: 
rewardSum:247.25
loss:[26.349602937698364, 20.94331467151642]
policies:[0, 4, 0]
qAverage:[0.0, 40.279869079589844]
ws:[-3.634004330635071, 1.5257673740386963]
memory len:10000
memory used:3561.0
now epsilon is 0.02616832891005612, the reward is 245.25 with loss [30.169944047927856, 38.57464027404785] in episode 2652
Report: 
rewardSum:245.25
loss:[30.169944047927856, 38.57464027404785]
policies:[0, 5, 1]
qAverage:[0.0, 42.082375844319664]
ws:[2.229142506917318, 4.824323852856954]
memory len:10000
memory used:3557.0
now epsilon is 0.026142170392633987, the reward is 247.25 with loss [32.507882595062256, 28.12308979034424] in episode 2653
Report: 
rewardSum:247.25
loss:[32.507882595062256, 28.12308979034424]
policies:[0, 4, 0]
qAverage:[0.0, 39.999009704589845]
ws:[-1.070411491394043, 2.790681171417236]
memory len:10000
memory used:3556.0
now epsilon is 0.026116038023921467, the reward is 247.25 with loss [25.233386874198914, 34.3968391418457] in episode 2654
Report: 
rewardSum:247.25
loss:[25.233386874198914, 34.3968391418457]
policies:[0, 4, 0]
qAverage:[0.0, 40.24529266357422]
ws:[-1.95126314163208, 1.88269100189209]
memory len:10000
memory used:3557.0
now epsilon is 0.026089931777779657, the reward is 247.25 with loss [19.471275329589844, 27.746574878692627] in episode 2655
Report: 
rewardSum:247.25
loss:[19.471275329589844, 27.746574878692627]
policies:[0, 4, 0]
qAverage:[0.0, 36.62430000305176]
ws:[-4.30532905459404, -0.04261469841003418]
memory len:10000
memory used:3557.0
now epsilon is 0.02606385162809578, the reward is 247.25 with loss [34.12642240524292, 26.115028381347656] in episode 2656
Report: 
rewardSum:247.25
loss:[34.12642240524292, 26.115028381347656]
policies:[0, 4, 0]
qAverage:[0.0, 40.374037170410155]
ws:[-3.2293364286422728, 2.0252976417541504]
memory len:10000
memory used:3557.0
now epsilon is 0.02603779754878316, the reward is 247.25 with loss [32.0637993812561, 26.265332221984863] in episode 2657
Report: 
rewardSum:247.25
loss:[32.0637993812561, 26.265332221984863]
policies:[0, 4, 0]
qAverage:[0.0, 40.309278106689455]
ws:[-3.984308786690235, 1.6208694458007813]
memory len:10000
memory used:3556.0
now epsilon is 0.0260117695137812, the reward is 247.25 with loss [26.693440437316895, 22.831910133361816] in episode 2658
Report: 
rewardSum:247.25
loss:[26.693440437316895, 22.831910133361816]
policies:[0, 4, 0]
qAverage:[0.0, 40.559224700927736]
ws:[-3.906877711415291, 2.3118735790252685]
memory len:10000
memory used:3557.0
now epsilon is 0.025979271055181087, the reward is 246.25 with loss [40.48454308509827, 25.81110668182373] in episode 2659
Report: 
rewardSum:246.25
loss:[40.48454308509827, 25.81110668182373]
policies:[1, 3, 1]
qAverage:[0.0, 36.94376754760742]
ws:[-5.331650670617819, 1.7879551649093628]
memory len:10000
memory used:3557.0
now epsilon is 0.025953301524728953, the reward is 247.25 with loss [25.341122150421143, 26.552349090576172] in episode 2660
Report: 
rewardSum:247.25
loss:[25.341122150421143, 26.552349090576172]
policies:[1, 3, 0]
qAverage:[0.0, 45.50539207458496]
ws:[-5.5505144745111465, 1.3755701780319214]
memory len:10000
memory used:3557.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2*		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.02592735795407032, the reward is 247.25 with loss [23.125444173812866, 30.933561325073242] in episode 2661
Report: 
rewardSum:247.25
loss:[23.125444173812866, 30.933561325073242]
policies:[0, 4, 0]
qAverage:[0.0, 46.43685684204102]
ws:[-4.934657496958971, 0.9397227525711059]
memory len:10000
memory used:3558.0
now epsilon is 0.025901440317255126, the reward is 247.25 with loss [23.003172397613525, 34.63307809829712] in episode 2662
Report: 
rewardSum:247.25
loss:[23.003172397613525, 34.63307809829712]
policies:[0, 4, 0]
qAverage:[0.0, 43.59762191772461]
ws:[-3.9979154407978057, 1.904648494720459]
memory len:10000
memory used:3560.0
now epsilon is 0.025875548588359255, the reward is 247.25 with loss [27.02568531036377, 22.00315749645233] in episode 2663
Report: 
rewardSum:247.25
loss:[27.02568531036377, 22.00315749645233]
policies:[0, 4, 0]
qAverage:[0.0, 45.595086669921876]
ws:[-2.9608704805374146, 2.2528215408325196]
memory len:10000
memory used:3560.0
now epsilon is 0.025836759515718927, the reward is 245.25 with loss [28.056689977645874, 38.43224048614502] in episode 2664
Report: 
rewardSum:245.25
loss:[28.056689977645874, 38.43224048614502]
policies:[1, 4, 1]
qAverage:[0.0, 44.38229217529297]
ws:[-2.4625842034816743, 1.91441068649292]
memory len:10000
memory used:3559.0
now epsilon is 0.025810932443373333, the reward is 247.25 with loss [34.10695266723633, 40.177578926086426] in episode 2665
Report: 
rewardSum:247.25
loss:[34.10695266723633, 40.177578926086426]
policies:[0, 4, 0]
qAverage:[0.0, 45.54310913085938]
ws:[-2.080562138557434, 2.8231600761413573]
memory len:10000
memory used:3559.0
now epsilon is 0.025785131188416546, the reward is 247.25 with loss [32.33760213851929, 23.223105430603027] in episode 2666
Report: 
rewardSum:247.25
loss:[32.33760213851929, 23.223105430603027]
policies:[0, 4, 0]
qAverage:[0.0, 44.393182373046876]
ws:[-2.622668409347534, 2.2796671390533447]
memory len:10000
memory used:3558.0
now epsilon is 0.025759355725040855, the reward is 247.25 with loss [27.027199745178223, 35.65858507156372] in episode 2667
Report: 
rewardSum:247.25
loss:[27.027199745178223, 35.65858507156372]
policies:[0, 4, 0]
qAverage:[0.0, 46.97954559326172]
ws:[-5.177188068628311, 0.6693633198738098]
memory len:10000
memory used:3557.0
now epsilon is 0.025733606027464356, the reward is 247.25 with loss [26.645832538604736, 25.6437771320343] in episode 2668
Report: 
rewardSum:247.25
loss:[26.645832538604736, 25.6437771320343]
policies:[0, 4, 0]
qAverage:[0.0, 44.93313064575195]
ws:[-5.275968837738037, -0.08036397397518158]
memory len:10000
memory used:3557.0
now epsilon is 0.025707882069930903, the reward is 247.25 with loss [24.102070808410645, 18.09213411808014] in episode 2669
Report: 
rewardSum:247.25
loss:[24.102070808410645, 18.09213411808014]
policies:[0, 4, 0]
qAverage:[0.0, 45.259359741210936]
ws:[-4.930755150318146, 0.4575741525739431]
memory len:10000
memory used:3556.0
now epsilon is 0.025682183826710107, the reward is 247.25 with loss [30.840667724609375, 30.97553253173828] in episode 2670
Report: 
rewardSum:247.25
loss:[30.840667724609375, 30.97553253173828]
policies:[0, 4, 0]
qAverage:[0.0, 46.96104907989502]
ws:[-4.641323685646057, 1.4710644483566284]
memory len:10000
memory used:3556.0
now epsilon is 0.025656511272097303, the reward is 247.25 with loss [23.286325216293335, 23.43215274810791] in episode 2671
Report: 
rewardSum:247.25
loss:[23.286325216293335, 23.43215274810791]
policies:[0, 4, 0]
qAverage:[0.0, 45.06937255859375]
ws:[-3.523891347646713, 1.7870257139205932]
memory len:10000
memory used:3557.0
now epsilon is 0.025630864380413503, the reward is 247.25 with loss [23.863625526428223, 24.110912680625916] in episode 2672
Report: 
rewardSum:247.25
loss:[23.863625526428223, 24.110912680625916]
policies:[0, 4, 0]
qAverage:[0.0, 48.039991760253905]
ws:[-3.504381449520588, 2.2654003620147707]
memory len:10000
memory used:3557.0
now epsilon is 0.025605243126005404, the reward is 247.25 with loss [26.387783527374268, 26.879365921020508] in episode 2673
Report: 
rewardSum:247.25
loss:[26.387783527374268, 26.879365921020508]
policies:[0, 4, 0]
qAverage:[0.0, 51.783242797851564]
ws:[-3.224000911414623, 2.6657696306705474]
memory len:10000
memory used:3556.0
now epsilon is 0.025579647483245347, the reward is 247.25 with loss [35.45343637466431, 22.149256229400635] in episode 2674
Report: 
rewardSum:247.25
loss:[35.45343637466431, 22.149256229400635]
policies:[0, 4, 0]
qAverage:[0.0, 48.175873565673825]
ws:[-2.524576540291309, 3.334658908843994]
memory len:10000
memory used:3557.0
now epsilon is 0.025554077426531284, the reward is 247.25 with loss [27.514747619628906, 32.056753158569336] in episode 2675
Report: 
rewardSum:247.25
loss:[27.514747619628906, 32.056753158569336]
policies:[0, 4, 0]
qAverage:[0.0, 50.98901596069336]
ws:[-2.831774614751339, 3.0889108180999756]
memory len:10000
memory used:3557.0
now epsilon is 0.02552853293028676, the reward is 247.25 with loss [30.13712501525879, 30.16385269165039] in episode 2676
Report: 
rewardSum:247.25
loss:[30.13712501525879, 30.16385269165039]
policies:[0, 4, 0]
qAverage:[0.0, 49.15915374755859]
ws:[-2.2936879068613054, 3.5721734285354616]
memory len:10000
memory used:3557.0
now epsilon is 0.025503013968960892, the reward is 247.25 with loss [27.201536178588867, 25.14013957977295] in episode 2677
Report: 
rewardSum:247.25
loss:[27.201536178588867, 25.14013957977295]
policies:[0, 4, 0]
qAverage:[0.0, 50.78775253295898]
ws:[-2.202630090713501, 3.9126492977142333]
memory len:10000
memory used:3558.0
now epsilon is 0.025477520517028333, the reward is 247.25 with loss [30.845940113067627, 32.52009868621826] in episode 2678
Report: 
rewardSum:247.25
loss:[30.845940113067627, 32.52009868621826]
policies:[0, 4, 0]
qAverage:[0.0, 49.202013397216795]
ws:[-1.3150462210178375, 4.690367603302002]
memory len:10000
memory used:3558.0
now epsilon is 0.025452052548989256, the reward is 247.25 with loss [18.743820190429688, 18.67129898071289] in episode 2679
Report: 
rewardSum:247.25
loss:[18.743820190429688, 18.67129898071289]
policies:[0, 3, 1]
qAverage:[0.0, 52.26458263397217]
ws:[-2.3853213489055634, 5.178260445594788]
memory len:10000
memory used:3558.0
now epsilon is 0.025426610039369323, the reward is 247.25 with loss [18.214442253112793, 32.31892204284668] in episode 2680
Report: 
rewardSum:247.25
loss:[18.214442253112793, 32.31892204284668]
policies:[0, 4, 0]
qAverage:[0.0, 48.82387084960938]
ws:[-2.8340735644102097, 3.678265023231506]
memory len:10000
memory used:3558.0
now epsilon is 0.025401192962719657, the reward is 247.25 with loss [22.36739158630371, 27.51555585861206] in episode 2681
Report: 
rewardSum:247.25
loss:[22.36739158630371, 27.51555585861206]
policies:[0, 4, 0]
qAverage:[0.0, 50.431678009033206]
ws:[-3.1331866323947906, 2.7909109234809875]
memory len:10000
memory used:3558.0
now epsilon is 0.025375801293616825, the reward is 247.25 with loss [19.430673360824585, 24.56984853744507] in episode 2682
Report: 
rewardSum:247.25
loss:[19.430673360824585, 24.56984853744507]
policies:[0, 4, 0]
qAverage:[0.0, 48.95982818603515]
ws:[-2.228459966182709, 3.130860078334808]
memory len:10000
memory used:3557.0
now epsilon is 0.02535043500666281, the reward is 247.25 with loss [23.8929203748703, 29.70900058746338] in episode 2683
Report: 
rewardSum:247.25
loss:[23.8929203748703, 29.70900058746338]
policies:[0, 3, 1]
qAverage:[0.0, 49.0657844543457]
ws:[-2.879522953182459, 1.9878863543272018]
memory len:10000
memory used:3557.0
now epsilon is 0.025325094076484974, the reward is 247.25 with loss [33.793174743652344, 21.689936876296997] in episode 2684
Report: 
rewardSum:247.25
loss:[33.793174743652344, 21.689936876296997]
policies:[0, 4, 0]
qAverage:[0.0, 50.064170837402344]
ws:[-2.801280748844147, 1.2584776133298874]
memory len:10000
memory used:3557.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24*		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.02529977847773605, the reward is 247.25 with loss [23.18857216835022, 23.377259016036987] in episode 2685
Report: 
rewardSum:247.25
loss:[23.18857216835022, 23.377259016036987]
policies:[0, 4, 0]
qAverage:[0.0, 55.73585891723633]
ws:[-3.5613268852233886, 1.062613034248352]
memory len:10000
memory used:3557.0
now epsilon is 0.025274488185094108, the reward is 247.25 with loss [25.746300220489502, 29.708457946777344] in episode 2686
Report: 
rewardSum:247.25
loss:[25.746300220489502, 29.708457946777344]
policies:[0, 4, 0]
qAverage:[0.0, 49.97999572753906]
ws:[-4.035846501588821, 1.4868997931480408]
memory len:10000
memory used:3556.0
now epsilon is 0.02524922317326253, the reward is 247.25 with loss [29.579976081848145, 31.29075336456299] in episode 2687
Report: 
rewardSum:247.25
loss:[29.579976081848145, 31.29075336456299]
policies:[0, 4, 0]
qAverage:[0.0, 54.22420654296875]
ws:[-2.260917213559151, 2.9814395368099214]
memory len:10000
memory used:3556.0
now epsilon is 0.02522398341696998, the reward is 247.25 with loss [16.9948992729187, 28.94875717163086] in episode 2688
Report: 
rewardSum:247.25
loss:[16.9948992729187, 28.94875717163086]
policies:[0, 4, 0]
qAverage:[0.0, 54.54978561401367]
ws:[-1.6800688087940217, 3.1208775639533997]
memory len:10000
memory used:3553.0
now epsilon is 0.025198768890970398, the reward is 247.25 with loss [25.299104690551758, 25.55873465538025] in episode 2689
Report: 
rewardSum:247.25
loss:[25.299104690551758, 25.55873465538025]
policies:[0, 4, 0]
qAverage:[0.0, 54.831651306152345]
ws:[-0.514247465133667, 3.3198973655700685]
memory len:10000
memory used:3553.0
now epsilon is 0.02516728617515043, the reward is 246.25 with loss [38.8569393157959, 33.385096073150635] in episode 2690
Report: 
rewardSum:246.25
loss:[38.8569393157959, 33.385096073150635]
policies:[1, 3, 1]
qAverage:[0.0, 55.491652488708496]
ws:[-1.2793894410133362, 3.4822197556495667]
memory len:10000
memory used:3553.0
now epsilon is 0.025142128325134742, the reward is 247.25 with loss [21.48594832420349, 19.04140603542328] in episode 2691
Report: 
rewardSum:247.25
loss:[21.48594832420349, 19.04140603542328]
policies:[1, 3, 0]
qAverage:[12.35047721862793, 41.93451499938965]
ws:[-2.047166556119919, 2.173369526863098]
memory len:10000
memory used:3553.0
now epsilon is 0.02511071637463056, the reward is 246.25 with loss [32.53242206573486, 29.41782236099243] in episode 2692
Report: 
rewardSum:246.25
loss:[32.53242206573486, 29.41782236099243]
policies:[0, 4, 1]
qAverage:[0.0, 54.199641418457034]
ws:[-1.6411779880523683, 2.835997438430786]
memory len:10000
memory used:3553.0
now epsilon is 0.025085615073205253, the reward is 247.25 with loss [32.18993616104126, 30.847703456878662] in episode 2693
Report: 
rewardSum:247.25
loss:[32.18993616104126, 30.847703456878662]
policies:[0, 4, 0]
qAverage:[0.0, 54.29885330200195]
ws:[-1.0426985502243042, 3.056393933296204]
memory len:10000
memory used:3553.0
now epsilon is 0.025060538863669945, the reward is 247.25 with loss [20.52011203765869, 22.836937189102173] in episode 2694
Report: 
rewardSum:247.25
loss:[20.52011203765869, 22.836937189102173]
policies:[0, 4, 0]
qAverage:[0.0, 54.40575256347656]
ws:[-0.8872676849365234, 3.1522037029266357]
memory len:10000
memory used:3553.0
now epsilon is 0.025035487720942162, the reward is 247.25 with loss [30.986523151397705, 28.990927696228027] in episode 2695
Report: 
rewardSum:247.25
loss:[30.986523151397705, 28.990927696228027]
policies:[0, 4, 0]
qAverage:[0.0, 54.437540435791014]
ws:[-1.2992145776748658, 2.878520202636719]
memory len:10000
memory used:3552.0
now epsilon is 0.0250104616199645, the reward is 247.25 with loss [35.7260422706604, 20.76497173309326] in episode 2696
Report: 
rewardSum:247.25
loss:[35.7260422706604, 20.76497173309326]
policies:[1, 3, 0]
qAverage:[0.0, 50.7590446472168]
ws:[-2.670445427298546, 1.7500683069229126]
memory len:10000
memory used:3552.0
now epsilon is 0.024985460535704593, the reward is 247.25 with loss [32.99358558654785, 26.514419317245483] in episode 2697
Report: 
rewardSum:247.25
loss:[32.99358558654785, 26.514419317245483]
policies:[0, 4, 0]
qAverage:[0.0, 62.42963790893555]
ws:[-1.9940319299697875, 2.2307222366333006]
memory len:10000
memory used:3553.0
now epsilon is 0.024960484443155097, the reward is 247.25 with loss [27.24528980255127, 34.802852153778076] in episode 2698
Report: 
rewardSum:247.25
loss:[27.24528980255127, 34.802852153778076]
policies:[1, 3, 0]
qAverage:[11.646605682373046, 48.851025390625]
ws:[-1.257940423488617, 2.8992745876312256]
memory len:10000
memory used:3553.0
now epsilon is 0.02493553331733368, the reward is 247.25 with loss [30.099313259124756, 31.781752586364746] in episode 2699
Report: 
rewardSum:247.25
loss:[30.099313259124756, 31.781752586364746]
policies:[1, 3, 0]
qAverage:[10.655326080322265, 48.58338470458985]
ws:[-0.6291289567947388, 3.4755526304244997]
memory len:10000
memory used:3553.0
now epsilon is 0.02491060713328297, the reward is 247.25 with loss [21.124021530151367, 27.43818187713623] in episode 2700
Report: 
rewardSum:247.25
loss:[21.124021530151367, 27.43818187713623]
policies:[1, 3, 0]
qAverage:[11.47513198852539, 49.59097900390625]
ws:[-0.5398204326629639, 3.63826162815094]
memory len:10000
memory used:3552.0
now epsilon is 0.02488570586607055, the reward is 247.25 with loss [22.877532958984375, 16.953895568847656] in episode 2701
Report: 
rewardSum:247.25
loss:[22.877532958984375, 16.953895568847656]
policies:[0, 4, 0]
qAverage:[0.0, 58.936309814453125]
ws:[0.1356485366821289, 4.6500352144241335]
memory len:10000
memory used:3555.0
now epsilon is 0.024860829490788924, the reward is 247.25 with loss [32.91454601287842, 26.975266933441162] in episode 2702
Report: 
rewardSum:247.25
loss:[32.91454601287842, 26.975266933441162]
policies:[0, 4, 0]
qAverage:[0.0, 60.981482696533206]
ws:[0.3012181043624878, 5.271248722076416]
memory len:10000
memory used:3556.0
now epsilon is 0.024835977982555494, the reward is 247.25 with loss [31.98794984817505, 24.19600534439087] in episode 2703
Report: 
rewardSum:247.25
loss:[31.98794984817505, 24.19600534439087]
policies:[0, 4, 0]
qAverage:[0.0, 58.41004867553711]
ws:[1.037533950805664, 6.713397598266601]
memory len:10000
memory used:3556.0
now epsilon is 0.02481115131651253, the reward is 247.25 with loss [36.13463878631592, 24.48049807548523] in episode 2704
Report: 
rewardSum:247.25
loss:[36.13463878631592, 24.48049807548523]
policies:[0, 4, 0]
qAverage:[0.0, 60.653227996826175]
ws:[1.394329881668091, 7.448248767852784]
memory len:10000
memory used:3556.0
now epsilon is 0.024786349467827165, the reward is 247.25 with loss [29.469319820404053, 33.08924865722656] in episode 2705
Report: 
rewardSum:247.25
loss:[29.469319820404053, 33.08924865722656]
policies:[0, 4, 0]
qAverage:[0.0, 59.33641357421875]
ws:[0.4332055807113647, 6.586780166625976]
memory len:10000
memory used:3556.0
now epsilon is 0.02476157241169134, the reward is 247.25 with loss [25.425896167755127, 22.27323293685913] in episode 2706
Report: 
rewardSum:247.25
loss:[25.425896167755127, 22.27323293685913]
policies:[0, 4, 0]
qAverage:[0.0, 60.075276947021486]
ws:[-1.3096608877182008, 3.6520827293395994]
memory len:10000
memory used:3556.0
now epsilon is 0.024736820123321804, the reward is 247.25 with loss [27.982834815979004, 18.932037830352783] in episode 2707
Report: 
rewardSum:247.25
loss:[27.982834815979004, 18.932037830352783]
policies:[0, 4, 0]
qAverage:[0.0, 60.13254852294922]
ws:[-2.220304864645004, 2.243815150856972]
memory len:10000
memory used:3557.0
now epsilon is 0.024712092577960077, the reward is 247.25 with loss [35.121460914611816, 34.824432373046875] in episode 2708
Report: 
rewardSum:247.25
loss:[35.121460914611816, 34.824432373046875]
policies:[0, 4, 0]
qAverage:[0.0, 59.78779983520508]
ws:[-1.8329343654215335, 2.525681105256081]
memory len:10000
memory used:3557.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24*		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.024675047598958857, the reward is 245.25 with loss [39.12387704849243, 46.38225507736206] in episode 2709
Report: 
rewardSum:245.25
loss:[39.12387704849243, 46.38225507736206]
policies:[0, 5, 1]
qAverage:[0.0, 66.78543599446614]
ws:[-0.5428640649964412, 3.3925232738256454]
memory len:10000
memory used:3557.0
now epsilon is 0.02465038180296066, the reward is 247.25 with loss [24.799871921539307, 21.413899421691895] in episode 2710
Report: 
rewardSum:247.25
loss:[24.799871921539307, 21.413899421691895]
policies:[0, 4, 0]
qAverage:[0.0, 65.23678588867188]
ws:[0.4496277332305908, 4.780618786811829]
memory len:10000
memory used:3557.0
now epsilon is 0.024625740663510327, the reward is 247.25 with loss [21.377045154571533, 29.912038326263428] in episode 2711
Report: 
rewardSum:247.25
loss:[21.377045154571533, 29.912038326263428]
policies:[0, 4, 0]
qAverage:[0.0, 65.86730651855468]
ws:[0.8753502607345581, 5.559734320640564]
memory len:10000
memory used:3556.0
now epsilon is 0.02460112415596056, the reward is 247.25 with loss [24.09769868850708, 30.90310287475586] in episode 2712
Report: 
rewardSum:247.25
loss:[24.09769868850708, 30.90310287475586]
policies:[0, 4, 0]
qAverage:[0.0, 66.01614151000976]
ws:[0.652201247215271, 5.665434885025024]
memory len:10000
memory used:3555.0
now epsilon is 0.024576532255688684, the reward is 247.25 with loss [20.65611505508423, 20.925415515899658] in episode 2713
Report: 
rewardSum:247.25
loss:[20.65611505508423, 20.925415515899658]
policies:[0, 4, 0]
qAverage:[0.0, 65.96836395263672]
ws:[0.07764347791671752, 5.385298824310302]
memory len:10000
memory used:3555.0
now epsilon is 0.02453969049012542, the reward is 245.25 with loss [29.232145071029663, 37.23851680755615] in episode 2714
Report: 
rewardSum:245.25
loss:[29.232145071029663, 37.23851680755615]
policies:[0, 5, 1]
qAverage:[0.0, 69.53270848592122]
ws:[1.4024557520945866, 6.092916011810303]
memory len:10000
memory used:3555.0
now epsilon is 0.024515160000485595, the reward is 247.25 with loss [24.15420150756836, 22.54660153388977] in episode 2715
Report: 
rewardSum:247.25
loss:[24.15420150756836, 22.54660153388977]
policies:[0, 4, 0]
qAverage:[0.0, 65.4054054260254]
ws:[0.973245108127594, 7.294137811660766]
memory len:10000
memory used:3554.0
now epsilon is 0.024490654032138008, the reward is 247.25 with loss [34.74645185470581, 20.357747554779053] in episode 2716
Report: 
rewardSum:247.25
loss:[34.74645185470581, 20.357747554779053]
policies:[0, 4, 0]
qAverage:[0.0, 66.2063102722168]
ws:[0.5597237944602966, 6.793220138549804]
memory len:10000
memory used:3555.0
now epsilon is 0.024466172560570567, the reward is 247.25 with loss [23.968984127044678, 26.73171901702881] in episode 2717
Report: 
rewardSum:247.25
loss:[23.968984127044678, 26.73171901702881]
policies:[0, 4, 0]
qAverage:[0.0, 65.07146759033203]
ws:[0.04887627363204956, 6.228714561462402]
memory len:10000
memory used:3553.0
now epsilon is 0.02444171556129567, the reward is 247.25 with loss [31.741994380950928, 20.175947666168213] in episode 2718
Report: 
rewardSum:247.25
loss:[31.741994380950928, 20.175947666168213]
policies:[0, 4, 0]
qAverage:[0.0, 65.5368751525879]
ws:[0.3471366822719574, 5.9257570743560795]
memory len:10000
memory used:3553.0
now epsilon is 0.0244172830098502, the reward is 247.25 with loss [29.08833646774292, 28.724726676940918] in episode 2719
Report: 
rewardSum:247.25
loss:[29.08833646774292, 28.724726676940918]
policies:[0, 4, 0]
qAverage:[0.0, 65.65101623535156]
ws:[-0.1001644104719162, 4.849745583534241]
memory len:10000
memory used:3553.0
now epsilon is 0.024392874881795496, the reward is 247.25 with loss [35.09724140167236, 38.88985824584961] in episode 2720
Report: 
rewardSum:247.25
loss:[35.09724140167236, 38.88985824584961]
policies:[0, 4, 0]
qAverage:[0.0, 66.43433532714843]
ws:[0.2933263838291168, 5.288168096542359]
memory len:10000
memory used:3553.0
now epsilon is 0.024368491152717327, the reward is 247.25 with loss [24.6014244556427, 27.62817144393921] in episode 2721
Report: 
rewardSum:247.25
loss:[24.6014244556427, 27.62817144393921]
policies:[0, 4, 0]
qAverage:[0.0, 72.71801681518555]
ws:[0.9806101083755493, 6.044535565376282]
memory len:10000
memory used:3553.0
now epsilon is 0.024344131798225862, the reward is 247.25 with loss [23.321813583374023, 38.21675634384155] in episode 2722
Report: 
rewardSum:247.25
loss:[23.321813583374023, 38.21675634384155]
policies:[0, 4, 0]
qAverage:[0.0, 71.45101165771484]
ws:[1.7281998872756958, 7.1305351734161375]
memory len:10000
memory used:3553.0
now epsilon is 0.02431979679395565, the reward is 247.25 with loss [22.232396125793457, 28.639843463897705] in episode 2723
Report: 
rewardSum:247.25
loss:[22.232396125793457, 28.639843463897705]
policies:[0, 4, 0]
qAverage:[0.0, 71.23959808349609]
ws:[1.5901128768920898, 7.207093048095703]
memory len:10000
memory used:3553.0
now epsilon is 0.0242954861155656, the reward is 247.25 with loss [31.40021324157715, 27.71386456489563] in episode 2724
Report: 
rewardSum:247.25
loss:[31.40021324157715, 27.71386456489563]
policies:[0, 4, 0]
qAverage:[0.0, 72.1458740234375]
ws:[1.4727532386779785, 7.2614984035491945]
memory len:10000
memory used:3553.0
now epsilon is 0.02427119973873896, the reward is 247.25 with loss [20.693970203399658, 14.913777351379395] in episode 2725
Report: 
rewardSum:247.25
loss:[20.693970203399658, 14.913777351379395]
policies:[0, 4, 0]
qAverage:[0.0, 71.72719039916993]
ws:[1.2324568271636962, 6.9980189323425295]
memory len:10000
memory used:3554.0
now epsilon is 0.024246937639183272, the reward is 247.25 with loss [20.53170657157898, 27.270588397979736] in episode 2726
Report: 
rewardSum:247.25
loss:[20.53170657157898, 27.270588397979736]
policies:[0, 4, 0]
qAverage:[0.0, 71.3587158203125]
ws:[0.4727948445826769, 6.145685434341431]
memory len:10000
memory used:3554.0
now epsilon is 0.024222699792630368, the reward is 247.25 with loss [19.107925176620483, 21.021226167678833] in episode 2727
Report: 
rewardSum:247.25
loss:[19.107925176620483, 21.021226167678833]
policies:[0, 4, 0]
qAverage:[0.0, 72.04229888916015]
ws:[0.34687621891498566, 5.823114967346191]
memory len:10000
memory used:3553.0
now epsilon is 0.024198486174836338, the reward is 247.25 with loss [33.41465187072754, 40.57149791717529] in episode 2728
Report: 
rewardSum:247.25
loss:[33.41465187072754, 40.57149791717529]
policies:[0, 4, 0]
qAverage:[0.0, 70.3632583618164]
ws:[1.0893279910087585, 6.526357507705688]
memory len:10000
memory used:3554.0
now epsilon is 0.02417429676158151, the reward is 247.25 with loss [30.681862354278564, 30.594738006591797] in episode 2729
Report: 
rewardSum:247.25
loss:[30.681862354278564, 30.594738006591797]
policies:[0, 4, 0]
qAverage:[0.0, 72.29618911743164]
ws:[1.0955652117729187, 5.553585433959961]
memory len:10000
memory used:3553.0
now epsilon is 0.02415013152867042, the reward is 247.25 with loss [28.62380886077881, 25.29662275314331] in episode 2730
Report: 
rewardSum:247.25
loss:[28.62380886077881, 25.29662275314331]
policies:[0, 3, 1]
qAverage:[0.0, 68.65242195129395]
ws:[0.10718243569135666, 4.8573541939258575]
memory len:10000
memory used:3552.0
now epsilon is 0.02412599045193179, the reward is 247.25 with loss [22.420942306518555, 25.427083730697632] in episode 2731
Report: 
rewardSum:247.25
loss:[22.420942306518555, 25.427083730697632]
policies:[0, 4, 0]
qAverage:[0.0, 72.5877197265625]
ws:[-0.27281752824783323, 4.826442241668701]
memory len:10000
memory used:3552.0
now epsilon is 0.0241018735072185, the reward is 247.25 with loss [23.650930643081665, 27.97114658355713] in episode 2732
Report: 
rewardSum:247.25
loss:[23.650930643081665, 27.97114658355713]
policies:[0, 4, 0]
qAverage:[0.0, 64.51776885986328]
ws:[0.09883560612797737, 5.156292378902435]
memory len:10000
memory used:3553.0
now epsilon is 0.02407778067040758, the reward is 247.25 with loss [32.37473964691162, 32.73346710205078] in episode 2733
Report: 
rewardSum:247.25
loss:[32.37473964691162, 32.73346710205078]
policies:[0, 3, 1]
qAverage:[0.0, 70.89545822143555]
ws:[0.9785963147878647, 6.519546568393707]
memory len:10000
memory used:3552.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2*		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.024053711917400158, the reward is 247.25 with loss [39.310224533081055, 29.240447759628296] in episode 2734
Report: 
rewardSum:247.25
loss:[39.310224533081055, 29.240447759628296]
policies:[0, 4, 0]
qAverage:[0.0, 80.97988433837891]
ws:[2.4356043100357057, 8.832913780212403]
memory len:10000
memory used:3552.0
now epsilon is 0.024029667224121466, the reward is 247.25 with loss [17.082998275756836, 20.936252117156982] in episode 2735
Report: 
rewardSum:247.25
loss:[17.082998275756836, 20.936252117156982]
policies:[0, 4, 0]
qAverage:[0.0, 76.21955718994141]
ws:[2.3041656017303467, 7.990473365783691]
memory len:10000
memory used:3552.0
now epsilon is 0.023993645243590447, the reward is 245.25 with loss [41.995675563812256, 46.83324718475342] in episode 2736
Report: 
rewardSum:245.25
loss:[41.995675563812256, 46.83324718475342]
policies:[0, 5, 1]
qAverage:[0.0, 82.86072413126628]
ws:[2.0343516568342843, 6.2432664434115095]
memory len:10000
memory used:3552.0
now epsilon is 0.02396966059446432, the reward is 247.25 with loss [25.480167627334595, 24.829765796661377] in episode 2737
Report: 
rewardSum:247.25
loss:[25.480167627334595, 24.829765796661377]
policies:[0, 4, 0]
qAverage:[0.0, 78.44534759521484]
ws:[2.373605465888977, 7.225665760040283]
memory len:10000
memory used:3553.0
now epsilon is 0.023945699920994566, the reward is 247.25 with loss [16.76169753074646, 30.679810047149658] in episode 2738
Report: 
rewardSum:247.25
loss:[16.76169753074646, 30.679810047149658]
policies:[0, 4, 0]
qAverage:[0.0, 77.77086639404297]
ws:[2.1558249950408936, 7.044040727615356]
memory len:10000
memory used:3553.0
now epsilon is 0.023933728567640315, the reward is -1.0 with loss [18.967360496520996, 17.2363224029541] in episode 2739
Report: 
rewardSum:-1.0
loss:[18.967360496520996, 17.2363224029541]
policies:[0, 1, 1]
qAverage:[0.0, 36.0346794128418]
ws:[0.19797322154045105, 1.2115117311477661]
memory len:10000
memory used:3553.0
now epsilon is 0.023909803812725124, the reward is 247.25 with loss [39.446417808532715, 29.821749687194824] in episode 2740
Report: 
rewardSum:247.25
loss:[39.446417808532715, 29.821749687194824]
policies:[0, 4, 0]
qAverage:[0.0, 77.45302734375]
ws:[2.239700984954834, 8.405330562591553]
memory len:10000
memory used:3552.0
now epsilon is 0.02388590297359456, the reward is 247.25 with loss [24.350226402282715, 38.19823694229126] in episode 2741
Report: 
rewardSum:247.25
loss:[24.350226402282715, 38.19823694229126]
policies:[0, 4, 0]
qAverage:[0.0, 79.50783996582031]
ws:[2.1213491916656495, 8.050894927978515]
memory len:10000
memory used:3552.0
now epsilon is 0.023862026026341812, the reward is 247.25 with loss [27.852379322052002, 18.242276430130005] in episode 2742
Report: 
rewardSum:247.25
loss:[27.852379322052002, 18.242276430130005]
policies:[0, 4, 0]
qAverage:[0.0, 77.35537261962891]
ws:[2.5997510313987733, 8.62758436203003]
memory len:10000
memory used:3552.0
now epsilon is 0.023826255350496223, the reward is 245.25 with loss [44.8704514503479, 38.12649583816528] in episode 2743
Report: 
rewardSum:245.25
loss:[44.8704514503479, 38.12649583816528]
policies:[0, 5, 1]
qAverage:[0.0, 82.49609375]
ws:[4.7416611313819885, 10.348757982254028]
memory len:10000
memory used:3553.0
now epsilon is 0.02380243802850244, the reward is 247.25 with loss [28.51071786880493, 24.882709503173828] in episode 2744
Report: 
rewardSum:247.25
loss:[28.51071786880493, 24.882709503173828]
policies:[0, 4, 0]
qAverage:[0.0, 77.67133178710938]
ws:[4.645372438430786, 10.915023136138917]
memory len:10000
memory used:3553.0
now epsilon is 0.02377864451490064, the reward is 247.25 with loss [28.823413372039795, 27.78496503829956] in episode 2745
Report: 
rewardSum:247.25
loss:[28.823413372039795, 27.78496503829956]
policies:[0, 4, 0]
qAverage:[0.0, 86.50036315917968]
ws:[3.3311221599578857, 9.641074943542481]
memory len:10000
memory used:3554.0
now epsilon is 0.023766756678808475, the reward is -1.0 with loss [15.527276039123535, 12.650629997253418] in episode 2746
Report: 
rewardSum:-1.0
loss:[15.527276039123535, 12.650629997253418]
policies:[0, 1, 1]
qAverage:[0.0, 41.7497673034668]
ws:[-0.8120040893554688, 1.1651296615600586]
memory len:10000
memory used:3554.0
now epsilon is 0.023742998833178094, the reward is 247.25 with loss [33.163240909576416, 32.42360496520996] in episode 2747
Report: 
rewardSum:247.25
loss:[33.163240909576416, 32.42360496520996]
policies:[0, 4, 0]
qAverage:[0.0, 81.7606689453125]
ws:[2.570929217338562, 10.76753854751587]
memory len:10000
memory used:3553.0
now epsilon is 0.023719264736485637, the reward is 247.25 with loss [26.423607110977173, 25.654566764831543] in episode 2748
Report: 
rewardSum:247.25
loss:[26.423607110977173, 25.654566764831543]
policies:[1, 3, 0]
qAverage:[0.0, 77.59328651428223]
ws:[2.381040759384632, 9.551015853881836]
memory len:10000
memory used:3555.0
now epsilon is 0.02369555436499107, the reward is 247.25 with loss [34.80423307418823, 32.806715965270996] in episode 2749
Report: 
rewardSum:247.25
loss:[34.80423307418823, 32.806715965270996]
policies:[0, 3, 1]
qAverage:[0.0, 85.34568786621094]
ws:[2.516237437725067, 10.14699125289917]
memory len:10000
memory used:3555.0
now epsilon is 0.023671867694978088, the reward is 247.25 with loss [38.79807710647583, 33.99143934249878] in episode 2750
Report: 
rewardSum:247.25
loss:[38.79807710647583, 33.99143934249878]
policies:[0, 4, 0]
qAverage:[0.0, 84.36616821289063]
ws:[2.4126043424010275, 8.985491037368774]
memory len:10000
memory used:3555.0
now epsilon is 0.0236482047027541, the reward is 247.25 with loss [27.941879272460938, 25.033374309539795] in episode 2751
Report: 
rewardSum:247.25
loss:[27.941879272460938, 25.033374309539795]
policies:[0, 4, 0]
qAverage:[0.0, 85.18241729736329]
ws:[2.596960261464119, 9.65044264793396]
memory len:10000
memory used:3555.0
now epsilon is 0.02362456536465019, the reward is 247.25 with loss [24.87313175201416, 30.125224113464355] in episode 2752
Report: 
rewardSum:247.25
loss:[24.87313175201416, 30.125224113464355]
policies:[0, 4, 0]
qAverage:[0.0, 78.03433609008789]
ws:[2.514976277947426, 8.845650792121887]
memory len:10000
memory used:3554.0
now epsilon is 0.02360094965702111, the reward is 247.25 with loss [24.33143901824951, 22.714648723602295] in episode 2753
Report: 
rewardSum:247.25
loss:[24.33143901824951, 22.714648723602295]
policies:[0, 4, 0]
qAverage:[0.0, 86.08575439453125]
ws:[0.6358357071876526, 5.994539642333985]
memory len:10000
memory used:3554.0
now epsilon is 0.02356557035105197, the reward is 245.25 with loss [37.05616807937622, 37.5922532081604] in episode 2754
Report: 
rewardSum:245.25
loss:[37.05616807937622, 37.5922532081604]
policies:[0, 5, 1]
qAverage:[0.0, 87.42666371663411]
ws:[0.3944321225086848, 4.837911327679952]
memory len:10000
memory used:3554.0
now epsilon is 0.023542013616317047, the reward is 247.25 with loss [22.066554069519043, 23.258795499801636] in episode 2755
Report: 
rewardSum:247.25
loss:[22.066554069519043, 23.258795499801636]
policies:[0, 4, 0]
qAverage:[0.0, 84.49347229003907]
ws:[1.602970814704895, 6.554989051818848]
memory len:10000
memory used:3553.0
now epsilon is 0.02351848042948455, the reward is 247.25 with loss [20.900651931762695, 22.80495262145996] in episode 2756
Report: 
rewardSum:247.25
loss:[20.900651931762695, 22.80495262145996]
policies:[0, 4, 0]
qAverage:[0.0, 86.5123062133789]
ws:[1.8723398804664613, 7.073621082305908]
memory len:10000
memory used:3552.0
now epsilon is 0.023494970767015413, the reward is 247.25 with loss [29.31732177734375, 29.586281299591064] in episode 2757
Report: 
rewardSum:247.25
loss:[29.31732177734375, 29.586281299591064]
policies:[0, 3, 1]
qAverage:[0.0, 71.67848014831543]
ws:[1.8791953027248383, 5.327119946479797]
memory len:10000
memory used:3552.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26*		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.023471484605394093, the reward is 247.25 with loss [25.1289541721344, 32.79368495941162] in episode 2758
Report: 
rewardSum:247.25
loss:[25.1289541721344, 32.79368495941162]
policies:[0, 4, 0]
qAverage:[0.0, 94.25464630126953]
ws:[2.5355149507522583, 8.015880680084228]
memory len:10000
memory used:3551.0
############# STATE ###############
0*		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.023448021921128555, the reward is 247.25 with loss [24.191800832748413, 31.623059272766113] in episode 2759
Report: 
rewardSum:247.25
loss:[24.191800832748413, 31.623059272766113]
policies:[1, 3, 0]
qAverage:[0.0, 83.61359786987305]
ws:[3.5941357016563416, 9.281969428062439]
memory len:10000
memory used:3553.0
now epsilon is 0.023424582690750238, the reward is 247.25 with loss [35.96065044403076, 29.588063716888428] in episode 2760
Report: 
rewardSum:247.25
loss:[35.96065044403076, 29.588063716888428]
policies:[0, 4, 0]
qAverage:[0.0, 90.00398101806641]
ws:[4.878609848022461, 11.384063625335694]
memory len:10000
memory used:3553.0
now epsilon is 0.023401166890814052, the reward is 247.25 with loss [27.642845153808594, 22.314979791641235] in episode 2761
Report: 
rewardSum:247.25
loss:[27.642845153808594, 22.314979791641235]
policies:[0, 4, 0]
qAverage:[0.0, 92.19598846435547]
ws:[6.415458297729492, 13.582462501525878]
memory len:10000
memory used:3553.0
now epsilon is 0.023377774497898345, the reward is 247.25 with loss [27.859687328338623, 27.724431037902832] in episode 2762
Report: 
rewardSum:247.25
loss:[27.859687328338623, 27.724431037902832]
policies:[0, 4, 0]
qAverage:[0.0, 90.72876586914063]
ws:[5.041321611404419, 12.590927505493164]
memory len:10000
memory used:3553.0
now epsilon is 0.023354405488604863, the reward is 247.25 with loss [30.004292964935303, 31.917419910430908] in episode 2763
Report: 
rewardSum:247.25
loss:[30.004292964935303, 31.917419910430908]
policies:[0, 4, 0]
qAverage:[0.0, 90.8105712890625]
ws:[3.1281581282615663, 9.82829828262329]
memory len:10000
memory used:3553.0
now epsilon is 0.02333105983955876, the reward is 247.25 with loss [22.62056303024292, 27.185945987701416] in episode 2764
Report: 
rewardSum:247.25
loss:[22.62056303024292, 27.185945987701416]
policies:[0, 4, 0]
qAverage:[0.0, 90.6836441040039]
ws:[1.4765370219945908, 8.197427892684937]
memory len:10000
memory used:3555.0
now epsilon is 0.023307737527408542, the reward is 247.25 with loss [32.36364269256592, 26.629190683364868] in episode 2765
Report: 
rewardSum:247.25
loss:[32.36364269256592, 26.629190683364868]
policies:[0, 4, 0]
qAverage:[0.0, 89.96914520263672]
ws:[3.179397428035736, 9.83595151901245]
memory len:10000
memory used:3555.0
now epsilon is 0.023284438528826067, the reward is 247.25 with loss [30.01465153694153, 29.409623384475708] in episode 2766
Report: 
rewardSum:247.25
loss:[30.01465153694153, 29.409623384475708]
policies:[0, 4, 0]
qAverage:[0.0, 91.22628021240234]
ws:[4.2687310695648195, 10.251476573944093]
memory len:10000
memory used:3555.0
now epsilon is 0.02326116282050651, the reward is 247.25 with loss [18.985272884368896, 21.839271068572998] in episode 2767
Report: 
rewardSum:247.25
loss:[18.985272884368896, 21.839271068572998]
policies:[0, 3, 1]
qAverage:[0.0, 90.96236801147461]
ws:[5.039438307285309, 11.20274043083191]
memory len:10000
memory used:3556.0
now epsilon is 0.023237910379168333, the reward is 247.25 with loss [24.45258867740631, 20.818373680114746] in episode 2768
Report: 
rewardSum:247.25
loss:[24.45258867740631, 20.818373680114746]
policies:[0, 4, 0]
qAverage:[0.0, 90.67879180908203]
ws:[3.7284082174301147, 9.422711181640626]
memory len:10000
memory used:3555.0
now epsilon is 0.023214681181553277, the reward is 247.25 with loss [21.2922523021698, 20.087040662765503] in episode 2769
Report: 
rewardSum:247.25
loss:[21.2922523021698, 20.087040662765503]
policies:[0, 4, 0]
qAverage:[0.0, 90.76535186767578]
ws:[3.932161021232605, 9.650783252716064]
memory len:10000
memory used:3554.0
now epsilon is 0.02319147520442634, the reward is 247.25 with loss [40.47797679901123, 33.04052734375] in episode 2770
Report: 
rewardSum:247.25
loss:[40.47797679901123, 33.04052734375]
policies:[0, 4, 0]
qAverage:[0.0, 99.4274398803711]
ws:[3.0921178311109543, 9.204561901092529]
memory len:10000
memory used:3554.0
now epsilon is 0.023168292424575743, the reward is 247.25 with loss [36.25460195541382, 27.796553134918213] in episode 2771
Report: 
rewardSum:247.25
loss:[36.25460195541382, 27.796553134918213]
policies:[1, 3, 0]
qAverage:[0.0, 94.21088218688965]
ws:[4.884167889133096, 10.638471841812134]
memory len:10000
memory used:3555.0
now epsilon is 0.0231451328188129, the reward is 247.25 with loss [26.99303364753723, 35.49709939956665] in episode 2772
Report: 
rewardSum:247.25
loss:[26.99303364753723, 35.49709939956665]
policies:[0, 4, 0]
qAverage:[0.0, 94.75602264404297]
ws:[5.672099304199219, 11.001529121398926]
memory len:10000
memory used:3553.0
now epsilon is 0.023110436810915205, the reward is 245.25 with loss [51.2174551486969, 47.05983209609985] in episode 2773
Report: 
rewardSum:245.25
loss:[51.2174551486969, 47.05983209609985]
policies:[0, 5, 1]
qAverage:[0.0, 96.49437459309895]
ws:[4.203158985823393, 7.361010471979777]
memory len:10000
memory used:3554.0
now epsilon is 0.023087335039073786, the reward is 247.25 with loss [27.695394039154053, 26.44603681564331] in episode 2774
Report: 
rewardSum:247.25
loss:[27.695394039154053, 26.44603681564331]
policies:[0, 4, 0]
qAverage:[0.0, 97.16819763183594]
ws:[4.7785131454467775, 9.29313588142395]
memory len:10000
memory used:3554.0
now epsilon is 0.023064256360342487, the reward is 247.25 with loss [29.533170223236084, 25.39589262008667] in episode 2775
Report: 
rewardSum:247.25
loss:[29.533170223236084, 25.39589262008667]
policies:[0, 4, 0]
qAverage:[0.0, 96.23991241455079]
ws:[5.902192187309265, 12.082973670959472]
memory len:10000
memory used:3554.0
now epsilon is 0.023041200751636858, the reward is 247.25 with loss [25.320743560791016, 32.31832790374756] in episode 2776
Report: 
rewardSum:247.25
loss:[25.320743560791016, 32.31832790374756]
policies:[0, 4, 0]
qAverage:[0.0, 97.2368377685547]
ws:[6.119647407531739, 13.407024765014649]
memory len:10000
memory used:3555.0
now epsilon is 0.02301816818989552, the reward is 247.25 with loss [27.013161659240723, 22.098934173583984] in episode 2777
Report: 
rewardSum:247.25
loss:[27.013161659240723, 22.098934173583984]
policies:[0, 4, 0]
qAverage:[0.0, 95.29669494628907]
ws:[4.601308512687683, 11.895839214324951]
memory len:10000
memory used:3555.0
now epsilon is 0.022995158652080155, the reward is 247.25 with loss [29.70142650604248, 23.801477432250977] in episode 2778
Report: 
rewardSum:247.25
loss:[29.70142650604248, 23.801477432250977]
policies:[0, 4, 0]
qAverage:[0.0, 97.66090240478516]
ws:[2.4122653245925902, 9.803922080993653]
memory len:10000
memory used:3555.0
now epsilon is 0.022972172115175465, the reward is 247.25 with loss [32.322030544281006, 28.0961332321167] in episode 2779
Report: 
rewardSum:247.25
loss:[32.322030544281006, 28.0961332321167]
policies:[0, 4, 0]
qAverage:[0.0, 96.19299926757813]
ws:[1.6505971252918243, 8.653500318527222]
memory len:10000
memory used:3555.0
now epsilon is 0.022949208556189163, the reward is 247.25 with loss [31.039566040039062, 33.10373020172119] in episode 2780
Report: 
rewardSum:247.25
loss:[31.039566040039062, 33.10373020172119]
policies:[0, 4, 0]
qAverage:[0.0, 96.78897552490234]
ws:[2.6964176416397097, 8.852171611785888]
memory len:10000
memory used:3555.0
now epsilon is 0.022926267952151948, the reward is 247.25 with loss [30.148542404174805, 31.523805618286133] in episode 2781
Report: 
rewardSum:247.25
loss:[30.148542404174805, 31.523805618286133]
policies:[0, 4, 0]
qAverage:[0.0, 97.70046844482422]
ws:[2.8160859823226927, 8.407532691955566]
memory len:10000
memory used:3555.0
now epsilon is 0.022903350280117477, the reward is 247.25 with loss [33.61984705924988, 27.79252052307129] in episode 2782
Report: 
rewardSum:247.25
loss:[33.61984705924988, 27.79252052307129]
policies:[0, 4, 0]
qAverage:[0.0, 103.35017395019531]
ws:[2.547462010383606, 7.772381162643432]
memory len:10000
memory used:3555.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22*		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.02288045551716235, the reward is 247.25 with loss [30.331363677978516, 27.11714506149292] in episode 2783
Report: 
rewardSum:247.25
loss:[30.331363677978516, 27.11714506149292]
policies:[0, 4, 0]
qAverage:[0.0, 104.91397552490234]
ws:[2.782504689693451, 7.777810335159302]
memory len:10000
memory used:3553.0
now epsilon is 0.022857583640386068, the reward is 247.25 with loss [23.54829502105713, 30.323038578033447] in episode 2784
Report: 
rewardSum:247.25
loss:[23.54829502105713, 30.323038578033447]
policies:[0, 4, 0]
qAverage:[0.0, 103.65423126220703]
ws:[1.9679688423871995, 6.375558233261108]
memory len:10000
memory used:3554.0
now epsilon is 0.02283473462691104, the reward is 247.25 with loss [21.434265613555908, 28.840447425842285] in episode 2785
Report: 
rewardSum:247.25
loss:[21.434265613555908, 28.840447425842285]
policies:[0, 4, 0]
qAverage:[0.0, 103.84736938476563]
ws:[1.1905230224132537, 5.017700028419495]
memory len:10000
memory used:3554.0
now epsilon is 0.022811908453882535, the reward is 247.25 with loss [21.24557137489319, 24.447680950164795] in episode 2786
Report: 
rewardSum:247.25
loss:[21.24557137489319, 24.447680950164795]
policies:[0, 4, 0]
qAverage:[0.0, 105.99058074951172]
ws:[0.883572781085968, 4.538721227645874]
memory len:10000
memory used:3554.0
now epsilon is 0.02278910509846867, the reward is 247.25 with loss [40.293622970581055, 30.26397132873535] in episode 2787
Report: 
rewardSum:247.25
loss:[40.293622970581055, 30.26397132873535]
policies:[0, 4, 0]
qAverage:[0.0, 104.91438598632813]
ws:[1.328587631136179, 4.8881042957305905]
memory len:10000
memory used:3554.0
now epsilon is 0.022766324537860386, the reward is 247.25 with loss [20.573114156723022, 28.54327154159546] in episode 2788
Report: 
rewardSum:247.25
loss:[20.573114156723022, 28.54327154159546]
policies:[0, 4, 0]
qAverage:[0.0, 105.46954650878907]
ws:[2.724533534049988, 6.383451318740844]
memory len:10000
memory used:3554.0
now epsilon is 0.022732196387369712, the reward is 245.25 with loss [33.81688332557678, 42.23877954483032] in episode 2789
Report: 
rewardSum:245.25
loss:[33.81688332557678, 42.23877954483032]
policies:[1, 4, 1]
qAverage:[0.0, 107.6108642578125]
ws:[3.748854285478592, 9.545754146575927]
memory len:10000
memory used:3554.0
now epsilon is 0.022709472714135315, the reward is 247.25 with loss [21.801941633224487, 25.05165433883667] in episode 2790
Report: 
rewardSum:247.25
loss:[21.801941633224487, 25.05165433883667]
policies:[0, 4, 0]
qAverage:[0.0, 105.45429077148438]
ws:[2.5974378898739814, 8.633793067932128]
memory len:10000
memory used:3554.0
now epsilon is 0.022686771756054196, the reward is 247.25 with loss [29.06639575958252, 24.4405517578125] in episode 2791
Report: 
rewardSum:247.25
loss:[29.06639575958252, 24.4405517578125]
policies:[0, 4, 0]
qAverage:[0.0, 104.0706802368164]
ws:[2.4749328970909117, 9.081457996368409]
memory len:10000
memory used:3555.0
now epsilon is 0.02266409349041972, the reward is 247.25 with loss [31.08462619781494, 29.575454711914062] in episode 2792
Report: 
rewardSum:247.25
loss:[31.08462619781494, 29.575454711914062]
policies:[0, 4, 0]
qAverage:[0.0, 105.91453552246094]
ws:[5.0679145634174345, 10.290800762176513]
memory len:10000
memory used:3554.0
now epsilon is 0.022641437894547943, the reward is 247.25 with loss [16.750091075897217, 26.89168691635132] in episode 2793
Report: 
rewardSum:247.25
loss:[16.750091075897217, 26.89168691635132]
policies:[1, 3, 0]
qAverage:[0.0, 97.65981483459473]
ws:[6.427663266658783, 10.264581322669983]
memory len:10000
memory used:3554.0
now epsilon is 0.02261880494577761, the reward is 247.25 with loss [27.749265909194946, 31.644719123840332] in episode 2794
Report: 
rewardSum:247.25
loss:[27.749265909194946, 31.644719123840332]
policies:[0, 4, 0]
qAverage:[0.0, 113.45174407958984]
ws:[4.242153811454773, 8.721938133239746]
memory len:10000
memory used:3554.0
now epsilon is 0.022596194621470098, the reward is 247.25 with loss [32.86028790473938, 28.41007137298584] in episode 2795
Report: 
rewardSum:247.25
loss:[32.86028790473938, 28.41007137298584]
policies:[0, 4, 0]
qAverage:[0.0, 114.57468719482422]
ws:[3.4208649516105654, 9.176751852035522]
memory len:10000
memory used:3554.0
now epsilon is 0.022573606899009436, the reward is 247.25 with loss [38.032270431518555, 31.605636596679688] in episode 2796
Report: 
rewardSum:247.25
loss:[38.032270431518555, 31.605636596679688]
policies:[0, 4, 0]
qAverage:[0.0, 110.76004028320312]
ws:[6.1213254749774935, 11.985018157958985]
memory len:10000
memory used:3555.0
now epsilon is 0.022551041755802255, the reward is 247.25 with loss [39.62077856063843, 27.2964768409729] in episode 2797
Report: 
rewardSum:247.25
loss:[39.62077856063843, 27.2964768409729]
policies:[0, 4, 0]
qAverage:[0.0, 114.10534973144532]
ws:[9.319949007034301, 14.345051860809326]
memory len:10000
memory used:3554.0
now epsilon is 0.02252849916927776, the reward is 247.25 with loss [32.170409202575684, 23.434699773788452] in episode 2798
Report: 
rewardSum:247.25
loss:[32.170409202575684, 23.434699773788452]
policies:[0, 4, 0]
qAverage:[0.0, 111.84112396240235]
ws:[8.931880474090576, 12.617160415649414]
memory len:10000
memory used:3553.0
now epsilon is 0.022505979116887728, the reward is 247.25 with loss [25.737073183059692, 29.501862049102783] in episode 2799
Report: 
rewardSum:247.25
loss:[25.737073183059692, 29.501862049102783]
policies:[0, 4, 0]
qAverage:[0.0, 114.03295440673828]
ws:[5.032864201068878, 9.19755392074585]
memory len:10000
memory used:3553.0
now epsilon is 0.022483481576106475, the reward is 247.25 with loss [32.83737373352051, 28.056947708129883] in episode 2800
Report: 
rewardSum:247.25
loss:[32.83737373352051, 28.056947708129883]
policies:[0, 4, 0]
qAverage:[0.0, 110.00236206054687]
ws:[5.022949314117431, 9.044783639907838]
memory len:10000
memory used:3553.0
now epsilon is 0.02246100652443083, the reward is 247.25 with loss [36.94736385345459, 31.756838083267212] in episode 2801
Report: 
rewardSum:247.25
loss:[36.94736385345459, 31.756838083267212]
policies:[0, 4, 0]
qAverage:[0.0, 113.89496459960938]
ws:[7.140242847800255, 10.421033573150634]
memory len:10000
memory used:3554.0
now epsilon is 0.022438553939380122, the reward is 247.25 with loss [28.49980068206787, 34.80294895172119] in episode 2802
Report: 
rewardSum:247.25
loss:[28.49980068206787, 34.80294895172119]
policies:[0, 4, 0]
qAverage:[0.0, 110.70333099365234]
ws:[6.496026712656021, 10.023863363265992]
memory len:10000
memory used:3554.0
now epsilon is 0.02241612379849615, the reward is 247.25 with loss [29.45607900619507, 30.31363868713379] in episode 2803
Report: 
rewardSum:247.25
loss:[29.45607900619507, 30.31363868713379]
policies:[0, 4, 0]
qAverage:[0.0, 111.6399658203125]
ws:[6.783952516037971, 11.04768967628479]
memory len:10000
memory used:3554.0
now epsilon is 0.022393716079343158, the reward is 247.25 with loss [22.79093837738037, 29.533432006835938] in episode 2804
Report: 
rewardSum:247.25
loss:[22.79093837738037, 29.533432006835938]
policies:[0, 4, 0]
qAverage:[0.0, 112.24968566894532]
ws:[6.706001704931259, 11.312971591949463]
memory len:10000
memory used:3554.0
now epsilon is 0.022371330759507826, the reward is 247.25 with loss [30.92702341079712, 26.248140335083008] in episode 2805
Report: 
rewardSum:247.25
loss:[30.92702341079712, 26.248140335083008]
policies:[0, 4, 0]
qAverage:[0.0, 110.25531311035157]
ws:[5.470319628715515, 10.34887638092041]
memory len:10000
memory used:3554.0
now epsilon is 0.022348967816599234, the reward is 247.25 with loss [26.483346939086914, 35.64437294006348] in episode 2806
Report: 
rewardSum:247.25
loss:[26.483346939086914, 35.64437294006348]
policies:[0, 4, 0]
qAverage:[0.0, 115.07545776367188]
ws:[3.4651740431785583, 8.708247685432434]
memory len:10000
memory used:3554.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26*		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.02232662722824884, the reward is 247.25 with loss [31.4602632522583, 27.76250696182251] in episode 2807
Report: 
rewardSum:247.25
loss:[31.4602632522583, 27.76250696182251]
policies:[0, 4, 0]
qAverage:[0.0, 119.5505584716797]
ws:[3.153577208518982, 8.05909572839737]
memory len:10000
memory used:3554.0
############# STATE ###############
0*		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.022304308972110477, the reward is 247.25 with loss [21.17741107940674, 23.995617866516113] in episode 2808
Report: 
rewardSum:247.25
loss:[21.17741107940674, 23.995617866516113]
policies:[0, 4, 0]
qAverage:[0.0, 116.50379791259766]
ws:[4.725060307979584, 8.819599485397339]
memory len:10000
memory used:3555.0
now epsilon is 0.0222820130258603, the reward is 247.25 with loss [20.486615657806396, 27.954773426055908] in episode 2809
Report: 
rewardSum:247.25
loss:[20.486615657806396, 27.954773426055908]
policies:[0, 4, 0]
qAverage:[0.0, 117.37760009765626]
ws:[5.173994386196137, 8.928044176101684]
memory len:10000
memory used:3555.0
now epsilon is 0.022259739367196787, the reward is 247.25 with loss [26.634672164916992, 27.98762607574463] in episode 2810
Report: 
rewardSum:247.25
loss:[26.634672164916992, 27.98762607574463]
policies:[0, 4, 0]
qAverage:[0.0, 117.87704010009766]
ws:[5.000738823413849, 8.781715440750123]
memory len:10000
memory used:3555.0
now epsilon is 0.02223748797384071, the reward is 247.25 with loss [23.91797685623169, 20.16151237487793] in episode 2811
Report: 
rewardSum:247.25
loss:[23.91797685623169, 20.16151237487793]
policies:[0, 4, 0]
qAverage:[0.0, 119.11334381103515]
ws:[4.4143914103508, 8.669585800170898]
memory len:10000
memory used:3555.0
now epsilon is 0.022215258823535106, the reward is 247.25 with loss [22.67751169204712, 37.89681339263916] in episode 2812
Report: 
rewardSum:247.25
loss:[22.67751169204712, 37.89681339263916]
policies:[0, 4, 0]
qAverage:[0.0, 119.0566650390625]
ws:[4.068881192803383, 8.512694835662842]
memory len:10000
memory used:3556.0
now epsilon is 0.022193051894045267, the reward is 247.25 with loss [23.14653253555298, 25.930260181427002] in episode 2813
Report: 
rewardSum:247.25
loss:[23.14653253555298, 25.930260181427002]
policies:[0, 4, 0]
qAverage:[0.0, 119.39446716308593]
ws:[3.70140410810709, 7.540652942657471]
memory len:10000
memory used:3556.0
now epsilon is 0.022170867163158702, the reward is 247.25 with loss [33.96666955947876, 29.141864776611328] in episode 2814
Report: 
rewardSum:247.25
loss:[33.96666955947876, 29.141864776611328]
policies:[0, 4, 0]
qAverage:[0.0, 119.11498413085937]
ws:[3.5783278226852415, 7.58222222328186]
memory len:10000
memory used:3556.0
now epsilon is 0.02214870460868514, the reward is 247.25 with loss [23.6050044298172, 31.0277841091156] in episode 2815
Report: 
rewardSum:247.25
loss:[23.6050044298172, 31.0277841091156]
policies:[0, 4, 0]
qAverage:[0.0, 119.10930786132812]
ws:[6.071210666000843, 9.873242616653442]
memory len:10000
memory used:3556.0
now epsilon is 0.02212656420845648, the reward is 247.25 with loss [29.855363845825195, 34.12034034729004] in episode 2816
Report: 
rewardSum:247.25
loss:[29.855363845825195, 34.12034034729004]
policies:[0, 4, 0]
qAverage:[0.0, 120.24856567382812]
ws:[9.265782535076141, 13.427436113357544]
memory len:10000
memory used:3557.0
now epsilon is 0.022104445940326775, the reward is 247.25 with loss [36.23061227798462, 18.929860830307007] in episode 2817
Report: 
rewardSum:247.25
loss:[36.23061227798462, 18.929860830307007]
policies:[0, 4, 0]
qAverage:[0.0, 118.40736846923828]
ws:[9.040106105804444, 12.284845542907714]
memory len:10000
memory used:3555.0
now epsilon is 0.022082349782172238, the reward is 247.25 with loss [28.082033157348633, 23.628303050994873] in episode 2818
Report: 
rewardSum:247.25
loss:[28.082033157348633, 23.628303050994873]
policies:[1, 3, 0]
qAverage:[0.0, 101.25677490234375]
ws:[2.034504771232605, 6.320994853973389]
memory len:10000
memory used:3554.0
now epsilon is 0.02206027571189118, the reward is 247.25 with loss [31.390228271484375, 43.39077186584473] in episode 2819
Report: 
rewardSum:247.25
loss:[31.390228271484375, 43.39077186584473]
policies:[0, 4, 0]
qAverage:[0.0, 119.79653472900391]
ws:[6.353163206577301, 11.157381057739258]
memory len:10000
memory used:3555.0
now epsilon is 0.022038223707404003, the reward is 247.25 with loss [34.60534954071045, 26.988200545310974] in episode 2820
Report: 
rewardSum:247.25
loss:[34.60534954071045, 26.988200545310974]
policies:[0, 4, 0]
qAverage:[0.0, 105.88363647460938]
ws:[0.20647487044334412, 5.514485657215118]
memory len:10000
memory used:3554.0
now epsilon is 0.022016193746653187, the reward is 247.25 with loss [21.60814666748047, 32.73706388473511] in episode 2821
Report: 
rewardSum:247.25
loss:[21.60814666748047, 32.73706388473511]
policies:[0, 4, 0]
qAverage:[0.0, 121.14344024658203]
ws:[8.780923318862914, 13.493983745574951]
memory len:10000
memory used:3554.0
now epsilon is 0.02199418580760326, the reward is 247.25 with loss [22.42739486694336, 28.061598777770996] in episode 2822
Report: 
rewardSum:247.25
loss:[22.42739486694336, 28.061598777770996]
policies:[0, 4, 0]
qAverage:[0.0, 120.93667144775391]
ws:[9.731850194931031, 15.368421268463134]
memory len:10000
memory used:3556.0
now epsilon is 0.021972199868240786, the reward is 247.25 with loss [35.461979389190674, 22.41224479675293] in episode 2823
Report: 
rewardSum:247.25
loss:[35.461979389190674, 22.41224479675293]
policies:[0, 4, 0]
qAverage:[0.0, 120.55457763671875]
ws:[7.760111904144287, 13.995164012908935]
memory len:10000
memory used:3556.0
now epsilon is 0.02195023590657432, the reward is 247.25 with loss [32.788694858551025, 22.934153079986572] in episode 2824
Report: 
rewardSum:247.25
loss:[32.788694858551025, 22.934153079986572]
policies:[0, 4, 0]
qAverage:[0.0, 122.30364685058593]
ws:[8.235601258277892, 14.644446659088135]
memory len:10000
memory used:3558.0
now epsilon is 0.021928293900634405, the reward is 247.25 with loss [27.881975173950195, 20.721609234809875] in episode 2825
Report: 
rewardSum:247.25
loss:[27.881975173950195, 20.721609234809875]
policies:[0, 4, 0]
qAverage:[0.0, 121.9091781616211]
ws:[8.5380655169487, 15.552911472320556]
memory len:10000
memory used:3559.0
now epsilon is 0.021906373828473557, the reward is 247.25 with loss [25.20709991455078, 24.96904945373535] in episode 2826
Report: 
rewardSum:247.25
loss:[25.20709991455078, 24.96904945373535]
policies:[0, 4, 0]
qAverage:[0.0, 122.47471466064454]
ws:[7.952511489391327, 15.001068210601806]
memory len:10000
memory used:3558.0
now epsilon is 0.021884475668166207, the reward is 247.25 with loss [37.18719148635864, 24.445701122283936] in episode 2827
Report: 
rewardSum:247.25
loss:[37.18719148635864, 24.445701122283936]
policies:[0, 4, 0]
qAverage:[0.0, 120.05206604003907]
ws:[6.520530343055725, 14.234914207458496]
memory len:10000
memory used:3557.0
now epsilon is 0.021851669464522282, the reward is 245.25 with loss [48.654775619506836, 31.608585357666016] in episode 2828
Report: 
rewardSum:245.25
loss:[48.654775619506836, 31.608585357666016]
policies:[0, 5, 1]
qAverage:[0.0, 129.0583012898763]
ws:[6.359061290820439, 12.998610337575277]
memory len:10000
memory used:3557.0
now epsilon is 0.021829825988068166, the reward is 247.25 with loss [29.824910163879395, 20.32279920578003] in episode 2829
Report: 
rewardSum:247.25
loss:[29.824910163879395, 20.32279920578003]
policies:[0, 4, 0]
qAverage:[0.0, 121.8024169921875]
ws:[11.095805644989014, 18.13201847076416]
memory len:10000
memory used:3558.0
now epsilon is 0.021808004346900566, the reward is 247.25 with loss [33.77158260345459, 27.39723014831543] in episode 2830
Report: 
rewardSum:247.25
loss:[33.77158260345459, 27.39723014831543]
policies:[0, 4, 0]
qAverage:[0.0, 121.47881317138672]
ws:[9.185883140563964, 17.070313262939454]
memory len:10000
memory used:3556.0
now epsilon is 0.021786204519192384, the reward is 247.25 with loss [32.733871936798096, 25.510480880737305] in episode 2831
Report: 
rewardSum:247.25
loss:[32.733871936798096, 25.510480880737305]
policies:[0, 4, 0]
qAverage:[0.0, 125.37977142333985]
ws:[4.147000312805176, 12.237023067474365]
memory len:10000
memory used:3558.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22*		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.021764426483138335, the reward is 247.25 with loss [25.589704751968384, 24.455554485321045] in episode 2832
Report: 
rewardSum:247.25
loss:[25.589704751968384, 24.455554485321045]
policies:[0, 4, 0]
qAverage:[0.0, 123.278759765625]
ws:[2.963762807846069, 10.419040989130735]
memory len:10000
memory used:3559.0
now epsilon is 0.021742670216954942, the reward is 247.25 with loss [30.68201780319214, 34.52037811279297] in episode 2833
Report: 
rewardSum:247.25
loss:[30.68201780319214, 34.52037811279297]
policies:[0, 4, 0]
qAverage:[0.0, 127.21044311523437]
ws:[6.2458510279655455, 13.842948579788208]
memory len:10000
memory used:3560.0
now epsilon is 0.02172093569888049, the reward is 247.25 with loss [34.50078248977661, 27.115912437438965] in episode 2834
Report: 
rewardSum:247.25
loss:[34.50078248977661, 27.115912437438965]
policies:[0, 4, 0]
qAverage:[0.0, 125.24185943603516]
ws:[9.496035480499268, 15.769291114807128]
memory len:10000
memory used:3564.0
now epsilon is 0.021699222907175023, the reward is 247.25 with loss [27.879669427871704, 30.527647018432617] in episode 2835
Report: 
rewardSum:247.25
loss:[27.879669427871704, 30.527647018432617]
policies:[0, 4, 0]
qAverage:[0.0, 127.32269287109375]
ws:[7.929230499267578, 12.564341926574707]
memory len:10000
memory used:3565.0
now epsilon is 0.02167753182012032, the reward is 247.25 with loss [23.350158214569092, 27.383753299713135] in episode 2836
Report: 
rewardSum:247.25
loss:[23.350158214569092, 27.383753299713135]
policies:[0, 4, 0]
qAverage:[0.0, 126.18041076660157]
ws:[6.33444037437439, 11.361466312408448]
memory len:10000
memory used:3565.0
now epsilon is 0.021655862416019876, the reward is 247.25 with loss [33.80212640762329, 28.691871643066406] in episode 2837
Report: 
rewardSum:247.25
loss:[33.80212640762329, 28.691871643066406]
policies:[0, 4, 0]
qAverage:[0.0, 125.81008453369141]
ws:[6.15619341135025, 11.296176624298095]
memory len:10000
memory used:3566.0
now epsilon is 0.021634214673198857, the reward is 247.25 with loss [22.813026189804077, 39.80727195739746] in episode 2838
Report: 
rewardSum:247.25
loss:[22.813026189804077, 39.80727195739746]
policies:[0, 4, 0]
qAverage:[0.0, 126.27025909423828]
ws:[6.826451110839844, 12.15132246017456]
memory len:10000
memory used:3567.0
now epsilon is 0.02161258857000411, the reward is 247.25 with loss [35.6282901763916, 32.93759489059448] in episode 2839
Report: 
rewardSum:247.25
loss:[35.6282901763916, 32.93759489059448]
policies:[0, 4, 0]
qAverage:[0.0, 121.18575668334961]
ws:[9.608462691307068, 15.913644075393677]
memory len:10000
memory used:3568.0
now epsilon is 0.021590984084804116, the reward is 247.25 with loss [28.524654388427734, 25.9360671043396] in episode 2840
Report: 
rewardSum:247.25
loss:[28.524654388427734, 25.9360671043396]
policies:[0, 4, 0]
qAverage:[0.0, 123.59605560302734]
ws:[7.44375205039978, 13.219033813476562]
memory len:10000
memory used:3568.0
now epsilon is 0.021569401195988994, the reward is 247.25 with loss [28.371633529663086, 28.783348560333252] in episode 2841
Report: 
rewardSum:247.25
loss:[28.371633529663086, 28.783348560333252]
policies:[0, 4, 0]
qAverage:[0.0, 122.36670837402343]
ws:[5.581831133365631, 11.649642944335938]
memory len:10000
memory used:3567.0
now epsilon is 0.02154783988197045, the reward is 247.25 with loss [29.98432207107544, 33.81901931762695] in episode 2842
Report: 
rewardSum:247.25
loss:[29.98432207107544, 33.81901931762695]
policies:[0, 4, 0]
qAverage:[0.0, 121.91235961914063]
ws:[3.4734001994132995, 9.554720878601074]
memory len:10000
memory used:3566.0
now epsilon is 0.021526300121181785, the reward is 247.25 with loss [26.15526056289673, 33.87069272994995] in episode 2843
Report: 
rewardSum:247.25
loss:[26.15526056289673, 33.87069272994995]
policies:[0, 4, 0]
qAverage:[0.0, 122.16005249023438]
ws:[3.449833470582962, 9.142813396453857]
memory len:10000
memory used:3566.0
now epsilon is 0.021494030845180673, the reward is 245.25 with loss [54.35146617889404, 44.89553928375244] in episode 2844
Report: 
rewardSum:245.25
loss:[54.35146617889404, 44.89553928375244]
policies:[0, 5, 1]
qAverage:[0.0, 126.78062693277995]
ws:[4.756240924199422, 9.063785711924234]
memory len:10000
memory used:3565.0
now epsilon is 0.021472544873253768, the reward is 247.25 with loss [33.726468086242676, 30.43813133239746] in episode 2845
Report: 
rewardSum:247.25
loss:[33.726468086242676, 30.43813133239746]
policies:[0, 4, 0]
qAverage:[0.0, 124.79211730957032]
ws:[5.1802558422088625, 9.057800436019898]
memory len:10000
memory used:3568.0
now epsilon is 0.02145108037924289, the reward is 247.25 with loss [33.057491302490234, 31.878814220428467] in episode 2846
Report: 
rewardSum:247.25
loss:[33.057491302490234, 31.878814220428467]
policies:[0, 4, 0]
qAverage:[0.0, 119.5128387451172]
ws:[4.554665958881378, 8.423357152938843]
memory len:10000
memory used:3568.0
now epsilon is 0.021429637341678184, the reward is 247.25 with loss [25.82941484451294, 30.117216110229492] in episode 2847
Report: 
rewardSum:247.25
loss:[25.82941484451294, 30.117216110229492]
policies:[1, 3, 0]
qAverage:[0.0, 112.13449096679688]
ws:[4.791783492080867, 7.741918593645096]
memory len:10000
memory used:3571.0
now epsilon is 0.021408215739111242, the reward is 247.25 with loss [22.01937484741211, 27.16469669342041] in episode 2848
Report: 
rewardSum:247.25
loss:[22.01937484741211, 27.16469669342041]
policies:[1, 3, 0]
qAverage:[0.0, 113.39997863769531]
ws:[4.4034534096717834, 7.395415782928467]
memory len:10000
memory used:3573.0
now epsilon is 0.021386815550115106, the reward is 247.25 with loss [32.51752829551697, 33.69541358947754] in episode 2849
Report: 
rewardSum:247.25
loss:[32.51752829551697, 33.69541358947754]
policies:[0, 4, 0]
qAverage:[0.0, 112.29140090942383]
ws:[5.60535791516304, 8.545827239751816]
memory len:10000
memory used:3575.0
now epsilon is 0.021365436753284235, the reward is 247.25 with loss [30.6141095161438, 28.72957754135132] in episode 2850
Report: 
rewardSum:247.25
loss:[30.6141095161438, 28.72957754135132]
policies:[0, 4, 0]
qAverage:[0.0, 118.58387756347656]
ws:[5.250739711523056, 8.771895265579223]
memory len:10000
memory used:3575.0
now epsilon is 0.021333408621575824, the reward is 245.25 with loss [37.07559108734131, 37.660425662994385] in episode 2851
Report: 
rewardSum:245.25
loss:[37.07559108734131, 37.660425662994385]
policies:[0, 6, 0]
qAverage:[0.0, 129.97565133231026]
ws:[7.1577896518366675, 11.282875197274345]
memory len:10000
memory used:3576.0
now epsilon is 0.021312083211649225, the reward is 247.25 with loss [34.357627868652344, 31.816277503967285] in episode 2852
Report: 
rewardSum:247.25
loss:[34.357627868652344, 31.816277503967285]
policies:[0, 4, 0]
qAverage:[0.0, 122.59362487792968]
ws:[5.215168839693069, 9.751829528808594]
memory len:10000
memory used:3576.0
now epsilon is 0.021290779119136863, the reward is 247.25 with loss [31.324486255645752, 33.37865734100342] in episode 2853
Report: 
rewardSum:247.25
loss:[31.324486255645752, 33.37865734100342]
policies:[0, 4, 0]
qAverage:[0.0, 118.41356506347657]
ws:[4.986262594163418, 9.905580186843872]
memory len:10000
memory used:3576.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26*		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.021237612011752546, the reward is 241.25 with loss [83.52420330047607, 82.2108964920044] in episode 2854
Report: 
rewardSum:241.25
loss:[83.52420330047607, 82.2108964920044]
policies:[0, 10, 0]
qAverage:[0.0, 132.6455841064453]
ws:[9.205673119425773, 14.368797874450683]
memory len:10000
memory used:3575.0
############# STATE ###############
0*		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.02120047405348633, the reward is 244.25 with loss [61.68155908584595, 49.683456897735596] in episode 2855
Report: 
rewardSum:244.25
loss:[61.68155908584595, 49.683456897735596]
policies:[0, 7, 0]
qAverage:[0.0, 126.43696975708008]
ws:[4.295433569699526, 10.748149544000626]
memory len:10000
memory used:3576.0
now epsilon is 0.021110552017730412, the reward is 234.25 with loss [132.14141941070557, 113.92140054702759] in episode 2856
Report: 
rewardSum:234.25
loss:[132.14141941070557, 113.92140054702759]
policies:[0, 17, 0]
qAverage:[0.0, 134.01757992015166]
ws:[5.395214475253049, 8.085043170872856]
memory len:10000
memory used:3577.0
now epsilon is 0.02081182281004867, the reward is 194.25 with loss [404.8507492542267, 408.59437918663025] in episode 2857
Report: 
rewardSum:194.25
loss:[404.8507492542267, 408.59437918663025]
policies:[8, 47, 2]
qAverage:[17.705851329456674, 115.85459899902344]
ws:[3.818851965259422, 6.616806780208241]
memory len:10000
memory used:3576.0
now epsilon is 0.020733914904250042, the reward is 236.25 with loss [107.0519585609436, 100.24540781974792] in episode 2858
Report: 
rewardSum:236.25
loss:[107.0519585609436, 100.24540781974792]
policies:[0, 14, 1]
qAverage:[0.0, 121.31917521158854]
ws:[1.651046331723531, 6.763786999384562]
memory len:10000
memory used:3575.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22*		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.02060471595290136, the reward is 226.25 with loss [178.70883417129517, 152.45344758033752] in episode 2859
Report: 
rewardSum:226.25
loss:[178.70883417129517, 152.45344758033752]
policies:[0, 25, 0]
qAverage:[0.0, 115.90338457547702]
ws:[0.9926994485923877, 5.415594907907339]
memory len:10000
memory used:3575.0
now epsilon is 0.0205121915012456, the reward is 233.25 with loss [145.97687029838562, 112.86126351356506] in episode 2860
Report: 
rewardSum:233.25
loss:[145.97687029838562, 112.86126351356506]
policies:[0, 17, 1]
qAverage:[0.0, 114.34276453653972]
ws:[-0.2809728220001691, 3.697094308005439]
memory len:10000
memory used:3576.0
now epsilon is 0.02043029639734095, the reward is 235.25 with loss [115.99255228042603, 105.09523630142212] in episode 2861
Report: 
rewardSum:235.25
loss:[115.99255228042603, 105.09523630142212]
policies:[1, 15, 0]
qAverage:[0.0, 110.20073413848877]
ws:[2.035535831004381, 4.780147820711136]
memory len:10000
memory used:3577.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22*		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.02017647564850095, the reward is 201.25 with loss [328.8535461425781, 334.1732417345047] in episode 2862
Report: 
rewardSum:201.25
loss:[328.8535461425781, 334.1732417345047]
policies:[1, 48, 1]
qAverage:[0.0, 108.00834017383809]
ws:[0.6658656949899635, 3.8899039427692794]
memory len:10000
memory used:3577.0
now epsilon is 0.02008587418862203, the reward is 233.25 with loss [112.94775772094727, 119.02590370178223] in episode 2863
Report: 
rewardSum:233.25
loss:[112.94775772094727, 119.02590370178223]
policies:[1, 17, 0]
qAverage:[0.0, 95.31328201293945]
ws:[-0.39388280888670124, 1.9970296379178762]
memory len:10000
memory used:3579.0
now epsilon is 0.019985682980035127, the reward is 231.25 with loss [115.30925583839417, 130.63839530944824] in episode 2864
Report: 
rewardSum:231.25
loss:[115.30925583839417, 130.63839530944824]
policies:[0, 20, 0]
qAverage:[0.0, 96.0866455804734]
ws:[-0.21933536852399507, 2.199741115172704]
memory len:10000
memory used:3578.0
now epsilon is 0.019876049786608032, the reward is 229.25 with loss [161.81860399246216, 143.93760323524475] in episode 2865
Report: 
rewardSum:229.25
loss:[161.81860399246216, 143.93760323524475]
policies:[0, 22, 0]
qAverage:[0.0, 88.59201604669744]
ws:[-1.5218943086537449, 1.0245577784424478]
memory len:10000
memory used:3579.0
now epsilon is 0.01983633245273584, the reward is 243.25 with loss [48.174185276031494, 52.23648977279663] in episode 2866
Report: 
rewardSum:243.25
loss:[48.174185276031494, 52.23648977279663]
policies:[0, 7, 1]
qAverage:[0.0, 81.08690166473389]
ws:[-1.094722400419414, 1.7125183045864105]
memory len:10000
memory used:3578.0
now epsilon is 0.019806596544420723, the reward is 245.25 with loss [41.063632011413574, 38.70157742500305] in episode 2867
Report: 
rewardSum:245.25
loss:[41.063632011413574, 38.70157742500305]
policies:[0, 6, 0]
qAverage:[0.0, 79.76532200404576]
ws:[-1.0920633801392146, 2.289125829935074]
memory len:10000
memory used:3579.0
now epsilon is 0.019776905212099957, the reward is 245.25 with loss [33.92888045310974, 35.90742015838623] in episode 2868
Report: 
rewardSum:245.25
loss:[33.92888045310974, 35.90742015838623]
policies:[0, 6, 0]
qAverage:[0.0, 80.16915021623883]
ws:[-0.8587226420640945, 2.766721765909876]
memory len:10000
memory used:3577.0
now epsilon is 0.019747258388951323, the reward is 245.25 with loss [42.04061031341553, 35.46770787239075] in episode 2869
Report: 
rewardSum:245.25
loss:[42.04061031341553, 35.46770787239075]
policies:[0, 6, 0]
qAverage:[0.0, 80.27867017473493]
ws:[-1.4075738191604614, 2.3292033842631747]
memory len:10000
memory used:3576.0
now epsilon is 0.019717656008252774, the reward is 245.25 with loss [43.08645296096802, 42.01017951965332] in episode 2870
Report: 
rewardSum:245.25
loss:[43.08645296096802, 42.01017951965332]
policies:[0, 6, 0]
qAverage:[0.0, 79.91793060302734]
ws:[-1.691550804036004, 1.9666882412774223]
memory len:10000
memory used:3575.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24*		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.019688098003382296, the reward is 245.25 with loss [39.045350074768066, 49.79012966156006] in episode 2871
Report: 
rewardSum:245.25
loss:[39.045350074768066, 49.79012966156006]
policies:[0, 6, 0]
qAverage:[0.0, 72.0617425101144]
ws:[-1.8294948211738042, 1.8654175315584456]
memory len:10000
memory used:3575.0
now epsilon is 0.019619301524528752, the reward is 237.25 with loss [86.86928367614746, 88.7607901096344] in episode 2872
Report: 
rewardSum:237.25
loss:[86.86928367614746, 88.7607901096344]
policies:[0, 14, 0]
qAverage:[0.0, 78.79608968098958]
ws:[-1.2111800171434879, 3.715298302968343]
memory len:10000
memory used:3578.0
now epsilon is 0.01958989095920726, the reward is 245.25 with loss [32.4031765460968, 33.55805206298828] in episode 2873
Report: 
rewardSum:245.25
loss:[32.4031765460968, 33.55805206298828]
policies:[0, 6, 0]
qAverage:[0.0, 73.76880754743304]
ws:[-1.9079182531152452, 2.5116873809269498]
memory len:10000
memory used:3578.0
now epsilon is 0.019540971291662587, the reward is 241.25 with loss [71.55899953842163, 55.128143072128296] in episode 2874
Report: 
rewardSum:241.25
loss:[71.55899953842163, 55.128143072128296]
policies:[0, 10, 0]
qAverage:[0.0, 77.764348116788]
ws:[-1.7012083442373709, 2.219605976885015]
memory len:10000
memory used:3578.0
now epsilon is 0.01950192352868602, the reward is 243.25 with loss [45.511377573013306, 58.72348165512085] in episode 2875
Report: 
rewardSum:243.25
loss:[45.511377573013306, 58.72348165512085]
policies:[0, 8, 0]
qAverage:[0.0, 76.18737284342448]
ws:[-2.7111055188708835, 0.7962705261177487]
memory len:10000
memory used:3590.0
now epsilon is 0.019462953792935975, the reward is 243.25 with loss [62.7262978553772, 63.16088533401489] in episode 2876
Report: 
rewardSum:243.25
loss:[62.7262978553772, 63.16088533401489]
policies:[0, 7, 1]
qAverage:[0.0, 73.58417892456055]
ws:[-3.216506689786911, 0.8733716793358326]
memory len:10000
memory used:3584.0
now epsilon is 0.019414351111534108, the reward is 241.25 with loss [76.28921842575073, 63.42067813873291] in episode 2877
Report: 
rewardSum:241.25
loss:[76.28921842575073, 63.42067813873291]
policies:[0, 10, 0]
qAverage:[0.0, 72.06052190607244]
ws:[-2.5677932013164866, 0.4640226526693864]
memory len:10000
memory used:3583.0
now epsilon is 0.019385247779755126, the reward is 245.25 with loss [33.923582553863525, 41.00265693664551] in episode 2878
Report: 
rewardSum:245.25
loss:[33.923582553863525, 41.00265693664551]
policies:[0, 6, 0]
qAverage:[0.0, 67.98133741106305]
ws:[-2.897840065615518, 0.9097614714077541]
memory len:10000
memory used:3582.0
now epsilon is 0.01936586980023179, the reward is 247.25 with loss [23.477417469024658, 25.011303901672363] in episode 2879
Report: 
rewardSum:247.25
loss:[23.477417469024658, 25.011303901672363]
policies:[0, 3, 1]
qAverage:[0.0, 58.3531379699707]
ws:[-0.9759715124964714, 1.8357081413269043]
memory len:10000
memory used:3582.0
now epsilon is 0.019336839144983686, the reward is 245.25 with loss [41.15745687484741, 46.44456911087036] in episode 2880
Report: 
rewardSum:245.25
loss:[41.15745687484741, 46.44456911087036]
policies:[0, 6, 0]
qAverage:[0.0, 68.34952000209263]
ws:[-1.933511963912419, 2.030097586768014]
memory len:10000
memory used:3583.0
now epsilon is 0.01930785200851128, the reward is 245.25 with loss [39.53486108779907, 28.725613355636597] in episode 2881
Report: 
rewardSum:245.25
loss:[39.53486108779907, 28.725613355636597]
policies:[0, 6, 0]
qAverage:[0.0, 67.94068472726005]
ws:[-2.541131240980966, 1.4481703894478934]
memory len:10000
memory used:3584.0
now epsilon is 0.019288551395740608, the reward is 247.25 with loss [29.179178953170776, 23.377418041229248] in episode 2882
Report: 
rewardSum:247.25
loss:[29.179178953170776, 23.377418041229248]
policies:[0, 4, 0]
qAverage:[0.0, 62.980055236816405]
ws:[-2.327157127857208, 1.9348008155822753]
memory len:10000
memory used:3583.0
now epsilon is 0.019259636645637394, the reward is 245.25 with loss [30.743915677070618, 42.35503625869751] in episode 2883
Report: 
rewardSum:245.25
loss:[30.743915677070618, 42.35503625869751]
policies:[0, 6, 0]
qAverage:[0.0, 68.20057896205357]
ws:[-3.5206357496125356, 0.17900273842470987]
memory len:10000
memory used:3583.0
now epsilon is 0.019230765240560783, the reward is 245.25 with loss [27.83253288269043, 35.6890013217926] in episode 2884
Report: 
rewardSum:245.25
loss:[27.83253288269043, 35.6890013217926]
policies:[0, 6, 0]
qAverage:[0.0, 66.95830535888672]
ws:[-2.6594991854258945, -0.5136889017054013]
memory len:10000
memory used:3584.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42*		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.01920193711553387, the reward is 245.25 with loss [37.54815912246704, 42.158894300460815] in episode 2885
Report: 
rewardSum:245.25
loss:[37.54815912246704, 42.158894300460815]
policies:[0, 6, 0]
qAverage:[0.0, 61.21546827043806]
ws:[-2.256480093513216, -0.26648521423339844]
memory len:10000
memory used:3585.0
now epsilon is 0.019173152205677137, the reward is 245.25 with loss [34.1767098903656, 37.79133367538452] in episode 2886
Report: 
rewardSum:245.25
loss:[34.1767098903656, 37.79133367538452]
policies:[0, 6, 0]
qAverage:[0.0, 63.59567369733538]
ws:[-2.9175289422273636, -0.08670135268143245]
memory len:10000
memory used:3587.0
now epsilon is 0.01914441044620833, the reward is 245.25 with loss [37.204749584198, 40.402618408203125] in episode 2887
Report: 
rewardSum:245.25
loss:[37.204749584198, 40.402618408203125]
policies:[0, 6, 0]
qAverage:[0.0, 62.45499965122768]
ws:[-2.8019667310374126, -0.4322908190744264]
memory len:10000
memory used:3586.0
now epsilon is 0.01911571177244231, the reward is 245.25 with loss [41.04901170730591, 33.81413555145264] in episode 2888
Report: 
rewardSum:245.25
loss:[41.04901170730591, 33.81413555145264]
policies:[0, 6, 0]
qAverage:[0.0, 62.311375209263396]
ws:[-3.393747636250087, 0.12343429667609078]
memory len:10000
memory used:3585.0
now epsilon is 0.01908705611979089, the reward is 245.25 with loss [34.58293867111206, 34.26785659790039] in episode 2889
Report: 
rewardSum:245.25
loss:[34.58293867111206, 34.26785659790039]
policies:[0, 6, 0]
qAverage:[0.0, 62.520968845912385]
ws:[-3.5034720046179637, 0.2184953306402479]
memory len:10000
memory used:3587.0
now epsilon is 0.01905844342376273, the reward is 245.25 with loss [36.86814093589783, 32.922059297561646] in episode 2890
Report: 
rewardSum:245.25
loss:[36.86814093589783, 32.922059297561646]
policies:[0, 6, 0]
qAverage:[0.0, 62.51912362234933]
ws:[-2.9438099350248064, 0.8815620648009437]
memory len:10000
memory used:3586.0
now epsilon is 0.019029873619963153, the reward is 245.25 with loss [36.908472537994385, 36.21969532966614] in episode 2891
Report: 
rewardSum:245.25
loss:[36.908472537994385, 36.21969532966614]
policies:[0, 6, 0]
qAverage:[0.0, 62.85759844098772]
ws:[-1.9607852867671423, 0.8346234469541481]
memory len:10000
memory used:3587.0
now epsilon is 0.018982352421767403, the reward is 241.25 with loss [73.93755030632019, 67.15362215042114] in episode 2892
Report: 
rewardSum:241.25
loss:[73.93755030632019, 67.15362215042114]
policies:[0, 10, 0]
qAverage:[0.0, 63.79056618430398]
ws:[-2.051997192881324, 2.3188899010419846]
memory len:10000
memory used:3596.0
now epsilon is 0.018930216155529585, the reward is 240.25 with loss [64.02726745605469, 61.57685422897339] in episode 2893
Report: 
rewardSum:240.25
loss:[64.02726745605469, 61.57685422897339]
policies:[0, 11, 0]
qAverage:[0.0, 60.02541255950928]
ws:[-2.2303003817796707, 2.492751012245814]
memory len:10000
memory used:3639.0
now epsilon is 0.018901838572459355, the reward is 245.25 with loss [36.8144006729126, 34.000789165496826] in episode 2894
Report: 
rewardSum:245.25
loss:[36.8144006729126, 34.000789165496826]
policies:[0, 6, 0]
qAverage:[0.0, 57.83336475917271]
ws:[-3.863718560763768, 0.7282521277666092]
memory len:10000
memory used:3663.0
now epsilon is 0.01887350352916861, the reward is 245.25 with loss [41.0809623003006, 34.9620304107666] in episode 2895
Report: 
rewardSum:245.25
loss:[41.0809623003006, 34.9620304107666]
policies:[0, 5, 1]
qAverage:[0.0, 54.8293571472168]
ws:[-3.3343667089939117, 1.613414078950882]
memory len:10000
memory used:3682.0
now epsilon is 0.01885463710202375, the reward is 247.25 with loss [19.957228660583496, 32.443602085113525] in episode 2896
Report: 
rewardSum:247.25
loss:[19.957228660583496, 32.443602085113525]
policies:[0, 4, 0]
qAverage:[0.0, 52.70382308959961]
ws:[-2.416290229558945, 2.185566318035126]
memory len:10000
memory used:3607.0
now epsilon is 0.018835789534232303, the reward is 247.25 with loss [27.37181329727173, 21.91934323310852] in episode 2897
Report: 
rewardSum:247.25
loss:[27.37181329727173, 21.91934323310852]
policies:[0, 4, 0]
qAverage:[0.0, 52.425682830810544]
ws:[-2.683026900142431, 2.0631579399108886]
memory len:10000
memory used:3608.0
now epsilon is 0.018807553502598568, the reward is 245.25 with loss [39.41485047340393, 38.45096826553345] in episode 2898
Report: 
rewardSum:245.25
loss:[39.41485047340393, 38.45096826553345]
policies:[0, 5, 1]
qAverage:[0.0, 54.407798767089844]
ws:[-3.244427338242531, 0.3286724140246709]
memory len:10000
memory used:3608.0
now epsilon is 0.018788753000753135, the reward is 247.25 with loss [18.968003511428833, 23.769811630249023] in episode 2899
Report: 
rewardSum:247.25
loss:[18.968003511428833, 23.769811630249023]
policies:[0, 4, 0]
qAverage:[0.0, 52.74906845092774]
ws:[-3.590297245979309, -0.4490718849003315]
memory len:10000
memory used:3608.0
now epsilon is 0.018769971292360535, the reward is 247.25 with loss [28.194764614105225, 34.495606422424316] in episode 2900
Report: 
rewardSum:247.25
loss:[28.194764614105225, 34.495606422424316]
policies:[0, 4, 0]
qAverage:[0.0, 52.745008087158205]
ws:[-3.6724893927574156, -0.8218324661254883]
memory len:10000
memory used:3610.0
now epsilon is 0.018751208358634364, the reward is 247.25 with loss [26.457706451416016, 23.97795867919922] in episode 2901
Report: 
rewardSum:247.25
loss:[26.457706451416016, 23.97795867919922]
policies:[0, 4, 0]
qAverage:[0.0, 51.49724807739258]
ws:[-3.557692122459412, -0.39836333990097045]
memory len:10000
memory used:3611.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22*		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.018732464180806988, the reward is 247.25 with loss [21.640331029891968, 24.746116876602173] in episode 2902
Report: 
rewardSum:247.25
loss:[21.640331029891968, 24.746116876602173]
policies:[0, 4, 0]
qAverage:[0.0, 48.10754165649414]
ws:[-2.978667149692774, -0.3616562485694885]
memory len:10000
memory used:3611.0
now epsilon is 0.01871373874012955, the reward is 247.25 with loss [28.816471576690674, 23.379883766174316] in episode 2903
Report: 
rewardSum:247.25
loss:[28.816471576690674, 23.379883766174316]
policies:[0, 4, 0]
qAverage:[0.0, 50.65121154785156]
ws:[-3.3123434126377105, -0.6280744671821594]
memory len:10000
memory used:3611.0
now epsilon is 0.01869503201787191, the reward is 247.25 with loss [27.16116428375244, 24.48556637763977] in episode 2904
Report: 
rewardSum:247.25
loss:[27.16116428375244, 24.48556637763977]
policies:[0, 4, 0]
qAverage:[0.0, 50.082299041748044]
ws:[-3.750046420097351, -0.9604804083704949]
memory len:10000
memory used:3611.0
now epsilon is 0.018676343995322676, the reward is 247.25 with loss [25.574077606201172, 24.007914066314697] in episode 2905
Report: 
rewardSum:247.25
loss:[25.574077606201172, 24.007914066314697]
policies:[0, 4, 0]
qAverage:[0.0, 50.34973983764648]
ws:[-3.5101938366889955, -0.7237713426351547]
memory len:10000
memory used:3609.0
now epsilon is 0.018657674653789154, the reward is 247.25 with loss [18.794943809509277, 21.00847029685974] in episode 2906
Report: 
rewardSum:247.25
loss:[18.794943809509277, 21.00847029685974]
policies:[1, 3, 0]
qAverage:[12.405536651611328, 39.01921463012695]
ws:[-3.7441979736089706, -0.8407198041677475]
memory len:10000
memory used:3609.0
now epsilon is 0.01863902397459733, the reward is 247.25 with loss [23.362473964691162, 29.191028594970703] in episode 2907
Report: 
rewardSum:247.25
loss:[23.362473964691162, 29.191028594970703]
policies:[1, 3, 0]
qAverage:[12.028706359863282, 38.85415267944336]
ws:[-3.8620313167572022, -0.6188323855400085]
memory len:10000
memory used:3608.0
now epsilon is 0.018620391939091856, the reward is 247.25 with loss [27.768661499023438, 21.669700145721436] in episode 2908
Report: 
rewardSum:247.25
loss:[27.768661499023438, 21.669700145721436]
policies:[1, 3, 0]
qAverage:[12.342837524414062, 38.78913879394531]
ws:[-3.508514746278524, 0.20711702704429627]
memory len:10000
memory used:3609.0
now epsilon is 0.018601778528636044, the reward is 247.25 with loss [26.250237464904785, 18.43738079071045] in episode 2909
Report: 
rewardSum:247.25
loss:[26.250237464904785, 18.43738079071045]
policies:[1, 3, 0]
qAverage:[12.182182312011719, 38.76023941040039]
ws:[-3.180657076835632, 0.8035345315933228]
memory len:10000
memory used:3608.0
now epsilon is 0.0185738932941985, the reward is 245.25 with loss [39.909773111343384, 41.47871160507202] in episode 2910
Report: 
rewardSum:245.25
loss:[39.909773111343384, 41.47871160507202]
policies:[1, 4, 1]
qAverage:[10.29074478149414, 41.79343477884928]
ws:[-3.574533681074778, 0.6767401297887167]
memory len:10000
memory used:3608.0
now epsilon is 0.018546049861478918, the reward is 245.25 with loss [28.0392107963562, 42.99304914474487] in episode 2911
Report: 
rewardSum:245.25
loss:[28.0392107963562, 42.99304914474487]
policies:[1, 4, 1]
qAverage:[0.0, 52.01967926025391]
ws:[-2.822859662771225, 1.0816577970981598]
memory len:10000
memory used:3607.0
now epsilon is 0.018527510765227087, the reward is 247.25 with loss [23.074538230895996, 26.091657161712646] in episode 2912
Report: 
rewardSum:247.25
loss:[23.074538230895996, 26.091657161712646]
policies:[0, 4, 0]
qAverage:[0.0, 49.93003616333008]
ws:[-3.352413547039032, 0.887893807888031]
memory len:10000
memory used:3608.0
now epsilon is 0.018499736862831826, the reward is 245.25 with loss [37.56056451797485, 36.96546792984009] in episode 2913
Report: 
rewardSum:245.25
loss:[37.56056451797485, 36.96546792984009]
policies:[2, 4, 0]
qAverage:[9.892727533976236, 38.403185526529946]
ws:[-2.502459662655989, -1.1157306830088298]
memory len:10000
memory used:3607.0
now epsilon is 0.018481244062214162, the reward is 247.25 with loss [19.152162313461304, 19.428447008132935] in episode 2914
Report: 
rewardSum:247.25
loss:[19.152162313461304, 19.428447008132935]
policies:[1, 3, 0]
qAverage:[11.215647125244141, 38.119503784179685]
ws:[-2.928783106803894, 0.02661636471748352]
memory len:10000
memory used:3607.0
now epsilon is 0.018462769747463467, the reward is 247.25 with loss [27.174646615982056, 22.359503746032715] in episode 2915
Report: 
rewardSum:247.25
loss:[27.174646615982056, 22.359503746032715]
policies:[1, 3, 0]
qAverage:[11.395560455322265, 36.61214599609375]
ws:[-2.3957546263933183, -0.5340230524539947]
memory len:10000
memory used:3607.0
now epsilon is 0.018439702821625787, the reward is 246.25 with loss [31.75960683822632, 25.392335653305054] in episode 2916
Report: 
rewardSum:246.25
loss:[31.75960683822632, 25.392335653305054]
policies:[0, 4, 1]
qAverage:[0.0, 48.92221603393555]
ws:[-4.066888380050659, -1.2015397757291795]
memory len:10000
memory used:3607.0
now epsilon is 0.018421270032540307, the reward is 247.25 with loss [21.167746543884277, 34.1909122467041] in episode 2917
Report: 
rewardSum:247.25
loss:[21.167746543884277, 34.1909122467041]
policies:[1, 3, 0]
qAverage:[11.284651947021484, 36.952885437011716]
ws:[-3.339649234712124, -0.5308694392442703]
memory len:10000
memory used:3608.0
now epsilon is 0.018402855669332772, the reward is 247.25 with loss [25.065301656723022, 28.470515727996826] in episode 2918
Report: 
rewardSum:247.25
loss:[25.065301656723022, 28.470515727996826]
policies:[1, 3, 0]
qAverage:[11.163397216796875, 37.61065521240234]
ws:[-2.3342411875724793, 0.07995535135269165]
memory len:10000
memory used:3608.0
now epsilon is 0.01838445971358421, the reward is 247.25 with loss [23.510250091552734, 17.19407820701599] in episode 2919
Report: 
rewardSum:247.25
loss:[23.510250091552734, 17.19407820701599]
policies:[1, 3, 0]
qAverage:[11.316606903076172, 36.99415512084961]
ws:[-1.356430673599243, 0.9799767971038819]
memory len:10000
memory used:3608.0
now epsilon is 0.018366082146894065, the reward is 247.25 with loss [33.47012138366699, 17.45445156097412] in episode 2920
Report: 
rewardSum:247.25
loss:[33.47012138366699, 17.45445156097412]
policies:[1, 3, 0]
qAverage:[13.888188362121582, 29.98008632659912]
ws:[1.5063721630722284, 2.5510583221912384]
memory len:10000
memory used:3608.0
now epsilon is 0.01834772295088017, the reward is 247.25 with loss [20.323015213012695, 18.500804901123047] in episode 2921
Report: 
rewardSum:247.25
loss:[20.323015213012695, 18.500804901123047]
policies:[1, 3, 0]
qAverage:[11.331437683105468, 36.681237030029294]
ws:[-2.4152996301651, 1.0827524185180664]
memory len:10000
memory used:3607.0
now epsilon is 0.018329382107178733, the reward is 247.25 with loss [26.71689486503601, 23.187634229660034] in episode 2922
Report: 
rewardSum:247.25
loss:[26.71689486503601, 23.187634229660034]
policies:[0, 4, 0]
qAverage:[0.0, 47.63264770507813]
ws:[-2.5635859936475756, 0.9147162556648254]
memory len:10000
memory used:3607.0
now epsilon is 0.01831105959744433, the reward is 247.25 with loss [27.54443645477295, 20.641425371170044] in episode 2923
Report: 
rewardSum:247.25
loss:[27.54443645477295, 20.641425371170044]
policies:[0, 4, 0]
qAverage:[0.0, 47.742496490478516]
ws:[-2.6589508309960364, 1.003834068775177]
memory len:10000
memory used:3607.0
now epsilon is 0.018292755403349868, the reward is 247.25 with loss [22.658910989761353, 30.641993522644043] in episode 2924
Report: 
rewardSum:247.25
loss:[22.658910989761353, 30.641993522644043]
policies:[0, 4, 0]
qAverage:[0.0, 46.591311645507815]
ws:[-3.046078422665596, 0.46896026134490965]
memory len:10000
memory used:3608.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24*		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.01827446950658657, the reward is 247.25 with loss [22.077332019805908, 28.224199056625366] in episode 2925
Report: 
rewardSum:247.25
loss:[22.077332019805908, 28.224199056625366]
policies:[0, 4, 0]
qAverage:[0.0, 43.71167221069336]
ws:[-3.2810548812150957, 0.19789026379585267]
memory len:10000
memory used:3607.0
now epsilon is 0.018256201888863968, the reward is 247.25 with loss [19.78316354751587, 25.686180591583252] in episode 2926
Report: 
rewardSum:247.25
loss:[19.78316354751587, 25.686180591583252]
policies:[0, 4, 0]
qAverage:[0.0, 45.12927322387695]
ws:[-2.8316278725862505, 0.763476037979126]
memory len:10000
memory used:3609.0
now epsilon is 0.018237952531909872, the reward is 247.25 with loss [24.485080242156982, 21.92311382293701] in episode 2927
Report: 
rewardSum:247.25
loss:[24.485080242156982, 21.92311382293701]
policies:[0, 4, 0]
qAverage:[0.0, 45.95555648803711]
ws:[-2.5121690928936005, 1.2249480485916138]
memory len:10000
memory used:3609.0
now epsilon is 0.018219721417470365, the reward is 247.25 with loss [17.047008275985718, 31.61707377433777] in episode 2928
Report: 
rewardSum:247.25
loss:[17.047008275985718, 31.61707377433777]
policies:[0, 4, 0]
qAverage:[0.0, 40.687350273132324]
ws:[0.5908943638205528, 2.6419762074947357]
memory len:10000
memory used:3610.0
now epsilon is 0.018201508527309763, the reward is 247.25 with loss [19.869138479232788, 27.319223880767822] in episode 2929
Report: 
rewardSum:247.25
loss:[19.869138479232788, 27.319223880767822]
policies:[0, 3, 1]
qAverage:[0.0, 41.56498146057129]
ws:[0.7554636532440782, 2.773115396499634]
memory len:10000
memory used:3610.0
now epsilon is 0.018183313843210626, the reward is 247.25 with loss [25.994176864624023, 32.13279438018799] in episode 2930
Report: 
rewardSum:247.25
loss:[25.994176864624023, 32.13279438018799]
policies:[0, 4, 0]
qAverage:[0.0, 45.515471649169925]
ws:[-2.491097445785999, 1.4500133037567138]
memory len:10000
memory used:3609.0
now epsilon is 0.018165137346973725, the reward is 247.25 with loss [21.504932165145874, 32.316237449645996] in episode 2931
Report: 
rewardSum:247.25
loss:[21.504932165145874, 32.316237449645996]
policies:[0, 4, 0]
qAverage:[0.0, 46.63010940551758]
ws:[-3.385103264451027, 0.9978883504867554]
memory len:10000
memory used:3611.0
now epsilon is 0.018146979020418008, the reward is 247.25 with loss [17.337032079696655, 27.82991313934326] in episode 2932
Report: 
rewardSum:247.25
loss:[17.337032079696655, 27.82991313934326]
policies:[0, 4, 0]
qAverage:[0.0, 45.45687713623047]
ws:[-3.412608563899994, 1.210467267036438]
memory len:10000
memory used:3612.0
now epsilon is 0.01811977555901035, the reward is 245.25 with loss [45.644104957580566, 38.18818736076355] in episode 2933
Report: 
rewardSum:245.25
loss:[45.644104957580566, 38.18818736076355]
policies:[0, 5, 1]
qAverage:[0.0, 47.90066719055176]
ws:[-3.1449959377447763, 0.1051118994752566]
memory len:10000
memory used:3611.0
now epsilon is 0.01810166257723476, the reward is 247.25 with loss [23.817652702331543, 22.964624404907227] in episode 2934
Report: 
rewardSum:247.25
loss:[23.817652702331543, 22.964624404907227]
policies:[0, 3, 1]
qAverage:[0.0, 42.73915672302246]
ws:[-5.435916513204575, -0.4042569100856781]
memory len:10000
memory used:3640.0
now epsilon is 0.018083567701649712, the reward is 247.25 with loss [28.368846893310547, 24.4457426071167] in episode 2935
Report: 
rewardSum:247.25
loss:[28.368846893310547, 24.4457426071167]
policies:[0, 4, 0]
qAverage:[0.0, 46.11943740844727]
ws:[-5.39670615196228, -0.46096024513244627]
memory len:10000
memory used:3655.0
now epsilon is 0.018056459297791903, the reward is 245.25 with loss [41.48619747161865, 34.58828830718994] in episode 2936
Report: 
rewardSum:245.25
loss:[41.48619747161865, 34.58828830718994]
policies:[1, 4, 1]
qAverage:[0.0, 44.0178726196289]
ws:[-2.617047905921936, 0.6715533018112183]
memory len:10000
memory used:3678.0
now epsilon is 0.01803840960853789, the reward is 247.25 with loss [19.70325517654419, 17.83799123764038] in episode 2937
Report: 
rewardSum:247.25
loss:[19.70325517654419, 17.83799123764038]
policies:[1, 3, 0]
qAverage:[9.4850830078125, 33.690818786621094]
ws:[-3.3955675840377806, 1.553511393070221]
memory len:10000
memory used:3691.0
now epsilon is 0.018020377962205625, the reward is 247.25 with loss [24.450406074523926, 38.00651788711548] in episode 2938
Report: 
rewardSum:247.25
loss:[24.450406074523926, 38.00651788711548]
policies:[1, 3, 0]
qAverage:[9.788949584960937, 34.813543701171874]
ws:[-3.8285951256752013, 1.6549357652664185]
memory len:10000
memory used:3706.0
now epsilon is 0.018002364340758956, the reward is 247.25 with loss [30.184536933898926, 23.68575656414032] in episode 2939
Report: 
rewardSum:247.25
loss:[30.184536933898926, 23.68575656414032]
policies:[0, 4, 0]
qAverage:[0.0, 43.6090591430664]
ws:[-4.799777323007584, 1.008368682861328]
memory len:10000
memory used:3721.0
now epsilon is 0.01798436872617975, the reward is 247.25 with loss [28.760562419891357, 19.06008744239807] in episode 2940
Report: 
rewardSum:247.25
loss:[28.760562419891357, 19.06008744239807]
policies:[0, 4, 0]
qAverage:[0.0, 43.555982208251955]
ws:[-4.76837671995163, 1.2690817356109618]
memory len:10000
memory used:3741.0
now epsilon is 0.017966391100467893, the reward is 247.25 with loss [18.828117847442627, 27.474939346313477] in episode 2941
Report: 
rewardSum:247.25
loss:[18.828117847442627, 27.474939346313477]
policies:[0, 4, 0]
qAverage:[0.0, 43.9312858581543]
ws:[-3.610088562965393, 2.9742835521698]
memory len:10000
memory used:3762.0
now epsilon is 0.01794843144564126, the reward is 247.25 with loss [20.568172931671143, 27.225651264190674] in episode 2942
Report: 
rewardSum:247.25
loss:[20.568172931671143, 27.225651264190674]
policies:[0, 4, 0]
qAverage:[0.0, 43.11624374389648]
ws:[-1.610164964199066, 4.451428413391113]
memory len:10000
memory used:3782.0
now epsilon is 0.017930489743735706, the reward is 247.25 with loss [27.399816036224365, 16.78950262069702] in episode 2943
Report: 
rewardSum:247.25
loss:[27.399816036224365, 16.78950262069702]
policies:[0, 4, 0]
qAverage:[0.0, 43.70088882446289]
ws:[-2.80146164894104, 3.3794273376464843]
memory len:10000
memory used:3807.0
now epsilon is 0.01791256597680504, the reward is 247.25 with loss [32.47526216506958, 19.915705680847168] in episode 2944
Report: 
rewardSum:247.25
loss:[32.47526216506958, 19.915705680847168]
policies:[0, 4, 0]
qAverage:[0.0, 43.16356201171875]
ws:[-4.469745917618274, 1.8474090456962586]
memory len:10000
memory used:3822.0
now epsilon is 0.01789466012692102, the reward is 247.25 with loss [25.4153470993042, 17.881799936294556] in episode 2945
Report: 
rewardSum:247.25
loss:[25.4153470993042, 17.881799936294556]
policies:[0, 4, 0]
qAverage:[0.0, 43.7671630859375]
ws:[-5.345377278327942, 0.22938755750656128]
memory len:10000
memory used:3823.0
now epsilon is 0.0178767721761733, the reward is 247.25 with loss [23.97787380218506, 24.336031436920166] in episode 2946
Report: 
rewardSum:247.25
loss:[23.97787380218506, 24.336031436920166]
policies:[0, 3, 1]
qAverage:[0.0, 41.837286949157715]
ws:[-4.913539871573448, 0.6501131057739258]
memory len:10000
memory used:3825.0
now epsilon is 0.017858902106669468, the reward is 247.25 with loss [21.336585760116577, 21.44468927383423] in episode 2947
Report: 
rewardSum:247.25
loss:[21.336585760116577, 21.44468927383423]
policies:[0, 4, 0]
qAverage:[0.0, 44.160743713378906]
ws:[-3.1614986300468444, 1.2885027647018432]
memory len:10000
memory used:3846.0
now epsilon is 0.017841049900534978, the reward is 247.25 with loss [31.151179313659668, 28.703721046447754] in episode 2948
Report: 
rewardSum:247.25
loss:[31.151179313659668, 28.703721046447754]
policies:[0, 4, 0]
qAverage:[0.0, 42.21753997802735]
ws:[-3.2071543097496034, 1.3912723064422607]
memory len:10000
memory used:3851.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24*		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.017823215539913163, the reward is 247.25 with loss [22.843900203704834, 31.027018547058105] in episode 2949
Report: 
rewardSum:247.25
loss:[22.843900203704834, 31.027018547058105]
policies:[0, 4, 0]
qAverage:[0.0, 39.59918441772461]
ws:[-3.1665574073791505, 1.6812319040298462]
memory len:10000
memory used:3861.0
now epsilon is 0.017796497420299155, the reward is 245.25 with loss [41.378238916397095, 34.883153676986694] in episode 2950
Report: 
rewardSum:245.25
loss:[41.378238916397095, 34.883153676986694]
policies:[1, 5, 0]
qAverage:[0.0, 44.28486887613932]
ws:[-3.2853167206048965, 0.7210765977700552]
memory len:10000
memory used:3870.0
now epsilon is 0.017778707595453178, the reward is 247.25 with loss [27.16675043106079, 25.87621831893921] in episode 2951
Report: 
rewardSum:247.25
loss:[27.16675043106079, 25.87621831893921]
policies:[0, 4, 0]
qAverage:[0.0, 41.08756332397461]
ws:[-5.690014004707336, 0.0025382041931152344]
memory len:10000
memory used:3879.0
now epsilon is 0.017760935553761976, the reward is 247.25 with loss [28.790794610977173, 29.439428329467773] in episode 2952
Report: 
rewardSum:247.25
loss:[28.790794610977173, 29.439428329467773]
policies:[0, 4, 0]
qAverage:[0.0, 41.63071060180664]
ws:[-4.949150252342224, 0.4965126752853394]
memory len:10000
memory used:3910.0
now epsilon is 0.017743181277449057, the reward is 247.25 with loss [29.820394039154053, 23.432228803634644] in episode 2953
Report: 
rewardSum:247.25
loss:[29.820394039154053, 23.432228803634644]
policies:[0, 4, 0]
qAverage:[0.0, 41.731407928466794]
ws:[-5.261696282029152, 0.9548607259988785]
memory len:10000
memory used:3911.0
now epsilon is 0.01772544474875571, the reward is 247.25 with loss [15.15474271774292, 25.489014625549316] in episode 2954
Report: 
rewardSum:247.25
loss:[15.15474271774292, 25.489014625549316]
policies:[0, 4, 0]
qAverage:[0.0, 41.856893920898436]
ws:[-4.837979853153229, 1.9202505052089691]
memory len:10000
memory used:3913.0
now epsilon is 0.017707725949940967, the reward is 247.25 with loss [16.81403088569641, 31.6336669921875] in episode 2955
Report: 
rewardSum:247.25
loss:[16.81403088569641, 31.6336669921875]
policies:[0, 4, 0]
qAverage:[0.0, 41.78253402709961]
ws:[-3.6571566313505173, 2.8220818042755127]
memory len:10000
memory used:3918.0
now epsilon is 0.017690024863281594, the reward is 247.25 with loss [25.72005033493042, 49.167664527893066] in episode 2956
Report: 
rewardSum:247.25
loss:[25.72005033493042, 49.167664527893066]
policies:[0, 4, 0]
qAverage:[0.0, 42.320516204833986]
ws:[-3.0996632814407348, 2.9576285243034364]
memory len:10000
memory used:3924.0
now epsilon is 0.01767234147107208, the reward is 247.25 with loss [32.947537899017334, 26.67814016342163] in episode 2957
Report: 
rewardSum:247.25
loss:[32.947537899017334, 26.67814016342163]
policies:[0, 4, 0]
qAverage:[0.0, 42.24137954711914]
ws:[-4.86330246925354, 1.2787851333618163]
memory len:10000
memory used:3922.0
now epsilon is 0.01765467575562461, the reward is 247.25 with loss [16.370198011398315, 35.13319444656372] in episode 2958
Report: 
rewardSum:247.25
loss:[16.370198011398315, 35.13319444656372]
policies:[0, 4, 0]
qAverage:[0.0, 42.69849548339844]
ws:[-7.065540742874146, 0.20542333126068116]
memory len:10000
memory used:3922.0
now epsilon is 0.01763702769926905, the reward is 247.25 with loss [35.19579362869263, 17.349581003189087] in episode 2959
Report: 
rewardSum:247.25
loss:[35.19579362869263, 17.349581003189087]
policies:[0, 4, 0]
qAverage:[0.0, 42.02631759643555]
ws:[-8.039922073483467, 0.4623364210128784]
memory len:10000
memory used:3624.0
now epsilon is 0.017619397284352925, the reward is 247.25 with loss [30.353330612182617, 24.486263036727905] in episode 2960
Report: 
rewardSum:247.25
loss:[30.353330612182617, 24.486263036727905]
policies:[0, 4, 0]
qAverage:[0.0, 41.94692840576172]
ws:[-5.8769707679748535, 1.237096405029297]
memory len:10000
memory used:3624.0
now epsilon is 0.017601784493241412, the reward is 247.25 with loss [23.172293186187744, 28.014745235443115] in episode 2961
Report: 
rewardSum:247.25
loss:[23.172293186187744, 28.014745235443115]
policies:[0, 4, 0]
qAverage:[0.0, 41.272587585449216]
ws:[-5.87226687669754, 1.4502615451812744]
memory len:10000
memory used:3625.0
now epsilon is 0.017584189308317313, the reward is 247.25 with loss [21.397408485412598, 30.03640079498291] in episode 2962
Report: 
rewardSum:247.25
loss:[21.397408485412598, 30.03640079498291]
policies:[0, 4, 0]
qAverage:[0.0, 42.66823806762695]
ws:[-6.100576269626617, 1.4001789569854737]
memory len:10000
memory used:3624.0
now epsilon is 0.017566611711981046, the reward is 247.25 with loss [12.590646505355835, 21.473666667938232] in episode 2963
Report: 
rewardSum:247.25
loss:[12.590646505355835, 21.473666667938232]
policies:[0, 4, 0]
qAverage:[0.0, 41.688865661621094]
ws:[-4.722710609436035, 2.1010666847229005]
memory len:10000
memory used:3624.0
now epsilon is 0.017549051686650614, the reward is 247.25 with loss [25.170726776123047, 29.042988300323486] in episode 2964
Report: 
rewardSum:247.25
loss:[25.170726776123047, 29.042988300323486]
policies:[0, 4, 0]
qAverage:[0.0, 42.785614013671875]
ws:[-4.373400583863258, 2.1938003420829775]
memory len:10000
memory used:3624.0
now epsilon is 0.0175315092147616, the reward is 247.25 with loss [20.31580924987793, 24.63742971420288] in episode 2965
Report: 
rewardSum:247.25
loss:[20.31580924987793, 24.63742971420288]
policies:[0, 4, 0]
qAverage:[0.0, 41.99844131469727]
ws:[-2.8656711339950562, 3.4409101963043214]
memory len:10000
memory used:3623.0
now epsilon is 0.017513984278767148, the reward is 247.25 with loss [25.88010311126709, 36.04599475860596] in episode 2966
Report: 
rewardSum:247.25
loss:[25.88010311126709, 36.04599475860596]
policies:[0, 3, 1]
qAverage:[0.0, 37.055644035339355]
ws:[0.9434806853532791, 3.9910053610801697]
memory len:10000
memory used:3624.0
now epsilon is 0.017496476861137932, the reward is 247.25 with loss [26.925670623779297, 27.677358627319336] in episode 2967
Report: 
rewardSum:247.25
loss:[26.925670623779297, 27.677358627319336]
policies:[0, 4, 0]
qAverage:[0.0, 39.19569969177246]
ws:[-7.096388109028339, 1.029434323310852]
memory len:10000
memory used:3624.0
now epsilon is 0.01747898694436216, the reward is 247.25 with loss [27.86410427093506, 25.722244262695312] in episode 2968
Report: 
rewardSum:247.25
loss:[27.86410427093506, 25.722244262695312]
policies:[0, 4, 0]
qAverage:[0.0, 42.29976196289063]
ws:[-5.78175448179245, 2.1617629289627076]
memory len:10000
memory used:3623.0
now epsilon is 0.017461514510945533, the reward is 247.25 with loss [27.733953952789307, 17.68633246421814] in episode 2969
Report: 
rewardSum:247.25
loss:[27.733953952789307, 17.68633246421814]
policies:[0, 4, 0]
qAverage:[0.0, 42.444523620605466]
ws:[-4.7481529712677, 3.0172553434967995]
memory len:10000
memory used:3624.0
now epsilon is 0.017444059543411255, the reward is 247.25 with loss [17.242748618125916, 35.282296895980835] in episode 2970
Report: 
rewardSum:247.25
loss:[17.242748618125916, 35.282296895980835]
policies:[0, 4, 0]
qAverage:[0.0, 42.31323394775391]
ws:[-4.300872707366944, 3.8017084687948226]
memory len:10000
memory used:3624.0
now epsilon is 0.017426622024299992, the reward is 247.25 with loss [31.991178035736084, 28.252687454223633] in episode 2971
Report: 
rewardSum:247.25
loss:[31.991178035736084, 28.252687454223633]
policies:[0, 4, 0]
qAverage:[0.0, 42.97564849853516]
ws:[-4.192815373465419, 3.9177790760993956]
memory len:10000
memory used:3623.0
now epsilon is 0.01740920193616986, the reward is 247.25 with loss [28.440933227539062, 36.663901805877686] in episode 2972
Report: 
rewardSum:247.25
loss:[28.440933227539062, 36.663901805877686]
policies:[0, 4, 0]
qAverage:[0.0, 42.51740493774414]
ws:[-5.846207708120346, 2.7376937866210938]
memory len:10000
memory used:3624.0
now epsilon is 0.01739179926159641, the reward is 247.25 with loss [32.33775877952576, 31.99266266822815] in episode 2973
Report: 
rewardSum:247.25
loss:[32.33775877952576, 31.99266266822815]
policies:[0, 4, 0]
qAverage:[0.0, 42.41695938110352]
ws:[-6.1098545551300045, 2.827905607223511]
memory len:10000
memory used:3623.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2*		12-		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.01737441398317262, the reward is 247.25 with loss [23.316229343414307, 30.58719778060913] in episode 2974
Report: 
rewardSum:247.25
loss:[23.316229343414307, 30.58719778060913]
policies:[0, 4, 0]
qAverage:[0.0, 43.218771362304686]
ws:[-6.402414989471436, 2.726763868331909]
memory len:10000
memory used:3624.0
now epsilon is 0.01735704608350886, the reward is 247.25 with loss [32.972901344299316, 30.125007152557373] in episode 2975
Report: 
rewardSum:247.25
loss:[32.972901344299316, 30.125007152557373]
policies:[0, 4, 0]
qAverage:[0.0, 44.133332061767575]
ws:[-6.590673643350601, 2.928290510177612]
memory len:10000
memory used:3625.0
now epsilon is 0.017339695545232885, the reward is 247.25 with loss [29.20443105697632, 30.767184257507324] in episode 2976
Report: 
rewardSum:247.25
loss:[29.20443105697632, 30.767184257507324]
policies:[0, 4, 0]
qAverage:[0.0, 44.02770462036133]
ws:[-4.361666801571846, 4.560561645030975]
memory len:10000
memory used:3625.0
now epsilon is 0.01732236235098982, the reward is 247.25 with loss [21.92267107963562, 29.950971364974976] in episode 2977
Report: 
rewardSum:247.25
loss:[21.92267107963562, 29.950971364974976]
policies:[0, 4, 0]
qAverage:[0.0, 44.312066650390626]
ws:[-2.8731613397598266, 5.020404911041259]
memory len:10000
memory used:3626.0
now epsilon is 0.017305046483442138, the reward is 37.15999999999997 with loss [27.57588005065918, 26.800801277160645] in episode 2978
Report: 
rewardSum:37.15999999999997
loss:[27.57588005065918, 26.800801277160645]
policies:[0, 3, 1]
qAverage:[0.0, 37.57357215881348]
ws:[1.0254405215382576, 5.339994192123413]
memory len:10000
memory used:3626.0
now epsilon is 0.01728774792526963, the reward is 247.25 with loss [34.21451139450073, 21.67922592163086] in episode 2979
Report: 
rewardSum:247.25
loss:[34.21451139450073, 21.67922592163086]
policies:[0, 4, 0]
qAverage:[0.0, 37.56528949737549]
ws:[-0.8408145606517792, 5.018784403800964]
memory len:10000
memory used:3626.0
now epsilon is 0.01727046665916942, the reward is 247.25 with loss [23.24511456489563, 19.440661191940308] in episode 2980
Report: 
rewardSum:247.25
loss:[23.24511456489563, 19.440661191940308]
policies:[0, 4, 0]
qAverage:[0.0, 44.78365936279297]
ws:[-6.469843959808349, 3.3757387042045592]
memory len:10000
memory used:3626.0
now epsilon is 0.01725320266785591, the reward is 247.25 with loss [35.96863317489624, 31.239224910736084] in episode 2981
Report: 
rewardSum:247.25
loss:[35.96863317489624, 31.239224910736084]
policies:[0, 4, 0]
qAverage:[0.0, 44.115074920654294]
ws:[-6.4645776450634, 3.648885005712509]
memory len:10000
memory used:3626.0
now epsilon is 0.0172359559340608, the reward is 247.25 with loss [25.7174015045166, 24.801105499267578] in episode 2982
Report: 
rewardSum:247.25
loss:[25.7174015045166, 24.801105499267578]
policies:[0, 4, 0]
qAverage:[0.0, 44.89620590209961]
ws:[-7.076005434989929, 3.1357588529586793]
memory len:10000
memory used:3626.0
now epsilon is 0.01721872644053304, the reward is 247.25 with loss [25.222618579864502, 28.932406425476074] in episode 2983
Report: 
rewardSum:247.25
loss:[25.222618579864502, 28.932406425476074]
policies:[0, 4, 0]
qAverage:[0.0, 44.35582046508789]
ws:[-6.7322488784790036, 2.986485314369202]
memory len:10000
memory used:3626.0
now epsilon is 0.01720151417003882, the reward is 247.25 with loss [31.36813735961914, 37.24480581283569] in episode 2984
Report: 
rewardSum:247.25
loss:[31.36813735961914, 37.24480581283569]
policies:[0, 4, 0]
qAverage:[0.0, 44.37206802368164]
ws:[-5.4761387426406145, 3.6094168424606323]
memory len:10000
memory used:3625.0
now epsilon is 0.017184319105361574, the reward is 247.25 with loss [30.42611598968506, 32.75705909729004] in episode 2985
Report: 
rewardSum:247.25
loss:[30.42611598968506, 32.75705909729004]
policies:[0, 4, 0]
qAverage:[0.0, 44.80274429321289]
ws:[-5.199061894416809, 3.303523635864258]
memory len:10000
memory used:3624.0
now epsilon is 0.017167141229301925, the reward is 247.25 with loss [37.33438301086426, 33.515624046325684] in episode 2986
Report: 
rewardSum:247.25
loss:[37.33438301086426, 33.515624046325684]
policies:[0, 4, 0]
qAverage:[0.0, 46.98541488647461]
ws:[-8.114460575580598, 2.369660663604736]
memory len:10000
memory used:3625.0
now epsilon is 0.017149980524677706, the reward is 247.25 with loss [24.920857191085815, 29.130839347839355] in episode 2987
Report: 
rewardSum:247.25
loss:[24.920857191085815, 29.130839347839355]
policies:[0, 4, 0]
qAverage:[0.0, 46.427286529541014]
ws:[-7.836358976364136, 3.3750821590423583]
memory len:10000
memory used:3626.0
now epsilon is 0.017132836974323922, the reward is 247.25 with loss [18.79256784915924, 25.02050018310547] in episode 2988
Report: 
rewardSum:247.25
loss:[18.79256784915924, 25.02050018310547]
policies:[0, 3, 1]
qAverage:[0.0, 49.051239013671875]
ws:[-5.686604030430317, 5.205697953701019]
memory len:10000
memory used:3625.0
now epsilon is 0.01711571056109273, the reward is 247.25 with loss [18.677574396133423, 36.852498054504395] in episode 2989
Report: 
rewardSum:247.25
loss:[18.677574396133423, 36.852498054504395]
policies:[0, 4, 0]
qAverage:[0.0, 47.08758316040039]
ws:[-2.275412750244141, 6.303127765655518]
memory len:10000
memory used:3625.0
now epsilon is 0.017098601267853435, the reward is 247.25 with loss [21.305655241012573, 17.932509899139404] in episode 2990
Report: 
rewardSum:247.25
loss:[21.305655241012573, 17.932509899139404]
policies:[0, 4, 0]
qAverage:[0.0, 47.85311660766602]
ws:[-1.9442776679992675, 5.970376110076904]
memory len:10000
memory used:3626.0
now epsilon is 0.017081509077492465, the reward is 247.25 with loss [36.05121183395386, 16.780926942825317] in episode 2991
Report: 
rewardSum:247.25
loss:[36.05121183395386, 16.780926942825317]
policies:[0, 4, 0]
qAverage:[0.0, 47.14136505126953]
ws:[-3.2940919011831284, 5.0280366897583]
memory len:10000
memory used:3625.0
now epsilon is 0.01706443397291335, the reward is 247.25 with loss [37.01461315155029, 18.981836080551147] in episode 2992
Report: 
rewardSum:247.25
loss:[37.01461315155029, 18.981836080551147]
policies:[0, 4, 0]
qAverage:[0.0, 47.51891098022461]
ws:[-5.216774922609329, 3.965616512298584]
memory len:10000
memory used:3626.0
now epsilon is 0.01704737593703672, the reward is 247.25 with loss [27.837882041931152, 32.16260623931885] in episode 2993
Report: 
rewardSum:247.25
loss:[27.837882041931152, 32.16260623931885]
policies:[0, 4, 0]
qAverage:[0.0, 47.138369750976565]
ws:[-6.408741450309753, 3.7331637382507323]
memory len:10000
memory used:3625.0
now epsilon is 0.017030334952800267, the reward is 247.25 with loss [22.53356122970581, 21.09804606437683] in episode 2994
Report: 
rewardSum:247.25
loss:[22.53356122970581, 21.09804606437683]
policies:[0, 4, 0]
qAverage:[0.0, 47.05794830322266]
ws:[-5.2248581647872925, 4.7395772457122805]
memory len:10000
memory used:3625.0
now epsilon is 0.017013311003158747, the reward is 247.25 with loss [28.661027431488037, 32.244542598724365] in episode 2995
Report: 
rewardSum:247.25
loss:[28.661027431488037, 32.244542598724365]
policies:[0, 4, 0]
qAverage:[0.0, 39.7614860534668]
ws:[0.4402591735124588, 5.378427028656006]
memory len:10000
memory used:3625.0
now epsilon is 0.01699630407108395, the reward is 247.25 with loss [35.58097743988037, 23.649306297302246] in episode 2996
Report: 
rewardSum:247.25
loss:[35.58097743988037, 23.649306297302246]
policies:[0, 4, 0]
qAverage:[0.0, 47.09452362060547]
ws:[-4.848521196842194, 4.964723920822143]
memory len:10000
memory used:3625.0
now epsilon is 0.016979314139564695, the reward is 247.25 with loss [34.75938272476196, 35.546783447265625] in episode 2997
Report: 
rewardSum:247.25
loss:[34.75938272476196, 35.546783447265625]
policies:[1, 3, 0]
qAverage:[0.0, 48.71040058135986]
ws:[-8.306159555912018, 5.022527098655701]
memory len:10000
memory used:3625.0
now epsilon is 0.016962341191606795, the reward is 247.25 with loss [21.913657188415527, 32.94763898849487] in episode 2998
Report: 
rewardSum:247.25
loss:[21.913657188415527, 32.94763898849487]
policies:[0, 4, 0]
qAverage:[0.0, 50.47057571411133]
ws:[-7.9206085205078125, 3.5621433526277544]
memory len:10000
memory used:3625.0
############# STATE ###############
0-		10-		20-		30-		40-		
1-		11-		21-		31-		41-		
2-		12*		22-		32-		42-		
3x		13-		23-		33-		43-		
4x		14x		24-		34-		44-		
5x		15x		25-		35-		45-		
6x		16x		26x		36-		46-		
7x		17x		27x		37-		47-		
8x		18x		28x		38x		48-		
9x		19x		29x		39x		49x		
-----------------------------------
1		41		251		640		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[41.15999999999997, -5]
[251.25, -8]
[640.1600000000001, -11]
[1000, -13]
now epsilon is 0.016945385210233057, the reward is 247.25 with loss [32.78551483154297, 18.178861141204834] in episode 2999
Report: 
rewardSum:247.25
loss:[32.78551483154297, 18.178861141204834]
policies:[0, 4, 0]
qAverage:[0.0, 51.62054290771484]
ws:[-3.1438324570655825, 6.249058270454407]
memory len:10000
memory used:3625.0
now epsilon is 0.01692844617848326, the reward is 247.25 with loss [24.31777286529541, 28.149224758148193] in episode 3000
Report: 
rewardSum:247.25
loss:[24.31777286529541, 28.149224758148193]
policies:[0, 4, 0]
qAverage:[0.0, 50.30300598144531]
ws:[-0.6108386039733886, 7.22864465713501]
memory len:10000
memory used:3626.0
Traceback (most recent call last):
  File "agentRunner.py", line 57, in <module>
    agent.train()
  File "/home/luke/MODRL/tensorflow_agents/deep_sea_graphical_wpddqn.py", line 234, in train
    
AttributeError: 'DeepSeaTreasure' object has no attribute 'close'
pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)
Hello from the pygame community. https://www.pygame.org/contribute.html
2021-03-25 15:45:15.209096: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
2021-03-25 15:45:17.048644: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 15:45:17.049432: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-25 15:45:18.012223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0001:00:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 149.31GiB/s
2021-03-25 15:45:18.012314: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 15:45:18.017836: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 15:45:18.017938: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 15:45:18.019103: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-25 15:45:18.020150: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-25 15:45:18.021255: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-25 15:45:18.022149: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-25 15:45:18.022337: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 15:45:18.023630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-25 15:45:18.024803: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-25 15:45:18.025452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0001:00:00.0 name: Tesla M60 computeCapability: 5.2
coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 149.31GiB/s
2021-03-25 15:45:18.025501: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 15:45:18.025545: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 15:45:18.025579: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 15:45:18.025618: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-25 15:45:18.025659: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-25 15:45:18.025699: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-25 15:45:18.025736: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-03-25 15:45:18.025772: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 15:45:18.026906: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-25 15:45:18.026975: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-03-25 15:45:18.612479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-25 15:45:18.612566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-25 15:45:18.612582: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-03-25 15:45:18.614493: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7468 MB memory) -> physical GPU (device: 0, name: Tesla M60, pci bus id: 0001:00:00.0, compute capability: 5.2)
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.9997000225, the reward is -1.0 with loss [0, 0] in episode 0
No handles with labels found to put in legend.
Report: 
rewardSum:-1.0
loss:[0, 0]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4
memory used:2765.0
now epsilon is 0.9994001349865005, the reward is -1.0 with loss [0, 0] in episode 1
Report: 
rewardSum:-1.0
loss:[0, 0]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:8
memory used:2764.0
now epsilon is 0.9985010120951063, the reward is -5.0 with loss [0, 0] in episode 2
Report: 
rewardSum:-5.0
loss:[0, 0]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:20
memory used:2764.0
2021-03-25 15:45:19.538790: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-03-25 15:45:20.314378: W tensorflow/stream_executor/gpu/asm_compiler.cc:98] *** WARNING *** You are using ptxas 9.1.108, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You do not need to update to CUDA 9.2.88; cherry-picking the ptxas binary is sufficient.
2021-03-25 15:45:20.365272: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-03-25 15:45:20.602991: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-03-25 15:45:21.003050: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-03-25 15:45:21.005596: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2596990000 Hz
now epsilon is 0.9971538442315864, the reward is 54.6875 with loss [2.1569728143513203, 0.6720322743058205] in episode 3
Report: 
rewardSum:54.6875
loss:[2.1569728143513203, 0.6720322743058205]
policies:[0, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:38
memory used:3476.0
now epsilon is 0.9964062031743763, the reward is -4.0 with loss [1.2265638709068298, 3.7577634900808334] in episode 4
Report: 
rewardSum:-4.0
loss:[1.2265638709068298, 3.7577634900808334]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:48
memory used:3476.0
now epsilon is 0.994763365437108, the reward is 551.6875 with loss [13.75651841238141, 9.911552000790834] in episode 5
Report: 
rewardSum:551.6875
loss:[13.75651841238141, 9.911552000790834]
policies:[0, 0, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:70
memory used:3177.0
now epsilon is 0.9929742678645566, the reward is 550.6875 with loss [39.6530368514359, 76.81239926815033] in episode 6
Report: 
rewardSum:550.6875
loss:[39.6530368514359, 76.81239926815033]
policies:[0, 0, 12]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:94
memory used:3181.0
now epsilon is 0.9920809260852758, the reward is -5.0 with loss [28.623371370136738, 58.82688665390015] in episode 7
Report: 
rewardSum:-5.0
loss:[28.623371370136738, 58.82688665390015]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:106
memory used:3181.0
now epsilon is 0.9916345566306516, the reward is -2.0 with loss [14.316797316074371, 23.08724617958069] in episode 8
Report: 
rewardSum:-2.0
loss:[14.316797316074371, 23.08724617958069]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:112
memory used:3194.0
now epsilon is 0.9901481084241877, the reward is 241.25 with loss [53.74668352305889, 54.657294899225235] in episode 9
Report: 
rewardSum:241.25
loss:[53.74668352305889, 54.657294899225235]
policies:[0, 0, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:132
memory used:3194.0
now epsilon is 0.9892573092347652, the reward is -5.0 with loss [41.78260290622711, 42.29728136211634] in episode 10
Report: 
rewardSum:-5.0
loss:[41.78260290622711, 42.29728136211634]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:144
memory used:3192.0
now epsilon is 0.9888122102171392, the reward is -2.0 with loss [24.05349636077881, 11.35535740852356] in episode 11
Report: 
rewardSum:-2.0
loss:[24.05349636077881, 11.35535740852356]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:150
memory used:3192.0
now epsilon is 0.9874781143909713, the reward is -8.0 with loss [37.06029896810651, 53.198040921241045] in episode 12
Report: 
rewardSum:-8.0
loss:[37.06029896810651, 53.198040921241045]
policies:[0, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:168
memory used:3193.0
now epsilon is 0.985849996956656, the reward is 240.25 with loss [43.02808950841427, 86.50667554885149] in episode 13
Report: 
rewardSum:240.25
loss:[43.02808950841427, 86.50667554885149]
policies:[0, 0, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:190
memory used:3193.0
now epsilon is 0.9849630646172317, the reward is 994.0 with loss [35.937763690948486, 40.79948949813843] in episode 14
Report: 
rewardSum:994.0
loss:[35.937763690948486, 40.79948949813843]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:202
memory used:3193.0
now epsilon is 0.9846675978595155, the reward is -1.0 with loss [21.100751876831055, 11.327178001403809] in episode 15
Report: 
rewardSum:-1.0
loss:[21.100751876831055, 11.327178001403809]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:206
memory used:3193.0
now epsilon is 0.984076930217633, the reward is 247.25 with loss [21.69316165521741, 31.936409521847963] in episode 16
Report: 
rewardSum:247.25
loss:[21.69316165521741, 31.936409521847963]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:214
memory used:3192.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17*		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.9831915930399835, the reward is 556.6875 with loss [57.94200277328491, 44.743618316948414] in episode 17
Report: 
rewardSum:556.6875
loss:[57.94200277328491, 44.743618316948414]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:226
memory used:3194.0
now epsilon is 0.9821597063091972, the reward is 56.6875 with loss [71.20776641368866, 58.935532093048096] in episode 18
Report: 
rewardSum:56.6875
loss:[71.20776641368866, 58.935532093048096]
policies:[0, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:240
memory used:3196.0
now epsilon is 0.9815705430637134, the reward is 247.25 with loss [27.56967367976904, 50.085959911346436] in episode 19
Report: 
rewardSum:247.25
loss:[27.56967367976904, 50.085959911346436]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:248
memory used:3196.0
now epsilon is 0.9802462176245047, the reward is 991.0 with loss [109.30400562286377, 108.37210541218519] in episode 20
Report: 
rewardSum:991.0
loss:[109.30400562286377, 108.37210541218519]
policies:[1, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:266
memory used:3198.0
now epsilon is 0.9799521658147573, the reward is -1.0 with loss [28.905189514160156, 20.594159364700317] in episode 21
Report: 
rewardSum:-1.0
loss:[28.905189514160156, 20.594159364700317]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:270
memory used:3197.0
now epsilon is 0.9786300238744079, the reward is 54.6875 with loss [63.44042903184891, 86.82979154586792] in episode 22
Report: 
rewardSum:54.6875
loss:[63.44042903184891, 86.82979154586792]
policies:[0, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:288
memory used:3203.0
now epsilon is 0.9771630693052544, the reward is 990.0 with loss [59.58547401428223, 83.5032520070672] in episode 23
Report: 
rewardSum:990.0
loss:[59.58547401428223, 83.5032520070672]
policies:[1, 1, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:308
memory used:3202.0
now epsilon is 0.9759910890501728, the reward is 55.6875 with loss [74.11922576278448, 139.54954487085342] in episode 24
Report: 
rewardSum:55.6875
loss:[74.11922576278448, 139.54954487085342]
policies:[0, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:324
memory used:3203.0
now epsilon is 0.9737974133806758, the reward is 236.25 with loss [163.75375986099243, 107.63223706185818] in episode 25
Report: 
rewardSum:236.25
loss:[163.75375986099243, 107.63223706185818]
policies:[0, 0, 15]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:354
memory used:3204.0
now epsilon is 0.9732132663821526, the reward is -3.0 with loss [33.86401164531708, 31.448739290237427] in episode 26
Report: 
rewardSum:-3.0
loss:[33.86401164531708, 31.448739290237427]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:362
memory used:3204.0
now epsilon is 0.9726294697929764, the reward is -3.0 with loss [27.556072041392326, 42.68291187286377] in episode 27
Report: 
rewardSum:-3.0
loss:[27.556072041392326, 42.68291187286377]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:370
memory used:3205.0
now epsilon is 0.9717544314669638, the reward is 556.6875 with loss [58.38994479179382, 56.93757748603821] in episode 28
Report: 
rewardSum:556.6875
loss:[58.38994479179382, 56.93757748603821]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:382
memory used:3204.0
now epsilon is 0.9701522386601689, the reward is 989.0 with loss [159.7557315826416, 114.84548497200012] in episode 29
Report: 
rewardSum:989.0
loss:[159.7557315826416, 114.84548497200012]
policies:[2, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:404
memory used:3204.0
now epsilon is 0.9688433186863147, the reward is 991.0 with loss [119.10733854398131, 96.72027583420277] in episode 30
Report: 
rewardSum:991.0
loss:[119.10733854398131, 96.72027583420277]
policies:[0, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:422
memory used:3204.0
now epsilon is 0.9685526874896835, the reward is -1.0 with loss [48.47876739501953, 24.52258014678955] in episode 31
Report: 
rewardSum:-1.0
loss:[48.47876739501953, 24.52258014678955]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:426
memory used:3203.0
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.9666657086245002, the reward is -12.0 with loss [175.51395577192307, 157.99728325009346] in episode 32
Report: 
rewardSum:-12.0
loss:[175.51395577192307, 157.99728325009346]
policies:[0, 1, 12]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:452
memory used:3203.0
now epsilon is 0.9657960356711724, the reward is -5.0 with loss [69.16910743713379, 52.563520431518555] in episode 33
Report: 
rewardSum:-5.0
loss:[69.16910743713379, 52.563520431518555]
policies:[0, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:464
memory used:3202.0
now epsilon is 0.9646376886973685, the reward is 992.0 with loss [107.87401700019836, 95.70493123680353] in episode 34
Report: 
rewardSum:992.0
loss:[107.87401700019836, 95.70493123680353]
policies:[1, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:480
memory used:3201.0
now epsilon is 0.9642036668472432, the reward is -2.0 with loss [24.110676288604736, 45.12720775604248] in episode 35
Report: 
rewardSum:-2.0
loss:[24.110676288604736, 45.12720775604248]
policies:[1, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:486
memory used:3201.0
now epsilon is 0.9631917084694069, the reward is 244.25 with loss [52.150242641568184, 90.68085502833128] in episode 36
Report: 
rewardSum:244.25
loss:[52.150242641568184, 90.68085502833128]
policies:[0, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:500
memory used:3203.0
now epsilon is 0.9620364850480109, the reward is 243.25 with loss [88.39347839355469, 86.93475437164307] in episode 37
Report: 
rewardSum:243.25
loss:[88.39347839355469, 86.93475437164307]
policies:[0, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:516
memory used:3201.0
now epsilon is 0.9605944039928576, the reward is -9.0 with loss [96.25961261987686, 136.7945351600647] in episode 38
Report: 
rewardSum:-9.0
loss:[96.25961261987686, 136.7945351600647]
policies:[0, 0, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:536
memory used:3201.0
now epsilon is 0.9600181770177388, the reward is -3.0 with loss [29.560665607452393, 63.264936447143555] in episode 39
Report: 
rewardSum:-3.0
loss:[29.560665607452393, 63.264936447143555]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:544
memory used:3202.0
now epsilon is 0.9585791213819113, the reward is 990.0 with loss [149.08324156701565, 137.86121201515198] in episode 40
Report: 
rewardSum:990.0
loss:[149.08324156701565, 137.86121201515198]
policies:[1, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:564
memory used:3203.0
now epsilon is 0.9571422228730763, the reward is 552.6875 with loss [107.45306915044785, 135.40051364898682] in episode 41
Report: 
rewardSum:552.6875
loss:[107.45306915044785, 135.40051364898682]
policies:[1, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:584
memory used:3219.0
now epsilon is 0.9567115734766531, the reward is -2.0 with loss [41.203866720199585, 41.67008876800537] in episode 42
Report: 
rewardSum:-2.0
loss:[41.203866720199585, 41.67008876800537]
policies:[1, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:590
memory used:3219.0
now epsilon is 0.9557074782577266, the reward is 56.6875 with loss [102.94523286819458, 69.77104511111975] in episode 43
Report: 
rewardSum:56.6875
loss:[102.94523286819458, 69.77104511111975]
policies:[0, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:604
memory used:3219.0
now epsilon is 0.9542748843072019, the reward is 53.6875 with loss [105.61711702495813, 137.866934299469] in episode 44
Report: 
rewardSum:53.6875
loss:[105.61711702495813, 137.866934299469]
policies:[1, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:624
memory used:3219.0
now epsilon is 0.9531303554588864, the reward is 55.6875 with loss [87.38661386072636, 105.00476717948914] in episode 45
Report: 
rewardSum:55.6875
loss:[87.38661386072636, 105.00476717948914]
policies:[0, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:640
memory used:3218.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20*		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.9525586059053422, the reward is 59.6875 with loss [53.94551467895508, 38.82236024737358] in episode 46
Report: 
rewardSum:59.6875
loss:[53.94551467895508, 38.82236024737358]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:648
memory used:3221.0
now epsilon is 0.9514161355101778, the reward is -7.0 with loss [90.70698817074299, 76.23374319076538] in episode 47
Report: 
rewardSum:-7.0
loss:[90.70698817074299, 76.23374319076538]
policies:[1, 0, 7]
qAverage:[-0.26066839694976807, 0.0]
ws:[-9.360556602478027, -11.268080711364746]
memory len:664
memory used:3221.0
now epsilon is 0.9508454142572065, the reward is 247.25 with loss [65.66512966156006, 50.10195350646973] in episode 48
Report: 
rewardSum:247.25
loss:[65.66512966156006, 50.10195350646973]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:672
memory used:3221.0
now epsilon is 0.9502750353599472, the reward is 247.25 with loss [83.76001739501953, 32.957409381866455] in episode 49
Report: 
rewardSum:247.25
loss:[83.76001739501953, 32.957409381866455]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:680
memory used:3220.0
now epsilon is 0.9487082569879339, the reward is 989.0 with loss [103.68613420426846, 118.78930830955505] in episode 50
Report: 
rewardSum:989.0
loss:[103.68613420426846, 118.78930830955505]
policies:[0, 0, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:702
memory used:3220.0
now epsilon is 0.9472861547854365, the reward is 990.0 with loss [124.97906478494406, 154.34871149808168] in episode 51
Report: 
rewardSum:990.0
loss:[124.97906478494406, 154.34871149808168]
policies:[1, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:722
memory used:3220.0
now epsilon is 0.9470019902529394, the reward is -1.0 with loss [34.2849178314209, 32.81310844421387] in episode 52
Report: 
rewardSum:-1.0
loss:[34.2849178314209, 32.81310844421387]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:726
memory used:3220.0
now epsilon is 0.9467179109634083, the reward is -1.0 with loss [28.43728542327881, 12.811364885419607] in episode 53
Report: 
rewardSum:-1.0
loss:[28.43728542327881, 12.811364885419607]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:730
memory used:3220.0
now epsilon is 0.94586618429694, the reward is -5.0 with loss [53.084815703332424, 117.00920724868774] in episode 54
Report: 
rewardSum:-5.0
loss:[53.084815703332424, 117.00920724868774]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:742
memory used:3221.0
now epsilon is 0.9455824457236401, the reward is -1.0 with loss [63.07558822631836, 26.48335838317871] in episode 55
Report: 
rewardSum:-1.0
loss:[63.07558822631836, 26.48335838317871]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:746
memory used:3220.0
now epsilon is 0.945298792265528, the reward is -1.0 with loss [48.45923614501953, 36.258087158203125] in episode 56
Report: 
rewardSum:-1.0
loss:[48.45923614501953, 36.258087158203125]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:750
memory used:3220.0
now epsilon is 0.9445900308316558, the reward is 58.6875 with loss [84.69933748245239, 66.19050693511963] in episode 57
Report: 
rewardSum:58.6875
loss:[84.69933748245239, 66.19050693511963]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:760
memory used:3220.0
now epsilon is 0.9424669334063491, the reward is -14.0 with loss [171.62016520649195, 217.55905485153198] in episode 58
Report: 
rewardSum:-14.0
loss:[171.62016520649195, 217.55905485153198]
policies:[0, 0, 15]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:790
memory used:3222.0
now epsilon is 0.9413365666623369, the reward is -7.0 with loss [94.19025182723999, 110.76316511631012] in episode 59
Report: 
rewardSum:-7.0
loss:[94.19025182723999, 110.76316511631012]
policies:[0, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:806
memory used:3220.0
now epsilon is 0.9406307760063, the reward is -4.0 with loss [38.353641986846924, 57.377508357167244] in episode 60
Report: 
rewardSum:-4.0
loss:[38.353641986846924, 57.377508357167244]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:816
memory used:3220.0
now epsilon is 0.9402075556464999, the reward is -2.0 with loss [43.6275110244751, 20.203328371047974] in episode 61
Report: 
rewardSum:-2.0
loss:[43.6275110244751, 20.203328371047974]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:822
memory used:3220.0
now epsilon is 0.9396435580284398, the reward is 59.6875 with loss [41.24935108423233, 30.466759994626045] in episode 62
Report: 
rewardSum:59.6875
loss:[41.24935108423233, 30.466759994626045]
policies:[2, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:830
memory used:3220.0
now epsilon is 0.9386573761631126, the reward is 244.25 with loss [141.85805797576904, 78.94175481796265] in episode 63
Report: 
rewardSum:244.25
loss:[141.85805797576904, 78.94175481796265]
policies:[1, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:844
memory used:3220.0
now epsilon is 0.9383758000700547, the reward is -1.0 with loss [37.70219802856445, 22.561132431030273] in episode 64
Report: 
rewardSum:-1.0
loss:[37.70219802856445, 22.561132431030273]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:848
memory used:3219.0
now epsilon is 0.9379535942972227, the reward is -2.0 with loss [27.014357715845108, 19.211227893829346] in episode 65
Report: 
rewardSum:-2.0
loss:[27.014357715845108, 19.211227893829346]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:854
memory used:3219.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34*		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.9365476132040197, the reward is -9.0 with loss [111.26799839735031, 103.97377351671457] in episode 66
Report: 
rewardSum:-9.0
loss:[111.26799839735031, 103.97377351671457]
policies:[0, 0, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:874
memory used:3222.0
now epsilon is 0.9359858110573822, the reward is 247.25 with loss [36.76760959625244, 32.92399461567402] in episode 67
Report: 
rewardSum:247.25
loss:[36.76760959625244, 32.92399461567402]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:882
memory used:3222.0
now epsilon is 0.9357050363737458, the reward is -1.0 with loss [25.91748809814453, 34.255990982055664] in episode 68
Report: 
rewardSum:-1.0
loss:[25.91748809814453, 34.255990982055664]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:886
memory used:3222.0
now epsilon is 0.9351437396594701, the reward is 247.25 with loss [27.498478055000305, 88.94826412200928] in episode 69
Report: 
rewardSum:247.25
loss:[27.498478055000305, 88.94826412200928]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:894
memory used:3224.0
now epsilon is 0.9343024258416738, the reward is -5.0 with loss [59.39546000212431, 56.9413560628891] in episode 70
Report: 
rewardSum:-5.0
loss:[59.39546000212431, 56.9413560628891]
policies:[1, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:906
memory used:3225.0
now epsilon is 0.9337419705043838, the reward is -3.0 with loss [51.514622926712036, 52.81858491897583] in episode 71
Report: 
rewardSum:-3.0
loss:[51.514622926712036, 52.81858491897583]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:914
memory used:3225.0
now epsilon is 0.9324821749105429, the reward is 54.6875 with loss [113.65805888175964, 54.27898080646992] in episode 72
Report: 
rewardSum:54.6875
loss:[113.65805888175964, 54.27898080646992]
policies:[0, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:932
memory used:3224.0
now epsilon is 0.9315035091145816, the reward is 56.6875 with loss [80.36558675765991, 90.49867236614227] in episode 73
Report: 
rewardSum:56.6875
loss:[80.36558675765991, 90.49867236614227]
policies:[0, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:946
memory used:3224.0
now epsilon is 0.931084395408823, the reward is -2.0 with loss [42.63748836517334, 16.30283761024475] in episode 74
Report: 
rewardSum:-2.0
loss:[42.63748836517334, 16.30283761024475]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:952
memory used:3224.0
now epsilon is 0.9302467336310976, the reward is 556.6875 with loss [46.11868143081665, 62.184518814086914] in episode 75
Report: 
rewardSum:556.6875
loss:[46.11868143081665, 62.184518814086914]
policies:[1, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:964
memory used:3223.0
now epsilon is 0.9298281853894785, the reward is -2.0 with loss [56.29120445251465, 39.691712379455566] in episode 76
Report: 
rewardSum:-2.0
loss:[56.29120445251465, 39.691712379455566]
policies:[1, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:970
memory used:3223.0
now epsilon is 0.9289916537768844, the reward is 994.0 with loss [34.55487161874771, 70.11943417787552] in episode 77
Report: 
rewardSum:994.0
loss:[34.55487161874771, 70.11943417787552]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:982
memory used:3222.0
now epsilon is 0.9278774488815476, the reward is 992.0 with loss [102.29531860351562, 77.96167707443237] in episode 78
Report: 
rewardSum:992.0
loss:[102.29531860351562, 77.96167707443237]
policies:[0, 1, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:998
memory used:3222.0
now epsilon is 0.9262086466828561, the reward is 550.6875 with loss [83.73223180323839, 105.9392842054367] in episode 79
Report: 
rewardSum:550.6875
loss:[83.73223180323839, 105.9392842054367]
policies:[1, 1, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1022
memory used:3222.0
now epsilon is 0.9241268639740783, the reward is 236.25 with loss [124.27394776046276, 182.15918225049973] in episode 80
Report: 
rewardSum:236.25
loss:[124.27394776046276, 182.15918225049973]
policies:[0, 0, 15]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1052
memory used:3235.0
now epsilon is 0.9230184937626065, the reward is -7.0 with loss [85.51910457015038, 96.36598032712936] in episode 81
Report: 
rewardSum:-7.0
loss:[85.51910457015038, 96.36598032712936]
policies:[0, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1068
memory used:3223.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34*		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.9216349002044633, the reward is 552.6875 with loss [129.25582218170166, 57.21838188171387] in episode 82
Report: 
rewardSum:552.6875
loss:[129.25582218170166, 57.21838188171387]
policies:[0, 0, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1088
memory used:3222.0
now epsilon is 0.9213584304711872, the reward is -1.0 with loss [13.722050428390503, 29.470251083374023] in episode 83
Report: 
rewardSum:-1.0
loss:[13.722050428390503, 29.470251083374023]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1092
memory used:3222.0
now epsilon is 0.9205295187800488, the reward is -5.0 with loss [58.0880773216486, 43.53049087524414] in episode 84
Report: 
rewardSum:-5.0
loss:[58.0880773216486, 43.53049087524414]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1104
memory used:3222.0
now epsilon is 0.9199773253278393, the reward is 247.25 with loss [49.82644081115723, 30.68223375082016] in episode 85
Report: 
rewardSum:247.25
loss:[49.82644081115723, 30.68223375082016]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1112
memory used:3233.0
now epsilon is 0.9184605007008297, the reward is 52.6875 with loss [114.11403101682663, 139.41272473335266] in episode 86
Report: 
rewardSum:52.6875
loss:[114.11403101682663, 139.41272473335266]
policies:[0, 0, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1134
memory used:3234.0
now epsilon is 0.9181849832159807, the reward is -1.0 with loss [3.5322852730751038, 4.062138646841049] in episode 87
Report: 
rewardSum:-1.0
loss:[3.5322852730751038, 4.062138646841049]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1138
memory used:3235.0
now epsilon is 0.9165336130693701, the reward is 988.0 with loss [85.00404430925846, 122.97102889418602] in episode 88
Report: 
rewardSum:988.0
loss:[85.00404430925846, 122.97102889418602]
policies:[0, 0, 12]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1162
memory used:3236.0
now epsilon is 0.9159838166211935, the reward is 59.6875 with loss [55.05035877227783, 50.452754974365234] in episode 89
Report: 
rewardSum:59.6875
loss:[55.05035877227783, 50.452754974365234]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1170
memory used:3235.0
now epsilon is 0.9154343499766707, the reward is 59.6875 with loss [33.379485219717026, 52.72462844848633] in episode 90
Report: 
rewardSum:59.6875
loss:[33.379485219717026, 52.72462844848633]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1178
memory used:3236.0
now epsilon is 0.9150224663079103, the reward is -2.0 with loss [26.541963011026382, 22.708899974822998] in episode 91
Report: 
rewardSum:-2.0
loss:[26.541963011026382, 22.708899974822998]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1184
memory used:3236.0
now epsilon is 0.913513811069404, the reward is 240.25 with loss [107.78343951702118, 106.65192985534668] in episode 92
Report: 
rewardSum:240.25
loss:[107.78343951702118, 106.65192985534668]
policies:[0, 0, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1206
memory used:3235.0
now epsilon is 0.9128288812208807, the reward is -4.0 with loss [48.45652532577515, 70.84251952171326] in episode 93
Report: 
rewardSum:-4.0
loss:[48.45652532577515, 70.84251952171326]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1216
memory used:3235.0
now epsilon is 0.911050466036409, the reward is 50.6875 with loss [106.23210632801056, 114.19892007112503] in episode 94
Report: 
rewardSum:50.6875
loss:[106.23210632801056, 114.19892007112503]
policies:[0, 1, 12]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1242
memory used:3237.0
now epsilon is 0.9098212855999127, the reward is 242.25 with loss [91.70141953229904, 84.00229209661484] in episode 95
Report: 
rewardSum:242.25
loss:[91.70141953229904, 84.00229209661484]
policies:[2, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1260
memory used:3237.0
now epsilon is 0.9080487299514735, the reward is -12.0 with loss [67.14930039644241, 116.24564403295517] in episode 96
Report: 
rewardSum:-12.0
loss:[67.14930039644241, 116.24564403295517]
policies:[0, 1, 12]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1286
memory used:3236.0
now epsilon is 0.9077763357635845, the reward is -1.0 with loss [15.94650912284851, 8.883551120758057] in episode 97
Report: 
rewardSum:-1.0
loss:[15.94650912284851, 8.883551120758057]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1290
memory used:3236.0
now epsilon is 0.9072317924996771, the reward is -3.0 with loss [49.6940221786499, 78.13257026672363] in episode 98
Report: 
rewardSum:-3.0
loss:[49.6940221786499, 78.13257026672363]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1298
memory used:3236.0
############# STATE ###############
0-		8*		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.9058718630157853, the reward is 241.25 with loss [95.5821076631546, 104.83027219772339] in episode 99
Report: 
rewardSum:241.25
loss:[95.5821076631546, 104.83027219772339]
policies:[0, 1, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1318
memory used:3236.0
now epsilon is 0.9050568840096854, the reward is -5.0 with loss [28.479852586984634, 87.07528805732727] in episode 100
Report: 
rewardSum:-5.0
loss:[28.479852586984634, 87.07528805732727]
policies:[1, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1330
memory used:3237.0
now epsilon is 0.9045139720497414, the reward is 247.25 with loss [29.758684635162354, 47.43517315387726] in episode 101
Report: 
rewardSum:247.25
loss:[29.758684635162354, 47.43517315387726]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1338
memory used:3238.0
now epsilon is 0.9032936105874196, the reward is 242.25 with loss [107.44025886058807, 83.38831561803818] in episode 102
Report: 
rewardSum:242.25
loss:[107.44025886058807, 83.38831561803818]
policies:[0, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1356
memory used:3238.0
now epsilon is 0.9016690228089048, the reward is 239.25 with loss [87.6633318066597, 154.97314596176147] in episode 103
Report: 
rewardSum:239.25
loss:[87.6633318066597, 154.97314596176147]
policies:[0, 0, 12]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1380
memory used:3238.0
now epsilon is 0.9008578249408162, the reward is 57.6875 with loss [47.57373023033142, 53.73220157623291] in episode 104
Report: 
rewardSum:57.6875
loss:[47.57373023033142, 53.73220157623291]
policies:[0, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1392
memory used:3238.0
now epsilon is 0.9000473568770844, the reward is 245.25 with loss [41.747161507606506, 98.56205582618713] in episode 105
Report: 
rewardSum:245.25
loss:[41.747161507606506, 98.56205582618713]
policies:[0, 2, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1404
memory used:3237.0
now epsilon is 0.8989678669085899, the reward is 243.25 with loss [67.46062850952148, 63.802613496780396] in episode 106
Report: 
rewardSum:243.25
loss:[67.46062850952148, 63.802613496780396]
policies:[0, 1, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1420
memory used:3237.0
now epsilon is 0.8970818739208629, the reward is -13.0 with loss [95.07352578639984, 112.02082842588425] in episode 107
Report: 
rewardSum:-13.0
loss:[95.07352578639984, 112.02082842588425]
policies:[2, 0, 12]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1448
memory used:3237.0
now epsilon is 0.8954684580485299, the reward is 988.0 with loss [91.45716762542725, 108.43197125196457] in episode 108
Report: 
rewardSum:988.0
loss:[91.45716762542725, 108.43197125196457]
policies:[0, 2, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1472
memory used:3243.0
now epsilon is 0.8951998376591557, the reward is -1.0 with loss [23.97786319255829, 23.037880897521973] in episode 109
Report: 
rewardSum:-1.0
loss:[23.97786319255829, 23.037880897521973]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1476
memory used:3243.0
now epsilon is 0.8943944598747885, the reward is -5.0 with loss [49.31903839111328, 53.5980007648468] in episode 110
Report: 
rewardSum:-5.0
loss:[49.31903839111328, 53.5980007648468]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1488
memory used:3242.0
now epsilon is 0.8931877515599663, the reward is 54.6875 with loss [66.01137411594391, 113.683509349823] in episode 111
Report: 
rewardSum:54.6875
loss:[66.01137411594391, 113.683509349823]
policies:[0, 1, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1506
memory used:3242.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18*		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.8917150965924903, the reward is 240.25 with loss [71.2542268037796, 137.0377458333969] in episode 112
Report: 
rewardSum:240.25
loss:[71.2542268037796, 137.0377458333969]
policies:[2, 2, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1528
memory used:3243.0
now epsilon is 0.8913138849867832, the reward is -2.0 with loss [14.031338453292847, 31.303247451782227] in episode 113
Report: 
rewardSum:-2.0
loss:[14.031338453292847, 31.303247451782227]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1534
memory used:3242.0
now epsilon is 0.8901113329536674, the reward is 54.6875 with loss [88.2579870223999, 74.80190181732178] in episode 114
Report: 
rewardSum:54.6875
loss:[88.2579870223999, 74.80190181732178]
policies:[2, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1552
memory used:3242.0
now epsilon is 0.8897108429333492, the reward is -2.0 with loss [7.2716532945632935, 6.983930051326752] in episode 115
Report: 
rewardSum:-2.0
loss:[7.2716532945632935, 6.983930051326752]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1558
memory used:3242.0
now epsilon is 0.8887770668315612, the reward is 244.25 with loss [48.84097868204117, 77.55557811260223] in episode 116
Report: 
rewardSum:244.25
loss:[48.84097868204117, 77.55557811260223]
policies:[1, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1572
memory used:3243.0
now epsilon is 0.8882439205643684, the reward is -3.0 with loss [48.02529716491699, 72.15765380859375] in episode 117
Report: 
rewardSum:-3.0
loss:[48.02529716491699, 72.15765380859375]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1580
memory used:3242.0
now epsilon is 0.8877110941129682, the reward is -3.0 with loss [30.76967477798462, 37.20872288942337] in episode 118
Report: 
rewardSum:-3.0
loss:[30.76967477798462, 37.20872288942337]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1588
memory used:3243.0
now epsilon is 0.8859816145558762, the reward is 50.6875 with loss [85.86717510223389, 140.60316228866577] in episode 119
Report: 
rewardSum:50.6875
loss:[85.86717510223389, 140.60316228866577]
policies:[0, 0, 13]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1614
memory used:3249.0
now epsilon is 0.8842555044503528, the reward is 50.6875 with loss [83.99012649059296, 79.16140818595886] in episode 120
Report: 
rewardSum:50.6875
loss:[83.99012649059296, 79.16140818595886]
policies:[0, 1, 12]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1640
memory used:3249.0
now epsilon is 0.8829300161443462, the reward is 53.6875 with loss [58.025813937187195, 52.582532823085785] in episode 121
Report: 
rewardSum:53.6875
loss:[58.025813937187195, 52.582532823085785]
policies:[2, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1660
memory used:3250.0
now epsilon is 0.8821356770591058, the reward is -5.0 with loss [31.572286427021027, 37.08087503910065] in episode 122
Report: 
rewardSum:-5.0
loss:[31.572286427021027, 37.08087503910065]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1672
memory used:3250.0
now epsilon is 0.8813420526110062, the reward is 556.6875 with loss [54.867852210998535, 41.36164569854736] in episode 123
Report: 
rewardSum:556.6875
loss:[54.867852210998535, 41.36164569854736]
policies:[0, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1684
memory used:3249.0
now epsilon is 0.8808133663487191, the reward is 247.25 with loss [27.14409577846527, 50.685462951660156] in episode 124
Report: 
rewardSum:247.25
loss:[27.14409577846527, 50.685462951660156]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1692
memory used:3249.0
now epsilon is 0.8798889283943382, the reward is -6.0 with loss [53.45740461349487, 61.63093042373657] in episode 125
Report: 
rewardSum:-6.0
loss:[53.45740461349487, 61.63093042373657]
policies:[0, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1706
memory used:3251.0
now epsilon is 0.8796249815133208, the reward is -1.0 with loss [11.831974506378174, 19.437264442443848] in episode 126
Report: 
rewardSum:-1.0
loss:[11.831974506378174, 19.437264442443848]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1710
memory used:3251.0
now epsilon is 0.8789654606631216, the reward is -4.0 with loss [21.810067176818848, 37.52801012992859] in episode 127
Report: 
rewardSum:-4.0
loss:[21.810067176818848, 37.52801012992859]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1720
memory used:3251.0
############# STATE ###############
0-		8-		16-		24-		32*		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.877911255692473, the reward is 243.25 with loss [65.03104346990585, 54.469157457351685] in episode 128
Report: 
rewardSum:243.25
loss:[65.03104346990585, 54.469157457351685]
policies:[2, 0, 6]
qAverage:[11.361793518066406, 0.0]
ws:[5.116921424865723, 4.074538230895996]
memory len:1736
memory used:3250.0
now epsilon is 0.8769898635833766, the reward is -6.0 with loss [44.90630221366882, 52.96963566541672] in episode 129
Report: 
rewardSum:-6.0
loss:[44.90630221366882, 52.96963566541672]
policies:[0, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1750
memory used:3251.0
now epsilon is 0.8762008686310405, the reward is 245.25 with loss [36.31450128555298, 31.03567361831665] in episode 130
Report: 
rewardSum:245.25
loss:[36.31450128555298, 31.03567361831665]
policies:[1, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1762
memory used:3251.0
now epsilon is 0.8755439150951931, the reward is 246.25 with loss [66.61273431777954, 12.40756893157959] in episode 131
Report: 
rewardSum:246.25
loss:[66.61273431777954, 12.40756893157959]
policies:[1, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1772
memory used:3250.0
now epsilon is 0.8744938138242987, the reward is -7.0 with loss [57.58862268924713, 53.986504316329956] in episode 132
Report: 
rewardSum:-7.0
loss:[57.58862268924713, 53.986504316329956]
policies:[0, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1788
memory used:3251.0
now epsilon is 0.8742314853562623, the reward is -1.0 with loss [19.533323287963867, 14.34025239944458] in episode 133
Report: 
rewardSum:-1.0
loss:[19.533323287963867, 14.34025239944458]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1792
memory used:3250.0
now epsilon is 0.8735760084148263, the reward is 246.25 with loss [31.762969970703125, 35.32241129875183] in episode 134
Report: 
rewardSum:246.25
loss:[31.762969970703125, 35.32241129875183]
policies:[1, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1802
memory used:3251.0
now epsilon is 0.8730519807307457, the reward is -3.0 with loss [20.458068370819092, 31.407068729400635] in episode 135
Report: 
rewardSum:-3.0
loss:[20.458068370819092, 31.407068729400635]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1810
memory used:3251.0
now epsilon is 0.8718740674814092, the reward is 54.6875 with loss [74.20030748844147, 38.742852330207825] in episode 136
Report: 
rewardSum:54.6875
loss:[74.20030748844147, 38.742852330207825]
policies:[1, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1828
memory used:3250.0
now epsilon is 0.8706977434611833, the reward is 991.0 with loss [117.94256567955017, 65.90726566314697] in episode 137
Report: 
rewardSum:991.0
loss:[117.94256567955017, 65.90726566314697]
policies:[0, 1, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1846
memory used:3250.0
now epsilon is 0.8704365537288442, the reward is -1.0 with loss [24.55301856994629, 10.905447483062744] in episode 138
Report: 
rewardSum:-1.0
loss:[24.55301856994629, 10.905447483062744]
policies:[1, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1850
memory used:3250.0
now epsilon is 0.8700449160311959, the reward is -2.0 with loss [19.503745555877686, 27.134453773498535] in episode 139
Report: 
rewardSum:-2.0
loss:[19.503745555877686, 27.134453773498535]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1856
memory used:3249.0
now epsilon is 0.8692621691882056, the reward is -5.0 with loss [51.52013897895813, 31.99113690853119] in episode 140
Report: 
rewardSum:-5.0
loss:[51.52013897895813, 31.99113690853119]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1868
memory used:3249.0
now epsilon is 0.8674384973797539, the reward is 237.25 with loss [71.52824157476425, 98.03883290290833] in episode 141
Report: 
rewardSum:237.25
loss:[71.52824157476425, 98.03883290290833]
policies:[1, 0, 13]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1896
memory used:3252.0
now epsilon is 0.8669181513738131, the reward is 247.25 with loss [24.11469268798828, 37.131611824035645] in episode 142
Report: 
rewardSum:247.25
loss:[24.11469268798828, 37.131611824035645]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1904
memory used:3269.0
now epsilon is 0.8661382175639426, the reward is 245.25 with loss [31.54816198348999, 41.03169393539429] in episode 143
Report: 
rewardSum:245.25
loss:[31.54816198348999, 41.03169393539429]
policies:[0, 2, 4]
qAverage:[0.0, 52.54078674316406]
ws:[3.3254222869873047, 3.9545605182647705]
memory len:1916
memory used:3270.0
now epsilon is 0.8647101608688204, the reward is 551.6875 with loss [84.0786942243576, 42.21152937412262] in episode 144
Report: 
rewardSum:551.6875
loss:[84.0786942243576, 42.21152937412262]
policies:[0, 2, 9]
qAverage:[0.0, 27.410375595092773]
ws:[-17.88113784790039, -17.84230613708496]
memory len:1938
memory used:3270.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3*		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.8641914514964977, the reward is -3.0 with loss [46.960153102874756, 29.86521863937378] in episode 145
Report: 
rewardSum:-3.0
loss:[46.960153102874756, 29.86521863937378]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1946
memory used:3269.0
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.8631549660320149, the reward is 992.0 with loss [47.961859345436096, 38.01051312685013] in episode 146
Report: 
rewardSum:992.0
loss:[47.961859345436096, 38.01051312685013]
policies:[0, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1962
memory used:3269.0
now epsilon is 0.8621197236972995, the reward is 55.6875 with loss [57.63345444202423, 40.77802395820618] in episode 147
Report: 
rewardSum:55.6875
loss:[57.63345444202423, 40.77802395820618]
policies:[0, 1, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1978
memory used:3270.0
now epsilon is 0.8609565601429287, the reward is -8.0 with loss [31.658929347991943, 56.02923583984375] in episode 148
Report: 
rewardSum:-8.0
loss:[31.658929347991943, 56.02923583984375]
policies:[0, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:1996
memory used:3270.0
now epsilon is 0.8604401024243562, the reward is -3.0 with loss [26.517357349395752, 26.771550178527832] in episode 149
Report: 
rewardSum:-3.0
loss:[26.517357349395752, 26.771550178527832]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2004
memory used:3271.0
now epsilon is 0.8587637534667804, the reward is -12.0 with loss [116.63197469711304, 118.09593534469604] in episode 150
Report: 
rewardSum:-12.0
loss:[116.63197469711304, 118.09593534469604]
policies:[0, 2, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2030
memory used:3270.0
now epsilon is 0.8582486111362141, the reward is -3.0 with loss [14.647837281227112, 35.20025134086609] in episode 151
Report: 
rewardSum:-3.0
loss:[14.647837281227112, 35.20025134086609]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2038
memory used:3271.0
now epsilon is 0.8572192533372972, the reward is -7.0 with loss [58.083117961883545, 35.90270209312439] in episode 152
Report: 
rewardSum:-7.0
loss:[58.083117961883545, 35.90270209312439]
policies:[0, 2, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2054
memory used:3277.0
now epsilon is 0.856448045262936, the reward is -5.0 with loss [50.677557826042175, 64.97904777526855] in episode 153
Report: 
rewardSum:-5.0
loss:[50.677557826042175, 64.97904777526855]
policies:[0, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2066
memory used:3277.0
now epsilon is 0.8559342920447028, the reward is 247.25 with loss [34.8133749961853, 25.442489624023438] in episode 154
Report: 
rewardSum:247.25
loss:[34.8133749961853, 25.442489624023438]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2074
memory used:3277.0
now epsilon is 0.855677531015611, the reward is -1.0 with loss [19.88365936279297, 17.152393579483032] in episode 155
Report: 
rewardSum:-1.0
loss:[19.88365936279297, 17.152393579483032]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2078
memory used:3278.0
now epsilon is 0.8554208470090507, the reward is -1.0 with loss [22.735431671142578, 11.351613998413086] in episode 156
Report: 
rewardSum:-1.0
loss:[22.735431671142578, 11.351613998413086]
policies:[1, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2082
memory used:3278.0
now epsilon is 0.8533701449965585, the reward is 984.0 with loss [69.00710356235504, 106.1140695810318] in episode 157
Report: 
rewardSum:984.0
loss:[69.00710356235504, 106.1140695810318]
policies:[2, 0, 14]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2114
memory used:3278.0
now epsilon is 0.8528582381030101, the reward is -3.0 with loss [31.889004707336426, 40.542274713516235] in episode 158
Report: 
rewardSum:-3.0
loss:[31.889004707336426, 40.542274713516235]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2122
memory used:3278.0
now epsilon is 0.8520909534708115, the reward is 245.25 with loss [55.91860055923462, 55.54134917259216] in episode 159
Report: 
rewardSum:245.25
loss:[55.91860055923462, 55.54134917259216]
policies:[1, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2134
memory used:3277.0
now epsilon is 0.8515798139195051, the reward is -3.0 with loss [27.169604063034058, 34.651419162750244] in episode 160
Report: 
rewardSum:-3.0
loss:[27.169604063034058, 34.651419162750244]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2142
memory used:3277.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2*		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.8499207269832564, the reward is 50.6875 with loss [55.702169477939606, 44.86022675037384] in episode 161
Report: 
rewardSum:50.6875
loss:[55.702169477939606, 44.86022675037384]
policies:[1, 0, 12]
qAverage:[56.97600555419922, 0.0]
ws:[2.3751182556152344, 2.1244261264801025]
memory len:2168
memory used:3277.0
now epsilon is 0.8491560851198536, the reward is 245.25 with loss [32.75737202167511, 35.836580753326416] in episode 162
Report: 
rewardSum:245.25
loss:[32.75737202167511, 35.836580753326416]
policies:[0, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2180
memory used:3277.0
now epsilon is 0.8486467060933901, the reward is 247.25 with loss [21.48643183708191, 16.38701629638672] in episode 163
Report: 
rewardSum:247.25
loss:[21.48643183708191, 16.38701629638672]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2188
memory used:3278.0
now epsilon is 0.8481376326255831, the reward is 59.6875 with loss [36.6055588722229, 43.80457925796509] in episode 164
Report: 
rewardSum:59.6875
loss:[36.6055588722229, 43.80457925796509]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2196
memory used:3277.0
now epsilon is 0.8473745949454282, the reward is 245.25 with loss [44.222917795181274, 61.88925075531006] in episode 165
Report: 
rewardSum:245.25
loss:[44.222917795181274, 61.88925075531006]
policies:[1, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2208
memory used:3283.0
now epsilon is 0.8463582791173648, the reward is 55.6875 with loss [80.00297331809998, 67.86198949813843] in episode 166
Report: 
rewardSum:55.6875
loss:[80.00297331809998, 67.86198949813843]
policies:[1, 2, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2224
memory used:3283.0
now epsilon is 0.8455968422549558, the reward is 245.25 with loss [36.67184603214264, 26.80595636367798] in episode 167
Report: 
rewardSum:245.25
loss:[36.67184603214264, 26.80595636367798]
policies:[1, 2, 3]
qAverage:[0.0, 75.67149353027344]
ws:[6.7505574226379395, 8.574738502502441]
memory len:2236
memory used:3283.0
now epsilon is 0.8443293028159995, the reward is 990.0 with loss [64.58981204032898, 62.62120223045349] in episode 168
Report: 
rewardSum:990.0
loss:[64.58981204032898, 62.62120223045349]
policies:[1, 1, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2256
memory used:3284.0
now epsilon is 0.8431901419246197, the reward is 54.6875 with loss [48.33721661567688, 47.975807428359985] in episode 169
Report: 
rewardSum:54.6875
loss:[48.33721661567688, 47.975807428359985]
policies:[0, 1, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2274
memory used:3283.0
now epsilon is 0.8423051905833541, the reward is 244.25 with loss [34.78225290775299, 47.46915650367737] in episode 170
Report: 
rewardSum:244.25
loss:[34.78225290775299, 47.46915650367737]
policies:[0, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2288
memory used:3284.0
now epsilon is 0.8420525179780459, the reward is -1.0 with loss [6.356908321380615, 8.1010901927948] in episode 171
Report: 
rewardSum:-1.0
loss:[6.356908321380615, 8.1010901927948]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2292
memory used:3284.0
now epsilon is 0.8414211680229619, the reward is 246.25 with loss [30.52498984336853, 13.168129682540894] in episode 172
Report: 
rewardSum:246.25
loss:[30.52498984336853, 13.168129682540894]
policies:[1, 1, 3]
qAverage:[0.0, 67.8207778930664]
ws:[6.014922142028809, 6.487001419067383]
memory len:2302
memory used:3285.0
now epsilon is 0.8409164289026471, the reward is -3.0 with loss [33.92499303817749, 13.003263473510742] in episode 173
Report: 
rewardSum:-3.0
loss:[33.92499303817749, 13.003263473510742]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2310
memory used:3285.0
now epsilon is 0.8404119925576715, the reward is -3.0 with loss [11.934067964553833, 21.072585821151733] in episode 174
Report: 
rewardSum:-3.0
loss:[11.934067964553833, 21.072585821151733]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2318
memory used:3285.0
now epsilon is 0.8392781168632294, the reward is 242.25 with loss [66.49167346954346, 58.381025433540344] in episode 175
Report: 
rewardSum:242.25
loss:[66.49167346954346, 58.381025433540344]
policies:[2, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2336
memory used:3285.0
now epsilon is 0.8381457709828569, the reward is 242.25 with loss [64.35421752929688, 48.4066858291626] in episode 176
Report: 
rewardSum:242.25
loss:[64.35421752929688, 48.4066858291626]
policies:[1, 1, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2354
memory used:3285.0
now epsilon is 0.837517350209133, the reward is -4.0 with loss [53.672449827194214, 52.32766652107239] in episode 177
Report: 
rewardSum:-4.0
loss:[53.672449827194214, 52.32766652107239]
policies:[1, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2364
memory used:3285.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18*		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.8363873799380217, the reward is 242.25 with loss [41.390835762023926, 56.62372398376465] in episode 178
Report: 
rewardSum:242.25
loss:[41.390835762023926, 56.62372398376465]
policies:[1, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2382
memory used:3286.0
now epsilon is 0.8347578915999744, the reward is 238.25 with loss [84.54659867286682, 72.41713881492615] in episode 179
Report: 
rewardSum:238.25
loss:[84.54659867286682, 72.41713881492615]
policies:[4, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2408
memory used:3291.0
now epsilon is 0.8336316443636062, the reward is 54.6875 with loss [45.41924595832825, 46.189804553985596] in episode 180
Report: 
rewardSum:54.6875
loss:[45.41924595832825, 46.189804553985596]
policies:[1, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2426
memory used:3292.0
now epsilon is 0.8313837064062297, the reward is 544.6875 with loss [89.10469722747803, 115.74756336212158] in episode 181
Report: 
rewardSum:544.6875
loss:[89.10469722747803, 115.74756336212158]
policies:[2, 3, 13]
qAverage:[0.0, 55.00044250488281]
ws:[10.092220306396484, 11.511571884155273]
memory len:2462
memory used:3291.0
now epsilon is 0.8308849884079631, the reward is 59.6875 with loss [18.01823365688324, 27.101218700408936] in episode 182
Report: 
rewardSum:59.6875
loss:[18.01823365688324, 27.101218700408936]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2470
memory used:3291.0
now epsilon is 0.8300129516651582, the reward is 993.0 with loss [47.63955998420715, 49.556969165802] in episode 183
Report: 
rewardSum:993.0
loss:[47.63955998420715, 49.556969165802]
policies:[0, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2484
memory used:3291.0
now epsilon is 0.8296395018599819, the reward is -2.0 with loss [17.44316816329956, 9.575889468193054] in episode 184
Report: 
rewardSum:-2.0
loss:[17.44316816329956, 9.575889468193054]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2490
memory used:3291.0
now epsilon is 0.8286444569738637, the reward is 992.0 with loss [46.263582706451416, 58.18573808670044] in episode 185
Report: 
rewardSum:992.0
loss:[46.263582706451416, 58.18573808670044]
policies:[0, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2506
memory used:3291.0
now epsilon is 0.8275264579240913, the reward is 242.25 with loss [67.50973796844482, 55.05349278450012] in episode 186
Report: 
rewardSum:242.25
loss:[67.50973796844482, 55.05349278450012]
policies:[2, 1, 6]
qAverage:[77.86609395345052, 0.0]
ws:[10.994564056396484, 10.31758975982666]
memory len:2524
memory used:3292.0
now epsilon is 0.8271541268732684, the reward is -2.0 with loss [26.71161937713623, 21.806039094924927] in episode 187
Report: 
rewardSum:-2.0
loss:[26.71161937713623, 21.806039094924927]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2530
memory used:3293.0
now epsilon is 0.8266579460517854, the reward is 59.6875 with loss [31.54980754852295, 24.355950951576233] in episode 188
Report: 
rewardSum:59.6875
loss:[31.54980754852295, 24.355950951576233]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2538
memory used:3292.0
now epsilon is 0.8264099672677737, the reward is -1.0 with loss [6.0473313331604, 6.683339834213257] in episode 189
Report: 
rewardSum:-1.0
loss:[6.0473313331604, 6.683339834213257]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2542
memory used:3293.0
now epsilon is 0.8254187957891695, the reward is 243.25 with loss [39.613733649253845, 37.371652364730835] in episode 190
Report: 
rewardSum:243.25
loss:[39.613733649253845, 37.371652364730835]
policies:[0, 1, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2558
memory used:3292.0
now epsilon is 0.8251711887223557, the reward is -1.0 with loss [14.92576003074646, 16.552993655204773] in episode 191
Report: 
rewardSum:-1.0
loss:[14.92576003074646, 16.552993655204773]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2562
memory used:3292.0
now epsilon is 0.82418150299781, the reward is 55.6875 with loss [50.960243463516235, 36.526909947395325] in episode 192
Report: 
rewardSum:55.6875
loss:[50.960243463516235, 36.526909947395325]
policies:[0, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2578
memory used:3292.0
now epsilon is 0.8235635522835858, the reward is 246.25 with loss [33.900649070739746, 42.22222375869751] in episode 193
Report: 
rewardSum:246.25
loss:[33.900649070739746, 42.22222375869751]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2588
memory used:3294.0
############# STATE ###############
0-		8-		16-		24-		32*		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.8223290404798016, the reward is 241.25 with loss [60.80209803581238, 47.21729135513306] in episode 194
Report: 
rewardSum:241.25
loss:[60.80209803581238, 47.21729135513306]
policies:[0, 2, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2608
memory used:3292.0
now epsilon is 0.8212195621285989, the reward is -8.0 with loss [76.3429024219513, 59.094929456710815] in episode 195
Report: 
rewardSum:-8.0
loss:[76.3429024219513, 59.094929456710815]
policies:[0, 1, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2626
memory used:3294.0
now epsilon is 0.8206038322036899, the reward is -4.0 with loss [32.68873453140259, 27.146989107131958] in episode 196
Report: 
rewardSum:-4.0
loss:[32.68873453140259, 27.146989107131958]
policies:[0, 2, 3]
qAverage:[0.0, 76.21580505371094]
ws:[8.416295051574707, 9.06298828125]
memory len:2636
memory used:3294.0
now epsilon is 0.8187594109982125, the reward is 236.25 with loss [110.96772980690002, 94.4098060131073] in episode 197
Report: 
rewardSum:236.25
loss:[110.96772980690002, 94.4098060131073]
policies:[2, 3, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2666
memory used:3294.0
now epsilon is 0.8179001003837847, the reward is -6.0 with loss [39.77336859703064, 64.56249618530273] in episode 198
Report: 
rewardSum:-6.0
loss:[39.77336859703064, 64.56249618530273]
policies:[0, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2680
memory used:3293.0
now epsilon is 0.8174094707290268, the reward is 59.6875 with loss [24.52026653289795, 17.652647018432617] in episode 199
Report: 
rewardSum:59.6875
loss:[24.52026653289795, 17.652647018432617]
policies:[0, 1, 3]
qAverage:[0.0, 74.53585815429688]
ws:[1.9508357048034668, 2.7899889945983887]
memory len:2688
memory used:3294.0
now epsilon is 0.8156945834826139, the reward is 49.6875 with loss [100.06268811225891, 86.60429990291595] in episode 200
Report: 
rewardSum:49.6875
loss:[100.06268811225891, 86.60429990291595]
policies:[2, 0, 12]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2716
memory used:3294.0
now epsilon is 0.8153275759766782, the reward is -2.0 with loss [12.916505336761475, 24.137796878814697] in episode 201
Report: 
rewardSum:-2.0
loss:[12.916505336761475, 24.137796878814697]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2722
memory used:3299.0
now epsilon is 0.8147162637158851, the reward is 246.25 with loss [20.79042387008667, 35.2118958234787] in episode 202
Report: 
rewardSum:246.25
loss:[20.79042387008667, 35.2118958234787]
policies:[1, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2732
memory used:3293.0
now epsilon is 0.8144718671678863, the reward is -1.0 with loss [21.724984169006348, 13.042368412017822] in episode 203
Report: 
rewardSum:-1.0
loss:[21.724984169006348, 13.042368412017822]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2736
memory used:3294.0
now epsilon is 0.8127631428778159, the reward is 237.25 with loss [70.86453706026077, 83.27285867929459] in episode 204
Report: 
rewardSum:237.25
loss:[70.86453706026077, 83.27285867929459]
policies:[2, 2, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2764
memory used:3294.0
now epsilon is 0.8125193322221234, the reward is -1.0 with loss [16.432013511657715, 12.771478176116943] in episode 205
Report: 
rewardSum:-1.0
loss:[16.432013511657715, 12.771478176116943]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2768
memory used:3294.0
now epsilon is 0.8115448207570989, the reward is 243.25 with loss [45.674793124198914, 36.30564731359482] in episode 206
Report: 
rewardSum:243.25
loss:[45.674793124198914, 36.30564731359482]
policies:[2, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2784
memory used:3294.0
now epsilon is 0.8109363447117282, the reward is -4.0 with loss [39.26746702194214, 33.86016392707825] in episode 207
Report: 
rewardSum:-4.0
loss:[39.26746702194214, 33.86016392707825]
policies:[1, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2794
memory used:3296.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36*		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.809720760939367, the reward is 241.25 with loss [56.763527393341064, 79.3094470500946] in episode 208
Report: 
rewardSum:241.25
loss:[56.763527393341064, 79.3094470500946]
policies:[0, 0, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2814
memory used:3296.0
now epsilon is 0.8089922854806285, the reward is 57.6875 with loss [36.49499583244324, 20.123945832252502] in episode 209
Report: 
rewardSum:57.6875
loss:[36.49499583244324, 20.123945832252502]
policies:[0, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2826
memory used:3294.0
now epsilon is 0.8085069993123777, the reward is 247.25 with loss [25.083890438079834, 20.93692111968994] in episode 210
Report: 
rewardSum:247.25
loss:[25.083890438079834, 20.93692111968994]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2834
memory used:3293.0
now epsilon is 0.8082644654039914, the reward is -1.0 with loss [19.016778469085693, 14.721999168395996] in episode 211
Report: 
rewardSum:-1.0
loss:[19.016778469085693, 14.721999168395996]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2838
memory used:3294.0
now epsilon is 0.8072950570993866, the reward is -7.0 with loss [58.21079504489899, 64.87847352027893] in episode 212
Report: 
rewardSum:-7.0
loss:[58.21079504489899, 64.87847352027893]
policies:[0, 1, 7]
qAverage:[0.0, 71.08245086669922]
ws:[5.899023532867432, 6.42087459564209]
memory len:2854
memory used:3293.0
now epsilon is 0.8066897674207059, the reward is -4.0 with loss [45.718337297439575, 36.42153000831604] in episode 213
Report: 
rewardSum:-4.0
loss:[45.718337297439575, 36.42153000831604]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2864
memory used:3294.0
now epsilon is 0.8058431242305535, the reward is 244.25 with loss [44.336755871772766, 50.360111236572266] in episode 214
Report: 
rewardSum:244.25
loss:[44.336755871772766, 50.360111236572266]
policies:[0, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2878
memory used:3293.0
now epsilon is 0.8052389231748884, the reward is 58.6875 with loss [31.698960781097412, 30.65876007080078] in episode 215
Report: 
rewardSum:58.6875
loss:[31.698960781097412, 30.65876007080078]
policies:[1, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2888
memory used:3294.0
now epsilon is 0.8035495691740886, the reward is 237.25 with loss [90.79531198740005, 80.67277908325195] in episode 216
Report: 
rewardSum:237.25
loss:[90.79531198740005, 80.67277908325195]
policies:[1, 1, 12]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2916
memory used:3294.0
now epsilon is 0.8028266457055782, the reward is 556.6875 with loss [25.55895960330963, 21.01642119884491] in episode 217
Report: 
rewardSum:556.6875
loss:[25.55895960330963, 21.01642119884491]
policies:[0, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2928
memory used:3294.0
now epsilon is 0.8016232182739391, the reward is 241.25 with loss [57.388930797576904, 92.78543376922607] in episode 218
Report: 
rewardSum:241.25
loss:[57.388930797576904, 92.78543376922607]
policies:[1, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2948
memory used:3294.0
now epsilon is 0.8011423525512877, the reward is 247.25 with loss [23.693235397338867, 13.218260288238525] in episode 219
Report: 
rewardSum:247.25
loss:[23.693235397338867, 13.218260288238525]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2956
memory used:3295.0
now epsilon is 0.8006617752831596, the reward is 247.25 with loss [21.083621740341187, 36.48529529571533] in episode 220
Report: 
rewardSum:247.25
loss:[21.083621740341187, 36.48529529571533]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2964
memory used:3295.0
now epsilon is 0.799581530195629, the reward is -8.0 with loss [50.88566279411316, 49.69181215763092] in episode 221
Report: 
rewardSum:-8.0
loss:[50.88566279411316, 49.69181215763092]
policies:[0, 1, 8]
qAverage:[0.0, 84.05667114257812]
ws:[4.073278903961182, 4.9825615882873535]
memory len:2982
memory used:3294.0
now epsilon is 0.7987423472967603, the reward is 993.0 with loss [30.616613626480103, 43.93526875972748] in episode 222
Report: 
rewardSum:993.0
loss:[30.616613626480103, 43.93526875972748]
policies:[0, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:2996
memory used:3295.0
now epsilon is 0.7975450421790362, the reward is 241.25 with loss [69.71152591705322, 78.48612213134766] in episode 223
Report: 
rewardSum:241.25
loss:[69.71152591705322, 78.48612213134766]
policies:[2, 2, 6]
qAverage:[0.0, 95.35926818847656]
ws:[9.940229415893555, 11.369441986083984]
memory len:3016
memory used:3293.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20*		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.7969470628181214, the reward is 246.25 with loss [61.808908462524414, 36.26294159889221] in episode 224
Report: 
rewardSum:246.25
loss:[61.808908462524414, 36.26294159889221]
policies:[2, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3026
memory used:3294.0
now epsilon is 0.7964690021575258, the reward is -3.0 with loss [15.171318769454956, 28.38195049762726] in episode 225
Report: 
rewardSum:-3.0
loss:[15.171318769454956, 28.38195049762726]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3034
memory used:3296.0
now epsilon is 0.7955137409799039, the reward is 554.6875 with loss [44.24523115158081, 24.40022325515747] in episode 226
Report: 
rewardSum:554.6875
loss:[44.24523115158081, 24.40022325515747]
policies:[0, 2, 6]
qAverage:[0.0, 78.79966735839844]
ws:[2.52498722076416, 5.315171718597412]
memory len:3050
memory used:3295.0
now epsilon is 0.795036540118932, the reward is -3.0 with loss [23.755998611450195, 26.395117044448853] in episode 227
Report: 
rewardSum:-3.0
loss:[23.755998611450195, 26.395117044448853]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3058
memory used:3295.0
now epsilon is 0.7930116279297903, the reward is 234.25 with loss [100.47197723388672, 98.26179730892181] in episode 228
Report: 
rewardSum:234.25
loss:[100.47197723388672, 98.26179730892181]
policies:[2, 2, 13]
qAverage:[0.0, 112.55082702636719]
ws:[11.340680758158365, 12.562082926432291]
memory len:3092
memory used:3293.0
now epsilon is 0.792535927998897, the reward is 247.25 with loss [23.27617120742798, 21.984370708465576] in episode 229
Report: 
rewardSum:247.25
loss:[23.27617120742798, 21.984370708465576]
policies:[0, 2, 2]
qAverage:[0.0, 94.90589141845703]
ws:[11.897584915161133, 12.850465774536133]
memory len:3100
memory used:3293.0
now epsilon is 0.7911105396560929, the reward is 239.25 with loss [79.07532727718353, 86.97256362438202] in episode 230
Report: 
rewardSum:239.25
loss:[79.07532727718353, 86.97256362438202]
policies:[1, 0, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3124
memory used:3294.0
now epsilon is 0.7906359801215427, the reward is -3.0 with loss [9.837310791015625, 24.754491806030273] in episode 231
Report: 
rewardSum:-3.0
loss:[9.837310791015625, 24.754491806030273]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3132
memory used:3294.0
now epsilon is 0.7892140088649057, the reward is 550.6875 with loss [94.3802250623703, 59.846625089645386] in episode 232
Report: 
rewardSum:550.6875
loss:[94.3802250623703, 59.846625089645386]
policies:[0, 1, 11]
qAverage:[0.0, 85.70555114746094]
ws:[5.6166157722473145, 5.779694557189941]
memory len:3156
memory used:3301.0
now epsilon is 0.7887405869928239, the reward is 247.25 with loss [39.78858947753906, 31.787282466888428] in episode 233
Report: 
rewardSum:247.25
loss:[39.78858947753906, 31.787282466888428]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3164
memory used:3300.0
now epsilon is 0.7872039263268716, the reward is 238.25 with loss [82.89170289039612, 72.05612647533417] in episode 234
Report: 
rewardSum:238.25
loss:[82.89170289039612, 72.05612647533417]
policies:[0, 1, 12]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3190
memory used:3300.0
now epsilon is 0.7857881276730124, the reward is 239.25 with loss [79.0147294998169, 88.41773593425751] in episode 235
Report: 
rewardSum:239.25
loss:[79.0147294998169, 88.41773593425751]
policies:[0, 1, 11]
qAverage:[0.0, 91.07539367675781]
ws:[7.1074676513671875, 8.57454776763916]
memory len:3214
memory used:3299.0
now epsilon is 0.7849634213310388, the reward is 56.6875 with loss [41.59755480289459, 40.82987582683563] in episode 236
Report: 
rewardSum:56.6875
loss:[41.59755480289459, 40.82987582683563]
policies:[1, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3228
memory used:3299.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3*		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.7843748753553198, the reward is -4.0 with loss [17.88012135028839, 28.046690225601196] in episode 237
Report: 
rewardSum:-4.0
loss:[17.88012135028839, 28.046690225601196]
policies:[0, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3238
memory used:3297.0
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.7834341195128458, the reward is 55.6875 with loss [38.58961170911789, 58.25753402709961] in episode 238
Report: 
rewardSum:55.6875
loss:[38.58961170911789, 58.25753402709961]
policies:[3, 0, 5]
qAverage:[78.9710922241211, 0.0]
ws:[7.5979905128479, 7.399425983428955]
memory len:3254
memory used:3299.0
now epsilon is 0.7831991069042596, the reward is -1.0 with loss [23.41939926147461, 18.368693351745605] in episode 239
Report: 
rewardSum:-1.0
loss:[23.41939926147461, 18.368693351745605]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3258
memory used:3298.0
now epsilon is 0.7827292931614238, the reward is 247.25 with loss [26.06357765197754, 15.212484002113342] in episode 240
Report: 
rewardSum:247.25
loss:[26.06357765197754, 15.212484002113342]
policies:[1, 1, 2]
qAverage:[0.0, 108.68061065673828]
ws:[16.62713623046875, 17.417142868041992]
memory len:3266
memory used:3299.0
now epsilon is 0.7822597612434151, the reward is 247.25 with loss [12.506015181541443, 16.91329264640808] in episode 241
Report: 
rewardSum:247.25
loss:[12.506015181541443, 16.91329264640808]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3274
memory used:3298.0
now epsilon is 0.7817905109811767, the reward is -3.0 with loss [22.06928777694702, 12.166320443153381] in episode 242
Report: 
rewardSum:-3.0
loss:[22.06928777694702, 12.166320443153381]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3282
memory used:3298.0
now epsilon is 0.7807357268200783, the reward is 242.25 with loss [60.87786066532135, 76.53484809398651] in episode 243
Report: 
rewardSum:242.25
loss:[60.87786066532135, 76.53484809398651]
policies:[1, 1, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3300
memory used:3301.0
now epsilon is 0.7793315613248558, the reward is 550.6875 with loss [87.30417740345001, 53.82236385345459] in episode 244
Report: 
rewardSum:550.6875
loss:[87.30417740345001, 53.82236385345459]
policies:[0, 1, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3324
memory used:3307.0
now epsilon is 0.7774632683006731, the reward is 546.6875 with loss [114.5874205827713, 119.93915796279907] in episode 245
Report: 
rewardSum:546.6875
loss:[114.5874205827713, 119.93915796279907]
policies:[3, 1, 12]
qAverage:[84.18877410888672, 0.0]
ws:[6.468469619750977, 5.927810192108154]
memory len:3356
memory used:3307.0
now epsilon is 0.7769968952867387, the reward is 59.6875 with loss [15.191874265670776, 24.819791793823242] in episode 246
Report: 
rewardSum:59.6875
loss:[15.191874265670776, 24.819791793823242]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3364
memory used:3314.0
now epsilon is 0.7760649883736138, the reward is 243.25 with loss [61.17238759994507, 34.73584771156311] in episode 247
Report: 
rewardSum:243.25
loss:[61.17238759994507, 34.73584771156311]
policies:[1, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3380
memory used:3319.0
now epsilon is 0.7755994541388866, the reward is 59.6875 with loss [26.698521852493286, 20.58231282234192] in episode 248
Report: 
rewardSum:59.6875
loss:[26.698521852493286, 20.58231282234192]
policies:[1, 0, 3]
qAverage:[97.01699829101562, 0.0]
ws:[11.441658973693848, 11.370823860168457]
memory len:3388
memory used:3320.0
now epsilon is 0.7751341991618595, the reward is 59.6875 with loss [15.910625457763672, 20.54957914352417] in episode 249
Report: 
rewardSum:59.6875
loss:[15.910625457763672, 20.54957914352417]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3396
memory used:3319.0
now epsilon is 0.7745530228915242, the reward is -4.0 with loss [17.235935926437378, 16.495088815689087] in episode 250
Report: 
rewardSum:-4.0
loss:[17.235935926437378, 16.495088815689087]
policies:[0, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3406
memory used:3319.0
now epsilon is 0.7740883956319913, the reward is 247.25 with loss [14.001362800598145, 30.111881732940674] in episode 251
Report: 
rewardSum:247.25
loss:[14.001362800598145, 30.111881732940674]
policies:[0, 1, 3]
qAverage:[0.0, 93.94661712646484]
ws:[12.10431957244873, 13.763433456420898]
memory len:3414
memory used:3320.0
now epsilon is 0.7732759684819193, the reward is 56.6875 with loss [38.084223985672, 46.18828725814819] in episode 252
Report: 
rewardSum:56.6875
loss:[38.084223985672, 46.18828725814819]
policies:[1, 2, 4]
qAverage:[0.0, 87.86282348632812]
ws:[8.842312812805176, 10.144052505493164]
memory len:3428
memory used:3320.0
now epsilon is 0.7728121072826472, the reward is 247.25 with loss [30.746455192565918, 40.76465892791748] in episode 253
Report: 
rewardSum:247.25
loss:[30.746455192565918, 40.76465892791748]
policies:[2, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3436
memory used:3318.0
now epsilon is 0.7723485243374795, the reward is 59.6875 with loss [28.648593425750732, 31.3016996383667] in episode 254
Report: 
rewardSum:59.6875
loss:[28.648593425750732, 31.3016996383667]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3444
memory used:3319.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17*		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.7707281728715356, the reward is 237.25 with loss [107.35963237285614, 77.04624915122986] in episode 255
Report: 
rewardSum:237.25
loss:[107.35963237285614, 77.04624915122986]
policies:[0, 5, 9]
qAverage:[0.0, 139.7557487487793]
ws:[12.010613441467285, 14.561975479125977]
memory len:3472
memory used:3319.0
now epsilon is 0.770381397215294, the reward is -2.0 with loss [15.127909183502197, 17.106348872184753] in episode 256
Report: 
rewardSum:-2.0
loss:[15.127909183502197, 17.106348872184753]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3478
memory used:3320.0
now epsilon is 0.7700347775846914, the reward is -2.0 with loss [16.97511053085327, 27.227402687072754] in episode 257
Report: 
rewardSum:-2.0
loss:[16.97511053085327, 27.227402687072754]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3484
memory used:3319.0
now epsilon is 0.7694574247333411, the reward is 246.25 with loss [17.887311458587646, 30.1930513381958] in episode 258
Report: 
rewardSum:246.25
loss:[17.887311458587646, 30.1930513381958]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3494
memory used:3319.0
now epsilon is 0.7687651726910295, the reward is 994.0 with loss [51.17051029205322, 41.839900732040405] in episode 259
Report: 
rewardSum:994.0
loss:[51.17051029205322, 41.839900732040405]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3506
memory used:3321.0
now epsilon is 0.7680735434419677, the reward is 57.6875 with loss [46.26050519943237, 30.718507766723633] in episode 260
Report: 
rewardSum:57.6875
loss:[46.26050519943237, 30.718507766723633]
policies:[0, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3518
memory used:3321.0
now epsilon is 0.767267429045388, the reward is 993.0 with loss [64.10583972930908, 40.625648617744446] in episode 261
Report: 
rewardSum:993.0
loss:[64.10583972930908, 40.625648617744446]
policies:[0, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3532
memory used:3321.0
now epsilon is 0.766807172158706, the reward is 247.25 with loss [35.8102445602417, 32.17338287830353] in episode 262
Report: 
rewardSum:247.25
loss:[35.8102445602417, 32.17338287830353]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3540
memory used:3321.0
now epsilon is 0.7663471913640276, the reward is 59.6875 with loss [22.959449768066406, 28.262287378311157] in episode 263
Report: 
rewardSum:59.6875
loss:[22.959449768066406, 28.262287378311157]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3548
memory used:3320.0
now epsilon is 0.7658874864957348, the reward is 247.25 with loss [28.851873636245728, 25.111273288726807] in episode 264
Report: 
rewardSum:247.25
loss:[28.851873636245728, 25.111273288726807]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3556
memory used:3319.0
now epsilon is 0.7649689038763309, the reward is 243.25 with loss [52.64906454086304, 57.441577434539795] in episode 265
Report: 
rewardSum:243.25
loss:[52.64906454086304, 57.441577434539795]
policies:[1, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3572
memory used:3320.0
now epsilon is 0.7645100257944805, the reward is 59.6875 with loss [23.0733380317688, 20.277090668678284] in episode 266
Report: 
rewardSum:59.6875
loss:[23.0733380317688, 20.277090668678284]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3580
memory used:3316.0
now epsilon is 0.7637076514080893, the reward is 244.25 with loss [56.3930447101593, 22.66607415676117] in episode 267
Report: 
rewardSum:244.25
loss:[56.3930447101593, 22.66607415676117]
policies:[0, 2, 5]
qAverage:[0.0, 101.71533203125]
ws:[5.575942516326904, 6.453384876251221]
memory len:3594
memory used:3323.0
now epsilon is 0.7629061191357768, the reward is -6.0 with loss [40.919188261032104, 26.50723695755005] in episode 268
Report: 
rewardSum:-6.0
loss:[40.919188261032104, 26.50723695755005]
policies:[0, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3608
memory used:3323.0
now epsilon is 0.7622197610578796, the reward is 245.25 with loss [38.248876094818115, 32.224631667137146] in episode 269
Report: 
rewardSum:245.25
loss:[38.248876094818115, 32.224631667137146]
policies:[1, 3, 2]
qAverage:[0.0, 84.00440216064453]
ws:[7.656010150909424, 9.52696704864502]
memory len:3620
memory used:3324.0
now epsilon is 0.7614197903675824, the reward is 244.25 with loss [25.137456834316254, 20.635955095291138] in episode 270
Report: 
rewardSum:244.25
loss:[25.137456834316254, 20.635955095291138]
policies:[0, 2, 5]
qAverage:[0.0, 134.12749735514322]
ws:[20.540807406107586, 21.75401433308919]
memory len:3634
memory used:3324.0
now epsilon is 0.7602784313112748, the reward is 241.25 with loss [59.56107318401337, 57.18335950374603] in episode 271
Report: 
rewardSum:241.25
loss:[59.56107318401337, 57.18335950374603]
policies:[3, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3654
memory used:3325.0
now epsilon is 0.7597083935247809, the reward is -4.0 with loss [38.29744392633438, 45.72296857833862] in episode 272
Report: 
rewardSum:-4.0
loss:[38.29744392633438, 45.72296857833862]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3664
memory used:3324.0
############# STATE ###############
0-		8-		16-		24*		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.7572053004647521, the reward is 229.25 with loss [133.05341017246246, 135.96174424886703] in episode 273
Report: 
rewardSum:229.25
loss:[133.05341017246246, 135.96174424886703]
policies:[2, 3, 17]
qAverage:[0.0, 122.35071309407552]
ws:[11.548501968383789, 13.160996119181315]
memory len:3708
memory used:3325.0
now epsilon is 0.7558434548117517, the reward is 239.25 with loss [69.02056366205215, 97.14993190765381] in episode 274
Report: 
rewardSum:239.25
loss:[69.02056366205215, 97.14993190765381]
policies:[3, 1, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3732
memory used:3325.0
now epsilon is 0.7543708858508277, the reward is 238.25 with loss [65.53816986083984, 77.66039031744003] in episode 275
Report: 
rewardSum:238.25
loss:[65.53816986083984, 77.66039031744003]
policies:[3, 2, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3758
memory used:3331.0
now epsilon is 0.7539183651492032, the reward is 247.25 with loss [18.663190603256226, 28.389197826385498] in episode 276
Report: 
rewardSum:247.25
loss:[18.663190603256226, 28.389197826385498]
policies:[3, 1, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3766
memory used:3331.0
now epsilon is 0.753127107003181, the reward is 56.6875 with loss [27.3508842587471, 47.326568245887756] in episode 277
Report: 
rewardSum:56.6875
loss:[27.3508842587471, 47.326568245887756]
policies:[1, 1, 5]
qAverage:[82.6618881225586, 0.0]
ws:[5.556532382965088, 4.981571674346924]
memory len:3780
memory used:3344.0
now epsilon is 0.752336679304436, the reward is 244.25 with loss [51.66441881656647, 50.356436252593994] in episode 278
Report: 
rewardSum:244.25
loss:[51.66441881656647, 50.356436252593994]
policies:[1, 1, 5]
qAverage:[78.22443389892578, 0.0]
ws:[7.811854839324951, 6.843761444091797]
memory len:3794
memory used:3344.0
now epsilon is 0.751547081181391, the reward is 244.25 with loss [21.71800458431244, 58.27944588661194] in episode 279
Report: 
rewardSum:244.25
loss:[21.71800458431244, 58.27944588661194]
policies:[2, 2, 3]
qAverage:[88.14306259155273, 44.817138671875]
ws:[20.689058780670166, 20.397122383117676]
memory len:3808
memory used:3344.0
now epsilon is 0.7496329341345517, the reward is 234.25 with loss [80.40387713909149, 102.98882484436035] in episode 280
Report: 
rewardSum:234.25
loss:[80.40387713909149, 102.98882484436035]
policies:[4, 0, 13]
qAverage:[93.76092529296875, 0.0]
ws:[6.673574924468994, 6.473641395568848]
memory len:3842
memory used:3345.0
now epsilon is 0.7486215366636737, the reward is 991.0 with loss [39.679768800735474, 37.92483627796173] in episode 281
Report: 
rewardSum:991.0
loss:[39.679768800735474, 37.92483627796173]
policies:[1, 2, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3860
memory used:3344.0
now epsilon is 0.7477236623097826, the reward is 243.25 with loss [44.03789496421814, 34.85047245025635] in episode 282
Report: 
rewardSum:243.25
loss:[44.03789496421814, 34.85047245025635]
policies:[2, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3876
memory used:3344.0
now epsilon is 0.7474993620348721, the reward is -1.0 with loss [2.450691819190979, 11.837939500808716] in episode 283
Report: 
rewardSum:-1.0
loss:[2.450691819190979, 11.837939500808716]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3880
memory used:3344.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2*		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.7458192532901744, the reward is 547.6875 with loss [74.74337255954742, 77.89169788360596] in episode 284
Report: 
rewardSum:547.6875
loss:[74.74337255954742, 77.89169788360596]
policies:[1, 1, 13]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3910
memory used:3344.0
now epsilon is 0.7455955242951205, the reward is -1.0 with loss [9.919206500053406, 9.294443130493164] in episode 285
Report: 
rewardSum:-1.0
loss:[9.919206500053406, 9.294443130493164]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3914
memory used:3344.0
now epsilon is 0.7433619222762508, the reward is -19.0 with loss [117.90725082159042, 116.53553700447083] in episode 286
Report: 
rewardSum:-19.0
loss:[117.90725082159042, 116.53553700447083]
policies:[1, 3, 16]
qAverage:[0.0, 75.43363952636719]
ws:[2.604288339614868, 3.797726631164551]
memory len:3954
memory used:3345.0
now epsilon is 0.7425817434085726, the reward is 244.25 with loss [56.156904220581055, 39.287434101104736] in episode 287
Report: 
rewardSum:244.25
loss:[56.156904220581055, 39.287434101104736]
policies:[0, 3, 4]
qAverage:[0.0, 123.63479105631511]
ws:[14.906052112579346, 16.37916374206543]
memory len:3968
memory used:3345.0
now epsilon is 0.7416911130026591, the reward is 55.6875 with loss [43.50947904586792, 54.06380558013916] in episode 288
Report: 
rewardSum:55.6875
loss:[43.50947904586792, 54.06380558013916]
policies:[0, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3984
memory used:3345.0
now epsilon is 0.7410238412716489, the reward is 245.25 with loss [30.97714453935623, 24.064720511436462] in episode 289
Report: 
rewardSum:245.25
loss:[30.97714453935623, 24.064720511436462]
policies:[2, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:3996
memory used:3344.0
now epsilon is 0.740579326995101, the reward is 247.25 with loss [20.998146057128906, 36.78272092342377] in episode 290
Report: 
rewardSum:247.25
loss:[20.998146057128906, 36.78272092342377]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4004
memory used:3343.0
now epsilon is 0.7398020685380204, the reward is 56.6875 with loss [30.575897455215454, 63.03397607803345] in episode 291
Report: 
rewardSum:56.6875
loss:[30.575897455215454, 63.03397607803345]
policies:[1, 3, 3]
qAverage:[0.0, 84.8079833984375]
ws:[3.359210252761841, 4.7559051513671875]
memory len:4018
memory used:3343.0
now epsilon is 0.7385823102181391, the reward is 52.6875 with loss [52.78037083148956, 78.11976063251495] in episode 292
Report: 
rewardSum:52.6875
loss:[52.78037083148956, 78.11976063251495]
policies:[2, 2, 7]
qAverage:[0.0, 75.90116119384766]
ws:[6.185622692108154, 6.763656139373779]
memory len:4040
memory used:3343.0
now epsilon is 0.7383607521431758, the reward is -1.0 with loss [6.16462254524231, 12.412047386169434] in episode 293
Report: 
rewardSum:-1.0
loss:[6.16462254524231, 12.412047386169434]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4044
memory used:3343.0
now epsilon is 0.7379178353606239, the reward is 59.6875 with loss [24.454728364944458, 24.04450750350952] in episode 294
Report: 
rewardSum:59.6875
loss:[24.454728364944458, 24.04450750350952]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4052
memory used:3344.0
now epsilon is 0.7372539583062651, the reward is 245.25 with loss [34.51572775840759, 22.782373189926147] in episode 295
Report: 
rewardSum:245.25
loss:[34.51572775840759, 22.782373189926147]
policies:[0, 1, 5]
qAverage:[0.0, 97.95018005371094]
ws:[21.758546829223633, 22.58892822265625]
memory len:4064
memory used:3344.0
now epsilon is 0.7362592624292934, the reward is 242.25 with loss [34.37363541126251, 47.640302777290344] in episode 296
Report: 
rewardSum:242.25
loss:[34.37363541126251, 47.640302777290344]
policies:[1, 2, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4082
memory used:3346.0
now epsilon is 0.7354865379992868, the reward is 244.25 with loss [59.11535656452179, 37.08191895484924] in episode 297
Report: 
rewardSum:244.25
loss:[59.11535656452179, 37.08191895484924]
policies:[1, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4096
memory used:3346.0
############# STATE ###############
0-		8-		16*		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.7350453453572412, the reward is 247.25 with loss [34.62415051460266, 22.185346841812134] in episode 298
Report: 
rewardSum:247.25
loss:[34.62415051460266, 22.185346841812134]
policies:[1, 2, 1]
qAverage:[0.0, 87.251953125]
ws:[2.262197256088257, 2.4064316749572754]
memory len:4104
memory used:3345.0
now epsilon is 0.7346044173712258, the reward is 247.25 with loss [15.610125303268433, 34.00810194015503] in episode 299
Report: 
rewardSum:247.25
loss:[15.610125303268433, 34.00810194015503]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4112
memory used:3345.0
now epsilon is 0.7339435212750025, the reward is 57.6875 with loss [49.41298866271973, 32.4299111366272] in episode 300
Report: 
rewardSum:57.6875
loss:[49.41298866271973, 32.4299111366272]
policies:[1, 0, 5]
qAverage:[75.77534484863281, 0.0]
ws:[2.3983078002929688, 2.1526620388031006]
memory len:4124
memory used:3346.0
now epsilon is 0.7332832197622579, the reward is 245.25 with loss [13.87472116947174, 30.84833788871765] in episode 301
Report: 
rewardSum:245.25
loss:[13.87472116947174, 30.84833788871765]
policies:[1, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4136
memory used:3348.0
now epsilon is 0.7326235122980675, the reward is 57.6875 with loss [55.124475955963135, 48.501999855041504] in episode 302
Report: 
rewardSum:57.6875
loss:[55.124475955963135, 48.501999855041504]
policies:[1, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4148
memory used:3347.0
now epsilon is 0.7321840370849729, the reward is 59.6875 with loss [34.961907386779785, 30.950402975082397] in episode 303
Report: 
rewardSum:59.6875
loss:[34.961907386779785, 30.950402975082397]
policies:[0, 3, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4156
memory used:3347.0
now epsilon is 0.7313058773780575, the reward is 243.25 with loss [57.26386845111847, 24.763629913330078] in episode 304
Report: 
rewardSum:243.25
loss:[57.26386845111847, 24.763629913330078]
policies:[1, 2, 5]
qAverage:[0.0, 94.34014892578125]
ws:[25.816925048828125, 26.794593811035156]
memory len:4172
memory used:3344.0
now epsilon is 0.7310865020692263, the reward is -1.0 with loss [10.605444192886353, 5.39341926574707] in episode 305
Report: 
rewardSum:-1.0
loss:[10.605444192886353, 5.39341926574707]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4176
memory used:3344.0
now epsilon is 0.7303192065940793, the reward is 56.6875 with loss [52.772178649902344, 42.14952540397644] in episode 306
Report: 
rewardSum:56.6875
loss:[52.772178649902344, 42.14952540397644]
policies:[1, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4190
memory used:3344.0
now epsilon is 0.729771631486309, the reward is 246.25 with loss [29.460446000099182, 24.339388012886047] in episode 307
Report: 
rewardSum:246.25
loss:[29.460446000099182, 24.339388012886047]
policies:[1, 2, 2]
qAverage:[0.0, 77.88372039794922]
ws:[3.566047191619873, 6.169741630554199]
memory len:4200
memory used:3344.0
now epsilon is 0.729005716004153, the reward is 555.6875 with loss [28.263071298599243, 42.066895484924316] in episode 308
Report: 
rewardSum:555.6875
loss:[28.263071298599243, 42.066895484924316]
policies:[0, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4214
memory used:3343.0
now epsilon is 0.7282406043714487, the reward is 555.6875 with loss [47.79349601268768, 47.10364532470703] in episode 309
Report: 
rewardSum:555.6875
loss:[47.79349601268768, 47.10364532470703]
policies:[0, 2, 5]
qAverage:[0.0, 74.93085479736328]
ws:[2.0243403911590576, 2.6105082035064697]
memory len:4228
memory used:3341.0
now epsilon is 0.7278037583114765, the reward is -3.0 with loss [25.77398991584778, 17.03967845439911] in episode 310
Report: 
rewardSum:-3.0
loss:[25.77398991584778, 17.03967845439911]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4236
memory used:3341.0
now epsilon is 0.7270399081665663, the reward is -6.0 with loss [58.99863839149475, 33.91626191139221] in episode 311
Report: 
rewardSum:-6.0
loss:[58.99863839149475, 33.91626191139221]
policies:[0, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4250
memory used:3342.0
now epsilon is 0.7263858175761156, the reward is 245.25 with loss [31.454519987106323, 38.03593587875366] in episode 312
Report: 
rewardSum:245.25
loss:[31.454519987106323, 38.03593587875366]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4262
memory used:3341.0
now epsilon is 0.7261679181745238, the reward is -1.0 with loss [15.100226402282715, 3.1934616565704346] in episode 313
Report: 
rewardSum:-1.0
loss:[15.100226402282715, 3.1934616565704346]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4266
memory used:3341.0
now epsilon is 0.7249706393380712, the reward is -10.0 with loss [87.81790846586227, 60.0249884724617] in episode 314
Report: 
rewardSum:-10.0
loss:[87.81790846586227, 60.0249884724617]
policies:[0, 0, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4288
memory used:3342.0
now epsilon is 0.7239925159956998, the reward is 553.6875 with loss [55.71516263484955, 41.028297424316406] in episode 315
Report: 
rewardSum:553.6875
loss:[55.71516263484955, 41.028297424316406]
policies:[1, 1, 7]
qAverage:[0.0, 69.70509338378906]
ws:[1.1197072267532349, 2.0484941005706787]
memory len:4306
memory used:3341.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20*		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.7230157123278379, the reward is 991.0 with loss [31.23533946275711, 57.02666413784027] in episode 316
Report: 
rewardSum:991.0
loss:[31.23533946275711, 57.02666413784027]
policies:[3, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4324
memory used:3360.0
now epsilon is 0.7223652421557476, the reward is 245.25 with loss [42.15038347244263, 49.41727685928345] in episode 317
Report: 
rewardSum:245.25
loss:[42.15038347244263, 49.41727685928345]
policies:[1, 3, 2]
qAverage:[0.0, 115.25748697916667]
ws:[17.31351598103841, 18.890762646993]
memory len:4336
memory used:3359.0
now epsilon is 0.721498858818762, the reward is 243.25 with loss [52.181551933288574, 33.63566529750824] in episode 318
Report: 
rewardSum:243.25
loss:[52.181551933288574, 33.63566529750824]
policies:[2, 2, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4352
memory used:3360.0
now epsilon is 0.7208497533029943, the reward is 245.25 with loss [22.54759055376053, 21.242310762405396] in episode 319
Report: 
rewardSum:245.25
loss:[22.54759055376053, 21.242310762405396]
policies:[2, 1, 3]
qAverage:[0.0, 91.33174133300781]
ws:[21.053544998168945, 22.27362632751465]
memory len:4364
memory used:3360.0
now epsilon is 0.7205254195689333, the reward is -2.0 with loss [21.36966848373413, 16.782729864120483] in episode 320
Report: 
rewardSum:-2.0
loss:[21.36966848373413, 16.782729864120483]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4370
memory used:3360.0
now epsilon is 0.7196612428603112, the reward is 243.25 with loss [85.28369426727295, 38.4822473526001] in episode 321
Report: 
rewardSum:243.25
loss:[85.28369426727295, 38.4822473526001]
policies:[1, 2, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4386
memory used:3360.0
now epsilon is 0.7190137905788347, the reward is 57.6875 with loss [40.30007600784302, 17.05838119983673] in episode 322
Report: 
rewardSum:57.6875
loss:[40.30007600784302, 17.05838119983673]
policies:[0, 3, 3]
qAverage:[0.0, 70.33082580566406]
ws:[-0.6735670566558838, 0.24234598875045776]
memory len:4398
memory used:3360.0
now epsilon is 0.7187981026194714, the reward is -1.0 with loss [5.985960483551025, 14.746903896331787] in episode 323
Report: 
rewardSum:-1.0
loss:[5.985960483551025, 14.746903896331787]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4402
memory used:3360.0
now epsilon is 0.7183669207859403, the reward is 59.6875 with loss [6.175972402095795, 14.845840215682983] in episode 324
Report: 
rewardSum:59.6875
loss:[6.175972402095795, 14.845840215682983]
policies:[0, 2, 2]
qAverage:[0.0, 93.04465738932292]
ws:[7.095678647359212, 8.839346567789713]
memory len:4410
memory used:3359.0
now epsilon is 0.7176129748626409, the reward is 244.25 with loss [41.35034143924713, 36.38723611831665] in episode 325
Report: 
rewardSum:244.25
loss:[41.35034143924713, 36.38723611831665]
policies:[1, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4424
memory used:3360.0
now epsilon is 0.7166447784096884, the reward is -8.0 with loss [46.62581670284271, 54.86902767419815] in episode 326
Report: 
rewardSum:-8.0
loss:[46.62581670284271, 54.86902767419815]
policies:[0, 3, 6]
qAverage:[0.0, 73.43370819091797]
ws:[4.095648765563965, 5.252615451812744]
memory len:4442
memory used:3361.0
now epsilon is 0.7160000399283644, the reward is -5.0 with loss [42.784895181655884, 40.65590524673462] in episode 327
Report: 
rewardSum:-5.0
loss:[42.784895181655884, 40.65590524673462]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4454
memory used:3361.0
now epsilon is 0.7151412908251772, the reward is 554.6875 with loss [39.67812216281891, 31.50278663635254] in episode 328
Report: 
rewardSum:554.6875
loss:[39.67812216281891, 31.50278663635254]
policies:[0, 2, 6]
qAverage:[0.0, 74.6331558227539]
ws:[3.740403175354004, 5.122174263000488]
memory len:4470
memory used:3360.0
now epsilon is 0.7143907302896074, the reward is 56.6875 with loss [46.43988072872162, 25.614980041980743] in episode 329
Report: 
rewardSum:56.6875
loss:[46.43988072872162, 25.614980041980743]
policies:[2, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4484
memory used:3361.0
now epsilon is 0.7133198672255352, the reward is 241.25 with loss [48.3691810965538, 54.802743911743164] in episode 330
Report: 
rewardSum:241.25
loss:[48.3691810965538, 54.802743911743164]
policies:[3, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4504
memory used:3359.0
now epsilon is 0.712143771780838, the reward is 989.0 with loss [91.21790909767151, 57.275199204683304] in episode 331
Report: 
rewardSum:989.0
loss:[91.21790909767151, 57.275199204683304]
policies:[0, 3, 8]
qAverage:[0.0, 99.23509216308594]
ws:[0.10599203904469807, 1.882372905810674]
memory len:4526
memory used:3361.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2*		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.7112896477707075, the reward is 55.6875 with loss [62.36548709869385, 60.497496008872986] in episode 332
Report: 
rewardSum:55.6875
loss:[62.36548709869385, 60.497496008872986]
policies:[0, 0, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4542
memory used:3361.0
now epsilon is 0.7103299826892266, the reward is 242.25 with loss [53.950873374938965, 64.39481937885284] in episode 333
Report: 
rewardSum:242.25
loss:[53.950873374938965, 64.39481937885284]
policies:[1, 2, 6]
qAverage:[0.0, 66.24944305419922]
ws:[3.295220375061035, 5.99431037902832]
memory len:4560
memory used:3360.0
now epsilon is 0.7096909253932336, the reward is 994.0 with loss [17.911794811487198, 28.60982471704483] in episode 334
Report: 
rewardSum:994.0
loss:[17.911794811487198, 28.60982471704483]
policies:[0, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4572
memory used:3361.0
now epsilon is 0.7092652066366922, the reward is 247.25 with loss [31.912924647331238, 28.083191394805908] in episode 335
Report: 
rewardSum:247.25
loss:[31.912924647331238, 28.083191394805908]
policies:[1, 2, 1]
qAverage:[0.0, 86.76870727539062]
ws:[16.427349090576172, 18.40638542175293]
memory len:4580
memory used:3362.0
now epsilon is 0.7087334172924503, the reward is 58.6875 with loss [29.958085536956787, 39.56617784500122] in episode 336
Report: 
rewardSum:58.6875
loss:[29.958085536956787, 39.56617784500122]
policies:[1, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4590
memory used:3361.0
now epsilon is 0.7080957963665814, the reward is 245.25 with loss [31.579434633255005, 44.12165594100952] in episode 337
Report: 
rewardSum:245.25
loss:[31.579434633255005, 44.12165594100952]
policies:[1, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4602
memory used:3361.0
now epsilon is 0.7073526302720291, the reward is 244.25 with loss [33.33006036281586, 46.057836294174194] in episode 338
Report: 
rewardSum:244.25
loss:[33.33006036281586, 46.057836294174194]
policies:[2, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4616
memory used:3360.0
now epsilon is 0.706928314176922, the reward is 59.6875 with loss [10.346475660800934, 28.453490257263184] in episode 339
Report: 
rewardSum:59.6875
loss:[10.346475660800934, 28.453490257263184]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4624
memory used:3359.0
now epsilon is 0.7063982769763031, the reward is 246.25 with loss [20.04417395591736, 25.646482944488525] in episode 340
Report: 
rewardSum:246.25
loss:[20.04417395591736, 25.646482944488525]
policies:[2, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4634
memory used:3362.0
now epsilon is 0.705868637184344, the reward is 246.25 with loss [29.469732880592346, 37.568880558013916] in episode 341
Report: 
rewardSum:246.25
loss:[29.469732880592346, 37.568880558013916]
policies:[1, 2, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4644
memory used:3362.0
now epsilon is 0.7055510439413617, the reward is -2.0 with loss [8.48753023147583, 13.425276756286621] in episode 342
Report: 
rewardSum:-2.0
loss:[8.48753023147583, 13.425276756286621]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4650
memory used:3361.0
now epsilon is 0.7051278085548633, the reward is 247.25 with loss [13.713233470916748, 16.982699513435364] in episode 343
Report: 
rewardSum:247.25
loss:[13.713233470916748, 16.982699513435364]
policies:[0, 1, 3]
qAverage:[0.0, 71.6458969116211]
ws:[5.503297328948975, 6.170797348022461]
memory len:4658
memory used:3361.0
now epsilon is 0.7047048270524657, the reward is -3.0 with loss [17.01229500770569, 34.39484143257141] in episode 344
Report: 
rewardSum:-3.0
loss:[17.01229500770569, 34.39484143257141]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4666
memory used:3361.0
now epsilon is 0.7042820992818727, the reward is 59.6875 with loss [38.01401627063751, 23.464320182800293] in episode 345
Report: 
rewardSum:59.6875
loss:[38.01401627063751, 23.464320182800293]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4674
memory used:3361.0
now epsilon is 0.7035429357677379, the reward is 244.25 with loss [36.05680561065674, 79.56019020080566] in episode 346
Report: 
rewardSum:244.25
loss:[36.05680561065674, 79.56019020080566]
policies:[1, 2, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4688
memory used:3361.0
now epsilon is 0.7030154368393298, the reward is -4.0 with loss [16.505797147750854, 31.992186069488525] in episode 347
Report: 
rewardSum:-4.0
loss:[16.505797147750854, 31.992186069488525]
policies:[1, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4698
memory used:3364.0
now epsilon is 0.7022776027224112, the reward is 56.6875 with loss [62.755826473236084, 30.141260385513306] in episode 348
Report: 
rewardSum:56.6875
loss:[62.755826473236084, 30.141260385513306]
policies:[1, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4712
memory used:3360.0
now epsilon is 0.7015405429827759, the reward is -6.0 with loss [23.616573333740234, 22.267995238304138] in episode 349
Report: 
rewardSum:-6.0
loss:[23.616573333740234, 22.267995238304138]
policies:[1, 2, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4726
memory used:3361.0
now epsilon is 0.7013300966045433, the reward is -1.0 with loss [16.204458236694336, 7.952762603759766] in episode 350
Report: 
rewardSum:-1.0
loss:[16.204458236694336, 7.952762603759766]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4730
memory used:3361.0
now epsilon is 0.7004889421940523, the reward is 243.25 with loss [41.07911378145218, 40.17749643325806] in episode 351
Report: 
rewardSum:243.25
loss:[41.07911378145218, 40.17749643325806]
policies:[1, 3, 4]
qAverage:[0.0, 94.60042317708333]
ws:[10.414674123128256, 12.261782964070639]
memory len:4746
memory used:3361.0
############# STATE ###############
0-		8-		16-		24-		32*		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.699753759703041, the reward is 56.6875 with loss [23.226253628730774, 44.958660781383514] in episode 352
Report: 
rewardSum:56.6875
loss:[23.226253628730774, 44.958660781383514]
policies:[0, 2, 5]
qAverage:[0.0, 65.53876495361328]
ws:[5.952503681182861, 6.312556743621826]
memory len:4760
memory used:3362.0
now epsilon is 0.6990193488063583, the reward is 244.25 with loss [32.99567884206772, 48.49393677711487] in episode 353
Report: 
rewardSum:244.25
loss:[32.99567884206772, 48.49393677711487]
policies:[0, 2, 5]
qAverage:[0.0, 91.8651123046875]
ws:[4.138794422149658, 4.574507395426433]
memory len:4774
memory used:3360.0
now epsilon is 0.6975528385575906, the reward is 237.25 with loss [92.6322410106659, 39.358841478824615] in episode 354
Report: 
rewardSum:237.25
loss:[92.6322410106659, 39.358841478824615]
policies:[0, 2, 12]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4802
memory used:3362.0
now epsilon is 0.6969252763798923, the reward is 57.6875 with loss [30.56710112094879, 44.13756263256073] in episode 355
Report: 
rewardSum:57.6875
loss:[30.56710112094879, 44.13756263256073]
policies:[1, 2, 3]
qAverage:[0.0, 65.80699920654297]
ws:[1.5932687520980835, 3.457599639892578]
memory len:4814
memory used:3361.0
now epsilon is 0.6965072152895686, the reward is 247.25 with loss [26.149503707885742, 40.316874504089355] in episode 356
Report: 
rewardSum:247.25
loss:[26.149503707885742, 40.316874504089355]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4822
memory used:3361.0
now epsilon is 0.6957762117309111, the reward is 244.25 with loss [36.54306197166443, 29.547291040420532] in episode 357
Report: 
rewardSum:244.25
loss:[36.54306197166443, 29.547291040420532]
policies:[2, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4836
memory used:3361.0
now epsilon is 0.6951502479178653, the reward is 245.25 with loss [30.396357655525208, 47.69220852851868] in episode 358
Report: 
rewardSum:245.25
loss:[30.396357655525208, 47.69220852851868]
policies:[1, 1, 4]
qAverage:[0.0, 69.9558334350586]
ws:[3.8978261947631836, 5.999149799346924]
memory len:4848
memory used:3362.0
now epsilon is 0.6944206685339415, the reward is 56.6875 with loss [32.07945704460144, 44.31562900543213] in episode 359
Report: 
rewardSum:56.6875
loss:[32.07945704460144, 44.31562900543213]
policies:[0, 2, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4862
memory used:3367.0
now epsilon is 0.6940041098702372, the reward is -3.0 with loss [21.089749097824097, 11.649107933044434] in episode 360
Report: 
rewardSum:-3.0
loss:[21.089749097824097, 11.649107933044434]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4870
memory used:3368.0
now epsilon is 0.6935878010855013, the reward is 59.6875 with loss [17.23876142501831, 19.48832881450653] in episode 361
Report: 
rewardSum:59.6875
loss:[17.23876142501831, 19.48832881450653]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4878
memory used:3369.0
now epsilon is 0.6931717420298401, the reward is 59.6875 with loss [28.583881378173828, 17.574007272720337] in episode 362
Report: 
rewardSum:59.6875
loss:[28.583881378173828, 17.574007272720337]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4886
memory used:3369.0
now epsilon is 0.6927559325534499, the reward is 247.25 with loss [25.60002827644348, 22.067580699920654] in episode 363
Report: 
rewardSum:247.25
loss:[25.60002827644348, 22.067580699920654]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4894
memory used:3369.0
now epsilon is 0.6923403725066168, the reward is 247.25 with loss [18.887538194656372, 19.003202855587006] in episode 364
Report: 
rewardSum:247.25
loss:[18.887538194656372, 19.003202855587006]
policies:[0, 2, 2]
qAverage:[0.0, 83.52378845214844]
ws:[18.212732315063477, 19.93047523498535]
memory len:4902
memory used:3368.0
now epsilon is 0.6913025626621601, the reward is 552.6875 with loss [49.74776101112366, 61.41330313682556] in episode 365
Report: 
rewardSum:552.6875
loss:[49.74776101112366, 61.41330313682556]
policies:[0, 4, 6]
qAverage:[0.0, 106.81000328063965]
ws:[6.506505966186523, 8.109212875366211]
memory len:4922
memory used:3370.0
now epsilon is 0.6901627685358352, the reward is 989.0 with loss [33.31423580646515, 63.64514046907425] in episode 366
Report: 
rewardSum:989.0
loss:[33.31423580646515, 63.64514046907425]
policies:[2, 0, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4944
memory used:3369.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35*		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.6887148389826636, the reward is 986.0 with loss [78.55005985498428, 63.30119487643242] in episode 367
Report: 
rewardSum:986.0
loss:[78.55005985498428, 63.30119487643242]
policies:[2, 2, 10]
qAverage:[31.71169662475586, 67.91289520263672]
ws:[-14.536805510520935, -16.314186453819275]
memory len:4972
memory used:3368.0
now epsilon is 0.6879920137381511, the reward is 244.25 with loss [26.151817679405212, 39.19001877307892] in episode 368
Report: 
rewardSum:244.25
loss:[26.151817679405212, 39.19001877307892]
policies:[0, 3, 4]
qAverage:[0.0, 108.12658182779948]
ws:[16.921748797098797, 17.692626953125]
memory len:4986
memory used:3369.0
now epsilon is 0.6875793113995426, the reward is 247.25 with loss [17.471583604812622, 14.718877494335175] in episode 369
Report: 
rewardSum:247.25
loss:[17.471583604812622, 14.718877494335175]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:4994
memory used:3368.0
now epsilon is 0.6868576779225898, the reward is 244.25 with loss [34.065840005874634, 42.985589027404785] in episode 370
Report: 
rewardSum:244.25
loss:[34.065840005874634, 42.985589027404785]
policies:[3, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5008
memory used:3368.0
now epsilon is 0.6866516360735108, the reward is -1.0 with loss [8.417947769165039, 3.486542224884033] in episode 371
Report: 
rewardSum:-1.0
loss:[8.417947769165039, 3.486542224884033]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5012
memory used:3368.0
now epsilon is 0.6862397377805681, the reward is 247.25 with loss [37.09493446350098, 23.625047087669373] in episode 372
Report: 
rewardSum:247.25
loss:[37.09493446350098, 23.625047087669373]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5020
memory used:3369.0
now epsilon is 0.6851082910528707, the reward is 52.6875 with loss [64.59758192300797, 86.58437085151672] in episode 373
Report: 
rewardSum:52.6875
loss:[64.59758192300797, 86.58437085151672]
policies:[1, 1, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5042
memory used:3368.0
now epsilon is 0.6840813220110402, the reward is 552.6875 with loss [41.35544592142105, 60.041802406311035] in episode 374
Report: 
rewardSum:552.6875
loss:[41.35544592142105, 60.041802406311035]
policies:[2, 0, 8]
qAverage:[83.95984141031902, 0.0]
ws:[4.156169573465983, 3.436396598815918]
memory len:5062
memory used:3369.0
now epsilon is 0.683055892383382, the reward is 241.25 with loss [57.50367760658264, 52.715765953063965] in episode 375
Report: 
rewardSum:241.25
loss:[57.50367760658264, 52.715765953063965]
policies:[3, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5082
memory used:3370.0
now epsilon is 0.6824413725654997, the reward is 57.6875 with loss [29.093797087669373, 35.91142475605011] in episode 376
Report: 
rewardSum:57.6875
loss:[29.093797087669373, 35.91142475605011]
policies:[0, 1, 5]
qAverage:[0.0, 78.48235321044922]
ws:[3.7614150047302246, 4.191054344177246]
memory len:5094
memory used:3370.0
now epsilon is 0.6818274056080946, the reward is 245.25 with loss [22.001527667045593, 21.488644242286682] in episode 377
Report: 
rewardSum:245.25
loss:[22.001527667045593, 21.488644242286682]
policies:[0, 1, 5]
qAverage:[0.0, 80.80936431884766]
ws:[11.396909713745117, 12.951118469238281]
memory len:5106
memory used:3370.0
now epsilon is 0.6816228727275289, the reward is -1.0 with loss [9.47053575515747, 4.377794086933136] in episode 378
Report: 
rewardSum:-1.0
loss:[9.47053575515747, 4.377794086933136]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5110
memory used:3370.0
now epsilon is 0.6809074906974677, the reward is 56.6875 with loss [31.243340730667114, 23.286172211170197] in episode 379
Report: 
rewardSum:56.6875
loss:[31.243340730667114, 23.286172211170197]
policies:[1, 1, 5]
qAverage:[0.0, 77.09623718261719]
ws:[1.1240614652633667, 1.7082276344299316]
memory len:5124
memory used:3370.0
now epsilon is 0.6804990381163686, the reward is 247.25 with loss [17.266059041023254, 31.16593074798584] in episode 380
Report: 
rewardSum:247.25
loss:[17.266059041023254, 31.16593074798584]
policies:[1, 3, 0]
qAverage:[0.0, 74.54863739013672]
ws:[2.9936726093292236, 4.270074367523193]
memory len:5132
memory used:3371.0
now epsilon is 0.6800908305516826, the reward is -3.0 with loss [20.287198662757874, 19.33487844467163] in episode 381
Report: 
rewardSum:-3.0
loss:[20.287198662757874, 19.33487844467163]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5140
memory used:3371.0
now epsilon is 0.6794789782889407, the reward is 245.25 with loss [44.82970428466797, 48.72547483444214] in episode 382
Report: 
rewardSum:245.25
loss:[44.82970428466797, 48.72547483444214]
policies:[1, 3, 2]
qAverage:[0.0, 83.17450714111328]
ws:[14.374815940856934, 15.76054859161377]
memory len:5152
memory used:3371.0
now epsilon is 0.6788676764867763, the reward is 245.25 with loss [25.502885103225708, 28.316896185278893] in episode 383
Report: 
rewardSum:245.25
loss:[25.502885103225708, 28.316896185278893]
policies:[1, 1, 4]
qAverage:[0.0, 83.3357925415039]
ws:[18.711578369140625, 20.005630493164062]
memory len:5164
memory used:3371.0
now epsilon is 0.678664031458353, the reward is -1.0 with loss [12.504883289337158, 16.569022178649902] in episode 384
Report: 
rewardSum:-1.0
loss:[12.504883289337158, 16.569022178649902]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5168
memory used:3370.0
now epsilon is 0.6784604475188561, the reward is -1.0 with loss [2.8877920508384705, 19.911328554153442] in episode 385
Report: 
rewardSum:-1.0
loss:[2.8877920508384705, 19.911328554153442]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5172
memory used:3371.0
now epsilon is 0.6780534628333464, the reward is 247.25 with loss [29.83078932762146, 15.45553982257843] in episode 386
Report: 
rewardSum:247.25
loss:[29.83078932762146, 15.45553982257843]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5180
memory used:3370.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12*		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.6776467222837107, the reward is 59.6875 with loss [13.956507205963135, 17.024267554283142] in episode 387
Report: 
rewardSum:59.6875
loss:[13.956507205963135, 17.024267554283142]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5188
memory used:3370.0
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.6772402257235002, the reward is 247.25 with loss [16.132984161376953, 24.16408199071884] in episode 388
Report: 
rewardSum:247.25
loss:[16.132984161376953, 24.16408199071884]
policies:[0, 1, 3]
qAverage:[0.0, 76.62217712402344]
ws:[5.1968183517456055, 6.150722503662109]
memory len:5196
memory used:3404.0
now epsilon is 0.6767324479104033, the reward is -4.0 with loss [25.1289244890213, 23.366652131080627] in episode 389
Report: 
rewardSum:-4.0
loss:[25.1289244890213, 23.366652131080627]
policies:[1, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5206
memory used:3405.0
now epsilon is 0.6761236170588107, the reward is -5.0 with loss [28.336899757385254, 47.33974361419678] in episode 390
Report: 
rewardSum:-5.0
loss:[28.336899757385254, 47.33974361419678]
policies:[0, 2, 4]
qAverage:[0.0, 61.14735794067383]
ws:[1.9884493350982666, 3.1617074012756348]
memory len:5218
memory used:3405.0
now epsilon is 0.6755153339495454, the reward is 57.6875 with loss [40.94420146942139, 35.22538912296295] in episode 391
Report: 
rewardSum:57.6875
loss:[40.94420146942139, 35.22538912296295]
policies:[0, 2, 4]
qAverage:[0.0, 89.33511352539062]
ws:[7.6682484944661455, 8.94500732421875]
memory len:5230
memory used:3394.0
now epsilon is 0.674907598089824, the reward is 245.25 with loss [38.032703042030334, 19.202245354652405] in episode 392
Report: 
rewardSum:245.25
loss:[38.032703042030334, 19.202245354652405]
policies:[0, 4, 2]
qAverage:[0.0, 85.69029235839844]
ws:[2.6649279594421387, 3.6267232100168862]
memory len:5242
memory used:3393.0
now epsilon is 0.673997019316264, the reward is 242.25 with loss [35.45030128955841, 37.91951194405556] in episode 393
Report: 
rewardSum:242.25
loss:[35.45030128955841, 37.91951194405556]
policies:[1, 1, 7]
qAverage:[0.0, 74.11470794677734]
ws:[7.397331714630127, 8.690788269042969]
memory len:5260
memory used:3392.0
now epsilon is 0.6730876690867376, the reward is 54.6875 with loss [49.01776897907257, 58.45689558982849] in episode 394
Report: 
rewardSum:54.6875
loss:[49.01776897907257, 58.45689558982849]
policies:[0, 1, 8]
qAverage:[0.0, 66.39449310302734]
ws:[3.4815609455108643, 3.839022159576416]
memory len:5278
memory used:3391.0
now epsilon is 0.671675562302111, the reward is 49.6875 with loss [62.68739980459213, 101.42174434661865] in episode 395
Report: 
rewardSum:49.6875
loss:[62.68739980459213, 101.42174434661865]
policies:[2, 1, 11]
qAverage:[70.55382537841797, 0.0]
ws:[6.208714485168457, 5.249020576477051]
memory len:5306
memory used:3397.0
now epsilon is 0.6710712809412083, the reward is 245.25 with loss [38.76086688041687, 31.049201607704163] in episode 396
Report: 
rewardSum:245.25
loss:[38.76086688041687, 31.049201607704163]
policies:[2, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5318
memory used:3396.0
now epsilon is 0.6706687287582074, the reward is 247.25 with loss [34.755453288555145, 22.322928190231323] in episode 397
Report: 
rewardSum:247.25
loss:[34.755453288555145, 22.322928190231323]
policies:[0, 1, 3]
qAverage:[0.0, 67.61780548095703]
ws:[4.048613548278809, 4.30104923248291]
memory len:5326
memory used:3396.0
now epsilon is 0.6701658780894693, the reward is 246.25 with loss [32.7550003528595, 17.690484285354614] in episode 398
Report: 
rewardSum:246.25
loss:[32.7550003528595, 17.690484285354614]
policies:[2, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5336
memory used:3395.0
now epsilon is 0.6691613075439409, the reward is 552.6875 with loss [55.86967673897743, 38.76249140501022] in episode 399
Report: 
rewardSum:552.6875
loss:[55.86967673897743, 38.76249140501022]
policies:[1, 1, 8]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5356
memory used:3379.0
now epsilon is 0.6675571258772673, the reward is 235.25 with loss [76.1739961206913, 118.94558095932007] in episode 400
Report: 
rewardSum:235.25
loss:[76.1739961206913, 118.94558095932007]
policies:[1, 3, 12]
qAverage:[0.0, 61.57222366333008]
ws:[3.2836716175079346, 3.7403976917266846]
memory len:5388
memory used:3379.0
now epsilon is 0.6673568737595394, the reward is -1.0 with loss [3.2745718359947205, 18.38776445388794] in episode 401
Report: 
rewardSum:-1.0
loss:[3.2745718359947205, 18.38776445388794]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5392
memory used:3379.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2*		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.6665564658197518, the reward is 243.25 with loss [39.81049311161041, 24.858042120933533] in episode 402
Report: 
rewardSum:243.25
loss:[39.81049311161041, 24.858042120933533]
policies:[1, 2, 5]
qAverage:[0.0, 92.80365244547527]
ws:[12.58167823155721, 14.116614500681559]
memory len:5408
memory used:3390.0
now epsilon is 0.6657570178653861, the reward is 243.25 with loss [53.50783169269562, 48.18525171279907] in episode 403
Report: 
rewardSum:243.25
loss:[53.50783169269562, 48.18525171279907]
policies:[0, 3, 5]
qAverage:[0.0, 103.41541290283203]
ws:[10.801741361618042, 11.509608507156372]
memory len:5424
memory used:3390.0
now epsilon is 0.6651580611973673, the reward is 245.25 with loss [34.86357307434082, 25.701955318450928] in episode 404
Report: 
rewardSum:245.25
loss:[34.86357307434082, 25.701955318450928]
policies:[0, 2, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5436
memory used:3391.0
now epsilon is 0.6648587849657528, the reward is -2.0 with loss [17.15702772140503, 17.604329705238342] in episode 405
Report: 
rewardSum:-2.0
loss:[17.15702772140503, 17.604329705238342]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5442
memory used:3392.0
now epsilon is 0.6640613731591937, the reward is 243.25 with loss [63.64740490913391, 46.228140234947205] in episode 406
Report: 
rewardSum:243.25
loss:[63.64740490913391, 46.228140234947205]
policies:[2, 2, 4]
qAverage:[28.224557876586914, 74.17495346069336]
ws:[8.426948308944702, 8.95880913734436]
memory len:5458
memory used:3393.0
now epsilon is 0.6638621696886269, the reward is -1.0 with loss [24.0461106300354, 12.186170101165771] in episode 407
Report: 
rewardSum:-1.0
loss:[24.0461106300354, 12.186170101165771]
policies:[1, 0, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5462
memory used:3392.0
now epsilon is 0.6634639419992447, the reward is 247.25 with loss [21.31953191757202, 24.13231074810028] in episode 408
Report: 
rewardSum:247.25
loss:[21.31953191757202, 24.13231074810028]
policies:[1, 2, 1]
qAverage:[0.0, 83.49549865722656]
ws:[2.937823931376139, 3.377171198527018]
memory len:5470
memory used:3391.0
now epsilon is 0.662668203125758, the reward is 55.6875 with loss [57.94649887084961, 59.64867448806763] in episode 409
Report: 
rewardSum:55.6875
loss:[57.94649887084961, 59.64867448806763]
policies:[0, 3, 5]
qAverage:[0.0, 86.02600606282552]
ws:[3.8708229064941406, 5.004114945729573]
memory len:5486
memory used:3391.0
now epsilon is 0.6622706916551444, the reward is 247.25 with loss [17.21720838546753, 27.384166717529297] in episode 410
Report: 
rewardSum:247.25
loss:[17.21720838546753, 27.384166717529297]
policies:[1, 1, 2]
qAverage:[0.0, 72.22162628173828]
ws:[5.808484077453613, 6.981374740600586]
memory len:5494
memory used:3392.0
now epsilon is 0.6615756202735894, the reward is 244.25 with loss [29.707751512527466, 46.102049112319946] in episode 411
Report: 
rewardSum:244.25
loss:[29.707751512527466, 46.102049112319946]
policies:[1, 3, 3]
qAverage:[0.0, 100.33271026611328]
ws:[6.4086912870407104, 7.5019203424453735]
memory len:5508
memory used:3391.0
now epsilon is 0.6609804254524638, the reward is -5.0 with loss [38.057718992233276, 38.69796133041382] in episode 412
Report: 
rewardSum:-5.0
loss:[38.057718992233276, 38.69796133041382]
policies:[0, 1, 5]
qAverage:[0.0, 64.49476623535156]
ws:[1.1621900796890259, 1.7401784658432007]
memory len:5520
memory used:3391.0
now epsilon is 0.6599896237893392, the reward is 990.0 with loss [59.56891703605652, 60.1496217250824] in episode 413
Report: 
rewardSum:990.0
loss:[59.56891703605652, 60.1496217250824]
policies:[1, 1, 8]
qAverage:[55.42499923706055, 0.0]
ws:[-37.666439056396484, -43.20043182373047]
memory len:5540
memory used:3392.0
now epsilon is 0.6595937191047553, the reward is 247.25 with loss [40.24960708618164, 25.889246940612793] in episode 414
Report: 
rewardSum:247.25
loss:[40.24960708618164, 25.889246940612793]
policies:[0, 2, 2]
qAverage:[0.0, 69.78531646728516]
ws:[3.9088356494903564, 5.649799823760986]
memory len:5548
memory used:3392.0
now epsilon is 0.6591980519095404, the reward is 247.25 with loss [17.3348650932312, 32.54136800765991] in episode 415
Report: 
rewardSum:247.25
loss:[17.3348650932312, 32.54136800765991]
policies:[2, 2, 0]
qAverage:[0.0, 75.7579574584961]
ws:[14.471463203430176, 16.936933517456055]
memory len:5556
memory used:3393.0
now epsilon is 0.6587038016679236, the reward is 58.6875 with loss [30.446874856948853, 31.21730837225914] in episode 416
Report: 
rewardSum:58.6875
loss:[30.446874856948853, 31.21730837225914]
policies:[1, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5566
memory used:3393.0
now epsilon is 0.6582099220027985, the reward is 58.6875 with loss [12.066438227891922, 50.74538564682007] in episode 417
Report: 
rewardSum:58.6875
loss:[12.066438227891922, 50.74538564682007]
policies:[1, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5576
memory used:3393.0
now epsilon is 0.6575191125111443, the reward is 244.25 with loss [42.72200655937195, 31.900023460388184] in episode 418
Report: 
rewardSum:244.25
loss:[42.72200655937195, 31.900023460388184]
policies:[1, 3, 3]
qAverage:[21.231063842773438, 81.15647888183594]
ws:[7.272538542747498, 8.197492218017578]
memory len:5590
memory used:3393.0
now epsilon is 0.6571246897998416, the reward is 247.25 with loss [22.94018316268921, 12.264324992895126] in episode 419
Report: 
rewardSum:247.25
loss:[22.94018316268921, 12.264324992895126]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5598
memory used:3393.0
now epsilon is 0.6562381035533579, the reward is 54.6875 with loss [52.031047344207764, 49.78687047958374] in episode 420
Report: 
rewardSum:54.6875
loss:[52.031047344207764, 49.78687047958374]
policies:[0, 3, 6]
qAverage:[0.0, 86.62091827392578]
ws:[2.027920881907145, 2.982102950414022]
memory len:5616
memory used:3392.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26*		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.6555493635396242, the reward is -6.0 with loss [29.1396544277668, 41.050450801849365] in episode 421
Report: 
rewardSum:-6.0
loss:[29.1396544277668, 41.050450801849365]
policies:[0, 2, 5]
qAverage:[0.0, 81.87803649902344]
ws:[4.275186538696289, 5.099735418955485]
memory len:5630
memory used:3392.0
now epsilon is 0.655352713480423, the reward is -1.0 with loss [26.73721694946289, 4.573489665985107] in episode 422
Report: 
rewardSum:-1.0
loss:[26.73721694946289, 4.573489665985107]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5634
memory used:3393.0
now epsilon is 0.6547631171756002, the reward is 245.25 with loss [63.00472688674927, 58.3764533996582] in episode 423
Report: 
rewardSum:245.25
loss:[63.00472688674927, 58.3764533996582]
policies:[2, 2, 2]
qAverage:[0.0, 54.9954833984375]
ws:[1.8280396461486816, 2.552262306213379]
memory len:5646
memory used:3392.0
now epsilon is 0.6539778138120265, the reward is -7.0 with loss [38.111132860183716, 38.41246175765991] in episode 424
Report: 
rewardSum:-7.0
loss:[38.111132860183716, 38.41246175765991]
policies:[1, 1, 6]
qAverage:[0.0, 56.52116775512695]
ws:[-0.838966965675354, -0.42672908306121826]
memory len:5662
memory used:3392.0
now epsilon is 0.6534874775746053, the reward is -4.0 with loss [28.901254892349243, 32.36155319213867] in episode 425
Report: 
rewardSum:-4.0
loss:[28.901254892349243, 32.36155319213867]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5672
memory used:3392.0
now epsilon is 0.6530954733000482, the reward is 247.25 with loss [30.256033658981323, 25.933069944381714] in episode 426
Report: 
rewardSum:247.25
loss:[30.256033658981323, 25.933069944381714]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5680
memory used:3393.0
now epsilon is 0.6527037041751408, the reward is 247.25 with loss [16.454361081123352, 19.960992515087128] in episode 427
Report: 
rewardSum:247.25
loss:[16.454361081123352, 19.960992515087128]
policies:[0, 3, 1]
qAverage:[0.0, 56.372314453125]
ws:[2.414360761642456, 3.114224910736084]
memory len:5688
memory used:3393.0
now epsilon is 0.652018673611168, the reward is -6.0 with loss [63.62288761138916, 27.59081867337227] in episode 428
Report: 
rewardSum:-6.0
loss:[63.62288761138916, 27.59081867337227]
policies:[0, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5702
memory used:3393.0
now epsilon is 0.651432076817214, the reward is 245.25 with loss [41.58271834254265, 33.95826196670532] in episode 429
Report: 
rewardSum:245.25
loss:[41.58271834254265, 33.95826196670532]
policies:[2, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5714
memory used:3393.0
now epsilon is 0.6510413055056601, the reward is 59.6875 with loss [18.970449209213257, 26.63376033306122] in episode 430
Report: 
rewardSum:59.6875
loss:[18.970449209213257, 26.63376033306122]
policies:[0, 2, 2]
qAverage:[0.0, 61.04196548461914]
ws:[3.846435308456421, 6.896566390991211]
memory len:5722
memory used:3392.0
now epsilon is 0.6506507686041443, the reward is 247.25 with loss [25.295636415481567, 25.584110498428345] in episode 431
Report: 
rewardSum:247.25
loss:[25.295636415481567, 25.584110498428345]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5730
memory used:3393.0
now epsilon is 0.6501629269021563, the reward is -4.0 with loss [26.962581872940063, 19.091960415244102] in episode 432
Report: 
rewardSum:-4.0
loss:[26.962581872940063, 19.091960415244102]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5740
memory used:3392.0
now epsilon is 0.6496754509716969, the reward is 246.25 with loss [23.711746215820312, 30.45461940765381] in episode 433
Report: 
rewardSum:246.25
loss:[23.711746215820312, 30.45461940765381]
policies:[0, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5750
memory used:3391.0
now epsilon is 0.6485069994457714, the reward is 51.6875 with loss [73.96328353881836, 64.09657073020935] in episode 434
Report: 
rewardSum:51.6875
loss:[73.96328353881836, 64.09657073020935]
policies:[2, 2, 8]
qAverage:[0.0, 59.58858108520508]
ws:[3.1099488735198975, 4.758134841918945]
memory len:5774
memory used:3392.0
now epsilon is 0.6477291994833012, the reward is 55.6875 with loss [54.31765127182007, 37.67610356211662] in episode 435
Report: 
rewardSum:55.6875
loss:[54.31765127182007, 37.67610356211662]
policies:[0, 1, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5790
memory used:3391.0
now epsilon is 0.6473406493983092, the reward is 247.25 with loss [29.534606218338013, 15.49179720878601] in episode 436
Report: 
rewardSum:247.25
loss:[29.534606218338013, 15.49179720878601]
policies:[0, 2, 2]
qAverage:[0.0, 68.15042114257812]
ws:[5.552816390991211, 7.592250823974609]
memory len:5798
memory used:3391.0
now epsilon is 0.6462733380504758, the reward is 240.25 with loss [79.1419335603714, 64.91211867332458] in episode 437
Report: 
rewardSum:240.25
loss:[79.1419335603714, 64.91211867332458]
policies:[1, 1, 9]
qAverage:[0.0, 66.1921157836914]
ws:[0.2995408773422241, 2.256603956222534]
memory len:5820
memory used:3391.0
now epsilon is 0.6456919101198635, the reward is 245.25 with loss [39.42049157619476, 26.11312484741211] in episode 438
Report: 
rewardSum:245.25
loss:[39.42049157619476, 26.11312484741211]
policies:[1, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5832
memory used:3391.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12*		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.6453045821334831, the reward is 59.6875 with loss [9.433196306228638, 27.012563705444336] in episode 439
Report: 
rewardSum:59.6875
loss:[9.433196306228638, 27.012563705444336]
policies:[1, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5840
memory used:3394.0
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.64453062305487, the reward is 243.25 with loss [41.5585914850235, 40.53005704283714] in episode 440
Report: 
rewardSum:243.25
loss:[41.5585914850235, 40.53005704283714]
policies:[0, 5, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5856
memory used:3394.0
now epsilon is 0.6441439916839704, the reward is 247.25 with loss [16.165066242218018, 12.84416663646698] in episode 441
Report: 
rewardSum:247.25
loss:[16.165066242218018, 12.84416663646698]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5864
memory used:3396.0
now epsilon is 0.6430819508672816, the reward is 240.25 with loss [42.843407452106476, 66.05865228176117] in episode 442
Report: 
rewardSum:240.25
loss:[42.843407452106476, 66.05865228176117]
policies:[1, 6, 4]
qAverage:[0.0, 100.39046173095703]
ws:[6.297140121459961, 8.946444320678712]
memory len:5886
memory used:3394.0
now epsilon is 0.6424070185991402, the reward is 244.25 with loss [32.18944764137268, 26.092406511306763] in episode 443
Report: 
rewardSum:244.25
loss:[32.18944764137268, 26.092406511306763]
policies:[0, 5, 2]
qAverage:[0.0, 98.08850479125977]
ws:[8.45967423915863, 11.247050046920776]
memory len:5900
memory used:3394.0
now epsilon is 0.6415402892916349, the reward is 242.25 with loss [57.74709725379944, 55.27781939506531] in episode 444
Report: 
rewardSum:242.25
loss:[57.74709725379944, 55.27781939506531]
policies:[2, 2, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5918
memory used:3397.0
now epsilon is 0.6405786281574847, the reward is 53.6875 with loss [47.6249053478241, 54.79117774963379] in episode 445
Report: 
rewardSum:53.6875
loss:[47.6249053478241, 54.79117774963379]
policies:[1, 2, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5938
memory used:3398.0
now epsilon is 0.6401943674500576, the reward is 247.25 with loss [29.418890118598938, 18.832435846328735] in episode 446
Report: 
rewardSum:247.25
loss:[29.418890118598938, 18.832435846328735]
policies:[2, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5946
memory used:3398.0
now epsilon is 0.6396184085417433, the reward is 245.25 with loss [27.815212845802307, 22.39827060699463] in episode 447
Report: 
rewardSum:245.25
loss:[27.815212845802307, 22.39827060699463]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:5958
memory used:3403.0
now epsilon is 0.6388512692902254, the reward is 55.6875 with loss [29.838188409805298, 41.49697780609131] in episode 448
Report: 
rewardSum:55.6875
loss:[29.838188409805298, 41.49697780609131]
policies:[1, 1, 6]
qAverage:[0.0, 56.496788024902344]
ws:[1.0417088270187378, 2.6738381385803223]
memory len:5974
memory used:3408.0
now epsilon is 0.6382765187170502, the reward is 57.6875 with loss [30.01831364631653, 36.62375211715698] in episode 449
Report: 
rewardSum:57.6875
loss:[30.01831364631653, 36.62375211715698]
policies:[1, 2, 3]
qAverage:[0.0, 83.44009399414062]
ws:[9.45262368520101, 11.04331080118815]
memory len:5986
memory used:3411.0
now epsilon is 0.6376066298826673, the reward is 56.6875 with loss [28.399906635284424, 28.840324640274048] in episode 450
Report: 
rewardSum:56.6875
loss:[28.399906635284424, 28.840324640274048]
policies:[1, 2, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6000
memory used:3411.0
now epsilon is 0.6370329990649769, the reward is 994.0 with loss [28.37763476371765, 55.963218688964844] in episode 451
Report: 
rewardSum:994.0
loss:[28.37763476371765, 55.963218688964844]
policies:[0, 2, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6012
memory used:3410.0
now epsilon is 0.6364598843214608, the reward is 245.25 with loss [20.89785099029541, 27.484474658966064] in episode 452
Report: 
rewardSum:245.25
loss:[20.89785099029541, 27.484474658966064]
policies:[1, 1, 4]
qAverage:[0.0, 70.42193603515625]
ws:[11.332983016967773, 13.23957347869873]
memory len:6024
memory used:3412.0
now epsilon is 0.6358872851878263, the reward is 245.25 with loss [40.301063776016235, 35.045174449682236] in episode 453
Report: 
rewardSum:245.25
loss:[40.301063776016235, 35.045174449682236]
policies:[3, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6036
memory used:3412.0
now epsilon is 0.6350293522412904, the reward is 54.6875 with loss [59.17218828201294, 65.92612743377686] in episode 454
Report: 
rewardSum:54.6875
loss:[59.17218828201294, 65.92612743377686]
policies:[2, 1, 6]
qAverage:[0.0, 54.687252044677734]
ws:[0.572349488735199, 1.5169531106948853]
memory len:6054
memory used:3412.0
############# STATE ###############
0*		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.6337921588682011, the reward is 549.6875 with loss [81.83859527111053, 72.64578498899937] in episode 455
Report: 
rewardSum:549.6875
loss:[81.83859527111053, 72.64578498899937]
policies:[2, 0, 11]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6080
memory used:3412.0
now epsilon is 0.6331269764933289, the reward is 56.6875 with loss [34.042094230651855, 35.261200189590454] in episode 456
Report: 
rewardSum:56.6875
loss:[34.042094230651855, 35.261200189590454]
policies:[1, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6094
memory used:3412.0
now epsilon is 0.6325573758521084, the reward is 245.25 with loss [33.31914699077606, 33.34087538719177] in episode 457
Report: 
rewardSum:245.25
loss:[33.31914699077606, 33.34087538719177]
policies:[0, 1, 5]
qAverage:[0.0, 61.349029541015625]
ws:[6.828965663909912, 7.366140842437744]
memory len:6106
memory used:3412.0
now epsilon is 0.6315144386195555, the reward is 989.0 with loss [36.40469402074814, 88.60044583678246] in episode 458
Report: 
rewardSum:989.0
loss:[36.40469402074814, 88.60044583678246]
policies:[1, 2, 8]
qAverage:[0.0, 69.56251017252605]
ws:[1.2785640557607014, 2.339167316754659]
memory len:6128
memory used:3413.0
now epsilon is 0.6311356152023079, the reward is 247.25 with loss [37.07009041309357, 33.39291167259216] in episode 459
Report: 
rewardSum:247.25
loss:[37.07009041309357, 33.39291167259216]
policies:[0, 3, 1]
qAverage:[0.0, 80.84075419108073]
ws:[8.708606243133545, 10.869340737660727]
memory len:6136
memory used:3413.0
now epsilon is 0.6305678061142991, the reward is 245.25 with loss [21.37259340286255, 32.452704191207886] in episode 460
Report: 
rewardSum:245.25
loss:[21.37259340286255, 32.452704191207886]
policies:[0, 1, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6148
memory used:3412.0
now epsilon is 0.6300005078628724, the reward is 57.6875 with loss [22.58027708530426, 32.0496039390564] in episode 461
Report: 
rewardSum:57.6875
loss:[22.58027708530426, 32.0496039390564]
policies:[1, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6160
memory used:3413.0
now epsilon is 0.6295281492108287, the reward is -4.0 with loss [12.980279803276062, 31.5099858045578] in episode 462
Report: 
rewardSum:-4.0
loss:[12.980279803276062, 31.5099858045578]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6170
memory used:3412.0
now epsilon is 0.6290561447215093, the reward is 58.6875 with loss [31.839274406433105, 28.152472019195557] in episode 463
Report: 
rewardSum:58.6875
loss:[31.839274406433105, 28.152472019195557]
policies:[1, 1, 3]
qAverage:[57.5583610534668, 0.0]
ws:[2.4805784225463867, 2.1903326511383057]
memory len:6180
memory used:3413.0
now epsilon is 0.6283959329242841, the reward is 244.25 with loss [84.17763948440552, 37.25938534736633] in episode 464
Report: 
rewardSum:244.25
loss:[84.17763948440552, 37.25938534736633]
policies:[4, 0, 3]
qAverage:[52.35948944091797, 0.0]
ws:[2.1367814540863037, 2.113128662109375]
memory len:6194
memory used:3411.0
now epsilon is 0.6269835256960498, the reward is 236.25 with loss [75.09059000015259, 62.31711769104004] in episode 465
Report: 
rewardSum:236.25
loss:[75.09059000015259, 62.31711769104004]
policies:[3, 3, 9]
qAverage:[21.859893798828125, 70.36493225097657]
ws:[7.8608087539672855, 9.21107292175293]
memory len:6224
memory used:3411.0
now epsilon is 0.6265134291019119, the reward is 246.25 with loss [53.934134006500244, 26.87453857064247] in episode 466
Report: 
rewardSum:246.25
loss:[53.934134006500244, 26.87453857064247]
policies:[3, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6234
memory used:3412.0
now epsilon is 0.6257620075720611, the reward is 243.25 with loss [51.528830766677856, 27.83359134197235] in episode 467
Report: 
rewardSum:243.25
loss:[51.528830766677856, 27.83359134197235]
policies:[0, 3, 5]
qAverage:[0.0, 83.67646789550781]
ws:[0.49734342098236084, 2.0585837364196777]
memory len:6250
memory used:3412.0
now epsilon is 0.6252928268417159, the reward is 58.6875 with loss [42.09746837615967, 28.994183629751205] in episode 468
Report: 
rewardSum:58.6875
loss:[42.09746837615967, 28.994183629751205]
policies:[0, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6260
memory used:3412.0
now epsilon is 0.6249177355517015, the reward is 59.6875 with loss [14.84048879146576, 23.53906559944153] in episode 469
Report: 
rewardSum:59.6875
loss:[14.84048879146576, 23.53906559944153]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6268
memory used:3417.0
############# STATE ###############
0-		8-		16*		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.6243555204572635, the reward is 245.25 with loss [39.13655111193657, 27.79366898536682] in episode 470
Report: 
rewardSum:245.25
loss:[39.13655111193657, 27.79366898536682]
policies:[1, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6280
memory used:3417.0
now epsilon is 0.6239809914245561, the reward is 247.25 with loss [11.989800930023193, 25.31386137008667] in episode 471
Report: 
rewardSum:247.25
loss:[11.989800930023193, 25.31386137008667]
policies:[0, 3, 1]
qAverage:[0.0, 86.02074178059895]
ws:[12.101816177368164, 14.642622311909994]
memory len:6288
memory used:3417.0
now epsilon is 0.6236066870587118, the reward is 59.6875 with loss [17.674910247325897, 16.264321327209473] in episode 472
Report: 
rewardSum:59.6875
loss:[17.674910247325897, 16.264321327209473]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6296
memory used:3418.0
now epsilon is 0.6230456514655272, the reward is 994.0 with loss [27.929491639137268, 27.630605220794678] in episode 473
Report: 
rewardSum:994.0
loss:[27.929491639137268, 27.630605220794678]
policies:[2, 2, 2]
qAverage:[48.129737854003906, 0.0]
ws:[-47.55735778808594, -53.56586837768555]
memory len:6308
memory used:3417.0
now epsilon is 0.6222983890847954, the reward is 992.0 with loss [41.094666481018066, 53.29790532588959] in episode 474
Report: 
rewardSum:992.0
loss:[41.094666481018066, 53.29790532588959]
policies:[0, 2, 6]
qAverage:[0.0, 56.44431686401367]
ws:[1.4165576696395874, 2.540498971939087]
memory len:6324
memory used:3452.0
now epsilon is 0.6219250940532263, the reward is 247.25 with loss [14.963404387235641, 40.60031294822693] in episode 475
Report: 
rewardSum:247.25
loss:[14.963404387235641, 40.60031294822693]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6332
memory used:3451.0
now epsilon is 0.6213655713263224, the reward is -5.0 with loss [39.79130971431732, 46.65981829166412] in episode 476
Report: 
rewardSum:-5.0
loss:[39.79130971431732, 46.65981829166412]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6344
memory used:3412.0
now epsilon is 0.6209928358594906, the reward is 247.25 with loss [25.324688971042633, 19.163504481315613] in episode 477
Report: 
rewardSum:247.25
loss:[25.324688971042633, 19.163504481315613]
policies:[0, 2, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6352
memory used:3412.0
now epsilon is 0.6202480355646003, the reward is 243.25 with loss [65.17337989807129, 54.212167382240295] in episode 478
Report: 
rewardSum:243.25
loss:[65.17337989807129, 54.212167382240295]
policies:[1, 2, 5]
qAverage:[0.0, 66.73515319824219]
ws:[9.415301322937012, 11.920475006103516]
memory len:6368
memory used:3413.0
now epsilon is 0.6193182912612549, the reward is 53.6875 with loss [48.33278262615204, 55.632694125175476] in episode 479
Report: 
rewardSum:53.6875
loss:[48.33278262615204, 55.632694125175476]
policies:[0, 2, 8]
qAverage:[0.0, 73.52169291178386]
ws:[-0.5438657204310099, 1.490927000840505]
memory len:6388
memory used:3419.0
now epsilon is 0.6187611137772439, the reward is 245.25 with loss [31.500771403312683, 34.567508697509766] in episode 480
Report: 
rewardSum:245.25
loss:[31.500771403312683, 34.567508697509766]
policies:[2, 1, 3]
qAverage:[0.0, 54.014015197753906]
ws:[5.285852909088135, 6.774898529052734]
memory len:6400
memory used:3415.0
now epsilon is 0.6178335983516734, the reward is -9.0 with loss [45.4940801858902, 63.42117786407471] in episode 481
Report: 
rewardSum:-9.0
loss:[45.4940801858902, 63.42117786407471]
policies:[0, 1, 9]
qAverage:[0.0, 54.953758239746094]
ws:[6.712608814239502, 7.195550918579102]
memory len:6420
memory used:3416.0
now epsilon is 0.6170925871520697, the reward is 554.6875 with loss [47.79624795913696, 41.325269758701324] in episode 482
Report: 
rewardSum:554.6875
loss:[47.79624795913696, 41.325269758701324]
policies:[0, 2, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6436
memory used:3428.0
now epsilon is 0.6162600118295039, the reward is 242.25 with loss [52.963865518569946, 49.95152044296265] in episode 483
Report: 
rewardSum:242.25
loss:[52.963865518569946, 49.95152044296265]
policies:[3, 2, 4]
qAverage:[0.0, 74.2719243367513]
ws:[3.134567936261495, 4.843615214029948]
memory len:6454
memory used:3428.0
now epsilon is 0.6156132299271537, the reward is 244.25 with loss [58.710089683532715, 34.877243638038635] in episode 484
Report: 
rewardSum:244.25
loss:[58.710089683532715, 34.877243638038635]
policies:[0, 2, 5]
qAverage:[0.0, 76.81509908040364]
ws:[4.349446058273315, 6.926262140274048]
memory len:6468
memory used:3434.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17*		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.6145982295764293, the reward is 240.25 with loss [55.513153411448, 55.751508355140686] in episode 485
Report: 
rewardSum:240.25
loss:[55.513153411448, 55.751508355140686]
policies:[2, 3, 6]
qAverage:[0.0, 75.65336036682129]
ws:[0.6650348138064146, 2.898414134979248]
memory len:6490
memory used:3436.0
now epsilon is 0.6142295536011476, the reward is -3.0 with loss [30.54506492614746, 19.849791944026947] in episode 486
Report: 
rewardSum:-3.0
loss:[30.54506492614746, 19.849791944026947]
policies:[1, 1, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6498
memory used:3437.0
now epsilon is 0.613216834604832, the reward is 240.25 with loss [55.10730481147766, 57.246487975120544] in episode 487
Report: 
rewardSum:240.25
loss:[55.10730481147766, 57.246487975120544]
policies:[4, 1, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6520
memory used:3436.0
now epsilon is 0.612573246601026, the reward is 244.25 with loss [48.919376730918884, 44.21500515937805] in episode 488
Report: 
rewardSum:244.25
loss:[48.919376730918884, 44.21500515937805]
policies:[1, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6534
memory used:3441.0
now epsilon is 0.6118385445104955, the reward is 55.6875 with loss [38.88011431694031, 42.02202844619751] in episode 489
Report: 
rewardSum:55.6875
loss:[38.88011431694031, 42.02202844619751]
policies:[1, 2, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6550
memory used:3438.0
now epsilon is 0.6113798032451372, the reward is 246.25 with loss [54.328683376312256, 25.381510257720947] in episode 490
Report: 
rewardSum:246.25
loss:[54.328683376312256, 25.381510257720947]
policies:[0, 1, 4]
qAverage:[0.0, 49.42351531982422]
ws:[4.075915336608887, 5.086477756500244]
memory len:6560
memory used:3438.0
now epsilon is 0.6108297677216368, the reward is 245.25 with loss [40.922184228897095, 42.033584117889404] in episode 491
Report: 
rewardSum:245.25
loss:[40.922184228897095, 42.033584117889404]
policies:[2, 2, 2]
qAverage:[0.0, 86.4484125773112]
ws:[7.681980609893799, 9.525211016337076]
memory len:6572
memory used:3438.0
now epsilon is 0.6105549355551099, the reward is -2.0 with loss [21.47981834411621, 18.701375126838684] in episode 492
Report: 
rewardSum:-2.0
loss:[21.47981834411621, 18.701375126838684]
policies:[0, 1, 2]
qAverage:[0.0, 50.963584899902344]
ws:[-1.415831446647644, -0.9067862033843994]
memory len:6578
memory used:3438.0
now epsilon is 0.6100056421341933, the reward is 245.25 with loss [23.1094753742218, 33.344202637672424] in episode 493
Report: 
rewardSum:245.25
loss:[23.1094753742218, 33.344202637672424]
policies:[1, 2, 3]
qAverage:[0.0, 82.95426686604817]
ws:[6.078429818153381, 8.5205370982488]
memory len:6590
memory used:3438.0
now epsilon is 0.609456842892006, the reward is 245.25 with loss [40.31818997859955, 24.979795455932617] in episode 494
Report: 
rewardSum:245.25
loss:[40.31818997859955, 24.979795455932617]
policies:[0, 4, 2]
qAverage:[0.0, 85.62331199645996]
ws:[7.923449218273163, 10.140905499458313]
memory len:6602
memory used:3438.0
now epsilon is 0.6090912510547173, the reward is 59.6875 with loss [37.60965323448181, 10.134382724761963] in episode 495
Report: 
rewardSum:59.6875
loss:[37.60965323448181, 10.134382724761963]
policies:[0, 3, 1]
qAverage:[0.0, 82.72412300109863]
ws:[5.580539107322693, 7.330246210098267]
memory len:6610
memory used:3437.0
now epsilon is 0.6080870039018144, the reward is -10.0 with loss [60.72735774517059, 44.33371549844742] in episode 496
Report: 
rewardSum:-10.0
loss:[60.72735774517059, 44.33371549844742]
policies:[1, 1, 9]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6632
memory used:3436.0
now epsilon is 0.6079045914826015, the reward is -1.0 with loss [6.163904219865799, 9.561646699905396] in episode 497
Report: 
rewardSum:-1.0
loss:[6.163904219865799, 9.561646699905396]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6636
memory used:3438.0
now epsilon is 0.6071754888378426, the reward is 243.25 with loss [54.2327356338501, 40.285468220710754] in episode 498
Report: 
rewardSum:243.25
loss:[54.2327356338501, 40.285468220710754]
policies:[1, 3, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6652
memory used:3438.0
now epsilon is 0.6063562935679621, the reward is 54.6875 with loss [40.166611552238464, 66.79522907733917] in episode 499
Report: 
rewardSum:54.6875
loss:[40.166611552238464, 66.79522907733917]
policies:[2, 2, 5]
qAverage:[0.0, 78.05172983805339]
ws:[7.995330810546875, 10.064149538675943]
memory len:6670
memory used:3438.0
now epsilon is 0.6059925616417354, the reward is 247.25 with loss [22.637052059173584, 29.264911651611328] in episode 500
Report: 
rewardSum:247.25
loss:[22.637052059173584, 29.264911651611328]
policies:[1, 2, 1]
qAverage:[0.0, 61.52505874633789]
ws:[0.8213388323783875, 1.083937168121338]
memory len:6678
memory used:3438.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26*		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.604539814528944, the reward is 235.25 with loss [74.49721139669418, 109.79956191778183] in episode 501
Report: 
rewardSum:235.25
loss:[74.49721139669418, 109.79956191778183]
policies:[3, 4, 9]
qAverage:[0.0, 73.90414428710938]
ws:[6.397167682647705, 7.592819372812907]
memory len:6710
memory used:3441.0
now epsilon is 0.6036336166589386, the reward is 241.25 with loss [58.749122858047485, 46.17541313171387] in episode 502
Report: 
rewardSum:241.25
loss:[58.749122858047485, 46.17541313171387]
policies:[2, 1, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6730
memory used:3441.0
now epsilon is 0.602909636494061, the reward is 992.0 with loss [61.55239987373352, 56.02679777145386] in episode 503
Report: 
rewardSum:992.0
loss:[61.55239987373352, 56.02679777145386]
policies:[0, 3, 5]
qAverage:[0.0, 62.926307678222656]
ws:[0.9760327935218811, 2.287262201309204]
memory len:6746
memory used:3441.0
now epsilon is 0.6027287771685796, the reward is -1.0 with loss [5.38303530216217, 11.79306936264038] in episode 504
Report: 
rewardSum:-1.0
loss:[5.38303530216217, 11.79306936264038]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6750
memory used:3440.0
now epsilon is 0.6021865246494108, the reward is 245.25 with loss [40.49791717529297, 33.70842054486275] in episode 505
Report: 
rewardSum:245.25
loss:[40.49791717529297, 33.70842054486275]
policies:[2, 2, 2]
qAverage:[0.0, 73.4715347290039]
ws:[5.231390515963237, 7.262447675069173]
memory len:6762
memory used:3441.0
now epsilon is 0.6018252940216728, the reward is 247.25 with loss [11.861236810684204, 7.462896108627319] in episode 506
Report: 
rewardSum:247.25
loss:[11.861236810684204, 7.462896108627319]
policies:[1, 1, 2]
qAverage:[0.0, 68.4638671875]
ws:[11.504951477050781, 13.26562213897705]
memory len:6770
memory used:3440.0
now epsilon is 0.6011034827050585, the reward is 243.25 with loss [47.4743914604187, 30.77863645553589] in episode 507
Report: 
rewardSum:243.25
loss:[47.4743914604187, 30.77863645553589]
policies:[1, 2, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6786
memory used:3439.0
now epsilon is 0.6005626924024796, the reward is 245.25 with loss [31.250241920351982, 53.482019782066345] in episode 508
Report: 
rewardSum:245.25
loss:[31.250241920351982, 53.482019782066345]
policies:[1, 1, 4]
qAverage:[0.0, 60.58906936645508]
ws:[3.3583290576934814, 5.520975589752197]
memory len:6798
memory used:3440.0
now epsilon is 0.6000223886286926, the reward is 245.25 with loss [33.19746255874634, 21.06083607673645] in episode 509
Report: 
rewardSum:245.25
loss:[33.19746255874634, 21.06083607673645]
policies:[2, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6810
memory used:3440.0
now epsilon is 0.5994825709459861, the reward is 245.25 with loss [45.72460436820984, 30.323423862457275] in episode 510
Report: 
rewardSum:245.25
loss:[45.72460436820984, 30.323423862457275]
policies:[0, 3, 3]
qAverage:[0.0, 49.157127380371094]
ws:[2.451974868774414, 4.299664497375488]
memory len:6822
memory used:3439.0
now epsilon is 0.5989432389170419, the reward is 245.25 with loss [43.350937604904175, 49.89793872833252] in episode 511
Report: 
rewardSum:245.25
loss:[43.350937604904175, 49.89793872833252]
policies:[1, 2, 3]
qAverage:[0.0, 49.051631927490234]
ws:[1.1499696969985962, 2.20831036567688]
memory len:6834
memory used:3440.0
now epsilon is 0.5987635694215897, the reward is -1.0 with loss [13.08608092367649, 5.4662230014801025] in episode 512
Report: 
rewardSum:-1.0
loss:[13.08608092367649, 5.4662230014801025]
policies:[0, 1, 1]
qAverage:[0.0, 49.7152099609375]
ws:[-4.057670593261719, -2.1832237243652344]
memory len:6838
memory used:3439.0
now epsilon is 0.5982248842514031, the reward is 245.25 with loss [25.122602850198746, 25.706403613090515] in episode 513
Report: 
rewardSum:245.25
loss:[25.122602850198746, 25.706403613090515]
policies:[1, 2, 3]
qAverage:[0.0, 54.35079574584961]
ws:[2.360355854034424, 4.819027900695801]
memory len:6850
memory used:3439.0
now epsilon is 0.5976866837160998, the reward is 57.6875 with loss [44.06229305267334, 29.258487939834595] in episode 514
Report: 
rewardSum:57.6875
loss:[44.06229305267334, 29.258487939834595]
policies:[0, 1, 5]
qAverage:[0.0, 49.25645065307617]
ws:[3.647655725479126, 5.781771659851074]
memory len:6862
memory used:3439.0
now epsilon is 0.597059395034565, the reward is 244.25 with loss [42.51696157455444, 27.107800006866455] in episode 515
Report: 
rewardSum:244.25
loss:[42.51696157455444, 27.107800006866455]
policies:[1, 2, 4]
qAverage:[0.0, 82.72097269694011]
ws:[5.437847137451172, 8.877115408579508]
memory len:6876
memory used:3439.0
now epsilon is 0.5967012399925025, the reward is 247.25 with loss [17.654475450515747, 28.57376539707184] in episode 516
Report: 
rewardSum:247.25
loss:[17.654475450515747, 28.57376539707184]
policies:[2, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6884
memory used:3439.0
now epsilon is 0.5961644102229049, the reward is 245.25 with loss [41.27411913871765, 26.323428213596344] in episode 517
Report: 
rewardSum:245.25
loss:[41.27411913871765, 26.323428213596344]
policies:[0, 5, 1]
qAverage:[0.0, 86.0901927947998]
ws:[3.8712113089859486, 7.472530961036682]
memory len:6896
memory used:3439.0
now epsilon is 0.595449388401562, the reward is 55.6875 with loss [57.77094292640686, 37.93177855014801] in episode 518
Report: 
rewardSum:55.6875
loss:[57.77094292640686, 37.93177855014801]
policies:[0, 2, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6912
memory used:3439.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19*		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.5950029353162783, the reward is 58.6875 with loss [23.92622220516205, 29.952797412872314] in episode 519
Report: 
rewardSum:58.6875
loss:[23.92622220516205, 29.952797412872314]
policies:[0, 2, 3]
qAverage:[0.0, 70.53566487630208]
ws:[2.083616591989994, 4.748910744984944]
memory len:6922
memory used:3439.0
now epsilon is 0.5940219164578963, the reward is 52.6875 with loss [61.9126957654953, 52.137964487075806] in episode 520
Report: 
rewardSum:52.6875
loss:[61.9126957654953, 52.137964487075806]
policies:[1, 3, 7]
qAverage:[0.0, 49.458919525146484]
ws:[1.1490017175674438, 2.9292662143707275]
memory len:6944
memory used:3439.0
now epsilon is 0.5933984740508128, the reward is 244.25 with loss [29.541951894760132, 35.19479179382324] in episode 521
Report: 
rewardSum:244.25
loss:[29.541951894760132, 35.19479179382324]
policies:[0, 2, 5]
qAverage:[0.0, 89.78564453125]
ws:[8.204607486724854, 10.355106353759766]
memory len:6958
memory used:3441.0
now epsilon is 0.5931314847898842, the reward is -2.0 with loss [19.726353645324707, 14.82327651977539] in episode 522
Report: 
rewardSum:-2.0
loss:[19.726353645324707, 14.82327651977539]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:6964
memory used:3440.0
now epsilon is 0.5925978665954177, the reward is 245.25 with loss [14.75842410326004, 40.1181538105011] in episode 523
Report: 
rewardSum:245.25
loss:[14.75842410326004, 40.1181538105011]
policies:[1, 3, 2]
qAverage:[0.0, 84.61520576477051]
ws:[7.467948317527771, 8.564098119735718]
memory len:6976
memory used:3440.0
now epsilon is 0.5920647284772659, the reward is 245.25 with loss [19.153670072555542, 37.18083381652832] in episode 524
Report: 
rewardSum:245.25
loss:[19.153670072555542, 37.18083381652832]
policies:[0, 4, 2]
qAverage:[0.0, 61.27963638305664]
ws:[4.848846912384033, 5.829890727996826]
memory len:6988
memory used:3439.0
now epsilon is 0.5911772306103642, the reward is 241.25 with loss [43.526410311460495, 72.07075357437134] in episode 525
Report: 
rewardSum:241.25
loss:[43.526410311460495, 72.07075357437134]
policies:[0, 4, 6]
qAverage:[0.0, 89.10999043782552]
ws:[4.696348428726196, 6.925421237945557]
memory len:7008
memory used:3452.0
now epsilon is 0.5907339806823327, the reward is 246.25 with loss [29.347230672836304, 29.64359140396118] in episode 526
Report: 
rewardSum:246.25
loss:[29.347230672836304, 29.64359140396118]
policies:[0, 2, 3]
qAverage:[0.0, 83.00776926676433]
ws:[8.016295909881592, 10.520788510640463]
memory len:7018
memory used:3454.0
now epsilon is 0.5902025194325672, the reward is 245.25 with loss [48.02606248855591, 37.36854910850525] in episode 527
Report: 
rewardSum:245.25
loss:[48.02606248855591, 37.36854910850525]
policies:[1, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7030
memory used:3454.0
now epsilon is 0.5898484775902804, the reward is 247.25 with loss [11.851936101913452, 14.776679515838623] in episode 528
Report: 
rewardSum:247.25
loss:[11.851936101913452, 14.776679515838623]
policies:[1, 2, 1]
qAverage:[0.0, 79.55147552490234]
ws:[6.498149553934733, 8.192790508270264]
memory len:7038
memory used:3454.0
now epsilon is 0.5887876258177923, the reward is 239.25 with loss [70.47989332675934, 78.48539853096008] in episode 529
Report: 
rewardSum:239.25
loss:[70.47989332675934, 78.48539853096008]
policies:[3, 4, 5]
qAverage:[0.0, 93.2528076171875]
ws:[11.325512409210205, 13.687865018844604]
memory len:7062
memory used:3454.0
now epsilon is 0.5882579156306412, the reward is 245.25 with loss [20.239278078079224, 41.65739917755127] in episode 530
Report: 
rewardSum:245.25
loss:[20.239278078079224, 41.65739917755127]
policies:[1, 0, 5]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7074
memory used:3453.0
now epsilon is 0.5879050402881404, the reward is 247.25 with loss [12.633270740509033, 17.56411623954773] in episode 531
Report: 
rewardSum:247.25
loss:[12.633270740509033, 17.56411623954773]
policies:[1, 3, 0]
qAverage:[0.0, 56.294158935546875]
ws:[2.745543956756592, 3.4458541870117188]
memory len:7082
memory used:3455.0
now epsilon is 0.5873761241301532, the reward is 245.25 with loss [33.625693678855896, 50.362065076828] in episode 532
Report: 
rewardSum:245.25
loss:[33.625693678855896, 50.362065076828]
policies:[0, 3, 3]
qAverage:[0.0, 67.77738189697266]
ws:[9.831548690795898, 12.579895973205566]
memory len:7094
memory used:3453.0
now epsilon is 0.5870237777435225, the reward is 247.25 with loss [31.66494381427765, 29.68999707698822] in episode 533
Report: 
rewardSum:247.25
loss:[31.66494381427765, 29.68999707698822]
policies:[0, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7102
memory used:3454.0
now epsilon is 0.5866716427171619, the reward is 59.6875 with loss [20.355454683303833, 30.77800214290619] in episode 534
Report: 
rewardSum:59.6875
loss:[20.355454683303833, 30.77800214290619]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7110
memory used:3455.0
now epsilon is 0.5861438362008, the reward is 57.6875 with loss [35.385746002197266, 24.58906102180481] in episode 535
Report: 
rewardSum:57.6875
loss:[35.385746002197266, 24.58906102180481]
policies:[3, 1, 2]
qAverage:[0.0, 48.6043815612793]
ws:[1.2529239654541016, 2.0949437618255615]
memory len:7122
memory used:3457.0
now epsilon is 0.5856165045322039, the reward is 245.25 with loss [28.57482945919037, 46.08749866485596] in episode 536
Report: 
rewardSum:245.25
loss:[28.57482945919037, 46.08749866485596]
policies:[0, 3, 3]
qAverage:[0.0, 55.88481521606445]
ws:[0.47633644938468933, 2.194676160812378]
memory len:7134
memory used:3459.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17*		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.5850896472841706, the reward is 57.6875 with loss [24.26971012353897, 17.9686678647995] in episode 537
Report: 
rewardSum:57.6875
loss:[24.26971012353897, 17.9686678647995]
policies:[0, 3, 3]
qAverage:[0.0, 87.80906867980957]
ws:[5.095836281776428, 7.696465492248535]
memory len:7146
memory used:3462.0
now epsilon is 0.5843879082033462, the reward is 243.25 with loss [46.318096578121185, 46.830299854278564] in episode 538
Report: 
rewardSum:243.25
loss:[46.318096578121185, 46.830299854278564]
policies:[1, 2, 5]
qAverage:[0.0, 54.78258514404297]
ws:[3.4671568870544434, 4.962975978851318]
memory len:7162
memory used:3464.0
now epsilon is 0.5840373543429029, the reward is 247.25 with loss [32.6079318523407, 22.092101097106934] in episode 539
Report: 
rewardSum:247.25
loss:[32.6079318523407, 22.092101097106934]
policies:[0, 4, 0]
qAverage:[0.0, 80.30650329589844]
ws:[7.405655384063721, 9.54149595896403]
memory len:7170
memory used:3464.0
now epsilon is 0.5831618894127368, the reward is 241.25 with loss [45.52332806587219, 44.39168894290924] in episode 540
Report: 
rewardSum:241.25
loss:[45.52332806587219, 44.39168894290924]
policies:[2, 3, 5]
qAverage:[0.0, 91.67792510986328]
ws:[6.419295787811279, 9.479759931564331]
memory len:7190
memory used:3465.0
now epsilon is 0.5827246491874222, the reward is 246.25 with loss [24.97469037771225, 38.18592953681946] in episode 541
Report: 
rewardSum:246.25
loss:[24.97469037771225, 38.18592953681946]
policies:[0, 2, 3]
qAverage:[0.0, 55.88680648803711]
ws:[3.3839664459228516, 4.692463397979736]
memory len:7200
memory used:3464.0
now epsilon is 0.5820257466148121, the reward is 55.6875 with loss [36.46484422683716, 64.9485445022583] in episode 542
Report: 
rewardSum:55.6875
loss:[36.46484422683716, 64.9485445022583]
policies:[1, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7216
memory used:3473.0
now epsilon is 0.581676609732462, the reward is 247.25 with loss [8.936818242073059, 11.836623787879944] in episode 543
Report: 
rewardSum:247.25
loss:[8.936818242073059, 11.836623787879944]
policies:[1, 3, 0]
qAverage:[0.0, 66.52631378173828]
ws:[8.123620986938477, 10.572888374328613]
memory len:7224
memory used:3471.0
now epsilon is 0.5811532970603, the reward is 57.6875 with loss [44.664809226989746, 40.31057667732239] in episode 544
Report: 
rewardSum:57.6875
loss:[44.664809226989746, 40.31057667732239]
policies:[1, 1, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7236
memory used:3469.0
now epsilon is 0.58063045519296, the reward is 245.25 with loss [34.60257053375244, 26.99630355834961] in episode 545
Report: 
rewardSum:245.25
loss:[34.60257053375244, 26.99630355834961]
policies:[2, 2, 2]
qAverage:[0.0, 82.54561614990234]
ws:[7.556969245274861, 9.410895029703775]
memory len:7248
memory used:3471.0
now epsilon is 0.5801080837068768, the reward is 245.25 with loss [28.810048401355743, 26.834728717803955] in episode 546
Report: 
rewardSum:245.25
loss:[28.810048401355743, 26.834728717803955]
policies:[1, 3, 2]
qAverage:[0.0, 67.19453430175781]
ws:[9.988845825195312, 12.5839204788208]
memory len:7260
memory used:3469.0
now epsilon is 0.5794123193649015, the reward is -7.0 with loss [53.89557218551636, 37.16079652309418] in episode 547
Report: 
rewardSum:-7.0
loss:[53.89557218551636, 37.16079652309418]
policies:[0, 2, 6]
qAverage:[0.0, 70.2881851196289]
ws:[1.394769271214803, 3.5761253039042153]
memory len:7276
memory used:3469.0
now epsilon is 0.5789778904735959, the reward is 246.25 with loss [34.707218170166016, 24.929980278015137] in episode 548
Report: 
rewardSum:246.25
loss:[34.707218170166016, 24.929980278015137]
policies:[1, 1, 3]
qAverage:[0.0, 53.962677001953125]
ws:[2.3845109939575195, 4.052061080932617]
memory len:7286
memory used:3470.0
now epsilon is 0.5784570057381312, the reward is 57.6875 with loss [44.532275438308716, 35.732224464416504] in episode 549
Report: 
rewardSum:57.6875
loss:[44.532275438308716, 35.732224464416504]
policies:[2, 0, 4]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7298
memory used:3468.0
now epsilon is 0.5781100096185752, the reward is 247.25 with loss [18.71259307861328, 16.93381667137146] in episode 550
Report: 
rewardSum:247.25
loss:[18.71259307861328, 16.93381667137146]
policies:[1, 1, 2]
qAverage:[0.0, 59.23843765258789]
ws:[0.5192061066627502, 1.899945855140686]
memory len:7306
memory used:3469.0
now epsilon is 0.5775899056830287, the reward is 245.25 with loss [24.926735162734985, 23.21918797492981] in episode 551
Report: 
rewardSum:245.25
loss:[24.926735162734985, 23.21918797492981]
policies:[1, 3, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7318
memory used:3469.0
now epsilon is 0.577243429706459, the reward is 247.25 with loss [15.58483362197876, 16.91856861114502] in episode 552
Report: 
rewardSum:247.25
loss:[15.58483362197876, 16.91856861114502]
policies:[3, 1, 0]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7326
memory used:3468.0
now epsilon is 0.5769837091250745, the reward is -2.0 with loss [16.695300340652466, 10.014299392700195] in episode 553
Report: 
rewardSum:-2.0
loss:[16.695300340652466, 10.014299392700195]
policies:[0, 0, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7332
memory used:3465.0
now epsilon is 0.5768106269944705, the reward is -1.0 with loss [5.609847068786621, 10.680694580078125] in episode 554
Report: 
rewardSum:-1.0
loss:[5.609847068786621, 10.680694580078125]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7336
memory used:3465.0
now epsilon is 0.5763781487871499, the reward is 246.25 with loss [14.611746311187744, 47.15441596508026] in episode 555
Report: 
rewardSum:246.25
loss:[14.611746311187744, 47.15441596508026]
policies:[0, 4, 1]
qAverage:[0.0, 84.07197189331055]
ws:[5.041444942355156, 7.7002968192100525]
memory len:7346
memory used:3466.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34*		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.5752552223845258, the reward is 238.25 with loss [103.2232837677002, 82.0716757774353] in episode 556
Report: 
rewardSum:238.25
loss:[103.2232837677002, 82.0716757774353]
policies:[1, 2, 10]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7372
memory used:3479.0
now epsilon is 0.5745652784197516, the reward is 243.25 with loss [40.52538061141968, 54.36624002456665] in episode 557
Report: 
rewardSum:243.25
loss:[40.52538061141968, 54.36624002456665]
policies:[1, 3, 4]
qAverage:[0.0, 83.11817169189453]
ws:[6.04865163564682, 8.109880145639181]
memory len:7388
memory used:3479.0
now epsilon is 0.5742206168112561, the reward is 247.25 with loss [10.373159408569336, 8.972507297992706] in episode 558
Report: 
rewardSum:247.25
loss:[10.373159408569336, 8.972507297992706]
policies:[1, 2, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7396
memory used:3479.0
now epsilon is 0.5737040120168285, the reward is 245.25 with loss [37.406853437423706, 38.549864053726196] in episode 559
Report: 
rewardSum:245.25
loss:[37.406853437423706, 38.549864053726196]
policies:[0, 2, 4]
qAverage:[0.0, 49.10383605957031]
ws:[0.11507577449083328, 1.1360176801681519]
memory len:7408
memory used:3485.0
now epsilon is 0.5731018938115982, the reward is 244.25 with loss [32.488122314214706, 33.88090169429779] in episode 560
Report: 
rewardSum:244.25
loss:[32.488122314214706, 33.88090169429779]
policies:[0, 1, 6]
qAverage:[0.0, 53.01621627807617]
ws:[3.7401342391967773, 5.603305339813232]
memory len:7422
memory used:3486.0
now epsilon is 0.5724145324849216, the reward is 243.25 with loss [25.22282063961029, 39.55748134851456] in episode 561
Report: 
rewardSum:243.25
loss:[25.22282063961029, 39.55748134851456]
policies:[0, 4, 4]
qAverage:[0.0, 79.72798919677734]
ws:[7.164669672648112, 9.23834753036499]
memory len:7438
memory used:3486.0
now epsilon is 0.5722428210045031, the reward is -1.0 with loss [5.516312122344971, 2.8339474201202393] in episode 562
Report: 
rewardSum:-1.0
loss:[5.516312122344971, 2.8339474201202393]
policies:[0, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7442
memory used:3488.0
now epsilon is 0.5718995525569563, the reward is 247.25 with loss [11.143762677907944, 30.738476514816284] in episode 563
Report: 
rewardSum:247.25
loss:[11.143762677907944, 30.738476514816284]
policies:[2, 1, 1]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7450
memory used:3488.0
now epsilon is 0.5717279955589292, the reward is -1.0 with loss [6.333659648895264, 13.645235538482666] in episode 564
Report: 
rewardSum:-1.0
loss:[6.333659648895264, 13.645235538482666]
policies:[0, 1, 1]
qAverage:[0.0, 48.676517486572266]
ws:[-0.13989318907260895, 0.15814842283725739]
memory len:7454
memory used:3488.0
now epsilon is 0.5707853515613661, the reward is 240.25 with loss [64.2851026058197, 74.28100085258484] in episode 565
Report: 
rewardSum:240.25
loss:[64.2851026058197, 74.28100085258484]
policies:[4, 0, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7476
memory used:3488.0
now epsilon is 0.5702718373464933, the reward is -5.0 with loss [17.43839854001999, 39.48656940460205] in episode 566
Report: 
rewardSum:-5.0
loss:[17.43839854001999, 39.48656940460205]
policies:[1, 0, 5]
qAverage:[42.19635772705078, 0.0]
ws:[1.1524534225463867, 0.9699283838272095]
memory len:7488
memory used:3489.0
now epsilon is 0.5701007686264058, the reward is -1.0 with loss [8.107789039611816, 19.903512954711914] in episode 567
Report: 
rewardSum:-1.0
loss:[8.107789039611816, 19.903512954711914]
policies:[0, 1, 1]
qAverage:[0.0, 48.63356018066406]
ws:[0.974767804145813, 1.7041810750961304]
memory len:7492
memory used:3489.0
now epsilon is 0.569587870305174, the reward is -5.0 with loss [38.259969830513, 31.802651166915894] in episode 568
Report: 
rewardSum:-5.0
loss:[38.259969830513, 31.802651166915894]
policies:[0, 0, 6]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7504
memory used:3488.0
now epsilon is 0.5692461944696643, the reward is 59.6875 with loss [21.002484798431396, 30.683369874954224] in episode 569
Report: 
rewardSum:59.6875
loss:[21.002484798431396, 30.683369874954224]
policies:[0, 1, 3]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7512
memory used:3488.0
now epsilon is 0.5679667345024866, the reward is 48.6875 with loss [77.6475739479065, 63.32641476392746] in episode 570
Report: 
rewardSum:48.6875
loss:[77.6475739479065, 63.32641476392746]
policies:[0, 5, 10]
qAverage:[0.0, 74.35634104410808]
ws:[2.644010066986084, 4.515834808349609]
memory len:7542
memory used:3486.0
now epsilon is 0.5676260311296272, the reward is -3.0 with loss [16.050771236419678, 26.826995372772217] in episode 571
Report: 
rewardSum:-3.0
loss:[16.050771236419678, 26.826995372772217]
policies:[1, 1, 2]
qAverage:[0.0, 46.9691276550293]
ws:[0.664435088634491, 1.9970176219940186]
memory len:7550
memory used:3486.0
now epsilon is 0.5671153592370857, the reward is 245.25 with loss [28.166677832603455, 28.635992527008057] in episode 572
Report: 
rewardSum:245.25
loss:[28.166677832603455, 28.635992527008057]
policies:[0, 2, 4]
qAverage:[0.0, 58.31403732299805]
ws:[4.6634087562561035, 4.997851848602295]
memory len:7562
memory used:3485.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34-		
3x		11-		19-		27-		35-		
4x		12x		20-		28*		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]
now epsilon is 0.5665201560049136, the reward is 244.25 with loss [15.401054501533508, 40.88034999370575] in episode 573
Report: 
rewardSum:244.25
loss:[15.401054501533508, 40.88034999370575]
policies:[0, 3, 4]
qAverage:[0.0, 48.28357696533203]
ws:[3.3092007637023926, 3.372008800506592]
memory len:7576
memory used:3486.0
now epsilon is 0.5660104790268261, the reward is 245.25 with loss [22.9767963886261, 26.82862365245819] in episode 574
Report: 
rewardSum:245.25
loss:[22.9767963886261, 26.82862365245819]
policies:[1, 1, 4]
qAverage:[0.0, 47.69092559814453]
ws:[1.4629091024398804, 1.5843260288238525]
memory len:7588
memory used:3486.0
now epsilon is 0.5654164353969492, the reward is 244.25 with loss [48.322253823280334, 42.046790599823] in episode 575
Report: 
rewardSum:244.25
loss:[48.322253823280334, 42.046790599823]
policies:[0, 2, 5]
qAverage:[0.0, 82.00616200764973]
ws:[5.436717828114827, 7.836256980895996]
memory len:7602
memory used:3487.0
now epsilon is 0.5650772618592969, the reward is 247.25 with loss [29.921874046325684, 31.647018909454346] in episode 576
Report: 
rewardSum:247.25
loss:[29.921874046325684, 31.647018909454346]
policies:[0, 2, 2]
qAverage:[0.0, 77.07543690999348]
ws:[6.270686944325765, 9.477051417032877]
memory len:7610
memory used:3487.0
now epsilon is 0.564568882999061, the reward is 245.25 with loss [19.832176566123962, 42.560348987579346] in episode 577
Report: 
rewardSum:245.25
loss:[19.832176566123962, 42.560348987579346]
policies:[0, 2, 4]
qAverage:[0.0, 80.29663594563802]
ws:[3.3358170787493386, 6.5065693855285645]
memory len:7622
memory used:3483.0
now epsilon is 0.5640609615082559, the reward is 245.25 with loss [40.11050319671631, 25.62131178379059] in episode 578
Report: 
rewardSum:245.25
loss:[40.11050319671631, 25.62131178379059]
policies:[2, 2, 2]
qAverage:[0.0, 65.95296732584636]
ws:[3.163084308306376, 5.696393380562465]
memory len:7634
memory used:3483.0
now epsilon is 0.5632999359397234, the reward is 242.25 with loss [31.063898742198944, 49.209641218185425] in episode 579
Report: 
rewardSum:242.25
loss:[31.063898742198944, 49.209641218185425]
policies:[1, 4, 4]
qAverage:[0.0, 63.381961822509766]
ws:[5.240301132202148, 9.119091033935547]
memory len:7652
memory used:3481.0
now epsilon is 0.5623711878155002, the reward is 240.25 with loss [101.98531091213226, 64.40919874608517] in episode 580
Report: 
rewardSum:240.25
loss:[101.98531091213226, 64.40919874608517]
policies:[0, 3, 8]
qAverage:[0.0, 80.72070503234863]
ws:[3.3916050493717194, 6.642722129821777]
memory len:7674
memory used:3486.0
now epsilon is 0.5621181587391404, the reward is -2.0 with loss [26.53454875946045, 14.26097822189331] in episode 581
Report: 
rewardSum:-2.0
loss:[26.53454875946045, 14.26097822189331]
policies:[0, 1, 2]
qAverage:[0.0, 47.735687255859375]
ws:[-1.7124160528182983, 0.3554631769657135]
memory len:7680
memory used:3485.0
now epsilon is 0.56178096372226, the reward is 247.25 with loss [22.497882604599, 25.641829013824463] in episode 582
Report: 
rewardSum:247.25
loss:[22.497882604599, 25.641829013824463]
policies:[1, 1, 2]
qAverage:[0.0, 51.44638442993164]
ws:[3.6469602584838867, 6.2385735511779785]
memory len:7688
memory used:3492.0
now epsilon is 0.561443970976873, the reward is 247.25 with loss [15.443597793579102, 18.326218843460083] in episode 583
Report: 
rewardSum:247.25
loss:[15.443597793579102, 18.326218843460083]
policies:[0, 3, 1]
qAverage:[0.0, 73.59110005696614]
ws:[4.768161694208781, 8.557241241137186]
memory len:7696
memory used:3498.0
now epsilon is 0.560938860852441, the reward is 994.0 with loss [30.560056686401367, 26.707406640052795] in episode 584
Report: 
rewardSum:994.0
loss:[30.560056686401367, 26.707406640052795]
policies:[1, 2, 3]
qAverage:[0.0, 73.99256896972656]
ws:[2.0127944548924765, 3.7709762255350747]
memory len:7708
memory used:3487.0
now epsilon is 0.5602660875049028, the reward is 243.25 with loss [44.71197187900543, 56.1321085691452] in episode 585
Report: 
rewardSum:243.25
loss:[44.71197187900543, 56.1321085691452]
policies:[1, 2, 5]
qAverage:[0.0, 69.85933176676433]
ws:[2.889784336090088, 4.343110084533691]
memory len:7724
memory used:3487.0
now epsilon is 0.5594262554162108, the reward is -9.0 with loss [51.82728815078735, 53.35292184352875] in episode 586
Report: 
rewardSum:-9.0
loss:[51.82728815078735, 53.35292184352875]
policies:[1, 2, 7]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7744
memory used:3480.0
now epsilon is 0.5590906751779536, the reward is 247.25 with loss [24.437412977218628, 31.699247360229492] in episode 587
Report: 
rewardSum:247.25
loss:[24.437412977218628, 31.699247360229492]
policies:[0, 2, 2]
qAverage:[0.0, 73.69498952229817]
ws:[2.1231300036112466, 4.092364311218262]
memory len:7752
memory used:3479.0
now epsilon is 0.5586714829481043, the reward is 246.25 with loss [31.601673483848572, 18.96554434299469] in episode 588
Report: 
rewardSum:246.25
loss:[31.601673483848572, 18.96554434299469]
policies:[0, 2, 3]
qAverage:[0.0, 63.9177360534668]
ws:[4.589121341705322, 7.987318992614746]
memory len:7762
memory used:3492.0
now epsilon is 0.5585038940733283, the reward is -1.0 with loss [14.473465919494629, 5.430264711380005] in episode 589
Report: 
rewardSum:-1.0
loss:[14.473465919494629, 5.430264711380005]
policies:[0, 0, 2]
qAverage:[0.0, 0.0]
ws:[0.0, 0.0]
memory len:7766
memory used:3493.0
############# STATE ###############
0-		8-		16-		24-		32-		
1-		9-		17-		25-		33-		
2-		10-		18-		26-		34*		
3x		11-		19-		27-		35-		
4x		12x		20-		28-		36-		
5x		13x		21x		29-		37-		
6x		14x		22x		30x		38-		
7x		15x		23x		31x		39x		
-----------------------------------
1		63		251		562		1000		
###################################


##### Pareto Solutions #####
[1, -3]
[63.6875, -5]
[251.25, -7]
[562.6875, -9]
[1000, -11]

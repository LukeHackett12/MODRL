# -*- coding: utf-8 -*-
"""Mario_TF.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yO-tvzmrO-Unp0EGfDc5TRNJaXE4IaT3
"""

# Commented out IPython magic to ensure Python compatibility.
from nes_py.wrappers import JoypadSpace
import gym_super_mario_bros
from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, RIGHT_ONLY

import gym
from gym.spaces import Box
from gym.wrappers import FrameStack, GrayScaleObservation, ResizeObservation

# %matplotlib inline
import matplotlib.pyplot as plt
import matplotlib
import numpy as np
from typing import NamedTuple
from collections import namedtuple, deque
import random
import datetime

from IPython import display as ipythondisplay

import tensorflow as tf
from tensorflow import keras, Tensor
from tensorboard.plugins.hparams import api as hp

log_dir="logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(
    log_dir=log_dir, histogram_freq=1)

class Transition(NamedTuple):
    currStates: Tensor
    actions: Tensor
    rewards: Tensor
    nextStates: Tensor
    dones: Tensor


class DQNAgent:
    def __init__(self, stateShape, actionSpace, numPicks, memorySize, sync=100000, burnin=10000, alpha=0.0001, epsilon=1, epsilon_decay=0.9999975, epsilon_min=0.01, gamma=0.9):
        self.numPicks = numPicks
        self.replayMemory = deque(maxlen=memorySize)
        self.stateShape = stateShape
        self.actionSpace = actionSpace

        self.step = 0

        self.sync = sync
        self.burnin = burnin
        self.alpha = alpha
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        self.gamma = gamma

        self.trainNetwork = self.createNetwork(
            stateShape, actionSpace.n, self.alpha)

        self.targetNetwork = self.createNetwork(
            stateShape, actionSpace.n, self.alpha)

        self.targetNetwork.set_weights(self.trainNetwork.get_weights())

    def createNetwork(self, n_input, n_output, learningRate):
        model = keras.models.Sequential()

        model.add(keras.layers.Conv2D(32, kernel_size=8, strides=4, activation='relu', input_shape=n_input, data_format="channels_first"))
        model.add(keras.layers.Conv2D(64, kernel_size=4, strides=2, activation='relu', data_format="channels_first"))
        model.add(keras.layers.Conv2D(64, kernel_size=3, strides=1, activation='relu', data_format="channels_first"))
        model.add(keras.layers.Flatten())
        model.add(keras.layers.Dense(512, activation='linear'))
        model.add(keras.layers.Dense(n_output, activation='linear'))

        model.compile(loss=keras.losses.Huber(), optimizer=keras.optimizers.Adam(lr=learningRate))
        return model

    def trainDQN(self):
        if len(self.replayMemory) <= self.numPicks or len(self.replayMemory) <= self.burnin:
            return 0

        samples = random.sample(self.replayMemory, self.numPicks)
        batch = Transition(*zip(*samples))
        currStates, actions, rewards, nextStates, _ = batch

        currStates = np.squeeze(np.array(currStates))
        Q_currents = self.trainNetwork(currStates, training=False).numpy()

        nextStates = np.squeeze(np.array(nextStates))
        Q_futures = self.targetNetwork(nextStates, training=False).numpy().max(axis=1)

        rewards = np.array(rewards).reshape(self.numPicks,).astype(float)
        actions = np.array(actions).reshape(self.numPicks,).astype(int)

        Q_currents[np.arange(self.numPicks), actions] = rewards + Q_futures * self.gamma

        if self.step%10000 == 0:
          hist = self.trainNetwork.fit(currStates, Q_currents, epochs=1, verbose=0, callbacks=[tensorboard_callback])
        else:
          hist = self.trainNetwork.fit(currStates, Q_currents, epochs=1, verbose=0)

        keras.backend.clear_session()
        return hist.history['loss'][0]

    def selectAction(self, state):
        self.step += 1
        self.epsilon = max(self.epsilon*self.epsilon_decay, self.epsilon_min)

        if self.step % self.sync == 0:
            self.targetNetwork.set_weights(self.trainNetwork.get_weights())

        q = -100000
        if np.random.rand(1) < self.epsilon:
            action = np.random.randint(0, 3)
        else:
            preds = np.squeeze(self.trainNetwork(np.expand_dims(np.squeeze(state),0), training=False).numpy(), axis=0)
            action = np.argmax(preds)
            q = preds[action]
        return action, q

    def addMemory(self, memory):
        self.replayMemory.append(memory)

    def save(self, ep):
        save_path = (
            f"./mario_{int(ep)}.chkpt"
        )
        self.trainNetwork.save(save_path)
        print(f"MarioNet saved to {save_path} done!")


class SkipFrame(gym.Wrapper):
    def __init__(self, env, skip):
        """Return only every `skip`-th frame"""
        super().__init__(env)
        self._skip = skip

    def step(self, action):
        """Repeat action, and sum reward"""
        total_reward = 0.0
        done = False
        for i in range(self._skip):
            # Accumulate reward and repeat the same action
            obs, reward, done, info = self.env.step(action)
            total_reward += reward
            if done:
                break
        return obs, total_reward, done, info

# Taken from OpenAI baselines: https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py
class NoopResetEnv(gym.Wrapper):
    def __init__(self, env, noop_max=30):
        """Sample initial states by taking random number of no-ops on reset.
        No-op is assumed to be action 0.
        """
        gym.Wrapper.__init__(self, env)
        self.noop_max = noop_max
        self.override_num_noops = None
        self.noop_action = 0
        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'

    def reset(self, **kwargs):
        """ Do no-op action for a number of steps in [1, noop_max]."""
        self.env.reset(**kwargs)
        if self.override_num_noops is not None:
            noops = self.override_num_noops
        else:
            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1)  # pylint: disable=E1101
        assert noops > 0
        obs = None
        for _ in range(noops):
            obs, _, done, _ = self.env.step(self.noop_action)
            if done:
                obs = self.env.reset(**kwargs)
        return obs

    def step(self, ac):
        return self.env.step(ac)

class MarioBaseline(object):
    def __init__(self, episodes):
        self.current_episode = 0
        self.episodes = episodes

        self.episode_score = []
        self.episode_qs = []
        self.episode_distance = []
        self.episode_loss = []

        self.fig, self.ax = plt.subplots(2, 2)
        self.fig.canvas.draw()
        plt.show(block=False)

        self.env = gym_super_mario_bros.make('SuperMarioBros-v0')
        # Apply Observation Wrappers
        self.env = GrayScaleObservation(self.env)
        self.env = ResizeObservation(self.env, 84)
        # Apply Control Wrappers
        self.env = JoypadSpace(self.env, SIMPLE_MOVEMENT)
        self.env = NoopResetEnv(self.env)
        # Apply Frame Wrappers
        self.env = SkipFrame(self.env, 4)
        self.env = FrameStack(self.env, 4)

        self.agent = DQNAgent(stateShape=(4, 84, 84),
                              actionSpace=self.env.action_space, numPicks=64, memorySize=100000)

    def train(self):
        for _ in range(self.episodes):
            self.episode()
            self.current_episode += 1

        self.env.close()

    def episode(self):
        done = False
        rewardsSum = 0
        qSum = 0
        qActions = 1
        lossSum = 0

        state = np.array(self.env.reset())
        maxDistance = -1000000
        lastX = 0

        while not done:
            action, q = self.agent.selectAction(state)
            if q != -100000:
                qSum += q
                qActions += 1

            obs, reward, done, info = self.env.step(action)

            if info['x_pos'] < lastX:
                reward -= 1
            if info['flag_get']:
                reward += 10

            lastX = info['x_pos']

            if info['x_pos'] > maxDistance:
                maxDistance = info['x_pos']

            nextState = np.array(obs)
            rewardsSum = np.add(rewardsSum, reward)

            self.agent.addMemory((state, action, reward, nextState, done))
            loss = self.agent.trainDQN()
            state = nextState
            lossSum += loss

        if self.current_episode % 200 == 0:
            self.agent.save(self.current_episode)

        self.plot()
        print("now epsilon is {}, the reward is {} with loss {} in episode {}".format(
            self.agent.epsilon, rewardsSum, lossSum, self.current_episode))

        self.episode_score.append(rewardsSum)
        self.episode_qs.append(qSum/qActions)
        self.episode_distance.append(maxDistance)
        self.episode_loss.append(lossSum)

    def plot(self):
        plt.figure(2)
        fig, ax = plt.subplots(2, 2)
        ax[0][0].title.set_text('Training Score')
        ax[0][0].set_xlabel('Episode')
        ax[0][0].set_ylabel('Score')
        ax[0][0].plot(self.episode_score, 'b')

        ax[0][1].title.set_text('Training Distance')
        ax[0][1].set_xlabel('Episode')
        ax[0][1].set_ylabel('Distance')
        ax[0][1].plot(self.episode_distance, 'g')

        ax[1][0].title.set_text('Training Loss')
        ax[1][0].set_xlabel('Episode')
        ax[1][0].set_ylabel('Loss')
        ax[1][0].plot(self.episode_loss, 'r')

        ax[1][1].title.set_text('Q vals')
        ax[1][1].set_xlabel('Episode')
        ax[1][1].set_ylabel('Q vals')
        ax[1][1].plot(self.episode_qs, 'c')
        ipythondisplay.display(plt.gcf())
        ipythondisplay.clear_output()
        plt.pause(0.001)  # pause a bit so that plots are updated

agent = MarioBaseline(5000)
agent.train()
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.9 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "caeb4b62dcf32e497ab1b4ba474b19e7bfa992b4c06523b39b11dd646faa0824"
        }
      }
    },
    "colab": {
      "name": "mountainCar.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Itaq5iEWgQ5R",
        "outputId": "7d53d400-1f74-422f-d711-fd2cd06befa2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/LukeHackett12/MODRL.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'MODRL'...\n",
            "remote: Enumerating objects: 61, done.\u001b[K\n",
            "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 61 (delta 21), reused 49 (delta 12), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (61/61), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xc0oZ3fAgQ5e",
        "outputId": "0994eb27-8cf9-4d05-c9ab-f854d8347c38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MODRL  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rsRtdgngQ5f",
        "outputId": "96c2b3bb-1eeb-4dc4-f6b0-2fb0570f9c60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install pyvirtualdisplay\n",
        "!pip install gym --upgrade\n",
        "!pip install piglet\n",
        "!pip install pygame \n",
        "\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/ad/05/6568620fed440941b704664b9cfe5f836ad699ac7694745e7787fbdc8063/PyVirtualDisplay-2.0-py2.py3-none-any.whl\n",
            "Collecting EasyProcess\n",
            "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.3 pyvirtualdisplay-2.0\n",
            "Collecting gym\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/f2/e7ee20bf02b2d02263becba1c5ec4203fef7cfbd57759e040e51307173f4/gym-0.18.0.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 17.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied, skipping upgrade: Pillow<=7.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.18.0-cp36-none-any.whl size=1656449 sha256=5fff151c69f4e1c2419ed5393efaf308640388f9bce739b4192e640025af5890\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/85/3b/480b828a4a697b37392740a040b8989f729d952b4e441a1877\n",
            "Successfully built gym\n",
            "Installing collected packages: gym\n",
            "  Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed gym-0.18.0\n",
            "Collecting piglet\n",
            "  Downloading https://files.pythonhosted.org/packages/11/56/6840e5f45626dc7eb7cd5dff57d11880b3113723b3b7b1fb1fa537855b75/piglet-1.0.0-py2.py3-none-any.whl\n",
            "Collecting piglet-templates\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/1e/49d7e0df9420eeb13a636487b8e606cf099f2ee0793159edd8ffe905125b/piglet_templates-1.1.0-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.3MB/s \n",
            "\u001b[?25hCollecting Parsley\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/d6/4fed8d65e28a970e1c5cb33ce9c7e22e3de745e1b2ae37af051ef16aea3b/Parsley-1.3-py2.py3-none-any.whl (88kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 10.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (20.3.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.1.1)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.6.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (0.36.2)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n",
            "Installing collected packages: Parsley, piglet-templates, piglet\n",
            "Successfully installed Parsley-1.3 piglet-1.0.0 piglet-templates-1.1.0\n",
            "Collecting pygame\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/da/4ff439558641a26dd29b04c25947e6c0ace041f56b2aa2ef1134edab06b8/pygame-2.0.1-cp36-cp36m-manylinux1_x86_64.whl (11.8MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8MB 14.6MB/s \n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "Successfully installed pygame-2.0.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  python-opengl\n",
            "0 upgraded, 1 newly installed, 0 to remove and 10 not upgraded.\n",
            "Need to get 496 kB of archives.\n",
            "After this operation, 5,416 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Fetched 496 kB in 1s (768 kB/s)\n",
            "Selecting previously unselected package python-opengl.\n",
            "(Reading database ... 146442 files and directories currently installed.)\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 10 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,270 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8 [784 kB]\n",
            "Fetched 784 kB in 1s (1,254 kB/s)\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 148797 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.8_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiMCB5z2gQ5f"
      },
      "source": [
        "import os\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6gAvd8Ai39V"
      },
      "source": [
        "from custom_envs.mountain_car.engine import MountainCar\n",
        "\n",
        "import torch\n",
        "from torch import FloatTensor, LongTensor, BoolTensor\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms as T\n",
        "\n",
        "import enum\n",
        "import random\n",
        "from collections import namedtuple, deque\n",
        "from typing import NamedTuple\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "from PIL import Image\n",
        "from dqn_utils.dqn_models import Experience"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ks7ol8jjJ8E"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN__yQJ-jNVZ"
      },
      "source": [
        "class StatusEnum(enum.Enum):\n",
        "    small = 1\n",
        "    tall = 2\n",
        "    fireball = 3\n",
        "\n",
        "class Transition(NamedTuple):\n",
        "    currStates: FloatTensor\n",
        "    actions: LongTensor\n",
        "    rewards: FloatTensor\n",
        "    nextStates: FloatTensor\n",
        "    dones: BoolTensor"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FvihSFCjW9v"
      },
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape, num_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        self._input_shape = input_shape\n",
        "        self._num_actions = num_actions\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_shape[0], out_channels=32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.feature_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x).view(x.size()[0], -1)\n",
        "        return self.fc(x)\n",
        "    \n",
        "    @property\n",
        "    def feature_size(self):\n",
        "        x = self.features(torch.zeros(1, *self._input_shape))\n",
        "        return x.view(1, -1).size(1)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyijJbWWjSai"
      },
      "source": [
        "class MarioAgent:\n",
        "    def __init__(self, stateShape, actionSpace, numPicks, memorySize):\n",
        "        self.numPicks = numPicks\n",
        "        self.memorySize = memorySize\n",
        "        self.replayMemory = Experience(memorySize, numPicks, 0.9)\n",
        "        self.stateShape = stateShape\n",
        "        self.actionSpace = actionSpace\n",
        "\n",
        "        self.step = 0\n",
        "        self.sync = 10\n",
        "\n",
        "        self.alpha = 0.00025\n",
        "        self.epsilon = 0.99\n",
        "        self.epsilon_decay = 0.9999975\n",
        "        self.epsilon_min=0.05\n",
        "        self.eps_threshold = 0\n",
        "\n",
        "        self.gamma = 0.999\n",
        "        self.tau = 1e-3\n",
        "\n",
        "        self.trainNetwork = DQN(stateShape, len(actionSpace)).to(device)\n",
        "        self.targetNetwork = DQN(stateShape, len(actionSpace)).to(device)\n",
        "        \n",
        "        self.targetNetwork.load_state_dict(self.trainNetwork.state_dict())\n",
        "        self.targetNetwork.eval()\n",
        "        '''\n",
        "        for p in self.trainNetwork.parameters():\n",
        "            p.requires_grad = True\n",
        "        for p in self.targetNetwork.parameters():\n",
        "            p.requires_grad = False\n",
        "        '''\n",
        "        self.optimizer = optim.RMSprop(self.trainNetwork.parameters())\n",
        "\n",
        "    def trainDQN(self):\n",
        "        if self.step <= self.numPicks:\n",
        "            return 1\n",
        "        \n",
        "        samples = self.replayMemory.select(0.9)\n",
        "        batch = Transition(*zip(*samples[0]))\n",
        "        currState,action,reward,nextState,done = batch\n",
        "\n",
        "        '''\n",
        "        states => actions\n",
        "        need ideal actions => Q LERN\n",
        "        '''\n",
        "        '''currState, nextState, action, reward, done = map(\n",
        "            torch.stack, zip(*samples))\n",
        "        '''\n",
        "        reward = torch.flatten(FloatTensor(reward)).to(device)\n",
        "        done = torch.flatten(BoolTensor(done)).to(device)\n",
        "\n",
        "        currState = torch.cat(currState)\n",
        "        action = torch.cat(action)\n",
        "\n",
        "        Q_current = self.trainNetwork(currState).gather(1, action.unsqueeze(1))\n",
        "        \n",
        "        nextStateNonFinal = torch.cat([s for s in nextState\n",
        "                                                if s is not None])\n",
        "        nonFinalMask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          nextState)), dtype=torch.bool).to(device)\n",
        "\n",
        "        Q_futures = torch.zeros(self.numPicks, device=device)\n",
        "        Q_futures[nonFinalMask] = self.targetNetwork(nextStateNonFinal.squeeze(1)).max(1)[0].detach()\n",
        "       # Q_futures_best = torch.argmax(Q_futures, axis=1)\n",
        "        #Q_next = self.trainNetwork(nextState)\n",
        "        '''(reward + (self.gamma * Q_futures)).unsqueeze(1).float() '''\n",
        "\n",
        "        Q_next = (reward * done + (reward + Q_futures * self.gamma)*~done).unsqueeze(1)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss_fn = nn.SmoothL1Loss()\n",
        "        loss = loss_fn(Q_current, Q_next)\n",
        "        for param in self.trainNetwork.parameters():\n",
        "            if param.grad != None:\n",
        "                param.grad.data.clamp_(-1, 1)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        '''self.epsilon *= self.epsilon_decay'''\n",
        "        self.soft_update()\n",
        "        self.epsilon = max(self.epsilon, self.epsilon_min)\n",
        "\n",
        "        return 1+loss.cpu().detach().numpy().max()\n",
        "\n",
        "    def selectAction(self, currState):\n",
        "      self.step += 1\n",
        "      '''self.eps_threshold = self.epsilon_min + (self.epsilon - self.epsilon_min) * \\\n",
        "        math.exp(-1. * self.step / self.epsilon_decay)'''\n",
        "      self.epsilon *= self.epsilon_decay\n",
        "\n",
        "      if np.random.rand(1) < self.epsilon:\n",
        "          action = random.sample(self.actionSpace, 1)[0]\n",
        "      else:\n",
        "          currState = currState.unsqueeze(0).to(device)\n",
        "          with torch.no_grad():\n",
        "              action = np.argmax(self.trainNetwork(currState.squeeze(1)).detach().cpu().numpy())\n",
        "      return action\n",
        "\n",
        "    def addMemory(self, memory, loss):\n",
        "        self.replayMemory.add(memory, loss)\n",
        "\n",
        "    def soft_update(self):\n",
        "        for target_param, train_param in zip(self.targetNetwork.parameters(), self.trainNetwork.parameters()):\n",
        "            target_param.data.copy_(self.tau*train_param.data + (1.0-self.tau)*target_param.data)\n",
        "\n",
        "    def save(self):\n",
        "        save_path = (\n",
        "            f\"./mountain_car_{int(self.step)}.chkpt\"\n",
        "        )\n",
        "        torch.save(\n",
        "            dict(model=self.trainNetwork.state_dict(), epsilon=self.epsilon),\n",
        "            save_path,\n",
        "        )\n",
        "        print(f\"MarioNet saved to {save_path} done!\")"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TURGuhUDjb_r",
        "outputId": "3393ea85-7080-4f85-c83f-d7f65605a5ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "source": [
        "resize = T.Compose([T.ToPILImage(),\n",
        "                    T.Resize(84, interpolation=Image.CUBIC),\n",
        "                    T.ToTensor()])\n",
        "\n",
        "def process_screen(observation):\n",
        "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
        "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
        "    screen = observation.transpose((2, 1, 0))\n",
        "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "    screen = torch.from_numpy(screen)\n",
        "    # Resize, and add a batch dimension (BCHW)\n",
        "    return resize(screen).unsqueeze(0).to(device)\n",
        "\n",
        "episode_score = []\n",
        "\n",
        "def plot_heights():\n",
        "    if is_ipython:\n",
        "        display.clear_output(wait=True)\n",
        "        plt.figure(2)\n",
        "        heights_t = torch.tensor(episode_score, dtype=torch.float)\n",
        "        plt.title('Training...')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Score')\n",
        "        plt.plot(heights_t.numpy())\n",
        "        # Take 100 episode averages and plot them too\n",
        "        if len(heights_t) >= 100:\n",
        "            means = heights_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "            means = torch.cat((torch.zeros(99), means))\n",
        "            plt.plot(means.numpy())\n",
        "        plt.show()\n",
        "\n",
        "env = MountainCar(speed=1e8, graphical_state=True, render=False, is_debug=True)\n",
        "agent = MarioAgent(stateShape=(3, 84, 84),\n",
        "                   actionSpace=env.get_action_space(), numPicks=128, memorySize=10000)\n",
        "\n",
        "def episode():\n",
        "    done = False\n",
        "    rewardSum = 0\n",
        "    lossSum = 0\n",
        "\n",
        "    env.reset()\n",
        "    currState = process_screen(env.get_state())\n",
        "    lastState = currState\n",
        "    state = currState - lastState\n",
        "\n",
        "    maxScore = -100000\n",
        "\n",
        "    while not done:\n",
        "        stateT = Variable(state)\n",
        "\n",
        "        action = agent.selectAction(stateT)\n",
        "        obs, reward, done, score = env.step_all(action)\n",
        "        maxScore =  max(  maxScore, score)\n",
        "       \n",
        "        reward = np.sum(reward)\n",
        "        lastState = currState\n",
        "        currState = process_screen(obs)\n",
        "\n",
        "        if not done:\n",
        "            nextState = Variable(currState - lastState).to(device)\n",
        "        else:\n",
        "            nextState = None\n",
        "\n",
        "        #reward += obs[0]\n",
        "\n",
        "        actionT = Variable(LongTensor([action])).to(device)\n",
        "        rewardT = Variable(FloatTensor([reward])).to(device)\n",
        "        doneT = Variable(BoolTensor([done])).to(device)\n",
        "\n",
        "        rewardSum += reward\n",
        "        state = nextState\n",
        "\n",
        "        loss = agent.trainDQN()\n",
        "        agent.addMemory((stateT, actionT, rewardT, nextState, doneT), loss)\n",
        "        lossSum += loss\n",
        "\n",
        "        if ep % 100 == 0:\n",
        "            env.render()\n",
        "\n",
        "    episode_score.append(maxScore)\n",
        "    plot_heights()\n",
        "\n",
        "    if ep % 200 == 0:\n",
        "        agent.save()\n",
        "\n",
        "    print(\"now epsilon is {}, the reward is {} with loss {} in episode {}\".format(agent.epsilon, rewardSum, lossSum, ep)) \n",
        "\n",
        "ep = 1\n",
        "while ep < 10000:\n",
        "    episode()\n",
        "    ep += 1\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "now epsilon is 0.9880022159439659, the reward is -330 with loss 202.12379405431096 in episode 4\n",
            "#################  RESET GAME  ##################\n",
            "Episode terminated after: 28 (s)\n",
            "Total score: -203\n",
            "#################################################\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}